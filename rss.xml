<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="/scripts/pretty-feed-v3.xsl" type="text/xsl"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:h="http://www.w3.org/TR/html4/"><channel><title>Axi&apos;s Blog</title><description>一只可爱小猫</description><link>https://axi404.github.io</link><item><title>创建 LeRobot 2.1 数据集</title><link>https://axi404.github.io/blog/lerobot</link><guid isPermaLink="true">https://axi404.github.io/blog/lerobot</guid><description>LeRobot 2.1 是目前最流行的数据集格式，创建 LeRobot 数据集的示例并不直观，在这里给出一些简单的指引。</description><pubDate>Wed, 29 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;因为对于 LeRobot 数据集最近包括了大量的使用需求，但是寻找许久之后都没有非常清晰的文档或者教程，在这里给出最简单的教程，来解释如何创建 LeRobot 2.1 数据集。&lt;/p&gt;
&lt;h2&gt;安装&lt;/h2&gt;
&lt;p&gt;首先，我们需要安装 LeRobot 2.1 的库，这里我们使用 pip 来安装：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install &quot;lerobot @ git+https://github.com/huggingface/lerobot.git@2b71789e15c35418b1ccecbceb81f4a598bfd883&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同时假如没有下载过，需要安装 ffmpeg&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt install ffmpeg
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;创建数据集&lt;/h2&gt;
&lt;p&gt;LeRobot 数据集一共包括两个环节，也不难理解，分别是创建数据集以及将每一个数据都存进去。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lerobot.common.datasets.lerobot_dataset import HF_LEROBOT_HOME, LeRobotDataset

def create_dataset(
    repo_id: str,
    robot_type: str,
    mode: Literal[&quot;video&quot;, &quot;image&quot;] = &quot;video&quot;,
) -&gt; LeRobotDataset:
    motors = [
        &quot;joint_0&quot;,
        &quot;joint_1&quot;,
        &quot;joint_2&quot;,
        &quot;joint_3&quot;,
        &quot;joint_4&quot;,
        &quot;joint_5&quot;,
        &quot;joint_6&quot;,
    ]
    cameras = [
        &quot;camera_0&quot;,
        &quot;camera_1&quot;,
        &quot;camera_2&quot;,
    ]

    features = {
        &quot;state.joints&quot;: {
            &quot;dtype&quot;: &quot;float32&quot;,
            &quot;shape&quot;: (len(motors),),
            &quot;names&quot;: [
                motors,
            ],
        },
        &quot;action.joints&quot;: {
            &quot;dtype&quot;: &quot;float32&quot;,
            &quot;shape&quot;: (len(motors),),
            &quot;names&quot;: [
                motors,
            ],
        },
    }
    for cam in cameras:
        features[f&quot;video.{cam}_view&quot;] = {
            &quot;dtype&quot;: mode,
            &quot;shape&quot;: (3, 480, 640),  # (channels, height, width)
            &quot;names&quot;: [
                &quot;channels&quot;,
                &quot;height&quot;,
                &quot;width&quot;,
            ],
        }

    if Path(HF_LEROBOT_HOME / repo_id).exists():
        shutil.rmtree(HF_LEROBOT_HOME / repo_id)

    return LeRobotDataset.create(
        repo_id=repo_id,
        fps=15,
        robot_type=robot_type,
        features=features,
        use_videos=True,
        tolerance_s=0.0001,
        image_writer_processes=10,
        image_writer_threads=5,
        video_backend=&quot;ffmpeg&quot;,
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这里我们使用将图像保存为视频，体现在输入的传参中，将 &lt;code&gt;mode&lt;/code&gt; 设置为 &lt;code&gt;&quot;video&quot;&lt;/code&gt;。如上的代码中定义了 joint 以及 camera 的基础信息。&lt;/p&gt;
&lt;p&gt;本身对于 LeRobot 数据集的定义，主要是通过 &lt;code&gt;features&lt;/code&gt; 来定义的，在此之后，features 就会被定义为一个 Dict，并且直接将其中存储数据即可。&lt;/p&gt;
&lt;p&gt;与此同时，需要定义环境变量 &lt;code&gt;export HF_LEROBOT_HOME=/path/to/your/lerobot/home&lt;/code&gt;，来设置 LeRobot 数据集的本地存储路径。&lt;/p&gt;
&lt;p&gt;之后存储数据：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def process_single_dataset(
    dataset: LeRobotDataset,
    state_joints: np.ndarray,
    action_joints: np.ndarray,
    video_dict: dict[str, np.ndarray],
    instruction: str,
) -&gt; LeRobotDataset:
    num_frames = state_joints.shape[0]

    for i in range(num_frames):
        frame = {
            &quot;state.joints&quot;: state_joints[i],
            &quot;action.joints&quot;: action_joints[i],
        }
        for camera, img_array in video_dict.items():
            frame[f&quot;video.{camera}_view&quot;] = img_array[i]
        dataset.add_frame(frame, task=instruction)

    dataset.save_episode()

    return dataset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这里，只要使用同一个 dataset，LeRobot 会自动管理诸如 id 以及其他的统计信息。&lt;/p&gt;
&lt;h2&gt;使用最新的 LeRobotDataset&lt;/h2&gt;
&lt;p&gt;同时在这里给出最新的 LeRobotDataset 的代码，还是先安装：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install lerobot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后对于创建数据集，直接：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;DROID_FEATURES = {
    &quot;observation.state.joint_position&quot;: {
        &quot;dtype&quot;: &quot;float32&quot;,
        &quot;shape&quot;: (7,),
        &quot;names&quot;: {
            &quot;axes&quot;: [&quot;joint_0&quot;, &quot;joint_1&quot;, &quot;joint_2&quot;, &quot;joint_3&quot;, &quot;joint_4&quot;, &quot;joint_5&quot;, &quot;joint_6&quot;],
        },
    },
    &quot;observation.state.gripper_position&quot;: {
        &quot;dtype&quot;: &quot;float32&quot;,
        &quot;shape&quot;: (1,),
        &quot;names&quot;: {
            &quot;axes&quot;: [&quot;gripper&quot;],
        },
    },
    &quot;observation.state&quot;: {
        &quot;dtype&quot;: &quot;float32&quot;,
        &quot;shape&quot;: (8,),
        &quot;names&quot;: {
            &quot;axes&quot;: [&quot;joint_0&quot;, &quot;joint_1&quot;, &quot;joint_2&quot;, &quot;joint_3&quot;, &quot;joint_4&quot;, &quot;joint_5&quot;, &quot;joint_6&quot;, &quot;gripper&quot;],
        },
    },
    &quot;observation.images.wrist_left&quot;: {
        &quot;dtype&quot;: &quot;video&quot;,
        &quot;shape&quot;: (180, 320, 3),
        &quot;names&quot;: [
            &quot;height&quot;,
            &quot;width&quot;,
            &quot;channels&quot;,
        ],
    },
    &quot;language_instruction&quot;: {
        &quot;dtype&quot;: &quot;string&quot;,
        &quot;shape&quot;: (1,),
        &quot;names&quot;: None,
    },
    &quot;action.joint_position&quot;: {
        &quot;dtype&quot;: &quot;float32&quot;,
        &quot;shape&quot;: (7,),
        &quot;names&quot;: {
            &quot;axes&quot;: [&quot;joint_0&quot;, &quot;joint_1&quot;, &quot;joint_2&quot;, &quot;joint_3&quot;, &quot;joint_4&quot;, &quot;joint_5&quot;, &quot;joint_6&quot;],
        },
    },
    &quot;action.gripper_position&quot;: {
        &quot;dtype&quot;: &quot;float32&quot;,
        &quot;shape&quot;: (1,),
        &quot;names&quot;: {
            &quot;axes&quot;: [&quot;gripper&quot;],
        },
    },
    &quot;action&quot;: {
        &quot;dtype&quot;: &quot;float32&quot;,
        &quot;shape&quot;: (8,),
        &quot;names&quot;: {
            &quot;axes&quot;: [&quot;joint_0&quot;, &quot;joint_1&quot;, &quot;joint_2&quot;, &quot;joint_3&quot;, &quot;joint_4&quot;, &quot;joint_5&quot;, &quot;joint_6&quot;, &quot;gripper&quot;],
        },
    },
    &quot;is_episode_successful&quot;: {
        &quot;dtype&quot;: &quot;bool&quot;,
        &quot;shape&quot;: (1,),
        &quot;names&quot;: None,
    },
}
dataset = LeRobotDataset.create(
    repo_id=&quot;your-repo-id&quot;,
    features=DROID_FEATURES,
    fps=15,
    robot_type=&quot;Franka&quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这里给出三种不同的 feature 类型。对于 str 或者 bool，其 shape 为 &lt;code&gt;(1,)&lt;/code&gt;，而 name 可以为 None；而对于如 joint 等一维数组，其 name 中设置了 axes 来表示每个位置的名称；而对于如 video 等二维数组，其 name 中设置了 height、width、channels 来表示每个维度的名称，同时指定 dtype 为 &lt;code&gt;video&lt;/code&gt;，以使用视频进行编码。&lt;/p&gt;
&lt;p&gt;对于添加每一帧，则为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def generate_lerobot_frames(tf_episode):
    m = tf_episode[&quot;episode_metadata&quot;]
    frame_meta = {
        &quot;is_episode_successful&quot;: np.array([is_episode_successful(m)]),
    }
    for f in tf_episode[&quot;steps&quot;]:
        frame = {
            &quot;language_instruction&quot;: f[&quot;language_instruction&quot;].numpy().decode(),
            &quot;observation.state.joint_position&quot;: f[&quot;observation&quot;][&quot;joint_position&quot;].numpy(),
            &quot;observation.state.gripper_position&quot;: f[&quot;observation&quot;][&quot;gripper_position&quot;].numpy(),
            &quot;action.gripper_position&quot;: f[&quot;action_dict&quot;][&quot;gripper_position&quot;].numpy(),
            &quot;action.joint_position&quot;: f[&quot;action_dict&quot;][&quot;joint_position&quot;].numpy(),
            &quot;observation.images.wrist_left&quot;: f[&quot;observation&quot;][&quot;wrist_image_left&quot;].numpy(),
        }

        # language_instruction is also stored as &quot;task&quot; to follow LeRobot standard
        frame[&quot;task&quot;] = frame[&quot;language_instruction&quot;]

        # Add this new feature to follow LeRobot standard of using joint position + gripper
        frame[&quot;observation.state&quot;] = np.concatenate(
            [frame[&quot;observation.state.joint_position&quot;], frame[&quot;observation.state.gripper_position&quot;]]
        )
        frame[&quot;action&quot;] = np.concatenate([frame[&quot;action.joint_position&quot;], frame[&quot;action.gripper_position&quot;]])

        # Meta data that are the same for all frames in the episode
        frame.update(frame_meta)

        # Cast fp64 to fp32
        for key in frame:
            if isinstance(frame[key], np.ndarray) and frame[key].dtype == np.float64:
                frame[key] = frame[key].astype(np.float32)

        yield frame
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这其中可以看到 LeRobot 标准强行定义了三个 Feature，分别是 &lt;code&gt;language_instruction&lt;/code&gt;、&lt;code&gt;observation.state&lt;/code&gt; 以及 &lt;code&gt;action&lt;/code&gt;。状态与动作都是 joint+gripper 的组合。&lt;/p&gt;
&lt;p&gt;最后结合起来即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for episode in raw_dataset:
    for frame in generate_lerobot_frames(episode):
        lerobot_dataset.add_frame(frame)
    lerobot_dataset.save_episode()
    logging.info(&quot;Save_episode&quot;)
lerobot_dataset.finalize()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;小结&lt;/h2&gt;
&lt;p&gt;LeRobot 数据集总体来说依然是比较好用的，在转录数据的时候，只需要从 Features 的角度入手即可，同时支持对于 Image 自动进行 Video 的编码，整体来说还是非常方便的。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/113398570_p0.webp"/><enclosure url="https://picr2.axi404.top/113398570_p0.webp"/></item><item><title>周记 Week68</title><link>https://axi404.github.io/blog/week-68</link><guid isPermaLink="true">https://axi404.github.io/blog/week-68</guid><description>2025-10-13 ~ 2025-10-19.</description><pubDate>Tue, 28 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;这次又是很久之后的一次周记，不过倒也不是因为最近又特别忙，而是在珍惜好不容易的一段闲暇的时间，在推进需要做的主线的同时，也去额外关注一下生活。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;因为大多数课都已经结课的原因，所以在课内的学习上就不用付出什么时间了，而是主要还是在科研这部分做一些探索。&lt;/p&gt;
&lt;p&gt;一方面还是积极展开了一些合作，目前细数下来可能能同时进行的有三四个项目，和实验室的一位学长一起做的项目可能是重中之重，某种程度上也是在 GenManip 上进行更进一步的迭代，来满足更多的需求。&lt;/p&gt;
&lt;p&gt;另一方面我也在积极补充一些论文的阅读，当然这里面可观的数量还是关于具身智能的。一个遗憾的发现是，目前整体 VLA 的领域还是偏向于非常浮躁的风格，基本上处理的还是那些陈词滥调的问题，以及不断做出一些不痛不痒的改进。其实我的一种看法在于，假如说我们已经拥有了在一些测评基础上面较为稳定的 baseline，那么按理来说就是时候在更深层面进行一些研究了，不过在这其中显然有两个关键问题没有解决。其一，目前大多数的更大规模预训练的模型并不具有稳定的性能，尤其在当下的 Benchmark 中，5% 的绝对性能的差异甚至都可能是噪声的一环，这使得留给我们进行消融的性能波动空间并不大；其二，不同于传统的图像或者文本模型，无论是 WM 还是 VLA，都涉及了在长时序下的任务，这些任务带来了更长的因果关系，甚至结果与环境反馈相关联，这使得一个对于模型内部有效的分析工具难以十分有效，我们只能根据现象来进行推测大多数内容。不过无论如何我已经开始尝试，包括完成一些项目相关的内容以及做出自己的一些探索。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活上也是没有其他事情，依然是除了科研就和乐小姐二人世界的快乐日子。当然，另一件事情是我申请了实验室那边的宿舍，等什么时候通过了，或许就是动身的时候。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/5tfa7WK.webp"/><enclosure url="https://picr2.axi404.top/5tfa7WK.webp"/></item><item><title>The Dark Side of the AI</title><link>https://axi404.github.io/blog/dark-side-of-ai</link><guid isPermaLink="true">https://axi404.github.io/blog/dark-side-of-ai</guid><description>关于我的 AI 失败主义思想</description><pubDate>Thu, 23 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在 ICLR 投稿之后，我因为长久以来的积劳成疾大病了一场，同时带来的问题也有心理上的低迷，以及对于科研激情的丧失。这其实并不是一个短期导致的问题，也不是出于某种来自科研长久以来的挫折。在和好友交流了之后，我也感觉是时候再次振作起来，所以决定写下来我的所思所想，然后再次出发。&lt;/p&gt;
&lt;p&gt;事实其实是反直觉的，在去年的 CVPR 中稿之后，我一直在 GenManip 的基础上进行进一步的开发以及改进，解决数据问题是当下 VLA 最为重要的事情，而我们搭建了一个 Scalable 的框架。我也有着若干的合作，可以确保持续的论文产出；当然，也有一些内容可以沿着我为自己规划的主线继续探索。&lt;/p&gt;
&lt;p&gt;然而遗憾的是，无论一切如何顺利地发展，问题依然在那里。任何一个深谙深度学习发展历史的人都会清晰地意识到，人工智能的黄金时代已经过去了，不再会有在这条漫长道路上的 milestone，不再会有下一个 Kaiming He 式的人物出现，即使是对于领域内具有短期卓越成果的论文，也仅仅是漫长线条上的一个点，而无数个这样的点组成的路线则通向了低垂果实之后的黑暗。&lt;/p&gt;
&lt;h2&gt;AGI 泡沫&lt;/h2&gt;
&lt;p&gt;在 2022 年的现在，那时候 ChatGPT 刚刚横空出世，Scaling Law 向世人展现了它的威力。绝大多数的人都相信这条道路向人们揭示了通向终点的道路。&lt;/p&gt;
&lt;p&gt;每个人都相信，不久之后，伴随着数据、模型设计以及算力的提升，AGI 就在这条道路的终点等待着我们。然而伴随着规模的扩大，提升则不再显著，之后 RL 出现，带来了更多的 Scaling 维度，下一次竞赛开始了。不过这就好似木桶原理一样，我们发现了其他的短板，并且可以在补全的过程中让模型变得更好，但是残酷的是，我们会清晰的意识到，这终究是一个木桶，而非一条无限延伸的道路。&lt;/p&gt;
&lt;p&gt;模型的性能一直在提升，这是一件从 Benchmark 指标上就可以看出来的事情。伴随着不断的改进方法的提出，越来越多的数据的注入，无论是工业界还是学界，似乎一切都还是积极的模样。我承认这种现象的存在，但是在大家感慨这些领域内的进步和发展如此之快的同时，AI 却开始逐渐显露疲态，那些主干的方向停滞不前，Vibe Coding 在为不懂程序的人产出缺乏结构性并且滥用面向对象的屎山，我们开始发展新的领域并且从中定义新的问题。&lt;/p&gt;
&lt;p&gt;当然，模型能力方面的问题，Researcher 自然有很多话可以说。这些问题可能来自于幻觉，某些可信领域的问题，某些数据的 imbalance，但是后人的智慧、某种训练范式、Scaling Law，他们能解决这些问题。不过这一问题确实也只是泡沫的冰山一角，毕竟从事实上来说，没有人能否认现如今 AI，尤其是 GenAI，已经完全重塑了这个社会的模样。没有人可以想象一个没有 AI 的世界，并且忍受那种下降的效率。然而一个更大的问题，一个在业界头顶的乌云，便是应用场景。&lt;/p&gt;
&lt;p&gt;假如说 LLM 的用处是 Coding Agent，然而依然并不能成功解决一些真正困难的编程问题；AIGC 可以生成图片或者视频，沦为短视频中的素材；WM 的投入可以生成逼真的视频，然后被投入到自动驾驶和具身智能中，那么究竟如何能够配得上市场的期待。Kimi 的 OK Computer 最近发布之后又有一系列的推广，但是展示的用例依然是编程或者制作 PPT，又或者某些其实没有实际价值的数据分析。科研这条道路对于研究者是令人兴奋的，探索未知并且理解模型、数据与表征的奥秘，然而对于市场，这一切都显得如此的苍白。&lt;/p&gt;
&lt;p&gt;AI 最美好的时代就是它的前景还处于一种微妙的朦胧感的时代。对于科研工作者来说，这种朦胧感意味着探索未知，设计新的算法解决新的问题；而对于投资人来讲，这种朦胧感搭配上现如今蓬勃生机的发展，带来了对于价值的美好畅想。然而一个糟糕的事情是某种自掘坟墓，似乎有些像是程序员热衷于开源最后将自己优化掉的幽默段子。越深入研究，一切就开始失去它的神秘感，而伴随着揭开这层神秘的面纱，这意味着科研工作者了解了这是什么，投资人理解了似乎这能做到什么，然后你需要兑现这些投资带来的期望的价值。&lt;/p&gt;
&lt;p&gt;他们在你身上压下了筹码，现在轮到了你出牌，然而你手上什么都没有。这是一个自我消亡的泡沫。&lt;/p&gt;
&lt;h2&gt;垃圾时间&lt;/h2&gt;
&lt;p&gt;时间回到半年前，大家在积极地讨论 o-series，那时候 Deepseek-R1 横空出世，将 RL 这一经久不衰的命题再次放到了聚光灯下。OpenAI 的姚顺雨写了一篇 Blog，叫做 The Second Half，AI 的下半场，RL finally generalize，关键在于如何定义问题。不过事实上对于一切来说，或许下半场已经提前结束了，其实早已经进入了比赛的垃圾时间。&lt;/p&gt;
&lt;p&gt;一些读者可能会赞同，架构已死。一些结构会带来少数的新 feature，带来一些领域中的提升，但是同样的问题是显著的。在 Transformer 出现并且发展之后，人工智能领域最核心的思想之一就已经出现，即 Fusion。在何种表征中，又或者是什么空间下，使用什么结构对于什么信息进行 Fusion，导致解决了什么问题，这就是当下的主基调。架构的改变可以带来效率的提升，性能的量变，但是本质不变。&lt;/p&gt;
&lt;p&gt;平庸的论文提出方法，优秀的论文提出问题，这是当下科研的常态，也是每一个 Senior 的共识。我们只需要敏锐察觉到问题所在，并且尝试用一个简单且有效的方法解决掉，就可以算是大功一件了。然而另一个令人沮丧的共识是，假如说你对于自己熟悉的领域，在缜密的思考之后罗列了那些存在的关键问题，你会惊讶的发现，大多数的论文，那些看上去还不错、没有通过对于方法进行排列组合进行 A+B 的论文，最后开始对于问题进行排列组合。&lt;/p&gt;
&lt;p&gt;于是我们应该从哪里找到出路，那些最为前沿的阵线，那么就去业界看看吧。不过，同样唏嘘的是，大多数时候，业界的更多算力的迭代之后也只是改变了领域内不同问题的权重。我们需要优先关注某些问题，但是问题的总数没有变多，也没有变少。因此我们只需要等待，等待算力的提升以及数据的增加，等待规模化的流程遍历全部的问题，然后答案留下，假如没有新的关键问题出现，那么一切的结束其实只是时间问题。你参与其中，或许稍有加速，但无论你是否在，结果都已经注定。&lt;/p&gt;
&lt;p&gt;比赛其实已经进入了垃圾时间。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;大多数参与到 AI 科研中的 Researcher，一定在某个阶段会认为自己的存在是可以改变一些事情的，但是当问题的解决变得清晰起来，解决问题的人倒是开始变得迷茫了。或许就像是 Pink Floyd 在他们的专辑《The Dark Side of the Moon》中唱到的那样。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&quot;There is no dark side of the moon really. Matter of fact it&apos;s all dark.&quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不过还是姑且 Dive into the dark，然后享受这个过程吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/3b54b55bd30543a12573945bb80418d8.webp"/><enclosure url="https://picr2.axi404.top/3b54b55bd30543a12573945bb80418d8.webp"/></item><item><title>Paper Reading: Embodied AI 5</title><link>https://axi404.github.io/blog/paper-reading-eai5</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-eai5</guid><description>从一些 Embodied AI 相关工作中扫过。</description><pubDate>Sat, 06 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Embodied AI Paper Reading&apos;,
items: [
{
title: &apos;Batch 1&apos;,
href: &apos;/blog/paper-reading-eai1&apos;,
order: &apos;1&apos;
},
{
title: &apos;Batch 2&apos;,
href: &apos;/blog/paper-reading-eai2&apos;,
order: &apos;2&apos;
},
{
title: &apos;Batch 3&apos;,
href: &apos;/blog/paper-reading-eai3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Batch 4&apos;,
href: &apos;/blog/paper-reading-eai4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Batch 5&apos;,
href: &apos;/blog/paper-reading-eai5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Batch 6&apos;,
href: &apos;/blog/paper-reading-eai6&apos;,
order: &apos;6&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;EO-1&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/5f4a4d91c16460a98bb6dd938e5091bb.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;EO-1 是一篇很大的工作，包括了大量的实验，以及基于目前开源数据训练的一个模型。本身其实论文的篇幅比较长，乍一看也是十分的专业，但是其实感觉一些内容写得非常不清楚，在这里给一个简单的理解。&lt;/p&gt;
&lt;p&gt;本身 EO-1 包括一个从 Qwen2.5VL 初始化的 Transformer 主干，支持 VL 以及 state, noisy action 的输入，其中 action 是直接过的 projection layer。然后 EO-1 支持两种不同的输出，也就是文本以及 action。其中文本就是直接使用 detokenizer 进行输出，使用 AR Loss；而 action 则经过一个很小的 Head 之后使用 FM Loss。本身数据就是交错的格式，论文中讲的非常乱七八糟的内容其实本身没有任何的含义，就是将 N-1 的 history 段中都是干净是数据，N 的是 noisy action，然后对 N 来进行 one-step 的 FM。&lt;/p&gt;
&lt;p&gt;本身论文还是在数据上做了一些构造，定义了不同类型的推理，但是在这方面在实验上来看，最后实验中展示的推理也是后训练进去的（也就是只使用特定任务上的数据进行训练任务分解，而不是模型在预训练上就具有这样的能力），并没有意料之外的 Feature。而其中对于 Action 的部分，可以说是用了一个很小的 Action Expert，肉眼可见的可能在 VLM 的 Transformer 中带来一些 conflict，这里面也没有着重去解决。综上所述，其实算是比较中规中矩的工作，出于论文写作的角度，让人看得云里雾里，还是不算很喜欢。&lt;/p&gt;
&lt;h2&gt;OpenVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/3d162ad7a91084d6c9448c1f064ffade.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;OpenVLA 的原始工作，如今补上。OpenVLA 继承了 RT-2 的思想，是十分经典的范式。简单来说，就是使用 Transformer 进行 Fusion，将图像、语言作为输入，输出动作 token，再过 Action detokenizer 得到动作。&lt;/p&gt;
&lt;p&gt;本身 OpenVLA 的 limitation 也非常明显，面对如今的 Co-training 需求，OpenVLA 选择使用最不常见的 256 个 Token 作为 Action token，这使得 OpenVLA 在面对灾难性遗忘的同时，还面临十分极致的双峰分布学习问题，也就是在学习 VL 的时候，模型需要尽可能少输出这些 Token，而学习 Action 的时候，模型需要完全输出这些 Token。这一问题使得某种程度上，OpenVLA 的 Llama2 可能更多地作为一个 VL Fusion 的模块，而其中的先验知识可能难以被充分利用。即使如此，OpenVLA 也是成为了奠基性的工作。&lt;/p&gt;
&lt;h2&gt;OpenVLA-OFT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/971e5cbf3efc95bc68eb14dc1be63ade.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;OpenVLA-OFT 可以说是在 OpenVLA 上面进行了若干的改进。主要包括几点，第一个是使用双向注意力进行并行解码，从而同时输出全部的 Action，这样全部的推理都变成了一步，也就是你的机器人有 N 维，就可以声称 N 倍加速。同时输出也改成了一个 MLP，之后输出连续动作，这在某一方面从而变成了使用 hidden state，避免了双峰分布的学习，并且可以使用 L1 损失，可以说非常的简单直接但是有效；第二个是 FiLM，使用 FiLM 来将 Language 的信息直接注入到 Image encoder 中，从而增强对于 Language 的感知。本身可以说广受认可而且确实不错，可惜 OpenVLA 的 Codebase 确实不太好，不然可以说是非常好的基石了。&lt;/p&gt;
&lt;h2&gt;Robix&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/906faa85c941a00801088b2c7e4821d1.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;虽然说标题里面说 A Unified Model，不过看上去其实只是一个单纯的 VLM Planner 而已，只是支持了不同的 VL 输出情景。本身比较简单，就是合成了一些数据，然后进行了继续预训练、SFT 以及强化学习。算是比较经典的套路。这里面值得一提的是其中 VLM 输出的 Action 应该是 primitive action，本质上也是 Text 输出。之后双系统给到下游的 VLA。在这里比较搞的是，这里面有一组给出的 Real World 的 Embodiment 部署是用人来代替的 VLA，比较神秘。&lt;/p&gt;
&lt;h2&gt;FLOWER&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/cbccdaec5b57ea961cf304e1cb4eee5c.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;FLOWER 使用 Transformer 的中间层的输出，同时让 Diffusion 的 AdaLN-Zero 的参数都共享。对于不了解 Diffusion 具体操作的读者，可以理解为 AdaLN-Zero 是一种类似于 FiLM 的内容，通过修改 LayerNorm 的参数来对于模型进行调制，是目前比较常用的添加 Condition 的做法。本身其实思想都比较通用了，比如说中间层，似乎 GR00T 已经是类似的操作了。总体来说中规中矩。&lt;/p&gt;
&lt;h2&gt;F1-VLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ed67cc587f4be39a76bb9c99120724f5.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;F1-VLA 可以说还是做了非常不错的探索。Pi-like 直接使用了 VLM 以及 Action Expert 进行 MoT，也就是直接使用 VL Data，那么另一条路线似乎也是显然的，就是如何使用 World Model 来吃 Video Data。&lt;/p&gt;
&lt;p&gt;F1-VLA 可以说是直接将这些东西整合在一起了，把 VLM, World Model 以及 Action Expert 来同时做 MoT。F1-VLA 带来的问题是，其将 MoT 描述为了一种因果关系，即，VL 带来理解能力，World Model 紧接着预测未来，最后 Action Expert 输出具体动作。虽然本身是 MoT，其实无所谓这些东西，但是如何进行这些数据和模型之间的 Balance，依然是一个严重的问题，但是目前看上去模型只在 VLA 数据上进行了训练，虽然经过了所谓的对齐，不过对于如何吃下更多的数据没有进行充分的探索。&lt;/p&gt;
&lt;p&gt;总体来说还是很不错，但是值得后续的探索，值得一读。&lt;/p&gt;
&lt;h2&gt;VLA-Adapter&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/62b0092bc299fc979fea1c92d68804be.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;VLA-Adapter 的主图如上，主要还是总结了之前的 VLA 的范式，这里是从 Pi-like 的角度出发，思考如何从 VLM 给 Action Expert 提供 Condition。VLA-Adapter 本身的范式其实是图中的 C 类型，但是使用的就不是常规的添加 Condition 的方式，而是使用了他们自己设计的 Bridge Attention 的连接方式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/fe8322e9c947a03dafbf5db3d8cdaa72.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Bridge Attention 的结构如上，在这里并不主要关注具体实现，本身的思想还是尽可能的如何更多地把 Condition 施加过去，感觉看上去效果还不错。Bridge Attention 本质上并非 DiT，而是直接从 Initial action 进行一次直接的预测。总体来说还可以，但是其实讲实话，并没有非常多的 insight。Bridge attention 阐述了施加 condition 的重要性，但是更多地证明为什么是这个结构，并没有很多有效性相关的阐述。&lt;/p&gt;
&lt;h2&gt;SimpleVLA-RL&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/b336354d1541e53f4647fd3f68a6b933.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SimpleVLA-RL 本身是使用直接的 online GRPO 对 VLA 进行强化学习，本身比较简单，但是效果还不错。其中其实有不少的细节也是值得品味的，比如说对于 GRPO 在 VLA 里面的改进，类似于动态采样或者 rollout 的时候增大采样温度。对于想要做 VLA RL 的读者来说值得细品。以及其中带来的一些有趣的 findings，也就是论文中说的 Pushcut，大概就是说因为 Goal 的反馈，导致模型在基础初始化的 Pick and Place Skill 的基础上自己学会了使用 Push 动作来解决任务，算是确实是 RL 的优点之一了，也就是 RL 可以超越数据本身自主探索。&lt;/p&gt;
&lt;h2&gt;RynnVLA-001&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/531aaf87aaaa1b6414acf2c21c06e9c0.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RynnVLA-001 本身是使用 I2V Model Pretrain 的 WM VLA。本身思路还是比较简单的，主要就是直接用 Transformer 训练一个 I2V 的模型，之后在 Ego 数据集上训练 Human action 的 action model，之后最后用 Robot action 来进行 post-training。问题其实也比较显著，就是其实大家对于 World Model 的关注点还是，因为其可以显式地展示很多 OOD 的泛化能力，因此这些能力如何，以及如何将这些能力 transfer 到 VLA，是否有效等。RynnVLA-001 的实验在 SO100 机械臂上进行，相对比较 Toy，也只说明了性能上的内容，感觉还是会稍微差些意思。不过本身这套流程还是没问题的，期待后续工作吧。&lt;/p&gt;
&lt;h2&gt;Evo-0&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/3dbc3fdde33a3853227658ab51078488.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Evo-0 的内容如 tldr 所示，就是将 VGGT 的信息作为 spatial encoder 加入了到了 Pi-0 里面，从而增强了 VLA 的 spatial 能力。本身模型是直接基于 Pi-0 改的，所以别的内容也没有很多的变化。模型本身在一些精细操作上似乎性能不错，但是实验本身的次数也比较有限，说服力不加。总体来说是一篇意料之中的论文，相似的思路或许可以在他们的 codebase 上进一步迭代。&lt;/p&gt;
&lt;h2&gt;Imagine2Act&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4ee42938c2ea0ccfe2b1de7d9c198e6d.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Imagine2Act 本身似乎还是有些古典，在 DP 上进行了一些探索，主要负责解决物体重排的任务。本身的思路如图所示，借助了类似于 4o-image 之类的模型，来生成模型的 Goal Image，并且通过 SAM 以及变换来得到 Goal 的 Depth，同时计算 Transformation info，分别进行 encode 然后作为 condition 输入到 DP 中。总体来说思路还是比较直接的，问题主要在于这种过于 modular 的方法并不能 scalable，而且会引入多余的不稳定性。&lt;/p&gt;
&lt;h2&gt;HDMI&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/eccb83554b1c8e939ffb5682c3acb66b.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;HDMI 本身比较直接，如图中所示，其实也就是整合了前人的几个工作。首先从视频中恢复人体数据以及比如说 articulation 或者别的物体的信息；然后根据这些 Goal 来去进行 RL，涉及了一些细节，比如设置接触点的 Reward 以及使用相对的 joint 去控制。本身的比如说 RL 以及其他的内容都是 follow 之前的工作，整体大概就是这样，大概是一种整合性质的工作，从效果上来看还是可以 Sim2Real 的，也算可以了。感觉 Locomotion 这种偏向传统 robotics 的可能其中的控制相关的细节很多，相关的读者可能会很西湖区按。不过从 CVer 的角度来看，确实似乎没有过多的启发。&lt;/p&gt;
&lt;h2&gt;World4RL&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/f6d76777735609f82cec4b661e68c07d.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;World4RL 本身讨论了一个很不错的话题，也就是使用 World Model 进行 RL，毕竟本身 Simulator 能够进行的 RL 还是不太 Scalable 的。本身的做法也是比较简单，首先需要训练一个可以根据 Action 来 Predict next frame 的 WM，然后再训练一个 Rewarwd Model，World4RL 本身也是按照这个思路进行的。本身使用 UNet 来训练 WM，使用 one-hot 来编码 action，然后在任务无关数据上训练。之后用一个 ResNet-18 + 分类器的模型作为 RM，在任务相关数据上训练。本身可以说思路不错，在有了 WM 和 RM 之后就可以正常的训练 Policy 了，不过确实模型选择的都比较古老，而且完全不是使用的 Scaling 的数据，所以其实效果也不是很显著，没有展现这种范式理论上的通用性。相关思路值得参考，但是实现上确实感觉还是仓促了很多。&lt;/p&gt;
&lt;h2&gt;Universal Manipulation Interface&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/37c78203e97e0d4aebd40d17ee0224b3.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;UMI 其实绝对来说是类似于机械的结构设计。本身的核心思路其实比较简单，也就是设计一种类似于通用的夹爪来统一全部的数据，于是 UMI 出现了，可以让一切夹爪看上去都是一样的。其中其实相对聪明的一点还是在于，本身 UMI 的运动数据不是从机械臂里面 FK 获得的，而是通过 IMU 获得的，这使得无论是什么机器人，UMI 的精度以及尺度都是一致的，而且也比较高。甚至 UMI 其实是可以直接手持来采集数据的。后续 UMI 类似的设计也有很多 Follow，虽然其实感觉在这个基础上设计自己的 UMI 反而破坏了社区，导致了数据的分岔，但是总的来说这个想法还是十分有意义的。&lt;/p&gt;
&lt;h2&gt;FastUMI&lt;/h2&gt;
&lt;h2&gt;MV-UMI&lt;/h2&gt;
&lt;h2&gt;DexUMI&lt;/h2&gt;
&lt;h2&gt;PEEK&lt;/h2&gt;
&lt;h2&gt;VisualMimic&lt;/h2&gt;
&lt;h2&gt;villa-X&lt;/h2&gt;
&lt;h2&gt;RoLA&lt;/h2&gt;
&lt;h2&gt;InSpire&lt;/h2&gt;
&lt;h2&gt;dVLA&lt;/h2&gt;
&lt;h2&gt;RoboPilot&lt;/h2&gt;
&lt;h2&gt;Gemini Robotics 1.5&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-11-zh.webp"/><enclosure url="https://picr2.axi404.top/template-11-zh.webp"/></item><item><title>Rclone 简易教程</title><link>https://axi404.github.io/blog/rclone</link><guid isPermaLink="true">https://axi404.github.io/blog/rclone</guid><description>Rclone 是一个用于同步文件的工具，可以用于同步本地文件到云端，也可以用于同步云端文件到本地。</description><pubDate>Mon, 20 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;因为要使用不少的 OSS 存储，Rclone 是非常常见的工具，一些简单的命令行十分容易掌握，但是还是有必要掌握一下整体的概念。&lt;/p&gt;
&lt;h2&gt;安装&lt;/h2&gt;
&lt;p&gt;本身 Rclone 的&lt;a href=&quot;https://rclone.org/install/&quot;&gt;官网&lt;/a&gt;给出了一键安装的指令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo -v ; curl https://rclone.org/install.sh | sudo bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后使用 &lt;code&gt;rclone config&lt;/code&gt; 来初始化。&lt;/p&gt;
&lt;h2&gt;配置&lt;/h2&gt;
&lt;p&gt;Rclone 本质上可以理解为使用密钥来访问 OSS 存储，包含了一系列的配置文件，一些信息是 OSS 存储中需要获得的，这些可以参照你自己使用的存储。你可以直接编辑 &lt;code&gt;~/.config/rclone/rclone.conf&lt;/code&gt; 来配置。&lt;/p&gt;
&lt;p&gt;以下是一个 OSS 存储的配置示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[oss]
type = s3
provider = AWS
access_key_id = my-access-key-id
secret_access_key = my-secret-access-key
endpoint = https://oss-cn-shanghai.aliyuncs.com
acl = private
bucket_acl = private
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;type&lt;/code&gt; 是存储的类型，&lt;code&gt;provider&lt;/code&gt; 是提供商，&lt;code&gt;access_key_id&lt;/code&gt; 是访问密钥，&lt;code&gt;secret_access_key&lt;/code&gt; 是密钥，&lt;code&gt;endpoint&lt;/code&gt; 是存储的 endpoint，&lt;code&gt;acl&lt;/code&gt; 是访问控制列表，&lt;code&gt;bucket_acl&lt;/code&gt; 是存储桶的访问控制列表。基本上我这里没有很多配置，都是直接同事给了配置文件，添加到里面就好。&lt;/p&gt;
&lt;h2&gt;使用&lt;/h2&gt;
&lt;p&gt;在配置之后，使用的逻辑就比较简单了，就是直接用 &lt;code&gt;rclone&lt;/code&gt; 命令来使用。rclone 的一个好处在于可以使用多线程来进行传输，所以速度比较快。&lt;/p&gt;
&lt;p&gt;以下是一些基础使用：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 查看目录
rclone lsd oss:/path/to/your/folder

# 上传/下载文件
rclone copy --progress --transfers 300 --checkers 300 -L /path/to/your/folder oss:/path/to/your/folder

# 删除文件
rclone delete oss:/path/to/your/folder
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;-L&lt;/code&gt; 的意思是 follow symlinks，也就是跟随符号链接。其他的都比较顾名思义&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;Rclone 是一个非常强大的工具，可以用于同步以及备份文件，对于有 OSS 使用需求的读者来说非常值得掌握。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/133927048_p0.webp"/><enclosure url="https://picr2.axi404.top/133927048_p0.webp"/></item><item><title>周记 Week67</title><link>https://axi404.github.io/blog/week-67</link><guid isPermaLink="true">https://axi404.github.io/blog/week-67</guid><description>2025-10-06 ~ 2025-10-12.</description><pubDate>Sun, 19 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;养病&lt;/h2&gt;
&lt;p&gt;这一周的周记也算是有点迟到，基本上大多数时候都是国庆的末尾，这时候大概还是在学习，但是剩下的时间基本上都是在养病。之前 GenManip 里面又查出来了一个相关比较严重的 bugs，又进行了一些修补。&lt;/p&gt;
&lt;p&gt;这一周实在是没有什么可以说的事情，还是照常阅读论文。主要是因为之前一直劳作，所以说居然积劳成疾，加上西安这鬼天气一直下雨，大病了一场，一直卧床。不过好歹是和乐小姐一起去逛了逛街，每天和乐小姐在一起都很开心。&lt;/p&gt;
&lt;p&gt;稍微值得一提的可能就是在周日的时候，在 AI 学组做了一次 Talk，估计也是我在西交的最后一次 Talk 了，关于保研以及科研自己目前的一些感想，也是最近自己一直以来思考的最后一次分享了，之后可能也很难有机会了。大抵上确实是因为分享了非常多很干货的内容（从我进入大学起参加的全部讲座大多数都是在说一些废话，甚至有的连正确的废话都算不上），大家还是给予了很多积极的反馈，令我感慨良多。&lt;/p&gt;
&lt;p&gt;除此之外大概也就别无他事了，好好休息，下周就要继续努力了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/9Xla7WK.webp"/><enclosure url="https://picr2.axi404.top/9Xla7WK.webp"/></item><item><title>Paper Reading: Embodied AI 4</title><link>https://axi404.github.io/blog/paper-reading-eai4</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-eai4</guid><description>从一些 Embodied AI 相关工作中扫过。</description><pubDate>Mon, 28 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Embodied AI Paper Reading&apos;,
items: [
{
title: &apos;Batch 1&apos;,
href: &apos;/blog/paper-reading-eai1&apos;,
order: &apos;1&apos;
},
{
title: &apos;Batch 2&apos;,
href: &apos;/blog/paper-reading-eai2&apos;,
order: &apos;2&apos;
},
{
title: &apos;Batch 3&apos;,
href: &apos;/blog/paper-reading-eai3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Batch 4&apos;,
href: &apos;/blog/paper-reading-eai4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Batch 5&apos;,
href: &apos;/blog/paper-reading-eai5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Batch 6&apos;,
href: &apos;/blog/paper-reading-eai6&apos;,
order: &apos;6&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;4D-VLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/nMA7ewK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;4D-VLA 本身的 VLM 主干是 InternVL-4B，使用投影到世界或者机器人坐标系下对齐的深度图以及视觉图一起编码成 Visual token 输入 VLM，之后后面接一个 Action head。&lt;/p&gt;
&lt;p&gt;其中 4D 的体现就是使用了 Memory Bank，这个 Bank 里面采样出来的帧之间的区别要尽可能大，这个思想也比较好理解，毕竟需要保证输入给模型的帧需要有尽可能大的信息量，自然需要差异。本身各种设计都还算挺有意思的。4D-VLA 里面另一个关键，也就是其中的 3D 部分，通过将深度图提供给 VLM 来表征三维信息，则是一个需要思考的话题。本身深度图是否要提供给 VLM 一直是一个有争议的问题，因为新训练的 Visual Encoder 需要 VLM 经过一定的 pre/post-train 去适应，以及进行对齐，而在这些训练过程中，则可能以损失模型本身的泛化能力为代价。从结果上来看，貌似这篇只和 OpenVLA 进行了比较，不知道带来的增益能否抵消这种适应带来的难度。&lt;/p&gt;
&lt;h2&gt;D(R,O) Grasp&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ixsiewK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;D(R,O) Grasp 提取灵巧手以及物体的点云的 Feature，然后用 CVAE 预测距离矩阵，然后进行优化，因为优化可以并行，所以性能很高也很快。&lt;/p&gt;
&lt;h2&gt;InstructVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755001452866_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;InstructVLA 本身还是 pi-like 的架构，其中 VLM 是一个 MOE，两个阶段去训练，使用了构建的大量的数据。&lt;/p&gt;
&lt;p&gt;本质上 InstructVLA 强调了多 Stage 的训练，第一阶段直接进行动作数据的预训练，这部分在文章的 setting 下也就是使用了 OXE，然后第二阶段冻结 DP，梯度回传到 VLM，去训练 MOE 以及 LoRA。InstructVLA 是近期开始讲 Generalist 故事的论文之一，是否有必要将模型的多模态泛化能力完全在 VLA 中保持，这一点依然是个问号。&lt;/p&gt;
&lt;p&gt;包括 InstructVLA 在内的一系列模型证明了训练 in-domain 的推理任务，可以在推理内容上从泛化角度具有一定的外拓，也就是训练了 &lt;code&gt;将猩猩爱吃的食物（香蕉）放到盘子里&lt;/code&gt; 之后大概率可以泛化到 &lt;code&gt;将海边比较常见的热带水果（椰子）放到盘子里&lt;/code&gt;。结果上来看，InstructVLA 也具有一定的 zero-shot 能力，还算是令人印象深刻。&lt;/p&gt;
&lt;h2&gt;Fast-in-Slow&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4c8b5cd469f7ee3662f3b2054dccbf57.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;FiS-VLA 算是比较意料之中的架构，一个 VLM 作为慢系统，本身 AR 输出 Language，然后一个 DP 作为快系统，Vision Encoder 的输出和 VLM 的输出 concat 并且作为 DP 的 condition。&lt;/p&gt;
&lt;p&gt;这里面有意思的一点在于，这种架构说明在 DP 中确实可以引入异步，DP 比 VLM 快一定的速度，而同时 DP 的 condition 也可以不只是 VLM 的输出，也可以和别的内容 concat 在一起，也就给加入诸如 3D Encoder 了空间。毕竟从第一性原理考量，VLA 需要 VLM 的能力越多越好，那么就尽量不要修改 VLM 的结构，也就不能在 VLM 的前面加入 3D Encoder，那么当整体的模型希望引入更多的 perception 能力的时候，一个合理的设计自然也就是在 DP 中加入新的 embedding 进行 concat 了。&lt;/p&gt;
&lt;h2&gt;NavDP&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/36de7de80b251c8c44514281714c03b3.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;第一次读 Navigation 的论文，还挺有意思的。&lt;/p&gt;
&lt;p&gt;NavDP 本身就是 Transformer 去 Fusion Goal 和 Observation，Observation 本身是 RGB-D 的，然后之后的输出作为 DP 的 condition 来输出一个 chunk 的轨迹，之后轨迹用 1D Conv 压成 Trajectory token 再次输入到同一个 shared weight 的 Transformer 中，这次后面接一个 Critic 来选择轨迹。&lt;/p&gt;
&lt;p&gt;这个导航以及对应的任务本身其实非常巧妙，本身感觉这种级别的 Navigation 是并不需要「脑子」，本质上任务是 2D Goal 或者视觉导航，而并没有语义理解的任务难度，作为一个导航领域的 foundation model，加上使用了大量的 simulation render 的数据，完全是 scalable 并且容易 work 的，确实不错，在 solid 的情况下可以成为一些方法的 base model。&lt;/p&gt;
&lt;p&gt;总的来说，这篇工作我还是挺喜欢的，在 DP 的范式下做出了一些探索，而且都是必要的，并且证明了一定的 scalable，很好地在自己的 scope 中闭环了。&lt;/p&gt;
&lt;h2&gt;Nav$A^3$&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/97978a25ed491c852ec2cd8174ce9bf1.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Nav$A^3$ 是导航中除了如 NavDP 这种 low-level 导航之外的另一种 topic，也就是进行 high-level 的 Navigation。&lt;/p&gt;
&lt;p&gt;本身相较于 Manipulation，Navigation 的 System 2 从故事上是更加闭环的，毕竟从一开始 SLAM 开始，一个可以被证实的事情是，导航本身至少在 Local Map 中只需要进行 2D 的规划，并且如 NavDP 这种 policy-based 还是如直接基于点云进行避障，早在很久以前都是已经验证十分有效的方案了。这也就意味着对于 Navigation 的 System 2 来说，已经可以输出最终需要的信息了，也就是最多只有三个自由度的 2D Nav Goal (x, y, yaw)。&lt;/p&gt;
&lt;p&gt;读者假如看了之前关于在 Manipulation 中引入 VLM 并且使用各种中间表征的 Paper reading，不难发现，对于 Manipulation 来说，这些表征确实是相对「中间」的，因为 Manipulation 最后的执行是使用 6 自由度的空间的 end-effector pose 进行。这对于模型来说要不然具有极大的难度，对于 VLM 几乎学不出来，要不然则少很多的信息（如 bounding box），或者具有歧义性（如 2D trajectory）。&lt;/p&gt;
&lt;p&gt;因此作为一个相对「粗粒度」的任务来说，VLM 确实在导航中进行了很大程度的运用。&lt;/p&gt;
&lt;p&gt;回到论文本身，Nav$A^3$ 的 System 2 是 VLM，System 1 也是 VLM 但是专门训练过 Affordance，因此本身只是构建了数据以及 System 来构建一个导航的 infra system，将不同的组件串联在了一起，这方面没有过多的 highlight。本身 Nav$A^3$ 似乎复用了 RoboBrain 的 Affordance 的训练数据，输出依然是 Point，并且具有了非常好的效果，之后通过 Point，可以变换到机器人坐标系下，并且进行正常的导航。&lt;/p&gt;
&lt;h2&gt;Genie Envisioner&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755005211487_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Genie Envisioner 本身是用 AgiBot 的数据训练了一个 video generation model，或者说 world model，称之为 GE-Base。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/479e0de9394aa5c696def137b11ea77d.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GE-Base模型本身就是经典的 DiT 设计，输入是三个视角的视频编码 token + 稀疏历史记忆 + 噪声图 + 文本嵌入，每次输出一个 video chuck。模型在 AgiBot 的数据上进行了预训练，作为了后续一系列设计的一个基石，这其中预训练包括两个环节，一个是在不同帧率的视频上预训练，之后为了对齐 Embodied 这边的频率，在 5Hz 的频率上进行了微调。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/292270f8fd32afbb641d238253246a45.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;另一个设计是将 GE-Base 外接一个相同架构，但是 Hidden state 更小的模型，来改造成一个 Action model，这里就是直接将 hidden state 拿出来作为 Action policy 的 condition。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/f6a46c260187caf7a8d739dd99523ad8.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;同时 GE-Base 也可以作为 World Model Simulator，也就是接受 Action condition 来输出视频，作为一个仿真器。&lt;/p&gt;
&lt;p&gt;这篇工作本身一气化三清的思路非常有意思，也是充分在 Large scale 的 pre-training 之后充分运用了 world model 的潜力。&lt;/p&gt;
&lt;h2&gt;StreamVLN&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755004919768_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;StreamVLN 本身算是如标题所示，也算是在 Navigation 这个任务中第一次引入这种 Memory bank，类比到 Manipulation 可能有点类似于 4D-VLA 里面的做法。本身模型就是使用 LLaVA-Video 的架构，然后进行连续的多轮自回归生成即可。同时引入了使用 KV-Cache 来进行加速，但是尽管进行了 KV-Cache 的复用，但是显存开销会随着历史长度的增加而线性增长，于是对于 KV-Cache 进行压缩，使用滑动窗口来缓存。同时对于记忆，把每一帧的图像 token 通过深度信息映射到 3D 空间，如果多个帧的 token 落在同一个体素里，就只保留最新的那个，来实现 Memory bank 的采样。&lt;/p&gt;
&lt;p&gt;StreamVLN 从思路上还是比较有趣的，不同于 4D-VLA 的 Memory 思路，StreamVLN 的 Memory 看起来更加可解释，类似于每个区域都只能包括最新的记忆，通过 3D 的空间来描述。&lt;/p&gt;
&lt;h2&gt;V-JEPA 2&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/9d3d0bc5f83dccdc03e713683f7fd7fa.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;V-JEPA 2 是 LeCun 的新作，本身还是按照 LeCun 一直在说的 World Model 的范式，使用自回归的范式进行训练，并且在大量的预训练之后支持了图中的动作分类、目标识别、动作预测和视频问答系统等，同时也可以作为一个 action model 来使用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755005701312_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;V-JEPA 2 本身本质上就是如图中左图的自回归范式，也就是将视频变为 token 之后，使用自回归的方式进行训练。如图中所示，其实本质上 V-JEPA 2 做的其实就是掩码降噪生成，也就是使用 DiT 的思路进行降噪。同时用停止梯度和 EMA 避免模型崩掉。模型本身都是 ViT 组成的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/61e3217c05fb8e748e574cac26d83877.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在此基础上，使用 V-JEPA 2 的 world model 做了 V-JEPA 2-AC，可以用于动作生成。这个本质上就是预测未来的 image，并且有一个可以以 Action 为 Condition 的 world model。本身在过程中就是使用交叉熵的方法来优化 action chunk，来确保此时的 action world model 的输出尽可能接近正常 world model 的输出。&lt;/p&gt;
&lt;p&gt;V-JEPA 2 在运动视频中理解能力很强，并且里面其实很多诸如约束 Gaussian 分布的内容很讲究，建议读者品读。&lt;/p&gt;
&lt;h2&gt;MolmoAct&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/6e0c9655991f2059e72a52c0ededf7f9.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;MolmoAct 是 AI2 在 Manipulation VLA 领域中的新作，本身是 follow OpenVLA 的设计，同时包括了经典的 reasoning 以及 VQA 等 VLM cotraining 的任务。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755007163356_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;MolmoAct 本身这图长得和 Pi 很像，但是实际上是 OpenVLA-like 的架构，也就是由 VLM 直接输出 Action Token，而并不是使用 DP 来输出。本身 MolmoAct 认为在 VLA 执行过程中有几个点是很关键的，比如说深度感知以及轨迹，因此本身 MolmoAct 在训练以及执行的时候会依次输出三种不同的 token，也就是深度感知 token，轨迹 token 以及 action token。在大量的数据上进行预训练之后，MolmoAct 同样需要构造自己的深度感知 token 以及轨迹 token 来让模型自回归地学习。&lt;/p&gt;
&lt;p&gt;其中深度感知 token 是使用 VQVAE 来自回归机器人数据集中使用 Depth-Anything V2 预测的结果，并且将 VQ 中的 codebook 结果作为 token。以及轨迹，直接使用 2D 点来表示。这些东西共同组成了 post training，而预训练依然是经典的各种 grounding 数据一起上。从结果上 MolmoAct 可以提供 in-domain trajectory，并且具有一定的 zero-shot 能力。&lt;/p&gt;
&lt;h2&gt;MimicGen&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755024593491_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;MimicGen 本身是经典的数据生成方法，本质上是将轨迹转化为 Object centric 的 grasp pose list，并且进行切片。这里的切片本质上就是将 trajectory 划分为若干的 primitive skill，比如说开抽屉或者抓起来某个物体。那么在新的 Layout 下面，根据当前的仿真可以获得每个物体的 pose，就可以根据 object centric 的 pose list 进行变换得到新的 world frame 下的 pose，并且将不连续的片段的首尾之间使用 motion planner 进行连接，也算是十分好用了。事实上如 GR00T 等模型使用了这类方法来生成数据。&lt;/p&gt;
&lt;h2&gt;SpatialVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1757557713910_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SpatialVLA 本身是十分 Work 的方法，但是这里的图是在不是很直观，简单可以理解为提出了一种将 Spatial 信息编码为 VLM 可以使用的输入，并且使用类 OpenVLA 范式的模型进行 inference。本身的输入包括了 text 的 token 以及 Ego3D visual token。这里的 Token 的获得大概是这样的流程，通过 SigLip 获得 semantic embedding，然后用 ZeoDepth 来获得 Depth，将 semantic 投影到 3D 空间，之后再用 MLP 拿到 Token。&lt;/p&gt;
&lt;p&gt;从本质上来说，这种 embedding 的方法综合了 depth 的表征以及本身的 semantic 信息，而不像是一些其他的工作一样，直接使用 Depth encoder 去进行处理，而且本身这里的范式因为引入了 ZeoDepth，算是从 input 上的 depth free 的范式，还是很有意思的，本身的点数也都很高。&lt;/p&gt;
&lt;h2&gt;TraceVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/b5013076f0d91ffacd72920aa95c87a5.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;TraceVLA 本身采用了 OpenVLA-like 的范式，如图中所示，不过不同的是加入了一张别的图片，也就是标注了 2D 视觉轨迹的图片。TraceVLA 的故事点在于，OpenVLA 是单帧输入多帧输出的模型，但是 VLA 模型有必要输入历史信息，但是使用 action history 进行输入不是很直观，于是直接计算 2D 视觉轨迹并且标注在图片上，是一个不错的做法。同时因为在图片上进行标注会遮挡本身的图像信息，所以原图也同样输入。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/bb2516155fb3d5919356b1e95a3caf3b.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;具体来说 Trace 的生成，就是保留一定的历史帧，使用 Co-tracker 来提取轨迹，并且保留运动的点，标注在图片上。从本质上，让 VLA 了解历史信息确实是十分必要的，不然对于单帧输入来说，假如说在 Pick and Place 的过程中前后有相似的动作，那么模型可能就难以分清前后了。标注 Trace 是一个很直接且有效的方法，问题只是在于，这看上去并不是最优雅的方法，会带来额外的计算开销。理论上类似的方案，可以直接使用机械臂的 URDF 的标注以及 Action 信息来直接计算出来这些轨迹点，并且投影到相机的像素坐标系下直接作为轨迹。&lt;/p&gt;
&lt;p&gt;当然，有一些有趣的 ablation 其实在论文中并没有体现，但是其实是值得关注的，例如 Trace 的条数能否更少，或者说这个 &lt;code&gt;&amp;#x3C;SEP&gt;&lt;/code&gt; Token（用于在 Trace image 和 Image 之间进行分割）是否必要。&lt;/p&gt;
&lt;h2&gt;Hume&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/5f06364c646f7da98e8c53fab103b08a.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Hume 本身也算是 Pi-like 的架构，直观理解，可以认为将 Pi 的 System 1 部分分为了两个环节，首先生成候选，再在选择的候选的基础上进行进一步的降噪。本身 System2 这里已经是 VLM + Flow Matching 的完整结构，但是在 FM Model 推理的过程中，降噪十步，把十步的输出组成一个十个 Chunk 的候选动作，评价网络从中选择 Best。System1 本身也是一个 Transformer + FM，不过这里更加轻量，同时 FM 在 Best of N Action 的基础上进行继续降噪。这里可以理解为这里的 Transformer 只是作为 fusion 存在。&lt;/p&gt;
&lt;p&gt;本身 Hume 算是在 Pi 的基础上创建了类似于快慢系统的设计，不过这里的候选动作确实在一个降噪过程中的不同步，而非不同降噪过程，那么和在降噪过程中直接输入 Fusion 的实时 Obs 信息，如 FiS-VLA 等设计，在某种程度上是类似的，而并非如 NavDP 一样的设计，那么这里所谓的 Best of N 个人感觉并非必要的。&lt;/p&gt;
&lt;h2&gt;RICL&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/e18a1f18f68d230da286b461d026b803.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;In-context learning 是一个 LLM 中非常重要的能力，也就是在推理的过程中根据历史输入而无需训练，泛化到未见任务上。这种能力明显在 VLA 中还没有进行充分的讨论，当下的领域聚焦于通过朴素的 Scaling up 来使得模型具有更多的动作能力，这篇文章对 ICL 进行了基本的讨论，并且给出了朴素的解法。具体来说，ICL 在 Manipulation 中的范式大概类似于，在开始正式任务之前，该任务已经采集了部分数据，这些数据大概与任务本身同分布，甚至完全相似，在执行的过程中，将这些可供参考的轨迹数据同样作为输入，使得模型可以参考这些数据，从而更好地完成任务。&lt;/p&gt;
&lt;p&gt;在具身系统中进行上下文学习，本身需要两个环节，一个是索引，也就是找到上下文数据，一个是推理，也就是如何将上下文数据嵌入到模型的推理流程中。本身 RICL 使用的方法相当朴素，对于索引，使用 DINO 编码当前帧的特征来对历史帧的特征进行 RAG 即可；对于推理，则通过比较历史和当前的 OBS 的 DINO 相似度，作为加权的权重，将 FAST 编码进行插值。这一方法的朴素在于，首先，对于 ICL 的实现过于显式，使用 DINO 特征来进行解码意味着使用较短的片段，并且希望找到与当前任务完全相似的任务，这使得这一方法在语义相似但是场景不相似的任务上效果不佳；同时，尽管对于索引的信息依然作为了 Transformer 的输入，从而可以隐式在模型中进行学习，然而依然采样相邻帧，本身学习的依然是一种相当 low-level 的动作信息，而不是较为 high-level 的动作模式；最后，模型本身的范式似乎是可拓展的，但是也没有在大规模的实验上进行 scaling up，还有继续讨论的空间。不过总的来说，RICL 对于 ICL 的 highlight 本身已经足够出彩，尽管本身的规模不大，但是依然值得较高的评价。&lt;/p&gt;
&lt;h2&gt;OmniVTLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/8e6da8a22d5cf2663bf336e0f3110c75.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;OmniVTLA 在 $\pi_0$ 的基础上引入了语义对齐的触觉编码器，本身的对齐就是通过对比损失，在论文中自己采集的触觉数据集（触觉传感器接触不同的材质，其中图像是 camera 信息，文字是材质名称）上进行训练。在这里事实上从结果来看，因为只有真机实验，和 baseline 的效果对比并不显著，但是事实上触觉的存在还是有必要的。根据目前笔者的经验来说，大量的失败主要在于对于抓取时机的把握不准，也就是可能还没有到达指定的深度，就已经触发了抓取，这里使用深度或者触觉在一定程度上都可以解决这个问题，通过更加显式的信号来进行引导，触觉可能是可以带来更多用处的一个方法，也是值得探索的。&lt;/p&gt;
&lt;h2&gt;Spatial Traces&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/cbea64bbd00be9127bf58bf55b3c39ca.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本身算是在 TraceVLA 的基础上进行的后续工作。本身的 Motivation 也是相当朴素，也就是将之前 TraceVLA 的时间维度拓展到时空，具体来说其实就像图里的内容，将 trace 标注在了 depth image 上面。之后的方法其实是比较直接，将 Vision 的各种信息拼接在一起，一起输入给 VLA。VLA 本身从 SpatialVLA 初始化，也就是一个 PaliGemma2，并且是 OpenVLA-like 的设计。&lt;/p&gt;
&lt;h2&gt;Embodied-R1&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4168fb5fb9ffd58aee3e42dffa2b21be.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，本身 Embodied-R1，顾名思义，就是用 R1，即 GRPO 方法训练的一个 VLM。本身其实还是有不少的有趣的点以及有趣的解释的。首先第一点在于为什么选取点而不是框，主要还是因为更加灵活，以及指向比较明确，同时在整体论文讲述的时候非常关注 embodiment 无关，这也很重要。另一方面在于为什么用 RFT 而不是 SFT，也很好理解，SFT 一般适用于选择题这种答案唯一的，但是比如说描述一个区域，一个点只要落于区域内就算对，这时候 RFT 可以给全部满足条件的答案都给奖励。&lt;/p&gt;
&lt;p&gt;模型本身的输出，在 reasoning 之后会给到 2d point，最后包括了两种模式进行动作执行。一种是 key point，也就是只输出关键点，中间过程使用 CuRobo 进行插值；一种是直接输出连续轨迹点。当然这里，感觉事实上即使是所谓的连续轨迹点，实际上最后还是要用 CuRobo 来插值。这里面 2d to 3d 的 projection 是直接使用的 depth 信息，这其实相对比较糟糕，意味着模型的全部过程点或者需要在空中的点，都会有一些问题。&lt;/p&gt;
&lt;p&gt;其中补充的一些试验，比如说 RFT 数据里面是否加入 common sense 的数据，不加入会带来性能损失，这些角度也都是读者很关注的，读起来很流畅。本身其他内容还是没什么问题的，用 R1 也是大势所趋，整体看上去很不错。&lt;/p&gt;
&lt;h2&gt;RynnEC&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/baf450d96899187ed011c136151a269a.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RynnEC 本身就是基于 VideoLLaMA3 构建的，本身的训练方法也就是标准的 SFT 方法。其中相对或许可以 highlight 的是引入了 Semantic Mask 的 encoder/decoder，从而可以让模型输出 mask token，从而使得模型具有 instance level 的 grounding 能力。&lt;/p&gt;
&lt;p&gt;同时这篇论文也提出了对应的数据管线以及 Benchmark，可以说是比较大的工作量了。不过问题也比较明显，貌似这个工作很难以部署到 Action level 的下游应用，而是确实是一个很单纯的 VLM，而且也没有展现任何的真机部署。&lt;/p&gt;
&lt;h2&gt;TinyVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/6706baaecb2bf058d4d920c78caee215.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;TinyVLA 实际上是首次提出了 VLM + DP 范式的模型，这个范式相当本质，有效缓解了模型的灾难性遗忘并且使得 VLM 和 action expert 各司其职，同时 leverage 了 DP 优秀的动作能力，后续的无数的工作都是基于此范式。&lt;/p&gt;
&lt;p&gt;具体从方法来说，模型使用 Pythia 作为 backbone，使用 LoRA 在 Finetune 的时候在 LLM 里面使用 LoRA，从而不过分破坏 LLM 的能力，这在 Pi0-KI 出现之前，可以说是相当有效的做法，并且使得模型可以高效微调。本身最后的性能也不错，可以说是领域内的奠基性工作了。&lt;/p&gt;
&lt;h2&gt;RoboRefer&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/e00dcbf2971e344369a5efa8d5868d84.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RoboRefer 使用 NVILA 作为基础模型，在输入的部分加入了 Depth 输入。SFT 训练先对齐深度，之后再进行全量微调，并且在 SFT 之后使用 GRPO 进行 RFT。RoboRefer 可以说是使用空间信息 VLM 数据训练 VLM 的集大成者，可以说考虑了很多的事情，既有对齐也有引入深度，同时输出也是 Point。同时论文还提出了 RefSpatial 的数据集以及成熟的数据管线，大量的细节在论文里都有提及，可以说是诚意满满的工作了，尽管更多的是 engineering，但是确实很闭环而且很清晰。&lt;/p&gt;
&lt;h2&gt;HERMES&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/5fca2b094a90cbf52d38b793d68a0966.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;老实说，HERMES 本身其实从图中来看也可以看出来，内容相当的 technical，在这里简单写一下整体的流程以及我比较 naive 的理解。从故事上来说，HERMES 本身想要实现的是使用 RL 来让模型从多源数据中学习，这里面使用的技术就是各种方法来提取 hand 以及 object 的 pose，但是提取之后需要放到 Sim 里面进行执行，这也就意味着需要 recover 出来模型本身的 mesh 并且放到仿真中，所以总体来说有点像一种版本的 mimic，而且并不 Scalable。在构造了 RL 模型之后，就可以使用 DAgger 来训练一个 Student 模型了，对于不了解 DAgger 的读者，可以理解为，从单个或者多个 RL 模型中蒸馏一个 learning based 模型。在之后就是弄了一个 VLN，组合在了一起，来实现 Moblie Manipulation。不过讲实话，整体的不少内容有点混乱。&lt;/p&gt;
&lt;h2&gt;Discrete Diffusion VLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/76def89d6f735b8bfcf6879be3777b26.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;尽管 Discrete Diffusion VLA 的论文中在不断强调自己的 Diffusion 身份，但是实际上在理解上，从范式上去看，本身就是 Bert-style 而已，因此并不具有真正的降噪或者加噪过程。这一过程在论文中被描述为对于 action 的加/减 Mask 的操作。同时因为采取了 Bert-style 的范式，具有了比较高的并行度。除此之外的内容主要就是一个 OpenVLA-like 的范式，还算有意思。&lt;/p&gt;
&lt;h2&gt;G0&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/99a615c358fdb7583fc4f005d8b5ae38.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;G0 是一个大型的 VLM + VLA 的 Pre-training 工作。本身的结构算是双系统，其实本质上也是类似于 Pi 的设计，从 PaliGemma 初始化，由 VLM + Flow Matching 的 DP 组成的。本身 G0 提出了一个大型的基于其本体的数据集，同时对于不同的任务进行了划分，来实现比较粗粒度的任务规划以及一些交互选项。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ea9a0db6827ba84f8ac22b6aed9bcb66.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于 VLA，分为两阶段。第一阶段在大规模异构数据上面预训练，其实还是类似于 Pi 的做法，只是没有 co-training VLM Data，而是只用了 FAST 进行 action tokenization 的离散动作来进行 next token prediction；第二阶段在他们自己的数据集上后训练，添加了 Action Expert，并且用 Flow Matching 来进行训练，本身没有什么额外的细节。在两阶段之后可以认为得出了一个预训练模型，这里之后就可以到具体的下游任务上进行验证使用了。实验部分的实现来看，其实预训练某种程度上没有那么有效，各种内容其实也没啥递增的一致性，只能说最后混在一起训练了一波，结果还算说得过去。&lt;/p&gt;
&lt;p&gt;对于 VLM，就是从 Qwen2.5VL 开始训练，用 G0 数据集中标注以及 rewrite 的 VL 任务拆解数据进行训练，本身没什么好说的。&lt;/p&gt;
&lt;p&gt;整体来说 G0 算是中规中矩的预训练工作，处于工作量来看应该算是还可以吧。&lt;/p&gt;
&lt;h2&gt;InternScenes&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4dbf10c7b95a3ee939a28b3fb5927798.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;非常巨大的场景资产数据集，包括了大量的场景资产，涵盖 40K 个场景以及 1.96M 个物体资产。整体源自于 EmbodiedScan, Infinigen indoors 以及设计师设计的资产，并且可以替换场景中的物体来带来更多的多样性。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-12-zh.webp"/><enclosure url="https://picr2.axi404.top/template-12-zh.webp"/></item><item><title>周记 Week66</title><link>https://axi404.github.io/blog/week-66</link><guid isPermaLink="true">https://axi404.github.io/blog/week-66</guid><description>2025-09-29 ~ 2025-10-05.</description><pubDate>Thu, 16 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;这一周总的来说也是没有发生很多的事情，可以说是在学习以及科研上的事情没有那么多，但是别的事情倒是有不少，重点还是经历了这么多之后，也需要慢慢的收敛，并且总结出来一些东西，给一些事情划上句号。&lt;/p&gt;
&lt;h2&gt;科研与学习&lt;/h2&gt;
&lt;p&gt;事实上，这一周在科研和学习上也没有做很多特别的事情，而是主要在收敛以前的一些内容。一方面还是将之前自己的博客又进行了反复的更新，以及把课内的一些提交的作业都整理了一下，这些事情其实也没什么可说的。&lt;/p&gt;
&lt;p&gt;另一方面就是在科研上了，也就是对于上一个阶段内做的一系列工作进行一个整体的收尾，这自然是多个角度的。&lt;/p&gt;
&lt;p&gt;在去年一年中我主要做的还是仿真的数据合成，GenManip 这个框架我花费了很多时间进行搭建，并且就像前几周的内容中写的一样，我也一直在进行 cleanup 和 refine。&lt;/p&gt;
&lt;p&gt;这些整理，一方面包括对于代码格式的整理，一些需要拆分的函数进行拆分，一些太长的文件也要切成不同的文件进行管理，同时先在必要的地方都加上注释；另一方面也包括从用户角度进行了一些考虑。比如说之前对于传参，其实需要传递一些参数，因此对于文件中本来的默认常数就没有做什么约束，现在将它改为了目前的最小 config，可以有一套很快速的流程要用户可以立刻地生成数据或者测试模型。&lt;/p&gt;
&lt;p&gt;当然还包括说资产管理，实际上单一的 benchmark 不需要考虑资产管理相关的事情的，大概率只需要将资产都集中放在一个文件夹里面，然后用一个脚本让用户可以进行下载，就已经可以了，因为用户在后续对于这些内容的使用是完全无感的。但是 GenManip 其实不一样，本身我目前在做的事情还是尽量趋近于一个平台，也就是希望用户可以自己管理资产，自己设计自己的任务或者测评，那么资产就需要通过一种区块的方式进行增删，这其实自然也不复杂，但是也需要去打通。&lt;/p&gt;
&lt;p&gt;其中重中之重还是在于文档。一个事实是天下没有免费的午餐，GenManip 具有比较高的上限，包括在我后续再过一段时间的开发之后，可以具有更大的潜力，但是熟练进行使用毫无疑问是需要学习成本的。如何通过一个合理的文档，直观向用户展示如何使用这个框架，从简单到困难一步一步搭建来整体的体系，并且说服他们这一切的设计是合理的，这就是我这一周在尝试去做的事情。比如说目前的文档里面展示了如何在7分钟内从零开始生成数据，或者一个 step by step 的 quickstart 示例。&lt;/p&gt;
&lt;p&gt;思考用户来看看文档之后，最关心的问题是什么，最多的类型的用户是什么，他们又能否在最短的时间内解决自己的问题，在解决的过程中每一步又能否得到明确的反馈，这毫无疑问是一门学问，而我还在努力的过程中。&lt;/p&gt;
&lt;p&gt;当然，另外一方面一段时间的空虚也告诉我，我事实上也必须要跳脱出这一切，我开始尝试从四个方向进行突围，希望能在下一段工作彻底被我锁定之前得到更多的合作和灵感。&lt;/p&gt;
&lt;p&gt;一方面我在数据生成方面寻求合作，和更加厉害的团队一起努力做更大事情；一方面我在 sell GenManip 给实验室内部的人，希望得到别人的使用，我通过提供支持来拉拢初始用户；一方面我迫切需要在模型方面进行更多的探索，无论是在之前 InternVLA-M1 的基础上，还是说目前新的项目，当务之急可能是自己上手去跑一些实验来获得更多理解；最后则是在其他领域上的一些探索一些合作。&lt;/p&gt;
&lt;p&gt;不过事实上说了这么多，做得却不是很多，我其实也在深刻反思，最近一段时间在漫长的努力得到了片刻喘息的机会之后，我是不是开始懈怠了？一些地盘还没有打下来，还不是休息的时候。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;接下来就是一些生活方面的事情，主要还是因为国庆假期，所以还是正经休息了一段时间，陪一陪乐小姐，并且全收集了之前想玩的丝之歌，大概花了 25 hrs，还是很有意思的。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/328d05abc6f590bb26cb20ed44db47bb.webp"/><enclosure url="https://picr2.axi404.top/328d05abc6f590bb26cb20ed44db47bb.webp"/></item><item><title>经验分享 PPT</title><link>https://axi404.github.io/blog/ppt-sharing</link><guid isPermaLink="true">https://axi404.github.io/blog/ppt-sharing</guid><description>本人 2025 年 10 月 12 日 AI 学组经验分享 PPT。</description><pubDate>Tue, 14 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Icon, Button } from &apos;@/components/user&apos;&lt;/p&gt;
&lt;h2&gt;关于&lt;/h2&gt;
&lt;p&gt;本文件为 2025 年 10 月 12 日本人进行 AI 学组相关分享所使用 PPT，仅代表个人的学习方法以及学习思路，以及部分常识性内容，这些内容仅供参考，部分文字内容包含一些为演讲考量的夸张成分，请以分享当天相关内容为准。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/2bja7WK.webp"/><enclosure url="https://picr2.axi404.top/2bja7WK.webp"/></item><item><title>如何在 Astro 中为你的鼠标添加指针动画</title><link>https://axi404.github.io/blog/cursor-feature</link><guid isPermaLink="true">https://axi404.github.io/blog/cursor-feature</guid><description>如何在 Astro 中为你的鼠标添加指针动画</description><pubDate>Sat, 11 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;这篇博客会简单讲解一下如何在 Astro 中为你的鼠标添加指针动画。&lt;/p&gt;
&lt;h2&gt;方法&lt;/h2&gt;
&lt;p&gt;在 Astro 中，Layout 这个概念非常重要。无论你的 Layout 如何设计，一般来说都会通过 &lt;code&gt;&amp;#x3C;slot /&gt;&lt;/code&gt; 来支持内容的插入，并且一般的全部界面最终都会在 &lt;code&gt;&amp;#x3C;BaseLayout /&gt;&lt;/code&gt; 中被包裹。&lt;/p&gt;
&lt;p&gt;因此假如你想要添加一些全局效果，都可以编辑 &lt;code&gt;src/layouts/BaseLayout.astro&lt;/code&gt; 文件。&lt;/p&gt;
&lt;p&gt;对于指针效果来说，你可以在添加：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-astro&quot;&gt;...
  &amp;#x3C;body class=&apos;flex justify-center bg-background&apos;&gt;
    &amp;#x3C;style define:vars={{ highlightColor }}&gt;
      ...

      /* 点击粒子效果样式 */
      :global(.click-particle) {
        position: fixed;
        pointer-events: none;
        width: 14px;
        height: 14px;
        border-radius: 50%;
        z-index: 9999;
        animation: particle-explosion 1.1s ease-out forwards;
        box-shadow: 0 0 6px rgba(255, 255, 255, 0.8);
      }

      /* 暗色主题下的粒子效果 */
      :global(.dark .click-particle) {
        box-shadow: 0 0 8px rgba(255, 255, 255, 0.4);
      }

      @keyframes particle-explosion {
        0% {
          opacity: 1;
          transform: scale(0.4) translate(0, 0);
        }
        15% {
          opacity: 1;
          transform: scale(1.3) translate(var(--dx), var(--dy));
        }
        60% {
          opacity: 0.8;
          transform: scale(1) translate(calc(var(--dx) * 2.5), calc(var(--dy) * 2.5));
        }
        100% {
          opacity: 0;
          transform: scale(0.3) translate(calc(var(--dx) * 3.5), calc(var(--dy) * 3.5));
        }
      }

      /* 亮色主题粒子颜色 */
      :global(.click-particle.color-1) {
        background: #ffc1cc;
        background: radial-gradient(circle, #ffb3ba 0%, #ffc1cc 100%);
      }

      :global(.click-particle.color-2) {
        background: #bde4ff;
        background: radial-gradient(circle, #a8d8ff 0%, #bde4ff 100%);
      }

      :global(.click-particle.color-3) {
        background: #fff2a8;
        background: radial-gradient(circle, #ffe66d 0%, #fff2a8 100%);
      }

      :global(.click-particle.color-4) {
        background: #e6ccff;
        background: radial-gradient(circle, #d4a5ff 0%, #e6ccff 100%);
      }

      :global(.click-particle.color-5) {
        background: #c1ffc1;
        background: radial-gradient(circle, #a8e6a8 0%, #c1ffc1 100%);
      }

      :global(.click-particle.color-6) {
        background: #ffd4a3;
        background: radial-gradient(circle, #ffcc99 0%, #ffd4a3 100%);
      }

      :global(.click-particle.color-7) {
        background: #c4e8ff;
        background: radial-gradient(circle, #b3deff 0%, #c4e8ff 100%);
      }

      :global(.click-particle.color-8) {
        background: #f8d7da;
        background: radial-gradient(circle, #f5c2c7 0%, #f8d7da 100%);
      }

      :global(.click-particle.color-9) {
        background: #d1f2d1;
        background: radial-gradient(circle, #c3e6c3 0%, #d1f2d1 100%);
      }

      :global(.click-particle.color-10) {
        background: #fff0d4;
        background: radial-gradient(circle, #ffe8b3 0%, #fff0d4 100%);
      }

      /* 暗色主题粒子颜色 - 更深更鲜艳 */
      :global(.dark .click-particle.color-1) {
        background: #ff6b8a;
        background: radial-gradient(circle, #ff4757 0%, #ff6b8a 100%);
      }

      :global(.dark .click-particle.color-2) {
        background: #4fc3f7;
        background: radial-gradient(circle, #29b6f6 0%, #4fc3f7 100%);
      }

      :global(.dark .click-particle.color-3) {
        background: #ffd54f;
        background: radial-gradient(circle, #ffca28 0%, #ffd54f 100%);
      }

      :global(.dark .click-particle.color-4) {
        background: #ba68c8;
        background: radial-gradient(circle, #ab47bc 0%, #ba68c8 100%);
      }

      :global(.dark .click-particle.color-5) {
        background: #81c784;
        background: radial-gradient(circle, #66bb6a 0%, #81c784 100%);
      }

      :global(.dark .click-particle.color-6) {
        background: #ffb74d;
        background: radial-gradient(circle, #ffa726 0%, #ffb74d 100%);
      }

      :global(.dark .click-particle.color-7) {
        background: #64b5f6;
        background: radial-gradient(circle, #42a5f5 0%, #64b5f6 100%);
      }

      :global(.dark .click-particle.color-8) {
        background: #f48fb1;
        background: radial-gradient(circle, #f06292 0%, #f48fb1 100%);
      }

      :global(.dark .click-particle.color-9) {
        background: #a5d6a7;
        background: radial-gradient(circle, #81c784 0%, #a5d6a7 100%);
      }

      :global(.dark .click-particle.color-10) {
        background: #ffe082;
        background: radial-gradient(circle, #ffd54f 0%, #ffe082 100%);
      }
    &amp;#x3C;/style&gt;

    ...
    &amp;#x3C;script&gt;
      document.addEventListener(&apos;DOMContentLoaded&apos;, () =&gt; {
        function createParticle(
          x: number,
          y: number,
          angle: number,
          distance: number,
          colorIndex: number
        ) {
          const particle = document.createElement(&apos;div&apos;)
          particle.className = `click-particle color-${colorIndex}`
          const dx = Math.cos(angle) * distance
          const dy = Math.sin(angle) * distance
          particle.style.setProperty(&apos;--dx&apos;, dx + &apos;px&apos;)
          particle.style.setProperty(&apos;--dy&apos;, dy + &apos;px&apos;)
          particle.style.left = x + &apos;px&apos;
          particle.style.top = y + &apos;px&apos;
          const size = Math.random() * 6 + 8
          particle.style.width = size + &apos;px&apos;
          particle.style.height = size + &apos;px&apos;
          document.body.appendChild(particle)
          setTimeout(() =&gt; {
            if (particle.parentNode) {
              particle.parentNode.removeChild(particle)
            }
          }, 1100)
        }
        document.addEventListener(&apos;click&apos;, (e) =&gt; {
          const particleCount = 10
          const baseDistance = 18
          for (let i = 0; i &amp;#x3C; particleCount; i++) {
            const angle = ((Math.PI * 2) / particleCount) * i
            const distance = baseDistance + Math.random() * 8
            const colorIndex = (i % 10) + 1
            setTimeout(() =&gt; {
              createParticle(e.clientX, e.clientY, angle, distance, colorIndex)
            }, i * 12)
          }
          for (let i = 0; i &amp;#x3C; 4; i++) {
            const randomAngle = Math.random() * Math.PI * 2
            const randomDistance = baseDistance + Math.random() * 10
            const colorIndex = Math.floor(Math.random() * 10) + 1
            setTimeout(
              () =&gt; {
                createParticle(e.clientX, e.clientY, randomAngle, randomDistance, colorIndex)
              },
              (particleCount + i) * 12
            )
          }
        })
      })
    &amp;#x3C;/script&gt;
  &amp;#x3C;/body&gt;
&amp;#x3C;/html&gt;
&lt;/code&gt;&lt;/pre&gt;</content:encoded><h:img src="https://picr2.axi404.top/112299886_p0.webp"/><enclosure url="https://picr2.axi404.top/112299886_p0.webp"/></item><item><title>Paper Reading: Embodied AI 6</title><link>https://axi404.github.io/blog/paper-reading-eai6</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-eai6</guid><description>从一些 Embodied AI 相关工作中扫过。</description><pubDate>Thu, 09 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Embodied AI Paper Reading&apos;,
items: [
{
title: &apos;Batch 1&apos;,
href: &apos;/blog/paper-reading-eai1&apos;,
order: &apos;1&apos;
},
{
title: &apos;Batch 2&apos;,
href: &apos;/blog/paper-reading-eai2&apos;,
order: &apos;2&apos;
},
{
title: &apos;Batch 3&apos;,
href: &apos;/blog/paper-reading-eai3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Batch 4&apos;,
href: &apos;/blog/paper-reading-eai4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Batch 5&apos;,
href: &apos;/blog/paper-reading-eai5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Batch 6&apos;,
href: &apos;/blog/paper-reading-eai6&apos;,
order: &apos;6&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;VLA-RFT&lt;/h2&gt;
&lt;h2&gt;ContextVLA&lt;/h2&gt;
&lt;h2&gt;VLA-R1&lt;/h2&gt;
&lt;h2&gt;Best of Sim and Real&lt;/h2&gt;
&lt;h2&gt;NOTVLA&lt;/h2&gt;
&lt;h2&gt;FlowVLA&lt;/h2&gt;
&lt;h2&gt;RLinf-VLA&lt;/h2&gt;
&lt;h2&gt;IntentionVLA&lt;/h2&gt;
&lt;h2&gt;X-VLA&lt;/h2&gt;
&lt;h2&gt;Spatial Forcing&lt;/h2&gt;
&lt;h2&gt;BridgeVLA&lt;/h2&gt;
&lt;h2&gt;RoVer&lt;/h2&gt;
&lt;h2&gt;VLA-0&lt;/h2&gt;
&lt;h2&gt;InternVLA-M1&lt;/h2&gt;
&lt;h2&gt;QDepth-VLA&lt;/h2&gt;
&lt;h2&gt;WoW&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/108249367_p0.webp"/><enclosure url="https://picr2.axi404.top/108249367_p0.webp"/></item><item><title>周记 Week65</title><link>https://axi404.github.io/blog/week-65</link><guid isPermaLink="true">https://axi404.github.io/blog/week-65</guid><description>2025-09-22 ~ 2025-09-28.</description><pubDate>Thu, 09 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;总的来说，这一周的周记又是迟到了，虽然似乎也是难免的事情，但是延期了两周也是在我的意料之外，好在这周的事情都令人印象深刻，所以还能记录一二。这周整体也是发生了不少大事。&lt;/p&gt;
&lt;h2&gt;科研与学习&lt;/h2&gt;
&lt;p&gt;一方面是和同事们一起投稿了 ICLR（说起来，假如说和实习的 groupmate 一起，这正常应该该用什么词汇进行表述？貌似用同学写得太年轻，用同事又显得太老成），也算是我在科研之后的第四篇主要参与的工作（ECCV，GenManip，InternVLA-M1 以及这篇，涉及双盲不说名字了），也是我在抛弃了独狼式科研之后合作的产物，虽然遗憾的是合作并没有带来个人工作量的减少，不过确实合力在一起带来了整体呈现的增加。本身这篇论文是在 tech report 的基础上进行了更加 research 的扩展与表达，也可以继续进行润色。&lt;/p&gt;
&lt;p&gt;另一方面也是终于结束了保研，在推免系统中确认了Offer，给一切画上了一个暂时的句号，最后的去向是 上海人工智能实验室 + SJTU。当然这方面的选择也是有一定考量的，虽然说并没有在我的保研经验帖中列出来，主要还是因为这边可以提供不在企业实习的去向中最顶级的一批资源，而且这边的老师相对来说对我也比较看好，再加上之前确实也很喜欢上海这座城市（在北京生活了十多年实在是有点无趣），以及老师的知遇之恩，各种因素综合考量之下做出了这样的选择。&lt;/p&gt;
&lt;p&gt;实际上，现在我身边进度比较靠前的同学，大多数也都去了大厂进行科研实习，在大厂这种人才密度最高的地方进行资源最多的科研似乎确实是更加正确的选择。不过感觉企业内部的竞争会更加剧烈一些，或者要不然就是拿企业的资源去做自己的科研，其实想来要是不在最核心的团队，和在这边貌似也没有本质区别，所以还是呆在自己的舒适区让我心情愉快一些。&lt;/p&gt;
&lt;p&gt;当然，随着保研结束，不少的课程也相继开始结课了。这学期有两门都是只上四周的课，基本也是可以混一下然后结束。再加上因为其实认清了一些现实，所以直接坐在教室的最前排拿电脑做自己的事情，反而相较那些在后排的同学，倒是有了更多和老师互动的机会。并且这学期的课程主要是自驾和机器人为主，也都算是我非常熟悉的领域，综上反倒是让老师记住了我的名字，也算是很有趣的经历了。&lt;/p&gt;
&lt;p&gt;当然，伴随这每一个句号，想要去总结一些事情的心理又开始发作了，所以也是重新将致新生的你里面补充了科研的部分，并且写了一下保研经验贴。事到如今，确实有种在高考结束之后，看着自己曾经刷过的题目卷子堆成小山一样的成就感，一个不经意间就走过这样远的路，还是颇为感慨的。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活上似乎还是没有什么可以说的，依然是和乐小姐的二人世界，但是因为 ICLR
在内的各种事情，确实也没有在一起很长时间，不过还是带她一起出去吃了饭。&lt;/p&gt;
&lt;p&gt;除此之外另一个有意思的事情反而是十几天前的《空洞骑士：丝之歌》发售了。HK 本身是我在高中期间还花费大量时间去玩单机游戏的时候为数不多全程就的游戏，也是花了很多心血和时间在上面。后面一直说要出续作，我也一直有在关注，但是却都是虚晃一枪，这次几个月没关注，居然就真的发售了，还是令人有些感慨，居然过去这么久了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/72a0929cb05d6594341b2a069b32a691.webp"/><enclosure url="https://picr2.axi404.top/72a0929cb05d6594341b2a069b32a691.webp"/></item><item><title>Isaac Sim 一百讲（7）：Camera</title><link>https://axi404.github.io/blog/isaac-7</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-7</guid><description>从零开始的 Isaac Sim 之路，第一季开始！</description><pubDate>Sun, 07 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Isaac 101&apos;,
items: [
{
title: &apos;安装&apos;,
href: &apos;/blog/isaac-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;万物皆 Prim&apos;,
href: &apos;/blog/isaac-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;USD&apos;,
href: &apos;/blog/isaac-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Transformation&apos;,
href: &apos;/blog/isaac-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Rigid and Collision&apos;,
href: &apos;/blog/isaac-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Collision 进阶&apos;,
href: &apos;/blog/isaac-6&apos;,
order: &apos;6&apos;
},
{
title: &apos;Camera&apos;,
href: &apos;/blog/isaac-7&apos;,
order: &apos;7&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在之前的若干章节中，我们已经十分 solid 地了解了从 Isaac Sim 的基础内容以及概念，到引入物体，距离完成一个比较简单的抓取物体的 demo，我们现在还需要的东西并不是很多了，也就只有相机以及机器人了。&lt;/p&gt;
&lt;p&gt;本章节中我们将讲解如何创建一个相机，以及如何使用相机来获得不同类型的数据。&lt;/p&gt;
&lt;h2&gt;创建相机&lt;/h2&gt;
&lt;p&gt;相机是 Isaac Sim 中的一种 Prim 类型，可以直接通过，详细的 API 可以看到 Isaac Sim 的 &lt;a href=&quot;https://docs.isaacsim.omniverse.nvidia.com/4.2.0/py/source/extensions/omni.isaac.sensor/docs/index.html?highlight=camera#omni.isaac.sensor.scripts.camera.Camera&quot;&gt;API 文档&lt;/a&gt;，在这里介绍所需的必要 API。&lt;/p&gt;
&lt;p&gt;Isaac Sim 的 Camera 本质上是对于 render product 的上层封装，render product 则可以简单理解为渲染的一个媒介，在使用 Camera 的接口创建新的 Camera 的时候，一般来说会直接创建一个新的 render product。&lt;/p&gt;
&lt;p&gt;然而因为 Isaac Sim 流水线的一些清理问题，被删除掉的相机的 render product 不能被很好地清理掉，假如说删除对应的相机，再在同样的 prim path 下创建同样的相机，就会在相当多的 world step 下无法获得到一些中间表征的渲染结果。在这里直接手动创建 render product 并指定给相机：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from omni.isaac.sensor import Camera
import omni.replicator.core as rep

rp = rep.create.render_product(rep.create.camera(), (1280, 720))
camera = Camera(
    prim_path=&quot;/World/camera&quot;,
    name=&quot;camera&quot;,
    frequency=1/60,
    resolution=(1280, 720),
    render_product_path=rp.path,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即可。&lt;/p&gt;
&lt;p&gt;创建相机一般来说需要在 World 的 reset 之前，并在在 reset 之后，就可以使用相机进行渲染了。&lt;/p&gt;
&lt;h2&gt;渲染 RGB 图片&lt;/h2&gt;
&lt;p&gt;Isaac Sim 想要进行更新，主要包括三种不同的形式，即，&lt;code&gt;step()&lt;/code&gt;, &lt;code&gt;render()&lt;/code&gt;, &lt;code&gt;world.step(render=False)&lt;/code&gt;，其中 &lt;code&gt;step()&lt;/code&gt; 是同时进行物理和渲染引擎的更新，&lt;code&gt;render()&lt;/code&gt; 是进行渲染的更新，&lt;code&gt;world.step(render=False)&lt;/code&gt; 是进行物理引擎的更新。&lt;/p&gt;
&lt;p&gt;值得注意的是，因为 Isaac Sim 不知名的 Bug，实际上在运行的过程中 &lt;code&gt;step()&lt;/code&gt; 是通过 omni 的一个闭源的 update 函数进行的更新，而 &lt;code&gt;step(render=False)&lt;/code&gt; 和 &lt;code&gt;render()&lt;/code&gt; 则相对正常。在绝大多数情况下，考虑物理，step 是否 render 为 True，不会有太大的影响，但是在 Articulation 物体的 stiffness 以及 damping 设置不合理的情况下，却会出现 &lt;code&gt;step(render=False)&lt;/code&gt; 超调而 &lt;code&gt;step(render=True)&lt;/code&gt; 正常的情况。&lt;/p&gt;
&lt;p&gt;在参数不合理的情况下，我们需要认为机械臂的超调反应了正确的物理现象，即使用 &lt;code&gt;step(render=False)&lt;/code&gt; 来更新物理引擎，并使用 &lt;code&gt;render()&lt;/code&gt; 来更新渲染引擎，是尽可能优雅的方式，而不是直接使用 &lt;code&gt;step()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;一个简单的函数可以帮助你获得相机的图像：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def get_rgb(camera: Camera) -&gt; np.ndarray | None:
    frame = camera.get_rgba()
    if isinstance(frame, np.ndarray) and frame.size &gt; 0:
        frame = frame[:, :, :3]
        return frame
    else:
        return None
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;只需要将 Camera 作为参数传入，即可获得 RGB 图片。&lt;/p&gt;
&lt;p&gt;同时值得注意的是，Camera 支持使用 &lt;code&gt;.set_world_pose()&lt;/code&gt; 以及 &lt;code&gt;.set_local_pose()&lt;/code&gt; 来设置相机的世界坐标以及局部坐标，你可以使用这些 API 来设置相机的位置和姿态。&lt;/p&gt;
&lt;h2&gt;更多参数&lt;/h2&gt;
&lt;p&gt;相机当然同时支持更多的参数，甚至包括设置鱼眼相机等，在这里先不进行过多的展开，介绍必要的内容。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def setup_camera(
    camera: Camera,
    focal_length: float = 4.5,
    clipping_range_min: float = 0.01,
    clipping_range_max: float = 10000.0,
    vertical_aperture: float = 5.625,
    horizontal_aperture: float = 10.0,
    with_distance: bool = True,
    with_semantic: bool = False,
    with_bbox2d: bool = False,
    with_bbox3d: bool = False,
    with_motion_vector: bool = False,
) -&gt; None:
    camera.initialize()
    camera.set_focal_length(focal_length)
    camera.set_clipping_range(clipping_range_min, clipping_range_max)
    camera.set_vertical_aperture(vertical_aperture)
    camera.set_horizontal_aperture(horizontal_aperture)
    if with_distance:
        camera.add_distance_to_image_plane_to_frame()
    if with_semantic:
        camera.add_semantic_segmentation_to_frame()
    if with_bbox2d:
        camera.add_bounding_box_2d_tight_to_frame()
        camera.add_bounding_box_2d_loose_to_frame()
    if with_bbox3d:
        camera.add_bounding_box_3d_to_frame()
    if with_motion_vector:
        camera.add_motion_vectors_to_frame()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如程序中所示，我们可以设置相机的焦距，剪裁范围，光圈，以及是否添加额外的表征，比如说深度图，语义，2D 和 3D 框，以及运动向量。&lt;/p&gt;
&lt;p&gt;其中裁剪范围也就是如 RGB 图，当裁剪范围过小，相机会在物体距离较近的时候看到物体的切面图，基本上此参数调整到大的很大，小的约 0.001 即可。&lt;/p&gt;
&lt;h2&gt;其他表征的获取&lt;/h2&gt;
&lt;p&gt;如下述代码所示：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def get_depth(camera: Camera) -&gt; np.ndarray | None:
    depth = camera._custom_annotators[&quot;distance_to_image_plane&quot;].get_data()
    if isinstance(depth, np.ndarray) and depth.size &gt; 0:
        return depth
    else:
        return None

def get_pointcloud(camera: Camera) -&gt; np.ndarray | None:
    cloud = camera._custom_annotators[&quot;pointcloud&quot;].get_data()[&quot;data&quot;]
    if isinstance(cloud, np.ndarray) and cloud.size &gt; 0:
        return cloud
    else:
        return None

def get_objectmask(camera: Camera) -&gt; dict | None:
    annotator = camera._custom_annotators[&quot;semantic_segmentation&quot;]
    annotation_data = annotator.get_data()
    mask = annotation_data[&quot;data&quot;]
    idToLabels = annotation_data[&quot;info&quot;][&quot;idToLabels&quot;]
    if isinstance(mask, np.ndarray) and mask.size &gt; 0:
        return dict(mask=mask.astype(np.int8), id2labels=idToLabels)
    else:
        return None

def get_rgb(camera: Camera) -&gt; np.ndarray | None:
    frame = camera.get_rgba()
    if isinstance(frame, np.ndarray) and frame.size &gt; 0:
        frame = frame[:, :, :3]
        return frame
    else:
        return None

def get_bounding_box_2d_tight(camera: Camera) -&gt; tuple[np.ndarray, dict]:
    annotator = camera._custom_annotators[&quot;bounding_box_2d_tight&quot;]
    annotation_data = annotator.get_data()
    bbox = annotation_data[&quot;data&quot;]
    info = annotation_data[&quot;info&quot;]
    return bbox, info[&quot;idToLabels&quot;]

def get_bounding_box_2d_loose(camera: Camera) -&gt; tuple[np.ndarray, dict]:
    annotator = camera._custom_annotators[&quot;bounding_box_2d_loose&quot;]
    annotation_data = annotator.get_data()
    bbox = annotation_data[&quot;data&quot;]
    info = annotation_data[&quot;info&quot;]
    return bbox, info[&quot;idToLabels&quot;]

def get_bounding_box_3d(camera: Camera) -&gt; tuple[list[dict], dict]:
    annotator = camera._custom_annotators[&quot;bounding_box_3d&quot;]
    annotation_data = annotator.get_data()
    bbox = annotation_data[&quot;data&quot;]
    info = annotation_data[&quot;info&quot;]
    bbox_data = []
    for box in bbox:
        extents = {}
        (
            extents[&quot;class&quot;],
            extents[&quot;x_min&quot;],
            extents[&quot;y_min&quot;],
            extents[&quot;z_min&quot;],
            extents[&quot;x_max&quot;],
            extents[&quot;y_max&quot;],
            extents[&quot;z_max&quot;],
            extents[&quot;transform&quot;],
            _,
        ) = box
        extents[&quot;corners&quot;] = get_world_corners_from_bbox3d(extents)
        bbox_data.append(extents)
    return bbox_data, info[&quot;idToLabels&quot;]

def get_motion_vectors(camera: Camera) -&gt; np.ndarray:
    annotator = camera._custom_annotators[&quot;motion_vectors&quot;]
    annotation_data = annotator.get_data()
    motion_vectors = annotation_data
    return motion_vectors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中如 bbox 的 label，源自于你需要为物体本身添加 semantic label，函数为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from omni.isaac.core.utils.prims import get_prim_at_path
from omni.isaac.core.utils.semantics import add_update_semantics


def set_semantic_label(prim_path: str, label: str) -&gt; None:
    prim = get_prim_at_path(prim_path)
    add_update_semantics(prim, semantic_label=label, type_label=&quot;class&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;详细的输出格式见 Isaac Sim 的官方文档。&lt;/p&gt;
&lt;h2&gt;与现实相机对齐&lt;/h2&gt;
&lt;p&gt;Isaac Sim 的相机自然也可以和现实中的相机进行对齐，此时外参通过 &lt;code&gt;.set_world_pose()&lt;/code&gt; 来设置，内参则通过下述代码来设置：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def set_camera_rational_polynomial(
    camera: Camera,
    fx: float,
    fy: float,
    cx: float,
    cy: float,
    width: int,
    height: int,
    pixel_size: float = 3,
    f_stop: float = 2.0,
    focus_distance: float = 0.3,
    D: np.ndarray | None = None,
) -&gt; Camera:
    if D is None:
        D = np.zeros(8)
    camera.initialize()
    camera.set_resolution([width, height])
    camera.set_clipping_range(0.02, 5)
    horizontal_aperture = pixel_size * 1e-3 * width
    vertical_aperture = pixel_size * 1e-3 * height
    focal_length_x = fx * pixel_size * 1e-3
    focal_length_y = fy * pixel_size * 1e-3
    focal_length = (focal_length_x + focal_length_y) / 2
    camera.set_focal_length(focal_length / 10.0)
    camera.set_focus_distance(focus_distance)
    camera.set_lens_aperture(f_stop * 100.0)
    camera.set_horizontal_aperture(horizontal_aperture / 10.0)
    camera.set_vertical_aperture(vertical_aperture / 10.0)
    camera.set_clipping_range(0.05, 1.0e5)
    diagonal = 2 * math.sqrt(max(cx, width - cx) ** 2 + max(cy, height - cy) ** 2)
    diagonal_fov = 2 * math.atan2(diagonal, fx + fy) * 180 / math.pi
    camera.set_projection_type(&quot;fisheyePolynomial&quot;)
    camera.set_rational_polynomial_properties(width, height, cx, cy, diagonal_fov, D)
    return camera
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在设置之后，Isaac Sim 不支持直接使用 &lt;code&gt;get_intrinsics()&lt;/code&gt; 来获得内参，此时需要自己实现对应的代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def get_intrinsic_matrix(camera: Camera) -&gt; np.ndarray:
    fx, fy = compute_fx_fy(
        camera, camera.get_resolution()[1], camera.get_resolution()[0]
    )
    cx, cy = camera.get_resolution()[0] / 2, camera.get_resolution()[1] / 2
    return np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)

def compute_fx_fy(camera: Camera, height: int, width: int) -&gt; tuple[float, float]:
    focal_length = camera.get_focal_length()
    horiz_aperture = camera.get_horizontal_aperture()
    vert_aperture = camera.get_vertical_aperture()
    near, far = camera.get_clipping_range()
    fov = 2 * np.arctan(0.5 * horiz_aperture / focal_length)
    focal_x = height * focal_length / vert_aperture
    focal_y = width * focal_length / horiz_aperture
    return focal_x, focal_y
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;小结&lt;/h2&gt;
&lt;p&gt;在本章节中，我们介绍了如何创建相机，以及如何使用相机来获得不同类型的数据。本身 Isaac Sim 的相机进行了比较良好的封装，所以使用起来还是比较方便的。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/1755456732964_104165485_p0.webp"/><enclosure url="https://picr2.axi404.top/1755456732964_104165485_p0.webp"/></item><item><title>乘凉，我的保研经验贴</title><link>https://axi404.github.io/blog/admission</link><guid isPermaLink="true">https://axi404.github.io/blog/admission</guid><description>关于这些年来走过的路，兴趣，进步，以及长期主义</description><pubDate>Fri, 03 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;一直以来都有一些同学向我询问保研经验贴的事情，因为各种事情的耽搁，反正是拖延了一段时间，但是细想下来，一直拖延似乎也不是办法。保研已经过去了，就像过去的三年时间一样，一些故事还在继续，一些故事也是时候需要划上一个句号了，综上，还是写下了这一篇经验贴。&lt;/p&gt;
&lt;p&gt;不过对于大多数的读者来说，可能这篇经验贴会让大家失望，我不打算讲什么经验，虽然并非是我自私，而是之前的博客 &lt;a href=&quot;/blog/advise&quot;&gt;致新生的你&lt;/a&gt; 已经将大部分的内容讲完了，因此这篇博客还是简单回顾下，这段时间以来都发生了什么，希望读者看后也可以从中有所收获。&lt;/p&gt;
&lt;h2&gt;过去的事&lt;/h2&gt;
&lt;p&gt;这实际上是一段相当无趣的故事，毕竟简短来说，我从大一开始就决定了我想要去的实验室，之后两度辗转，在本校实习之后前往 SHAILAB，几乎就已经锁定了 Offer，接下来更多的时间反而是在做一些有趣的事情，而非保研。接下来从起点开始从头说起。&lt;/p&gt;
&lt;p&gt;故事可能要从高中讲起，因为学校秉承的素质教育，早在高中时期我就接触了 RoboMaster 比赛。讲实话，当下回头来看，这个比赛其实相当无聊，使用 DJI 提供的 RoboMaster S1 机器人套组，使用神似 Scratch 的图形化编程来写一些巡线以及识别的程序，然后打比赛。比赛说是机器人比赛，其实贯彻了 RM 的基本理念，也就是对抗游戏，类似 FPS。机器人有压控的装甲板，并且可以发射水弹，从此可以计算血量，之后在一系列的规则加持下，就成为了对抗以及推塔的竞技比赛了。&lt;/p&gt;
&lt;p&gt;这些比赛讲实话没有给我什么技术沉淀，但是让我记住了这个比赛，以至于到了大学，提前学完了高数线代的我百无聊赖，打算找一些机器人比赛参加，第一时间就找到了学校的 RoboMaster 队伍。&lt;/p&gt;
&lt;p&gt;对于本来也不是很愿意和别人主动交流的我来说，这可能是一个影响深远的决定，彼时正值口罩时期，本来也不需要去线下上课，而是在学校上着网课，于是我就在 RM 的实验室里一边写程序，一边听着课。时间在口罩和核酸排队中匆匆过去，在回过神来的时候，确实已经和舍友们都算不上熟络了。&lt;/p&gt;
&lt;p&gt;我可能相对更加喜欢 technical 的内容，这与大多数的同做科研的朋友们可能并不是很相似，究其根本可能还是因为科研本身也不是因为什么崇高的理想才去做的，而是因为相对来说更加有趣一些。RM 的前前任视觉组组长 WJH 学长，虽然事实上在当时以及之后也没有很多的交流，但是还是有效帮助我打破了信息差。一方面，当时意外考得比较好的我了解了绿群，从而开始了保研之路；另一方面，也是在当时开始了解了科研，从之前广泛学习 Machine Learning 以及 Traditional CV 到开始聚焦于科研领域，当时的我确实叹为观止，原来之前的道路背后居然有着这番光景。&lt;/p&gt;
&lt;p&gt;接下来就开始了在 Deep Learning 中广泛的学习，其中最令我印象深刻的还是 Diffusion 的过程，后续还甚至艰难地推导了一遍 Score-based Diffusion。本校的老师很少有做 GenAI 方向，现在想来大概率也是因为算力问题，XJTU 虽然「贵为」 C9，但是事实上大西北前不着村后不着店。我在本校接触的算力，大多数也以混用的四卡 2080Ti 以及 3090 为主。当时我找到了本校的周老师，还是因为看到其 scholar 中包含了一些 inpainting 领域的论文，心想能否做些相关的内容。&lt;/p&gt;
&lt;p&gt;实际上事与愿违，周老师其实更多做的是相关多目标检测的 topic，也是打算在医学领域安排一些注意力，于是让我看一篇半监督医学影像分割方面的论文。当天吩咐完任务之后我就复现了论文，然后尝试自己进行了一些修改，在此之后也是进行了若干次组会，忽然间一个 idea 涌上心头，于是有了这篇论文 &lt;a href=&quot;https://arxiv.org/abs/2409.05122&quot;&gt;PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation&lt;/a&gt;。验证了 Idea 确实 work 之后，接下来就是一系列紧张的对比以及消融实验，最后投稿了 CVPR 2024，不过因为赶稿，确实犯下了不少的粗心失误，不出意外被拒稿了。&lt;/p&gt;
&lt;p&gt;回看第一段科研经历，其实大多数时候的 coding 以及探索都是自己进行，然后主动找老师进行 meeting，来看下接下来是不是有什么需要修改的内容，而没有前辈引路。好在之后再打磨，然后转投 ECCV 2024，还是遇到了心软审稿人，最后 443 初分，然后在 rebuttal 之后拿到了 554，虽然说貌似 meta reviewer 不是很喜欢，但是依然是中稿了。&lt;/p&gt;
&lt;p&gt;在当时 ECCV 2024 的期间，事实上我还在进行周老师课题组中继续进行另一个关于所谓 meta learning 的工作，尝试投稿了 ICLR 2025 之后便不再进行。&lt;/p&gt;
&lt;p&gt;在医学影像中受限于沉没成本而摸爬滚打了一年之后，我最终还是决定跳出自己的舒适区。一方面，半监督医学影像某种程度上依然是一个伪命题，我们的优化限于使用五张 labelled data 超过使用 80 张 labelled data 的效果，这种 limited data 的实验条件显然和更广泛的半监督领域 leverage 大量 unlabelled data 的思想相左；另一方面，一些 hack 的现象也是令人哭笑不得，诸如单纯换了 K-fold 之后模型性能就下降 10 个点，此类现象层出不穷。&lt;/p&gt;
&lt;p&gt;在向朋友们进行了请教之后，对于将来从事的领域，我大概也有了两个选择。在经典二择，选择 MLLM 还是 Embodied AI 中，因为早些年一直都在进行机器人领域的探索，于是选择了如今看来可能泡沫更大的 EAI，义无反顾地前往了大一时候 WJH 学长和我聊起科研的时候就向我提起过的 SHAILAB。当时我的印象十分深刻，实习工资一天三百，还有单人间的多人套间公寓，满足生活需要之后岂不是可以更加自由地探索技术以及科研了？虽然当下来看，这些待遇在若干相似联培中已经不是最具竞争力的，但是好歹也算是白月光。&lt;/p&gt;
&lt;p&gt;带着这篇 ECCV，我申请了 SHAILAB 具身智能中心（当时还是 OpenRobotLab）的实习，在和庞江淼老师套瓷了之后，经过了两轮实际上算是聊天的面试，就进入了如今 mentor 伦哥的麾下，在大二暑假进行了线下实习，并且在之后转到线上。大概从这时候起，其实也就算是锁定了 offer 了。&lt;/p&gt;
&lt;p&gt;其实这是一个很有意思的事情，实际上我没有什么波澜壮阔的面试经历，大概是在恰当的时候，在运气的推波助澜之下，获得了稳定的Offer。要是想从其中进行一些复盘，大概我们可以得到的结论是，积极探索，趁早开始做自己认为对的事，永远对知识保持兴趣。&lt;/p&gt;
&lt;p&gt;当然另一方面，对于一些同学来说，我的去向已经足够好，从中学习我做对了什么，已经可以带来不错的提升，但是对于另外一些人来说，他们想去更远的地方，因此不妨再仔细思考一下，在这一过程中，我有哪里做得还不够好。&lt;/p&gt;
&lt;p&gt;一方面一定在于不要给自己设限。大一上学期结束的时候，也算是因为运气，恰好成绩不错，考到了年级第二，加上第一学期事实上已经学过了传统计算机视觉以及机器学习的大多数内容，并且刷了一遍李沐的 d2l，其实可以直接申请外校很好的课题组。从后面和我接触的低年级同学来看，相似的 bg 已经可以申请清北非常厉害的 ap 组，从而省去在本校科研获取论文当作跳板的环节。在自己手中尚且存有优势的时候，勇敢跳出自己的舒适区，抛下沉默成本，去做自己真正相信更正确的事情，而不是自欺欺人，这实际上很重要。&lt;/p&gt;
&lt;p&gt;另一方面则是在于没有明确的指引。在这里再再再次给自己写的博客，致新生的你，打个广告。当然，无论这篇博客是否对你有帮助，甚至你并不是从事这个领域的读者，依然成立的要点是在于多去向别人请教，了解更加清晰而且正确的路线，省去试错的时间。乔布斯的一次大学演讲中提到自己早期在书法上的积累，在后续的字体设计中起到了作用，然而一个悲伤的事实是，在内卷节奏不断加速的现在，假如你还不能处于一个安稳的科研环境，那么现在绝非是后期而爆发的时候，而是要尽可能的加速。&lt;/p&gt;
&lt;p&gt;在之后的保研故事就显得平平无奇了，到了 AILAB 的夏令营期间，在 hr（还是说叫 ur 之类的岗位）的提醒下报名，然后线下参加了笔试和面试。&lt;/p&gt;
&lt;p&gt;值得称道的是，在 AILAB 的夏令营，也是进行了我的唯一一次和绿群群 u 的面基，当天一起去吃了火锅，但是只能说运气不佳，那天选择的海底捞麻酱一股怪味，或者说上海的麻酱大多如此，实在是糟糕。&lt;/p&gt;
&lt;p&gt;当天晚上向久经沙场的群 u 请教了机试的心得，传授给我了代码随想录的刷题列表，并且着重学习了 dp，搞定了状态转移方程，并且定下了使用 Python 从而放弃卡时长题目的方法，好在最后的题目还算仁慈，也是成功 AK。&lt;/p&gt;
&lt;p&gt;之后参加上交的 CS 的夏令营，也基本上没什么悬念，面试基本上就是在和老师聊天，后续听说只要入营就一定是优营，倒也就不奇怪了。&lt;/p&gt;
&lt;h2&gt;科研的路&lt;/h2&gt;
&lt;p&gt;说完保研，也是简单再说一下科研，尽管说貌似并不是这篇应该讲的正式内容，但是毕竟保研的路还是和一直以来做的科研紧密相关，所以也可以简单聊一聊。&lt;/p&gt;
&lt;p&gt;我其实已经很久没有从科研中获取本质的乐趣了，而是一直在一种内耗中，尝试去寻找更大的价值。&lt;/p&gt;
&lt;p&gt;在长时间的竞争之后，我逐渐发现超越我认识的每一个人，甚至大多数人，都是一件不切实际的事情，遂放弃了 peer pressure，并且丧失了 paper 焦虑，但是失去了这些焦虑的推动，一种更大的徘徊在不远之后的日子的阴影开始笼罩了我：假如说从事我目前所在的领域并且读博是一件必须要做的事情，花费了五年时间之后，我究竟是否真正完成了一些事情，改变了某些领域，或者对某些领域的某些人带来了影响？&lt;/p&gt;
&lt;p&gt;在 SHAILAB 的第一段项目的阶段性产出是一篇叫做 GenManip 的论文，在投稿了 CVPR2025 之后以 5442 遗憾获得了 poster，大概原因是 rebuttal 期间漏掉了一些问题，导致 3 分审稿人降分。&lt;/p&gt;
&lt;p&gt;GenManip 的初衷其实相当明确，在去年的当下，我们看到了一些模块化的方法开始出现，比如说 CoPA 以及 MOKA，并且认为在短时间内由端到端模型实现通用的操作智能还是不现实的，因此推断使用大模型进行推理并且实现的分层式架构以及双系统可能是未来的一个趋势，因此在仿真中制作一个对应的 Benchmark 是比较有价值的。&lt;/p&gt;
&lt;p&gt;我们使用了 Isaac Sim 作为仿真器，并且搭建了一套根据 scene graph 来验证是否成功或者生成使用 layout 的算法，这套算法相当 naive，但是好在有效。在这套内容的基础上，我们搭建了模块化的方法，并且意识到了这套方法可以用来收集数据。&lt;/p&gt;
&lt;p&gt;本身 CVPR 的内容还算收敛，关于如何搭建 Benchmark，批量生成，以及我们的实验结果，但是在此过程中，我开始投身于我认为很有价值的内容，也就是数据生成。&lt;/p&gt;
&lt;p&gt;在短期之内，具身智能领域贫瘠的数据依然是限制其智能程度的一个重大缺口，而从仿真出发对于数据的研究。最次的角度来说，sim2sim 具有更低的成本，可以快速生成大量的数据，研究模型在 in-domain 上的规律，仿真对于变量更加可控，可以为 real2real 的模型迭代提供超前的 insight；而假如可以打破 sim2real 的 gap，这甚至是唯二不需要人工参与的数据飞轮（另一个是 world model 的视频生成）。&lt;/p&gt;
&lt;p&gt;而长期来看，假如任何备受关注的领域，最后的发展方向都是沿着 scaling law 进行探索，对于具身这一包含大量 technical 细节的长链路领域，自己走通数据-模型-实验的闭环，确保自己对于真机以及仿真都具有足够的工程能力以及 insight，这也是必不可少的。&lt;/p&gt;
&lt;p&gt;使用 skill set 的方式实现泛化的数据生成是可能的，至少对于大多数场景并不需要过分的 hardcode，GenManip 在投稿之后的后续开发一直聚焦于此，并且做出了很多的改进以及完善，说起来在内部迭代之后，最近可能也要进行更正式的完全开源。&lt;/p&gt;
&lt;p&gt;在此之前，我们基于 GenManip 已经产出了大规模的 scaling up pick-and-place 数据集 &lt;a href=&quot;https://huggingface.co/datasets/InternRobotics/InternData-M1&quot;&gt;InternData-M1&lt;/a&gt;，Evaluation 部分 contribute 给了 &lt;a href=&quot;https://github.com/InternRobotics/InternManip&quot;&gt;InternManip&lt;/a&gt;，一个 all in one 的 manipulation 训测一体工具链，而 well-designed 的十个双臂上的任务也作为我们 &lt;a href=&quot;https://internrobotics.shlab.org.cn/challenge/2025/&quot;&gt;IROS Workshop&lt;/a&gt; 的 Manipulation 赛道赛题存在。&lt;/p&gt;
&lt;p&gt;GenManip 在内部的迭代中从用户的角度出发，从而设计了一套 Config，可以进行定制化的 Layout 设计到 Object Scaling，可以简单改动就提高数据的 horizon 长度；而假如说想要快速设计数据生成任务，也可以在三分钟内完成，并且在之后会支持 Articulation object 的 general scaling 的支持。这一方面的算法实现事实上早在今年年初笔者就已经有了雏形，而论文 ArticuBot 也用类似的发表了论文，被撞车让我失去了紧迫感，一拖再拖。&lt;/p&gt;
&lt;p&gt;一个有趣的视角是从长期主义出发。过去的一年，我都在进行仿真以及数据生成的工作，直到今年上半年也并没有过多的论文产出，但是一个简单的事实是，只要你做的事情是必要的，那么就会被其他人需要，合作以及更多的内容也就纷至沓来，因此读者不难在这段时间实验室具身智能中心的一些成果中看到我的身影，比如说，欢迎关注我们 &lt;a href=&quot;https://internrobotics.github.io/internvla-m1.github.io/&quot;&gt;InternVLA-M1&lt;/a&gt; 的技术报告。&lt;/p&gt;
&lt;p&gt;事实上，在写下这段文字的时候，我自身依然处于一定的焦虑以及困扰之中，在科研的道路上，我远远称不上一个 senior 的 researcher，而是相当 junior，甚至可能连合格的 researcher 也还差得远。&lt;/p&gt;
&lt;p&gt;仿真的道路不是可以一直进行下去的，至少 general scaling 是这样的。伴随着技术的发展，对于软体及流体的仿真会越来越真实，渲染的质量也会越来越高，sim2real 的 gap 会越来越低，但是 general action scaling 是有极限的。对于 pick-and-place 以及 1-dof articulation 来说，我可以确信他们有一个通用的算法可以适用于绝大多数的物体以及场景，但是当我们将目光放到软体在内的一系列操作中，general 的成本会变得越来越大，并且 corner case 也会越来越多，这意味着时间成本，以及对于精通仿真的高级人才缺口。&lt;/p&gt;
&lt;p&gt;从成本的角度来看，只有深耕于仿真以及数据生成这一领域的人才能写出相对更加
general 的 skill；现如今支持数据采集的大多数 Benchmark，大多数都是通过单任务的 hardcode 实现，这降低了 engineer 的技术栈要求，并且在降低了项目管理难度的同时增大了开发的并行度（可以一人一个 skill）；而在资本入局的当下，大量的数据工厂开始建立，需要的成本变得更低，对于劳动力的培训需求也更小，即将成为不可阻挡的未来。&lt;/p&gt;
&lt;p&gt;似乎是时候从数据和测评中脱身出来，带着目前的理解前往模型领域了，然而比起一篇或许可以中稿的论文，实际上现在往往困扰我的是，这篇论文所选择的路线是否值得我的下一个一年，是否像是上一个一年一样，即使我回头去看，也依然是有必要以及有价值的。&lt;/p&gt;
&lt;p&gt;这个问题也同样留给读者，假如说你和我一样处于 junior 的阶段，并且想要长期从事科研，你现在在做的事情是否值得你的一年时间，而不是一个审稿周期，而假如说放眼整个博士生涯，这件事情又该是什么。&lt;/p&gt;
&lt;h2&gt;尾声&lt;/h2&gt;
&lt;p&gt;行文至此，也是时候给一切画上一个句号了，保研结束对我来说只是一切的开始，对于过去现在或者将来的读者们来说应该也是如此。&lt;/p&gt;
&lt;p&gt;转眼间已经三年过去，经历了多少个日夜之后，下一条路又在哪里？这是一个问题。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/105216073_p0.webp"/><enclosure url="https://picr2.axi404.top/105216073_p0.webp"/></item><item><title>致新生的你</title><link>https://axi404.github.io/blog/advise</link><guid isPermaLink="true">https://axi404.github.io/blog/advise</guid><description>致新生的你</description><pubDate>Mon, 16 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Aside, Steps } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文为计算机科学与人工智能专业主要关于学习的相关内容。更多资讯，如其他学科、学生事务或者生活类内容，请见 &lt;a href=&quot;https://survivexjtu.cc/&quot;&gt;西安交大生存指南&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当你看到这篇博客的时候，笔者的大三生活已经快要结束了，期末考试已经完结，剩下的只有几篇实验报告需要去写，甚至往远去说，可能将来的去向也已经在脑海中形成了一个模糊的轮廓，一切都将尘埃落定，并且踏上一个新的起点。&lt;/p&gt;
&lt;p&gt;而读者们，抱着审视的态度来看这篇文章的 Seniors，以及希望从中获得一些知识的 Juniors，这是我这三年的感受，关于如何在大学中找到自己想要做什么，找到方向以及自己的爱好，假如你恰好喜欢做学问，那么如何做研究，如何看待这一切，在这个奔涌的时代面前如何去适应一切，我会给出一些我的看法。&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;关于向新生做一些介绍和科普的事情，如何去规划自己的大学生活，如何适应大学的节奏，假如你是想要升学，想要「卷」，那么应该按照什么顺序去做什么，这些内容我其实做过很多。&lt;/p&gt;
&lt;p&gt;我在不同的班会上给目前大二以及大三的同学们都进行过一些分享，以及在学组的研讨中，也给大一的同学带来了一些自己当时的想法，当然时过境迁，我对一切又有了一些新的视野和新的看法，所以打算写这篇博客。&lt;/p&gt;
&lt;p&gt;其实讲实话，之前的一些学习心得或者总体的方法论，我在另一个网站中其实有过一些介绍，就是我搭建的&lt;a href=&quot;https://survivexjtu.github.io/&quot;&gt;西安交大生存指南&lt;/a&gt;。这其实更像是一个 wiki 一样的内容，里面大大小小塞满了我各种的私货。&lt;/p&gt;
&lt;p&gt;在某个时刻，我看到其他学校的类似搭建，于是萌生了这个念头，并且凭藉着当初远超现在的执行力，搭建了那个网站。当时我还相信开源社区的力量，事实上这也确实是一片热忱的土壤，然而总归大多数的人是消费者而非生产者，网站获得了数万的浏览量，但是投稿者寥寥无几。&lt;/p&gt;
&lt;p&gt;我逐渐意识到，搭建一个大而全的生存指南网站是不现实的，而我当初的视角也存在一些偏差。实际上对于目前大三年级的同学们来说，甚至可以说 80% 的同学都没有在 Github[^github] 上给一个公开的项目提交过 PR[^pr]，而人们总是对于没有做过的事情充满畏惧，难以参与贡献流程。和计算机非常相关的专业尚且如此，更遑论广大的文科或者理科的专业呢？&lt;/p&gt;
&lt;p&gt;[^github]: GitHub 是一个基于 Git 的代码托管平台，由微软旗下的 GitHub 公司运营。它提供分布式版本控制与源代码管理功能，支持协作开发、问题追踪、文档编写和持续集成等流程。作为全球最大的开源社区，GitHub 汇集了数量众多的开源项目，是开发者进行代码共享、协同开发与技术交流的重要平台。&lt;/p&gt;
&lt;p&gt;[^pr]: PR（Pull Request） 是一种代码协作机制，主要用于在 GitHub 等平台上提交代码修改请求。开发者在分支上完成修改后，可以通过 PR 请求将这些更改合并到主项目中。PR 支持审查、讨论、测试等流程，是开源项目和团队协作中管理代码变更的核心手段之一。&lt;/p&gt;
&lt;p&gt;因而西安交大生存指南中充满了我的一面之辞，而我的视角总是有限的。相对来说我可以这样定性，我在大学的三年时光中更多的是学会了如何「卷」，而恰好科研以及各种技术也是我的兴趣所在，因此对于更广大的对学习弃之如敝履的同学来说，对于如何学习竞赛和科研的长篇大论并不在他们的兴趣范围内，而如何在这些课本与屏幕之外的丰富生活中获得乐趣，显然并不在我的认知范围之内。&lt;/p&gt;
&lt;p&gt;更何况不同专业之间的「卷」度也是有区别的，这取决于专业内部的升学压力以及整个专业的学习节奏。&lt;/p&gt;
&lt;p&gt;对于人工智能来说，大一或者大二进入课题组实在是家常便饭的事情，而即使读者甚至整个年级都排斥这种现象的出现，大环境不等人。一方面做出科研成果的门槛在不断下降，一方面全部的学校中具有前沿视野的同学都在进行着率先科研的军备竞赛，在这样的浪潮下，很难有人可以幸免。&lt;/p&gt;
&lt;p&gt;然而对于不少的文理或者医学的专业来说，学好课内的知识已经是需要做到的全部，当然这也不意味着轻松，看着我女朋友医学考试的大部头资料，我每一次都不禁感慨，她可以坚持下来每一次复习和考试，简直是一个奇迹。&lt;/p&gt;
&lt;p&gt;因此，在一个面向全体学生的百科全书式的内容中大谈进组科研的必要性，显得有些不切实际，且缺乏「泛化性」。&lt;/p&gt;
&lt;p&gt;这使我萌生了写作这篇博客的想法，部分学习方法与节奏复用了我在‘西安交大生存指南’中的总结，更多内容则面向人工智能专业进行特别撰写，不只是如何学，如何科研，也包括你需要认识到自己需要的是什么，因为人生不只有一条路，甚至说，通往成功的人生也不止一条路。&lt;/p&gt;
&lt;h2&gt;濒临奔溃的本科教学&lt;/h2&gt;
&lt;p&gt;你好，欢迎来到西安交通大学。&lt;/p&gt;
&lt;p&gt;假如你已经决定正式入学西安交通大学，你也就抛下了之前十八年来所积累的大多数的成绩，无论你来自于哪个省份或者哪所重点高中，现在一切退回起跑线，我们不得不重新开始。&lt;/p&gt;
&lt;p&gt;事实上，在人的一生之中，除了掌握的知识与能力之外，很少有什么能够始终伴随我们的一生，成绩更是其中贬值的最为迅速的一项。&lt;/p&gt;
&lt;p&gt;{/* 读者在来到这里之前，无论是从哪里听说了本篇博客，可能都不免同时听说过本校一些不好的传闻，而这些事情多半是真实的。笔者无意在这里批驳教务系统的低效，教学的质量参差不齐，又或者是基础设施的缺乏、学生间的压力的扩散，正如 &lt;a href=&quot;https://survivesjtu.gitbook.io/survivesjtumanual&quot;&gt;《上海交通大学生存手册》&lt;/a&gt; 说的那样，「国内绝大部分大学的本科教学，不是濒临崩溃，而是早已崩溃」，更加遗憾的是，我们可能崩溃的更加严重一些。 */}&lt;/p&gt;
&lt;p&gt;{/* 假如说揭开高中时期对大学自由生活的美好滤镜，读者可能难以接受在此之下掩盖的是怎样的一幅图景。同质化严重的课程以及大多以念PPT为主的讲师，莫名其妙的教学要求带来的无意义内耗，绩点为王的氛围以及繁多的考试，学生们被混乱裹挟其中，然后很快的经过一个人生的分岔路口，从此在“卷”与“摆”的道路上一骑绝尘。 */}&lt;/p&gt;
&lt;p&gt;一个可以透露的统计数据是，再比你们大三届的 2022 级中，大一结束的时候学分绩的中位数才仅有88分左右，而在两年之后的2024级，这个数字已经跃升至了90分以上。考试难度的波动以及教学方案的调整带来的影响确实不可忽略，但是学生们在笔头下的功夫确实肉眼可见的增加了。&lt;/p&gt;
&lt;p&gt;在繁重考试以及绩点排名压力的背后，是AI浪潮的席卷。伴随着 ChatGPT[^chatgpt] 的横空出世，全世界对于人工智能的期冀，连带着学界加快研究节奏的热忱一同高涨。在比你们大五届的2020级，本科生第一作者身份发表顶级会议的最早记录是大三下学期，而且两年之后的 2022 级，这个记录被笔者刷新到了大二下学期。放眼全国，在大一发表顶会甚至 spotlight[^spotlight] 或者 oral[^oral]，也很难说是一件稀罕事。&lt;/p&gt;
&lt;p&gt;[^chatgpt]: ChatGPT 是由 OpenAI 开发的大型语言模型，基于 GPT（Generative Pre-trained Transformer）架构。它能够理解自然语言并生成连贯、有逻辑的文本回复，广泛应用于问答、写作辅助、编程帮助等领域。ChatGPT 是目前最知名的人工智能对话系统之一，代表了当前自然语言处理技术的前沿水平。&lt;/p&gt;
&lt;p&gt;[^spotlight]: Spotlight 是部分顶级学术会议（如 NeurIPS、ICLR）采用的一种论文展示形式，介于口头报告（Oral）和墙报展示（Poster）之间。被选为 Spotlight 的论文通常具有较高的评价，会获得在主会场短时段内向全体与会者展示的机会。这种形式既提升了论文的可见度，也体现了其在同行评审中的认可度。&lt;/p&gt;
&lt;p&gt;[^oral]: Oral 是顶级学术会议中的最高级别论文展示形式，表示该论文被评为最具影响力或最具创新性的少数成果之一。Oral 通常安排在大会主会场，由作者进行约 10 至 15 分钟的口头报告，并接受现场提问。这类论文往往代表了当年研究的前沿方向或关键突破。&lt;/p&gt;
&lt;p&gt;在本科培养方案的规划下，假如读者继续按部就班的一步一步学习，直到大三上学期才会开始介绍计算机视觉[^cv]以及自然语言处理[^nlp]，而课程的内容涉及的技术多半来自上个世纪或者五六年前，而对于当下最时髦且流行的技术则鲜有提及。事实上在国内即使是最顶尖的大学诸如清华北大，也很难规划出让学生可以在大一发表顶会的培养方案。&lt;/p&gt;
&lt;p&gt;[^cv]: 计算机视觉（Computer Vision）是人工智能的一个重要分支，旨在使计算机具备“看”与“理解”图像和视频的能力。其研究内容包括图像识别、目标检测、三维重建、视频分析等，广泛应用于自动驾驶、人脸识别、医疗影像分析、工业检测等领域。计算机视觉结合了图像处理、模式识别、机器学习等多种技术，是推动智能系统感知世界的核心方向之一。&lt;/p&gt;
&lt;p&gt;[^nlp]: 自然语言处理（Natural Language Processing，NLP）是人工智能领域的一个核心方向，研究如何使计算机理解、生成和处理人类语言。NLP 涉及语言建模、语义理解、文本生成、机器翻译、对话系统等任务，广泛应用于搜索引擎、智能客服、文本分析、语言模型等场景。随着大规模预训练模型的发展，NLP 在语言理解与生成方面取得了显著进展。&lt;/p&gt;
&lt;p&gt;因此对于那些希望在学术体系下做出一番成就的读者来说，如何达到这一目标，那个答案已经昭然若揭，脱离这套已经生锈的、难以跟上科研节奏的本科教学体系。&lt;/p&gt;
&lt;h2&gt;学问的学问，生活的学问&lt;/h2&gt;
&lt;p&gt;对于那些早已经对科研跃跃欲试、摩拳擦掌的读者来说，后面的内容定然具有一番参考价值；那些还憧憬着大学美好生活以及多元发展的读者来说，则不免微微皱起了眉头。&lt;/p&gt;
&lt;p&gt;笔者对于科研的执着以及描写无疑是对焦虑的渲染，然而也别无他法，笔者的大学三年时光主要就集中在这一系列的探索之中，也难免将其作为重点，而但凡皆为同好的读者看后，必然也能感同身受的给予认同。不过在开始对于如何做学问的学问长篇大论之前，还是有必要先讲讲生活的学问。&lt;/p&gt;
&lt;p&gt;一组简单的数据可以作为参考，正常一家三口买菜做饭，一天的开销大约是五十元；一线城市一居室公寓一个月的租金大约是三千元；本专业的同学在本科可以获得的实习工资大约在每天两百元或以上；C9 应届毕业生的工资在一万元左右。&lt;/p&gt;
&lt;p&gt;善于观察与思考的读者不难获得一个答案，即在本专业中获得一个像样的学位，养家糊口，并且培养自己的兴趣爱好并非一件难事，而投身科研之中，消耗大把的时间进行研究，可能会让你放弃几乎全部的课余休息时间，然而结果依然是抽奖以及千军万马过独木桥，换来的薪资确实更高，但这是否是你最后想要的生活依然是一个问号。&lt;/p&gt;
&lt;p&gt;每天按部就班接受培养方案的教育，认真听讲做笔记，完成作业，在考试中取得好成绩，这是一种活法；参加机器人竞赛，或者其他的团体活动，和队员一起起早贪黑磨练技术打磨能力，在激烈的角逐中取得优胜，这是一种活法；投身科研之中，加入国内外顶尖课题组，发表论文，交流学术，这是一种活法。然而对于圆满与美好的定义总非是狭隘的，和好朋友们加入二次元社团，每天在欢声笑语之中度过，在考试前三三两两聚在一起研究如何低分飘过期末考试，然后畅想假期一同的出游，这也是一种活法；或者只是一个人默默的学习，学到够用为止，找一个自己的爱好，又或者干脆就漫无目的，在校园里投下阴影的梧桐树下行走，偶然间走到思源活动中心的南门，那里没有枝干的遮挡，那片湛蓝的天空是那样的动人，霎时间豁然开朗，这也是一种活法。&lt;/p&gt;
&lt;p&gt;假如我们为了薪资或者前途发展功利去看，并且选择一种期望值最大的途径，那么几个月后将有七八十名都读过本篇博客的同学一同踏入这个学校，手握相同的武功秘籍，陷入水深火热。&lt;/p&gt;
&lt;p&gt;不过实际上反直觉的是，利益最大化绝非大多数人对于成功的定义。设想科研这条路走到最后是怎样一副情景，你或许成为高校的教授、企业的 CTO[^CTO]，领导数十甚至上百人的科研团队，每天早上起来一杯咖啡，然后阅读论文、开会、讨论，为你的组织拉取资源，思考将来前沿的方向，然后在忙碌奔波中度过一天，你能否想象你的面孔出现在那个模糊的身影上，那是否是你想要的生活，而且假如不是，如何达到那个你的梦想，这是一个留给每个人思考的问题。&lt;/p&gt;
&lt;p&gt;[^CTO]: CTO（Chief Technology Officer），即首席技术官，是企业中负责技术战略与研发管理的高级管理职位。CTO 通常负责公司的技术方向制定、关键技术选型、研发团队建设与技术体系架构等工作，在技术型公司中常与产品、业务密切协作。其角色介于技术专家与战略决策者之间，是推动技术落地与创新的重要核心人物。&lt;/p&gt;
&lt;h2&gt;Learn to learn&lt;/h2&gt;
&lt;p&gt;在传统的机器学习话题中，有一个概念叫做元学习，meta learning[^metalearning]，有的时候也被称为 learn to learn，学会如何学习。虽然事实上这一领域中的绝大多数内容都是通过梯度传播来改变那些本来固定的超参数，在当下的应用场景也着实有限，然而这个概念本身却一直被学界所重视，并且在此之后的如持续学习[^continuallearning]等领域也都继承了这一思想。&lt;/p&gt;
&lt;p&gt;[^metalearning]: Meta Learning（元学习）是一种“学习如何学习”的机器学习方法，旨在使模型能够在面对新任务时以极少的数据迅速适应。与传统机器学习方法依赖大量训练样本不同，元学习通过在多个任务上进行训练，学习通用的学习策略或初始化，从而提高模型在低资源场景下的泛化能力。&lt;/p&gt;
&lt;p&gt;[^continuallearning]: 持续学习（Continual Learning 或 Lifelong Learning）是指模型在不断接收新任务或新数据的过程中，能够保持对已有知识的记忆，并持续学习新知识的能力。这一研究方向旨在解决传统模型在训练新任务时容易遗忘旧任务（即“灾难性遗忘”）的问题。&lt;/p&gt;
&lt;p&gt;学习如何去学习这一内容在人们心中是如此被看重，大概还是由于在人的成长与发展中学习是最不可或缺的一环。在本科的教育中学习那些培养方案中的课程，在竞赛中你学习那些课程之外的技术，在科研中学习那些前沿的论文和思想，而哪怕是在生活中，如何交往如何沟通也是关键的一环。&lt;/p&gt;
&lt;p&gt;在这个章节中笔者会介绍几个关键的学习心法，一些是在课程学习中适用的，一些则在更广泛的学习场景中都具有价值，希望可以给读者带来一定的启发和帮助。&lt;/p&gt;
&lt;h3&gt;信息差&lt;/h3&gt;
&lt;p&gt;我在大一结束的时候，有不少的新生前来向我请教，包括学习社团生活科研竞赛的种种经验，我当时便放出过这个论断，大学最大的差距不是天赋，不是努力，而是信息差。&lt;/p&gt;
&lt;p&gt;不同于高中生成长路线的单一，及拜诸多辅导机构所赐，网络上或现实中均充斥着大量的有用信息，大学的信息依然繁杂，但是质量却参差不齐。&lt;/p&gt;
&lt;p&gt;笔者在一个没有很多人指导的环境下独自摸索，踩过很多坑，也做过很多错事，回头来看，很多的努力和付出并不是必要的，而在一个追求快节奏的时代里，如何从这些庞杂的信息中识别出路径，并且在诸多路径中挑选到最短的那一条，毫无疑问是一个有价值且意义重大的学问。&lt;/p&gt;
&lt;p&gt;在后续具体的学习方法中，将会有笔者这些年来积累的信息的汇总，而在这里让我们着眼于这抽象的思想，简单思考下如何减小信息差，并且获得到更多的信息。&lt;/p&gt;
&lt;p&gt;对于绝大多数的减少信息的方法，我们可以粗略划分为两种类型，一种是提问，一种是搜索。&lt;/p&gt;
&lt;h4&gt;提问的艺术&lt;/h4&gt;
&lt;p&gt;当你明确了你的疑问，明确了你想要获得怎样的信息，提问是最快的解决问题的方法。提问的关键有三点，如何形成正确的问题，如何找到可以回答问题的人，以及如何整理答案，并且从中获得你需要的内容。&lt;/p&gt;
&lt;h5&gt;正确的问题&lt;/h5&gt;
&lt;p&gt;排除少数的有偿咨询之外，绝大多数的提问是无偿的，而这往往意味着提问者没有付出任何代价就获得了自己想要的内容而被提问者则没有收获任何价值，而付出了时间和自己的知识。在这种不平衡之下，意味着提问者需要向被提问者表达最基本的尊重。&lt;a href=&quot;https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md&quot;&gt;提问的艺术&lt;/a&gt;是起初源于黑客社区的册子，里面讲述了新手如何向老鸟获得答案的方法，而这其中的内容极具参考价值，笔者也强烈推荐每一个读者去将其通篇阅读，这会为你将来的提问之旅带来更好的结果。&lt;/p&gt;
&lt;p&gt;简单来说，提问的艺术中要求提问者需要向被提问者准确描述自己的状况，自己的需求以及目前已经尝试的方法，并且在获得自己需要的答案或者获得了帮助，即使并不是自己需要的内容后都表达感谢，这是最基本的尊重的体现。&lt;/p&gt;
&lt;p&gt;对于那些乐于回答别人问题的人来说，他们很喜欢一个好问题，并且愿意为之付出大量的时间，因为这不仅可以帮助到提问者，也可以帮助被提问者整理自己的思绪，对这个问题产生更深刻的认知。然而相对的，假如读者希望被提问者可以花费十分钟回答自己的问题，毫无疑问自己也有必要花费十分钟来准备自己的问题，这才是公平的体现。&lt;/p&gt;
&lt;p&gt;「我具有不错的课内成绩，现在正在抉择是参加机器人竞赛还是直接参与科研，我在网络上搜索了一些科研的知识，感觉具有比较高的上手门槛，而且领域繁多，我不知道如何选择，所以想要询问你的意见」，并且在提问结束后说上一句谢谢，听上去总比「怎么变成像你一样的人」，并且在获得回复之后已读不回听上去要好的多。&lt;/p&gt;
&lt;h5&gt;正确的人&lt;/h5&gt;
&lt;p&gt;相比起来形成一个正确的问题，这种只需要勤加练习，并且付出时间的事情来说，找到合适的人总是一件更难的挑战。&lt;/p&gt;
&lt;p&gt;对于人的信息的积累是一件需要持之以恒的事情，并且鲜有捷径可走。了解每一个人擅长做什么，对什么的理解深刻，并且在哪些方面可以帮到自己。在日常的交流中读者需要细心注意这些内容，并且记在心中。&lt;/p&gt;
&lt;p&gt;一个好的开始是在阅读读完这篇博客之后，对于自己不懂的地方向笔者提问，不过在这里依然有一个小小挑战，毕竟有的时候你不仅需要知道你需要向谁提问，还需要知道你怎样才能和他取得联系。&lt;/p&gt;
&lt;h4&gt;搜索与整合&lt;/h4&gt;
&lt;p&gt;除了积极的提问，从搜索引擎以及各路论坛讨论中获得信息也是一项十分关键的技能，而甚至对于一些技术问题来说，也只有在那些深奥的技术文档和技术论坛中才能获得答案。这是一项十分后期的技能，不得通过这种方式获得信息的时候，意味着你在这个领域已经打磨良久，并且有必要具备更 Senior 的能力了。而在此之前一些视频平台或者诸如知乎的社区，也可以给你理想的答案。&lt;/p&gt;
&lt;p&gt;在三年以前，这个回复显然是唯一的正解，但是时过境迁，伴随大语言模型的兴起，面对搜索这个困难的开放性问题，显然有了不一样的答案，那提问与搜索集合成一体的究极答案，向大模型提问。&lt;/p&gt;
&lt;p&gt;得益于大语言模型经过的大量预训练&lt;a href=&quot;%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88Pre-training%EF%BC%89%E6%98%AF%E6%8C%87%E5%9C%A8%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%88%9D%E6%AD%A5%E8%AE%AD%E7%BB%83%EF%BC%8C%E4%BB%A5%E5%AD%A6%E4%B9%A0%E9%80%9A%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E6%88%96%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E5%9C%A8%E7%89%B9%E5%AE%9A%E4%BB%BB%E5%8A%A1%E4%B8%8A%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%EF%BC%88Fine-tuning%EF%BC%89%E3%80%82%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E6%9C%80%E6%97%A9%E5%B9%BF%E6%B3%9B%E5%BA%94%E7%94%A8%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%92%8C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%EF%BC%8C%E6%98%BE%E8%91%97%E6%8F%90%E9%AB%98%E4%BA%86%E6%A8%A1%E5%9E%8B%E5%9C%A8%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BD%BF%E5%BE%97%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%A4%9F%E5%9C%A8%E9%9D%A2%E5%AF%B9%E6%95%B0%E6%8D%AE%E7%A8%80%E7%BC%BA%E4%BB%BB%E5%8A%A1%E6%97%B6%E4%BB%8D%E5%85%B7%E6%9C%89%E8%89%AF%E5%A5%BD%E7%9A%84%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%EF%BC%8C%E6%98%AF%E7%8E%B0%E4%BB%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%9E%84%E5%BB%BA%E5%BC%BA%E5%A4%A7%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E7%AD%96%E7%95%A5%E4%B9%8B%E4%B8%80%E3%80%82&quot;&gt;^pretrain&lt;/a&gt;，海量的互联网信息被它整合在自己的权重中，这意味着你可以通过日常的对话，就像和一个好朋友聊天一样，获得绝大多数你需要的信息。&lt;/p&gt;
&lt;p&gt;有的时候你需要学会辨别信息获取的边界，那在互联网上广泛存在但是可能稍显晦涩的内容使大语言模型输出的强项，而假如这些信息更具实效性或者出现频率较低，则可能传统的提问与搜索才是更好的答案。比如说如何学习Python这门语言，向一位及时已经深谙到多年的学长进行数小时的提问，倒不如和大模型在对话中持续学习来的更有效率；而西安交通大学人工智能专业历届的保研比例和分数线，无论你用何种口吻或者要求向模型提问，他都很难给出正确的答案。&lt;/p&gt;
&lt;p&gt;同时不容忽视的是大模型的幻觉问题，也就是模型输出的内容往往不一定是完全真实的，对于那些可以通过实践验证正确性的内容，最好的做法应当是立刻将之付诸实践，并且检查正误；而对于那些包含事实性内容的答案来说，将模型的输出粘贴到搜索引擎并进行二次确认，毫无疑问是更好的做法。&lt;/p&gt;
&lt;p&gt;读者至少需要学会如何向 &lt;a href=&quot;https://chat.deepseek.com/&quot;&gt;DeepSeek&lt;/a&gt; 或者 &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;Qwen&lt;/a&gt; 这样的国产模型提问，进一步了解如何访问并获得国际上最前沿的模型服务，如 &lt;a href=&quot;https://chatgpt.com/&quot;&gt;ChatGPT&lt;/a&gt; 或者 &lt;a href=&quot;https://gemini.google.com/&quot;&gt;Gemini&lt;/a&gt;，则是更加进阶的做法。当然，这些内容会在后续的详细阐述中有更具体的体现。&lt;/p&gt;
&lt;h3&gt;持续学习&lt;/h3&gt;
&lt;p&gt;假如读者喜欢学习新技术，那么描绘无数种广袤的前景是不难的事情。读者可以学习传统的前端后端框架[^frontbackend]，搭建非常炫酷的网页（比如本博客就是在模板的基础上加入了大量自己的修改形成的）；也可以深入 Linux 系统[^linux]，折腾 ArchLinux[^archlinux]，搭建美观且实用的开发与生产环境；也可以苦练算法，了解那些前人创造的精妙思想，并且在算法竞赛上一较高低；或者学习大模型的一系列技术栈，训练自己的模型，并且在科研上有所建树。同样的路还有无数条，学习计算机图形学[^graphic]了解渲染的机理；学习 CPP[^cpp] 或者 C#[^csharp] 之后用 Unity[^unity] 或者 Unreal[^unreal] 搭建自己的独立游戏，甚至参加 GameJam[^gamejam]，和其他开发者们即兴创作；甚至是研究 TCS[^tcs] 或者计算机体系结构，有趣的知识是无穷的，等待着你的探索。&lt;/p&gt;
&lt;p&gt;[^frontbackend]: 前端框架是用于构建网页界面和交互效果的工具，常见的有 React、Vue 等，负责用户在浏览器中看到和操作的部分；后端框架则用于处理服务器端的逻辑，如数据存储、接口响应等，常见的有 Django、Express 等，负责支撑整个应用的功能实现。&lt;/p&gt;
&lt;p&gt;[^linux]: Linux 系统是一种基于 Unix 设计思想开发的自由开源操作系统，具有高稳定性、高安全性和良好的可定制性，广泛应用于服务器、开发环境、嵌入式设备等领域，是现代互联网和云计算基础设施的重要组成部分。&lt;/p&gt;
&lt;p&gt;[^archlinux]: Arch Linux 是一个轻量、灵活、以滚动更新为特征的 Linux 发行版，强调极简主义、用户控制与 KISS（Keep It Simple, Stupid）理念。它适合有一定技术基础的用户，提供几乎纯净的系统环境，用户通过手动配置构建出完全符合自身需求的系统。&lt;/p&gt;
&lt;p&gt;[^graphic]: 计算机图形学（Computer Graphics）是研究如何使用计算机生成、表示和处理图像与几何信息的学科，涵盖图形建模、渲染、动画、图像处理等内容，广泛应用于游戏、电影特效、虚拟现实、CAD、科学可视化等领域。&lt;/p&gt;
&lt;p&gt;[^gamejam]: GameJam 是一种限时游戏开发活动，参与者需在规定时间内（通常为 24 至 72 小时）围绕指定主题独立或协作完成一款可玩的游戏，旨在激发创意、锻炼开发能力，并促进游戏开发者之间的交流与合作。&lt;/p&gt;
&lt;p&gt;[^cpp]: C++（CPP） 是一种通用的编程语言，兼具过程式和面向对象编程特性，以性能高、可操作性强著称，广泛应用于系统开发、游戏引擎、图形渲染、嵌入式系统等对效率要求极高的领域。&lt;/p&gt;
&lt;p&gt;[^csharp]: C# 是由微软开发的现代化、面向对象的编程语言，运行在 .NET 平台上，语法风格与 C++ 和 Java 接近，广泛用于桌面应用、Web 开发、企业级软件以及 Unity 引擎中的游戏开发。&lt;/p&gt;
&lt;p&gt;[^unity]: Unity 是一款由 Unity Technologies 开发的跨平台游戏引擎，以易用性强、上手快和良好的社区支持著称，广泛应用于 2D/3D 游戏开发、虚拟现实、交互式应用和教育可视化等领域，支持使用 C# 进行脚本编程。&lt;/p&gt;
&lt;p&gt;[^unreal]: Unreal（全称 Unreal Engine）是由 Epic Games 开发的高性能游戏引擎，以强大的图形渲染能力、蓝图可视化编程系统和跨平台支持著称，广泛应用于游戏开发、影视制作、虚拟现实和建筑可视化等领域。&lt;/p&gt;
&lt;p&gt;[^tcs]: TCS（理论计算机科学） 是研究计算本质及其基本原理的计算机科学分支，涵盖计算模型、复杂性理论、算法设计与分析、可计算性理论、自动机与语言理论等内容，是整个计算机科学的数学基础和理论核心。&lt;/p&gt;
&lt;p&gt;经过了笔者的一系列报菜名中，那些热爱学习的读者心中不难涌现出一团烈火，想要用自己有限的大学时光，充分了解这些技术，把自己发展成一个更强大的人。但是在经历了无数学弟学妹的请教以及后续的观察之后，笔者不得不得出一个结论，在绝大多数时候，兴趣所带来的学习动力都是不能持久的，然而只有持久的学习才能让你真正掌握某个技能。而这一切的关键在于正反馈。&lt;/p&gt;
&lt;h4&gt;记笔记是一种正反馈&lt;/h4&gt;
&lt;p&gt;记笔记是笔者推荐的一种极佳的学习方法。&lt;/p&gt;
&lt;p&gt;笔者不了解读者在高中期间对于记笔记的看法，例如是否会在复习的时候回顾这些笔记，或者认为记笔记是否是一种累赘，但是在高中的生活中，笔者从来没有记过一行笔记。&lt;/p&gt;
&lt;p&gt;尽管如此，笔者依然推荐在大学期间使用记笔记作为最核心的学习方法。&lt;/p&gt;
&lt;p&gt;一种广泛的论调是，大学的学习以自学为主，这一点是不可置否的，而在自学的过程中，能否让自己获得持续学习的动力，这成为了自学过程中最关键的一环。&lt;/p&gt;
&lt;p&gt;一种常见的现象是，每每开学，无论之前在高中，还是如今的大学，都有很多同学向笔者咨询学习相关的事宜，仿佛准备大干一场，在学业中一展宏图。而在开学两个月之后，一般他们的朋友圈或空间便被二次元或者现充生活占据了，而全然不见当初的拼搏模样。有时候学习的难点并不在于如何高效的学习，反而在于更为基础的一点：如何坚持学下去。&lt;/p&gt;
&lt;p&gt;假如并非十分自律的人，在学习的过程中，毫无疑问，正反馈是十分重要的。适当的正反馈不但可以让读者有学习的动力，也可以让读者保持学习的节奏。&lt;/p&gt;
&lt;p&gt;令人遗憾的是，在大学中，正反馈往往不是如此的常见。在不经常存在考试，甚至课程质量良莠不齐的大学当下，在学习中获得的反馈是十分难得的。更加经常的情况是，你有数不尽的内容可以学习（学完课内可以拓展课外知识），但是每每学习结束之后你并不知道自己的学习是否为自己带来了改变，这与高中阶段学习少量的内容，但是在重复性的刷题以验证自己的学习成果不同，而一味地向前冲有时会带来迷茫，从而让读者失去兴趣。&lt;/p&gt;
&lt;p&gt;关于笔记的用处，不排除一些人通过记笔记起到加深记忆，以及获得一手的复习资料的作用，但是在此之外，记笔记也是一种记录，如日记一般，可以告诉读者：“我今天学习了很多内容”。&lt;/p&gt;
&lt;p&gt;故也便不难阐述记笔记带来的正反馈了。事实上，正反馈的缺失往往来自于难以对自己的学习成果带来一种定量的总结，这是显而易见的，因为并不存在大量的考试，而学习的内容也远远多于考试的内容。但是通过频繁的记笔记，笔记的字数以及内容的积累可以带来一种最为直观的反馈，尤其是当读者自己的笔记从千字到万字再到数十万字的时候，便可以很明显的感知到，如今的自己已经今非昔比。&lt;/p&gt;
&lt;p&gt;在后文中，笔者会介绍自己使用的工具流。也就是 &lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsidian&lt;/a&gt;，以及使用 Markdown[^markdown] 进行记录笔记。使用Markdown 进行笔记的记录是一件非常常见的做法，这一属于程序员的记录语言提供了极佳的记录效率以及内容的规整性，而在如今的编程世界中，基本上全部的文档，包括本篇博客的背后都是 Markdown 正在书写。&lt;/p&gt;
&lt;p&gt;[^markdown]: Markdown 是一种轻量级标记语言，用于使用纯文本格式快速编写格式化内容，如标题、列表、链接、图片和代码块。它语法简洁，易于阅读和编写，广泛应用于文档写作、博客撰写、代码说明和 README 文件等场景。&lt;/p&gt;
&lt;h4&gt;一条精心设计的路线&lt;/h4&gt;
&lt;p&gt;尽管我们有了记笔记这一带来正反馈的利器，但是在学习的过程中依然可能存在不少的挫折，使得读者难以集中精神，或者最后也会半途而废。怀着满腔热血冲进一个自己喜欢的领域，即使自己一无所知，但是闷头苦学，这种想法毫无疑问是不可取的。&lt;/p&gt;
&lt;p&gt;事实上每一个领域，伴随着它涉及的知识与你目前掌握的知识面的交集越来越少，越来越多的内容将涉及不断地自我递归式的学习，即对于自己不懂的内容进行查询，然后再将查询内容中不懂的内容进行查询，以此类推，直到了解整体的知识脉络。这种频繁的递归式学习只会让你的学习负担越来越大，而难以对于整体的知识脉络有一个清晰的认识，我们推荐在一些局部使用递归式学习，这有助于你在小范围内规避多余的学习内容，而聚焦在必要的内容上，但是当将视野放大到整个领域之后，我们需要的是如庖丁解牛般的游刃有余，将领域的大块知识进行分解，了解自己需要掌握的知识边界，再自底向上进行学习。&lt;/p&gt;
&lt;p&gt;没有了解全貌，就难以进行笔记的记录，至少大多数人都难以提起兴致记录笔记。人们往往因为内心清楚自己对于这部分知识还没有完全掌握，便断定自己梳理出来的记录也一定毫无章法可言，从而拒绝进行笔记的记录，而笔记的缺失有进一步带来了正反馈的缺失，从而影响了学习的节奏。&lt;/p&gt;
&lt;p&gt;在学习之前，读者有必要详细了解自己需要学习的领域。就像之前所说到的那样，你需要通过提问或者搜索来让自己先了解某个领域的学习内容，以及如何阶段式的完成一个一个目标。尽管其中的很多中间状态并不是我们目的所在，但是阶梯式的划分有助于成就感的增加，而且在前人基础上总结出来的阶梯会使得学习更具条理性，并进一步降低学习的门槛。在后文中会详细介绍人工智能这类专业如何进行详细的阶梯式分解，并且假如上且有余力，给出一些其他的领域的介绍。&lt;/p&gt;
&lt;h4&gt;听课是一种糟糕的学习方法&lt;/h4&gt;
&lt;p&gt;行文至此，读者们已经开始逐渐接近笔者的思想了，我们在追求的是一种高效的学习方法，并且可以长久进行学习，而在这一语境下，听课是一种糟糕的学习方法，也是一种平庸的学习方法。&lt;/p&gt;
&lt;p&gt;这并非一种暴论，笔者并不是高傲地站在一种高位者的视角上，将那些只会听课的学生们评价为庸才与平庸之辈，事实上他们之中不少人都天赋异禀，但是学习的方法或许不是最本质的。我们需要意识到，学习这一过程本质上就是接受知识、提出疑问、获得答案的过程，并且在这个过程中不断迭代自己的认知，从而获得更好的理解。提出疑问以及获得答案的过程往往是通过自己的悟性以及之前学习的知识，包括提问以及请教他人，而接受知识的速度往往很大程度上取决于你受到的教育的知识密度。&lt;/p&gt;
&lt;p&gt;回想一下课堂的场景，即使是最为经验丰富的老师，也不可避免地要解决一个难题，他需要将文字甚至符号化的知识进行口述，并且在课堂中再现接受知识、提出疑问、获得答案的循环，这一循环需要关照课堂里面的每一个学生，自然也包括某一位悟性比较差，并且距离前排比较远的同学。优秀的教师在教授知识的时候总是娓娓道来深入浅出，并且受到同学们的广泛好评，但是读者需要注意的是，我们在追求的本质上只有一件事情，那便是尽可能高效的获取知识，而上述的过程显然不是最快的。&lt;/p&gt;
&lt;p&gt;一个有趣的事实是，假如说读者在期末周进行过考试前的突击，对于那些广泛存在在各个专业之间的通识课程，网络上往往充斥着无数的速通课，那些仅需不到三四个小时的视频便可以概括你横跨整个学期，高达四个学分的大课，并且假如选择恰当，几乎可以 cover 老师课堂上讲解的 99% 的内容。这个现象从侧面反映了常规课堂中知识密度之低，其中充斥着提问、重复的回顾、对于大量练习题的讲解甚至还有课堂小测，而我们的需求其实更加朴素，掌握知识，并且再次搜索出来的时候知道如何去用，毕竟实践是一场开卷而非闭卷考试。&lt;/p&gt;
&lt;p&gt;一种笔者推荐的最高效的学习方法当然是看书，书籍中包含最凝练的符号化表达，而同时，一个令人惊喜的事实是，虽然说那些符号乍一看令人生畏，但是假如沉下心来仔细阅读，事实上一切内容也并没有那么难懂。&lt;/p&gt;
&lt;h2&gt;人工智能的学习路线&lt;/h2&gt;
&lt;p&gt;看完了上述啰嗦的长篇大论，那些对于看似读者早已经了解的技巧和知识的反复强调，我们终于来到了这些充满干货的环节，尽管实际上在我看来，上述的心法才是重点。那些可能你认为是废话的内容，正是因为我在反复强调，所以你有必要在学习过程中反复回看，注意自己是否真的做到了，而无论如何，在一段时间过后，再度回看你的学习过程，也很难不由衷的感慨这些内容在学习过程中的重要性。&lt;/p&gt;
&lt;p&gt;说回学习路线，对笔者当前的阶段来说，大概是对于人工智能的某个领域（具身智能）十分了解，并且在其他领域也进行过非常广泛的阅读，在 insight 上面通过和别人的讨论也可以具有一定的深度。&lt;/p&gt;
&lt;p&gt;尽管笔者对于自己的一些思想具有一些信心，但是可以肯定的是我的分享也是具有局限性的，毕竟假如说作为一个完美的指南，那么笔者想必也已经是在学术圈叱咤风云的大佬，然而事实是笔者虽然相对领先，但依然有所不足。其中一个明显的不足在于笔者目前的绝大多数工作都是所谓的独狼型科研，也就是在论文的整个过程中，笔者承担了绝大多数的工作，而并没有和合作者均摊任务，并且达成更良性循环的合作关系。&lt;/p&gt;
&lt;p&gt;在科研中，假如硬要和考试应试对比，不可忽视的一个重点便是在于其允许的团队以及资源不平衡的现象，也就是即使你的个人水平很强，但你一个人同时没有资源而尽力去做，而另外一个团队只有七八人甚至十几人同时工作，即使他们人均水平远不如你，但奈何人数众多，且资源广集，最后项目的成果自然远好于你，那么他的论文可以中稿而你的不能，也自然无处说理。毕竟效果以及方法是一切的关键，但是至于你是怎么得出这个方法，或者你在背后付出多少努力，这些东西还是留到成功之后在 talk 上漫谈时不经意间讲出吧。&lt;/p&gt;
&lt;p&gt;因而如何通过社交结识更多具有能力的合作伙伴，通过合作，充分实现你和他人的创意，做出更多有影响力的工作，并且利用共同的计算资源以及其他资源，这是一门不得不学的学问，相应的经验，欢迎大家在评论区补充。&lt;/p&gt;
&lt;p&gt;回归正题，让我们正式开始学习路线的分享，我们假设你是一名之前仅受到高中教育，没有接触过计算机相关内容的大一新生，以下内容你只需要按照顺序去学便可以从中获得收获，而对于那些更有经验的人来说，以下内容也可以作为一个 checklist，看看有什么内容是你还没有涉及的，假如有什么更好的选择和建议，欢迎评论补充。&lt;/p&gt;
&lt;p&gt;值得一提的是，由于是学习路线而非大而全的指南，因此比如说在练习某些课程的时候，我会首先给出我推荐的内容正常情况下，你需要把推荐内容学完即可，而不需要涉及任何拓展知识。它们的知识点大致都是同质化的，假如想要展开研究或许可以，不过在这个节奏下，先掌握必须的要点，并写在实践中查漏补缺或许才是真理。&lt;/p&gt;
&lt;p&gt;当然在此之前，&lt;strong&gt;你需要学会如何访问谷歌&lt;/strong&gt;，假如你不知道方法，问问身边的朋友，这是一切的起点。&lt;/p&gt;
&lt;h3&gt;一切开始之前&lt;/h3&gt;
&lt;p&gt;哦，我的朋友，等下，在一切开始之前，让我们明确我们的目标，我们是不是真的喜欢科研。&lt;/p&gt;
&lt;p&gt;假如你不喜欢科研，那么好的，这并不妨碍你继续看下去。从绝对功利的角度来看，科研依然是当下提升个人竞争力最具性价比的方式，可以在短时间内让你学习更加超前的知识，从而以更好的视角回看课内以及其他的内容，起到事半功倍的效果。&lt;/p&gt;
&lt;p&gt;当然，假如你喜欢科研，也不妨看看谢赛宁老师在 CVPR 上给出的 talk，&lt;a href=&quot;https://x.com/sainingxie/status/1933008515854901735&quot;&gt;Research as an Infinite Game&lt;/a&gt;，用心体会，给自己一些更加长远的动力。同时，一些心法也同样重要，Omar Khattab 在 Github 的 Blog，&lt;a href=&quot;https://github.com/okhat/blog/blob/main/2024.09.impact.md&quot;&gt;On Impactful AI Research
&lt;/a&gt; 讲述了如何做出有影响力的研究，言之有理，值得细品。&lt;/p&gt;
&lt;p&gt;那么接下来，不要脱离你的主线，沿着这些前人的道路走下去，然后让我们开始 from scratch 的 Deep Learning 学习之旅~&lt;/p&gt;
&lt;h3&gt;基础数理知识&lt;/h3&gt;
&lt;p&gt;无论是什么理工科，事实上都是建立在最基础的数理知识之上，也就是以高等数学、线性代数以及概率论为代表的这三门科目，他们也是每个专业在大学期间的必修课。&lt;/p&gt;
&lt;p&gt;网络上存在着大量的相关课程，假如有条件的话可以观看 MIT 的相关课程，即 &lt;a href=&quot;https://ocw.mit.edu/courses/18-01sc-single-variable-calculus-fall-2010/&quot;&gt;高等数学-18.01&lt;/a&gt;、&lt;a href=&quot;https://ocw.mit.edu/courses/18-02sc-multivariable-calculus-fall-2010/&quot;&gt;高等数学-18.02&lt;/a&gt;、&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/&quot;&gt;线性代数-18.06&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;事实上尽管在网络上对于国内教材的批判呼声很大，但是对于了解以及学习来说，其实无论是哪些教材或者课程并没有本质的区别，只要认真学都可以熟练运用。有的时候我们要将课程知识当作工具而非一种艺术，对于这种起步时需要学习的基础知识尤其如此，在课程学习中患上「收集癖」，还没怎么开始学习就在挑选课程中看花了眼，不如花上一周先学完再说。&lt;/p&gt;
&lt;p&gt;在这里笔者推荐 &lt;a href=&quot;https://space.bilibili.com/66607740&quot;&gt;宋浩&lt;/a&gt; 的相关课程，属于按照国内课程的教学大纲进行的常规讲解，尽管说不上鞭辟入里，但是胜在详细。同时对于线性代数这门可以理解为几乎最重要的数学课程，同样推荐读者观看 &lt;a href=&quot;https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&quot;&gt;3Blue1Brown 的相关讲解&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;认识你的电脑&lt;/h3&gt;
&lt;p&gt;对于电脑小白来说，一般还是建议首先使用 Windows 电脑，假如没有玩游戏的需求，购买一个商务本即可，一般来说对于较重的渲染或者作业任务，在后期都是直接在服务器下运行，而不需要本地的计算资源。&lt;/p&gt;
&lt;p&gt;本地的办公环境还是要保证一定的轻便性，而由于在此之前绝大多数的读者对电脑的使用环境均是在 Windows下进行，并且网络上对于 Windows 的资料远大于其他系统，因此建议选择 Windows 作为自己第一台电脑的操作系统。&lt;/p&gt;
&lt;p&gt;假如读者已经领悟了便捷开发的重要性，并且具备一定的电脑使用能力，或许可以考虑使用 Mac。与此同时在本电脑上在后续也可以选择加装 Linux 作为双系统，以进一步提高生产力，并且贴近实际的开发环境，然而这一部分内容并不属于新手，将在后续展开说明。&lt;/p&gt;
&lt;p&gt;对于之前电脑接触不多的读者来说，可以阅读 &lt;a href=&quot;https://www.criwits.top/missing/&quot;&gt;你缺失的那门计算机课&lt;/a&gt; 来进行一些基础知识的扫盲，其中的前几个篇章都很有阅读的必要，而超越篇作为科普读物也尚且可以。假如读者不是对自己电脑的基本功绝对自信，也同样建议扫一遍目录，学习一下基础使用。&lt;/p&gt;
&lt;p&gt;对于电脑的基本熟悉流程，预期在三天之内完成。&lt;/p&gt;
&lt;h3&gt;Markdown，记录你的笔记&lt;/h3&gt;
&lt;p&gt;在正式学习某些专业知识或者编写程序之前，了解并学会使用 Markdown 可以说是十分重要的技能。&lt;/p&gt;
&lt;p&gt;简单来理解一下 Markdown 是什么。按照官方的说法，Markdown 是一种轻量化的标记语言，将带有特殊标记的纯文本转化为可以直观显示的 HTML 或者 PDF 文件。因为其便捷的特性，事实上当前互联网中绝大多数编程的文档，Github 上面的 README，甚至是本篇教程，都是使用 Markdown 完成的。&lt;/p&gt;
&lt;p&gt;以下是一个简单使用 Markdown 的示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;## 这是一个二级标题

### 这是一个三级标题

- 这是一个列表
- 这是一个列表
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以被渲染为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4n7z4amfbx.webp&quot; alt=&quot;Markdown 示例&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们不难简单理解 Markdown 流行的原因。在了解 Markdown 之前，读者可能接触最多的两类记录文本的内容分别是记事本以及 Word。记事本没有任何的渲染功能，作为临时的记录尚可，但是一旦要整理成可视化更好的文档便有点难以胜任了；而 Word 虽然功能强大，但是第一在于微软将其闭源，一般来说必须使用其软件才可以打开对应的文件，并不具备这类软件的机器上难以显示。同时相较于简单的纯文本，Word 对于正常的文字记录需求有些太重了。&lt;/p&gt;
&lt;p&gt;读者不妨回忆下自己使用 Word 的需求，一般来说也就只有改变字体，以及设置不同的标题和插入图片超链接，大量 Word 涉及的丰富功能往往为出版行业使用（事实上在这一方面，$\LaTeX$ 是更好的工具，将在后面进行介绍），而我们需要的其实很简单，完全没有必要等待漫长的软件打开时间以及安装庞大的软件。当我们渲染文件的需求移动到了服务器上时，这一劣势变得越发明显，&lt;/p&gt;
&lt;p&gt;通过 &lt;a href=&quot;https://markdown.com.cn/intro.html&quot;&gt;Markdown 官网&lt;/a&gt;，读者可以了解 Markdown 的全部语法，虽然说是全部，但实际上也非常简单，基本上两三次之后就可以熟练使用。&lt;/p&gt;
&lt;p&gt;尽管这些标记符号十分清晰，一个可以实时将这些内容渲染为可视化的文档的软件依然有必要存在，在这里推荐 &lt;a href=&quot;https://www.typora.io/&quot;&gt;Typora&lt;/a&gt;，作为专业的 Markdown 编辑工具，可以从外面导入不同的外观模板，同时对于导出 PDF 可以做到所见即所得；而 &lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsidian&lt;/a&gt; 相较于 Typora，导出的内容和软件中的呈现并不完全一致，不过其对于双向链接的支持，可以使得你使用特殊的语法连接不同的文档，并且构建属于你自己的知识库，这也是上述提到的记笔记软件中笔者的首选推荐。&lt;/p&gt;
&lt;p&gt;作为我自己的一个参考，我同时使用这两个软件，在面对常规的课程作业的需求时我使用 Typora 撰写文档并且导出美观的文件，而整理个人笔记及自身的一些记录时则使用 Obsidian。&lt;/p&gt;
&lt;h3&gt;VSCode，Not IDE&lt;/h3&gt;
&lt;p&gt;在学习了基础的数理知识，并且了解了如何整理自己的知识库之后，接下来我们终于可以来了解首选也是唯一的那一把属于程序员的瑞士军刀，&lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;VSCode&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;假如读者刚刚开始接触编程，往往不难经常听说一个词汇，也就是所谓的 IDE，对于 C++ 来说，&lt;a href=&quot;https://visualstudio.microsoft.com/&quot;&gt;Visual Studio&lt;/a&gt; 以及 &lt;a href=&quot;https://www.jetbrains.com/clion/&quot;&gt;CLion&lt;/a&gt; 都是属于这类的内容。在 IDE 中，软件内置了需要开发使用到的全部环境以及众多额外的高端开发工具，但是缺点也很明显，太重。试想一下一个软件巨大，功能繁杂，并且就大多数功能你都用不到的开发环境，我们还是提出那个基本的需求，极简的满足我们要求的开发环境，所以全世界绝大多数成员的首选，VSCode 应运而生。&lt;/p&gt;
&lt;p&gt;要是用某一个词汇去形容 VSCode，那么文本编辑器可能是比较贴切的之一，VSCode 并不支持任何的编程语言，也就是在你下载了它之后，你并不能直接在里面编辑并且运行 C，也不能直接运行 Python，本质上它只是一个文本编辑器而已，而恰好我们说的这些编程语言的文件都是直接的文本，只是后缀名不同。&lt;/p&gt;
&lt;p&gt;VSCode 实际的工作原理是将你本地配置好的环境和它自身的插件结合在一起，你在本地具有 Python 环境，同时你在 VSCode 中安装了 Python 插件，那么你就可以具有直接运行程序，实现代码高亮，错误提示在内的一系列功能。&lt;/p&gt;
&lt;p&gt;诸如此类，我们不难想象这样一个场景，当我们在本地分别为各种语言配置好了环境，并且为我们的 VSCode 安装了插件，那么一个兼容无数编程语言的强大 IDE 就诞生了，而且还很简洁。当然假如你有额外的需求，也不用担心，再安装一个插件就好，VSCode 具有极其庞大的社区，开源社区为其制作了无数的插件，可以覆盖你的绝大多数需求。&lt;/p&gt;
&lt;p&gt;关于配置环境，尽管我们如此描述，并且形容了 VSCode 的诸多好处，在 Windows 配置环境对于新手也绝非一件简单的事情（当后续我们介绍 Linux 的时候这一差距会变得更直观），因此在这里给出教程链接。对于 C++，可以见 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/401188789&quot;&gt;这个链接&lt;/a&gt;；而对于 Python，我们建议你配置 miniconda，见 &lt;a href=&quot;https://www.anaconda.com/docs/getting-started/miniconda/install#windows-installation&quot;&gt;这个链接&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;Chat with LLM&lt;/h3&gt;
&lt;p&gt;大模型出现之前学习一门新语言或者写一个项目有很多方法，你可能需要查阅文档或者论坛去找到一些在浅层搜索中难以发现的使用逻辑，然后再进行漫长的代码编写和 debug，其中可能还包括大量的重复性工作，但是大模型在出现之后，一切逻辑都被改变了。&lt;/p&gt;
&lt;p&gt;对于国产的模型，读者可以尝试使用 &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;Qwen&lt;/a&gt; 作为首选，或者使用 &lt;a href=&quot;https://chat.deepseek.com/&quot;&gt;Deepseek&lt;/a&gt;，但是不得不承认的是当前 Deepseek 具有极大的幻觉率。在有条件的情况下，笔者依然推荐读者使用海外模型，对于处理大量内容编写，&lt;a href=&quot;https://gemini.google.com/&quot;&gt;Gemini&lt;/a&gt;（Google 的模型）是一个好的选择，而 &lt;a href=&quot;https://chatgpt.com/&quot;&gt;OpenAI&lt;/a&gt; 的 GPT 系列在本档写作的当下对于回复长内容显得极其懒惰，但是响应速度以及使用体感上尚可。这两个海外模型的会员费用都是二十美元一个月，对于学生党算不上便宜，但是在我的视角来看绝对是物超所值。&lt;/p&gt;
&lt;p&gt;同时，伴随着大模型在代码编程领域的深度应用，代码补全插件应运而生，也就是在你输入了几行代码之后，模型会尝试理解并且预测你将来想写的程序，并且给出提示，此时你只需要按下 Tab 键就可以把它的提示应用到你的文件中。不过笔者在体验了诸多 VSCode 提供的代码补全插件之后，要是硬要说推荐，&lt;a href=&quot;https://www.cursor.com/&quot;&gt;Cursor&lt;/a&gt; 是唯一的选择。&lt;/p&gt;
&lt;p&gt;Cursor 并非一个插件，而是一个单独的软件，它的逻辑和 VSCode 完全相同，因为它实际上就是 VSCode 的套壳。VSCode 完全开源，也就是任何人都可以在它的基础上进行进一步的修改，而 Cursor 就是其中之一。它在其中内置了他们设计的代码补全以及写程序的流程框架，这套十分强大的流程远超任何代码补全插件的体验。要是用某种量化去描述，代码补全插件可以将我的编程效率提升一倍，而 Cursor 可以到三四倍，并且可以使我具有之前并没有能力做的，开发不了解的语言的小型项目的能力。&lt;/p&gt;
&lt;p&gt;事实上本博客的大多数代码也是在 Cursor 的帮助下完成了重构。Cursor 每月可以有一些免费额度，同时付费会员是二十美金每月，同样物超所值，并且假如你只有二十美元，我建议你为 Cursor 消费，而不是大模型会员。&lt;/p&gt;
&lt;p&gt;学会与大模型交流是一件十分重要的事情，尤其是在一切博客或者教程的撰写者都带有一定的 bias 的前提下，很有可能在你作为初学者阅读的过程中就忽然发现了自己难以理解的全新词汇，在此之前你需要在网上搜索资料，并且尝试理解，而如今你只需要打开网页，并且询问模型：「用浅显易懂的话来解释一下这个词汇的含义」，一切就迎刃而解，包括对本博客以上以及以下的内容都可以这样对待。&lt;/p&gt;
&lt;h3&gt;氛围编程，编程是一种思想&lt;/h3&gt;
&lt;p&gt;在诸如 Cursor 这样的 AI 代码编辑器兴起之后，一个新的概念开始出现，也就是氛围编程。作为氛围编程，无外乎就是使用文字的提示让模型去替你生成程序，但是其背后却隐藏着一个深刻的思想，那就是程序的语言本身只是外在，但背后的思想是统一的。&lt;/p&gt;
&lt;p&gt;没有读者没有学过任何编程语言的情况下，理解这些抽象内容显然是复杂的。事实上不同的语言确实有不同的特性，但是对于基础的使用来说，其实都是通用的。比如他们可能都有变量和逻辑语句，都有类以及函数，都可以分文件编写。这些共同的特性组成了程序设计的思想暗线，也就是如何组织你的功能，如何将不同的模块进行封装，使得整体的项目便于维护以及继续开发，而这才是体现编程功底的地方所在。至于语言以及具体的语法，模型能完成其中的大多数，而尽管更多的需求要你在进行语言的学习，但语言只是工具而思想不变。&lt;/p&gt;
&lt;p&gt;回到具体的语言学习，再根据上面的教程配置好开发环境后，读者大概率会面临本科教学中三大常见语言之一，也就是 C、C++ 或者 Python。在这里笔者并不会给出任何的教程，而是建议读者直接询问大模型并且获得答案。&lt;/p&gt;
&lt;p&gt;编程是互联网生产力的根本所在，也是众多大模型企业集中力量去攻关的场景之一，每一个模型都无外乎具有速通你的本科语言课程的能力，而与模型进行交互式的问答，效果好过任何的单方面视频课程是显而易见的。假如你是需要满足课内需求，可以直接将你的教学大纲发给大模型；而假如只是想要去学习，「我想要学习 xx 语言，我现在是零基础，请先列出一个详细的教学大纲，然后一步一步教会我」，这是一个好的提问，也是一个好的开始。&lt;/p&gt;
&lt;p&gt;在古早时期，对于大模型的使用存在大量的所谓提示词，可以进一步强化模型的能力，也就是所谓的提示词工程[^prompt-engineering]，而目前随着模型的能力不断变强，往往不需要提示词或者任何精心的设计，就可以让模型解决你的问题，因此也无需担心竭虑。&lt;/p&gt;
&lt;p&gt;[^prompt-engineering]: 提示词工程（Prompt Engineering）是指设计和优化输入内容（提示词）以引导大语言模型生成符合预期输出的技术方法。它涉及语言表述、上下文控制、格式设计等技巧，是有效利用生成式 AI 的关键手段之一。&lt;/p&gt;
&lt;p&gt;遇到报错，直接将信息复制或者截图发给模型；遇到看不懂的地方直接询问；你想要的功能也可以直接让它去写，对于初学者的需求来说，模型不会出任何问题。不过读者也不要完全依赖于模型，而是要在其中尝试自己学到一些内容，因为到了更深的层次之后，模型的能力会逐渐不适应，此时个人能力的体现就至关重要了。&lt;/p&gt;
&lt;h3&gt;Linux (and Git)&lt;/h3&gt;
&lt;p&gt;当你看到这一章的时候，你应该已经熟练掌握了如何使用 Windows 系统，并且已经学完了一门语言，这时候开始实践这一篇章才是合适的。当然，在此之前，先看一看介绍来了解一下详情也是可以的。&lt;/p&gt;
&lt;p&gt;有一种说法，Windows 是「弱智」的操作系统，Mac 是设计者的操作系统，而 Linux 是开发者的操作系统。前两者的描述并不准确，但最后一个却十分的恰当。市面上基本上全部的服务器都是用 Linux 进行管理，包括将来你在人工智能领域中训练模型所使用的服务器，因此学习 Linux 的基本使用是非常重要的一件事情。&lt;/p&gt;
&lt;p&gt;笔者推荐读者可以在自己的 Windows 电脑上安装 Ubuntu[^ubuntu] 双系统，我 &lt;a href=&quot;/blog/install-ubuntu&quot;&gt;之前的博客&lt;/a&gt; 非常详细的讲解了这些内容；同时直接租赁一个服务器也是值得推荐的，市面上现在大多数的显卡服务器都是按量付费，1小时也就几块钱，随用随停倒也不算很贵；而与此同时，网络上使用虚拟机安装 Ubuntu 的教程也很多。&lt;/p&gt;
&lt;p&gt;[^ubuntu]: Ubuntu 是一个基于 Debian 的开源 Linux 发行版，以易用性和社区支持著称，适合个人用户、开发者和服务器部署使用。它由 Canonical 公司维护，提供定期更新和长期支持版本，是最流行的桌面 Linux 系统之一，也广泛用于云计算和容器化环境。&lt;/p&gt;
&lt;p&gt;在这里简单解释 Ubuntu 和 Linux 的关系，本质上我们在说 Linux 系统的时候，实际上是在描述使用了 Linux 内核的一系列操作系统，这些操作系统在内核的基础上提供了图形化支持以及各种各样的包，使得其成为一个真实可以供开发使用的系统。在众多操作系统中，Ubuntu 以其易用性脱颖而出成为了深度学习研究者的首选，不出意外在将来的时间里读者只会见到这一个操作系统，因此也没有必要学习其他来节外生枝。但可以简单提一句的是，对于一些更加极客的开发者来说，他们会使用 Arch[^archlinux]。&lt;/p&gt;
&lt;p&gt;当你开始使用 Linux 操作系统，这意味着需要熟练使用命令行操作，简单向大模型提问，「向我介绍一下 Ubuntu 操作系统常见的命令行」，就可以获得你想要的答案，因此在这里不展开介绍。事实上，绝大多数时候你还是在图形化界面中按照 Windows 的操作逻辑进行操作，只有在需要运行你的代码的时候，你需要用命令行连到服务器，并使用 &lt;code&gt;cd&lt;/code&gt; 移到路境下，&lt;code&gt;conda&lt;/code&gt; 激活环境，并且 &lt;code&gt;python&lt;/code&gt; 运行程序。&lt;/p&gt;
&lt;p&gt;此时也可以向推荐另一个&lt;a href=&quot;https://missing-semester-cn.github.io/&quot;&gt;计算机扫盲的网站&lt;/a&gt;，这其中讲解了一些计算机的其他常识，同时简单介绍了 &lt;code&gt;git&lt;/code&gt; 这个工具，这一版本管理的工具将在日后派上巨大用处，而廖雪峰专门为其 &lt;a href=&quot;https://liaoxuefeng.com/books/git/introduction/index.html&quot;&gt;撰写的教程&lt;/a&gt; 也值得推荐。&lt;/p&gt;
&lt;p&gt;Git 是一个非常重要的工具，关于如何进行版本管理、多人协作以及参与开源社区，可以说某种程度上，在任何的编程项目中初始化一个 git 仓库都是很正常的事情，上述的扫盲网站中关于 &lt;a href=&quot;https://missing-semester-cn.github.io/2020/version-control/&quot;&gt;Git 的章节&lt;/a&gt; 笔者尤为推荐，这是你几乎必须了解的概念以及必须掌握的技巧。&lt;/p&gt;
&lt;p&gt;当然，事实上上述的全部需求，在实战中一步一步向大模型提问也可以解决，所以实践出真章，在实践中学习也是一个好的选择。同时，对于想要进一步了解 Linux 系统的读者来说，阅读 &lt;a href=&quot;https://101.lug.ustc.edu.cn/&quot;&gt;Linux 101&lt;/a&gt; 是一个很不错的选择。&lt;/p&gt;
&lt;h3&gt;机器学习与计算机视觉是否真的必要&lt;/h3&gt;
&lt;p&gt;笔者可以简单给出结论，机器学习以及传统的计算机视觉在大多数时候并不会起到什么作用，或者说这些内容已经通过理论完善，并且具有一定的上限。尽管有的时候可以为你设计新的算法带来启发，但是并不在我们学习的最短路线中，不过这里给出参考资料。&lt;/p&gt;
&lt;p&gt;对于系统学习机器学习的书籍，相较于网络较火的西瓜书，笔者更加推荐李航老师的《统计学习方法》。而传统的计算机视觉相关的内容，则推荐冈萨雷斯的《数字图像处理》。&lt;/p&gt;
&lt;p&gt;同时，尽管这些内容有的时候依然实用，但是多半在实际的使用中，我们都是以使用者的需求而非创新性的追求来看待。好消息是 Python 的众多库，如机器学习的 scikit-learn 以及计算机视觉的 OpenCV，其实都支持了这些算法，因此往往只需要简单向大模型进行提问，并且调动这些就可以实现对应的功能。&lt;/p&gt;
&lt;p&gt;一些反对者或者保守主义者不免担心，跳过了这些打基础的环节，是否会导致将来自己学习一些东西的时候不太牢靠。尽管在笔者的视野中并没看到这样的案例，但是进行适当的学习倒也无所谓，只是看到互联网上繁多的课程，没有必要因此看花了眼，选中几个课程并且进行快速的学习即可。想当初笔者在视频网站中比较不同视频合集介绍算法的异同点并且都看了一遍，如今回过头来看，除了浪费时间，没有得到任何的帮助。&lt;/p&gt;
&lt;p&gt;尽管传统机器学习与计算机视觉方法在当下并非主流，但在实际工程应用中依然有着广泛的使用，例如工业检测、图像处理、边缘设备部署等场景；而在理论研究领域，它们更是构建现代 AI 的基础。如果你对工程落地或算法原理感兴趣，这些内容仍然值得系统学习。&lt;/p&gt;
&lt;h3&gt;深度学习&lt;/h3&gt;
&lt;p&gt;到了深度学习的领域，说白了，你正式开始入门了，需要更深度的理解各个方面的知识，并且融会贯通。当然，这只是一个开始。&lt;/p&gt;
&lt;p&gt;假如说你擅长英语，那么在这里推荐你观看 &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&quot;&gt;CS231n&lt;/a&gt;，这门经典的计算机视觉课程现如今已经涵盖了对于计算机视觉以及泛深度学习中的绝大多数基础内容，对于初学者来说绝对是最好的教材。当然，假如你并不擅长英语，这门课程也依然推荐，请在磨砺你的英语水平的同时观看（顺带一提，YouTube 提供了实时字幕翻译的功能）。&lt;/p&gt;
&lt;p&gt;与此同时，假如说一定要选择一个中文教程，在这里照例推荐李沐的 &lt;a href=&quot;https://www.bilibili.com/video/BV1if4y147hS/&quot;&gt;动手学深度学习&lt;/a&gt;，同时并不建议观看其中任何的动手部分以及比赛部分，而且快速过掉全部知识点。至此你便了解了深度学习的基础知识，可以投身于特定领域以及代码的学习了。&lt;/p&gt;
&lt;p&gt;与此同时，假如说你对于 LLM 具有浓厚的兴趣，在这里同样推荐 &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&quot;&gt;CS336&lt;/a&gt;，这些内容看上去更加的专业，需要更多的时间仔细学习。&lt;/p&gt;
&lt;p&gt;在这里笔者建议读者自行实现几个程序，比如说经典的 MNIST 识别数字，或者可以手动实现一个 ViT 来识别 ImageNet，这个环节可以在AI的辅助下完成，来确保自己已经可以熟练融入编码中带有AI协助的工作流程。不过必须说明的是，这些代码说到底只是练手而已，而实际的项目往往从需求中产生，在此之前先学一学如何了解一个新领域吧。&lt;/p&gt;
&lt;h3&gt;论文阅读 and so on&lt;/h3&gt;
&lt;p&gt;伴随着人工智能技术的发展，人工智能所能涉及的领域也越来越多，从本来的图像处理以及自然语言变到了如今越发宽广的模样，包括大语言/多模态模型、具身智能、3DV、图像生成以及其他的无数的领域，而再向下细分更是有无穷多的分枝。从这样一个庞杂的枝杈中找到自己感兴趣的方向并非一件易事，读者不妨给自己放一到两周的假，在大量的新闻中了解AI领域的各个方面，包括但不限于各类新闻和公众号的推送，以及各类的每日论文推荐，并且尝试从中找到自己感兴趣的方向。在这里给出一些推荐的信息渠道。&lt;/p&gt;
&lt;p&gt;当读者找到了一个领域之后，往往就需要开始了解这个领域相关的论文，而弄懂一个领域在做什么，核心思想就是弄清楚当前的前沿工作都是用什么架构的模型，在怎样的数据上训练，并且存在哪些应用场景。人工智能本质上作为实践学科，是用方法解决某个问题，因此搞清楚这个关系就明白了大半，接下来就需要开始关注方法的细节，并且进一步精读论文了。&lt;/p&gt;
&lt;p&gt;阅读论文的方法依然可以参考李沐的论文精读系列，只是众多论文不一定和你的口味，你只需要了解阅读的节奏，并且在熟悉一个领域之后开始加快这一节奏，同时他们的论文串讲做得非常不错，值得一听。&lt;/p&gt;
&lt;p&gt;尽管前面给出了不少的捷径，但是到了最后这里，笔者却难以给出更快的方法，本身对于领域内方法的理解，就是在大量的阅读以及大量的实验基础上积累得到的，二者缺一不可，而在读者尚且没有进行实验的条件下，阅读也自然就成了唯一的途径。&lt;/p&gt;
&lt;p&gt;对于如何获得需要阅读的论文列表，大多数流行的领域在 Github 上均存在对应的所谓 awesome list，如 &lt;a href=&quot;https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models&quot;&gt;多模态大模型&lt;/a&gt;、&lt;a href=&quot;https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln&quot;&gt;具身智能&lt;/a&gt; 或者 &lt;a href=&quot;https://github.com/Purshow/Awesome-Unified-Multimodal&quot;&gt;Unified Models&lt;/a&gt;，也就是领域中其他人为这个领域收集的论文推荐列表。这些列表往往大而全，但是并不是每一篇都有必要精读，一个很好的参考是读者可以前往 &lt;a href=&quot;https://scholar.google.com/&quot;&gt;谷歌学术&lt;/a&gt; 查看论文的引用情况。同时读者也需要紧密关注大量人工智能相关资讯，如公众号机器之心、新智元以及量子位。关注 &lt;a href=&quot;https://arxiv.org/&quot;&gt;arxiv&lt;/a&gt; 上的最新论文也是一个好的习惯，同时 &lt;a href=&quot;https://papers.cool/&quot;&gt;papers.cool&lt;/a&gt; 是一个每日推荐论文的网站，值得每日上去刷一刷。&lt;/p&gt;
&lt;p&gt;而当你阅读了大量论文之后，可以前往一些租卡平台，诸如 &lt;a href=&quot;https://www.autodl.com/&quot;&gt;autodl&lt;/a&gt;，使用按量计费的方法租卡来跑一些实验，将自己看过的领域内比较前沿的论文跑一跑，从而增加自己的上手经验。&lt;/p&gt;
&lt;p&gt;实际上任何研究者都是从新手做起的，所以完全无需为难情绪，尽管实际上在这个介绍篇幅并不是很大的练手环节，才是篇章中可能带来最多挫折的部分，但是现在不同的领域有不同的代码体系，大量的经验只可意会不可言传，都是十分分散的，也是读者也只能在实践中自己摸索了。&lt;/p&gt;
&lt;p&gt;不过假如读者在摸索之后获得了提升，并且按照笔者上述的建议进行了近期工作的汇总，将自己如何解决问题记录了下来，笔者自然也欢迎读者将自己的博客发在评论区，供后来者参考。笔者当前在具身智能领域进行了大量仿真器的开发，相关内容都在本站中汇总成的博客，假如读者恰好对这个领域有兴趣也可以前去一观。&lt;/p&gt;
&lt;h3&gt;加入课题组&lt;/h3&gt;
&lt;p&gt;在学习了大量的基础知识之后，加入课题组自然也有相关的一番学问，当然事实上，笔者并没有加入过很多课题组，但是在与很多朋友的交流中也有一些心得，并且总结了一些经验。&lt;/p&gt;
&lt;p&gt;就像是把大象放到冰箱里分三步一样，加入课题组也有类似的三个步骤，找到想要加入的课题组，准备简历，申请与面试。&lt;/p&gt;
&lt;h4&gt;寻找课题组&lt;/h4&gt;
&lt;p&gt;找到想要加入的课题组往往是最令人烦恼的一个环节，因为剩下的两部分只是不知道如何做好，但是如何去做大致上还是有一个思路的，然而对于课题组来说，这毕竟是一个长期的考虑和决定，找到合适的往往是困难的。&lt;/p&gt;
&lt;p&gt;以往在加入课题组之后，浪费大量时间进行打杂，甚至将时间浪费在老师评优所需要参加的竞赛中，这些例子比比皆是，而如何对于目标的老师进行检索和筛选，笔者本人有几个方法。&lt;/p&gt;
&lt;p&gt;对于检索来说，两个常见的方法分别是在论文阅读中积累或者请教前辈。&lt;/p&gt;
&lt;p&gt;在读者找到了感兴趣的领域，并且广泛阅读论文之后，往往对于领域内的工作会有一些自己的初步见解，并且和以简单的对于不同工作的质量进行划分。一个有趣的现象是，读者有时会发现一些优质的工作，总集中在某些课题组中进行发表，或者至少挂在某位老师名下，这通常从侧面反映了课题组对于科研的 taste 以及课题组的学术水平，可以成为是否是一个好的课题组的标准。在笔者认识的大多数精通科研的朋友们中，一个广泛的共性是大家对于科研圈的各种脉络发展以及很多人的科研主线均具有着广泛的了解。具体来说在讨论到某篇工作的时候，往往不止于这篇工作本身提出的创新点，包括核心作者本人在这篇工作之前的内容，某某工作和这篇的思想如出一辙，本身是在遵循一条一致的路线。对于这类脉络的理解，在短期内可以帮助读者了解课题组的好坏，而在长期更可以帮助读者甄别合作者的能力以及相对应的人际关系。&lt;/p&gt;
&lt;p&gt;向前辈请教则是另一个很直观的解决思路。本校的同学可以很清楚地说出他了解到的部分老师是如何对待学生的，哪些老师是有能力，哪些老师没有资源；而对于这个课题组里面的学长来说，则可以进一步了解其中的细节，老师的为人处事以及合作模式。就像前面提到的，按照提问的智慧，礼貌的向他们咨询，并且表达感谢。值得注意的是，能够提供这些信息的前辈往往是老师的利益相关。在校的学生不敢对老师进行锐评，课题组中的学生更是如此，甚至向你推荐一些老师，这是否意味着其他老师的能力不够，往往容易得罪人。这一事实导致大多数人并不愿意在线上提供咨询，一个合理的借口，和前辈在校园里转一转，喝一杯咖啡或者请他出去吃一顿饭，线下的交流往往更加私密，更容易咨询到你需要的信息。&lt;/p&gt;
&lt;p&gt;通过咨询的方式可以了解到一定范围内能力尚可的老师，而那些往往能产出优质工作的课题组则门槛更高，只要你有足够的勇气，完全可以申请他们的课题组，就算失败也不会有什么损失，不过假如你决定退一步讲，那么一个问题是在若干能力尚可的老师中如何选择那位相对更好的，也就是一种对于能力的评价策略。&lt;/p&gt;
&lt;p&gt;简单介绍一下对于老师的职位，排除各种头衔不谈，一般来说，主要还是教授副教授以及助理教授。全世界的教职制度基本都是如此，向上的升迁是需要时间积累的，而不只是成果足够，因此一个现象是，现在在学术圈中一批更具影响力，并且在工业界也颇有人脉的学者，往往都是某高校的助理教授。在一般的选择上笔者也更推荐读者前往这些老师的课题组，一方面这些老师更加年轻，甚至依然奋斗在学术一线，和自己之前就读的高校或者工业界有着紧密的联系，而近些年来出色的学者往往也有着更加亮眼的成果，这些成果在现如今有着深远的影响力，并且可以为他带来学界以及工业界的合作和资源；另一方面，这些老师和学生的代沟更小，课题组中的人员关系积累也更少，相对纯粹而没有勾心斗角，甚至还可以有机会成为老师的开门大弟子。对于绝大多数的老师，论文引用量以及项目的影响力是不深入了解下评判他们能力的唯一标准，这方面读者可以前往谷歌学术查看，通常两到三千引用已经算是比较厉害的老师，更有甚者可以更多。而对于更加年长的老师来说，五千以上的引用往往意味着他们可以在这个学校里站稳脚跟，并且提供稳定的资源，也是可以投奔的对象。&lt;/p&gt;
&lt;h4&gt;准备简历&lt;/h4&gt;
&lt;p&gt;在选择了合适的老师之后，下一个目标就是撰写简历，这其中自然也有不少的学问。通常来说简历的撰写使用 PPT 或者 $\LaTeX$，而很少使用 Word，$\LaTeX$ 可以在 overleaf 上进行编辑，这是一个在线免费的 $\LaTeX$ 编辑器，支持在线编译。大多 GitHub 上的简历模板可以下载为压缩包，并且直接导入。&lt;/p&gt;
&lt;p&gt;简历在编辑的过程中，自然也有轻重缓急之分，姓名在内的个人信息放在最上方，之后是教育经历，接下来依次摆放科研项目以及其他奖项，在另一方面也反映了通常来说老师对于学生的期望。科研成果以及科研能力往往是排在首位的，其次是工程能力，再然后才是其他从侧面可以体现出学生个人水平的事项。一般来说比较前期的简历可以尽量排满一页，如果有所超出，就要对内容进行适当的取舍以及压缩。对于科研以及项目的描述，往往从两个核心点出发，一方面是项目实现了什么，这里主要凸现相较于以往内容的不同以及更加量化的进展，比如说相较于之前 SOTA 模型提升了 10 个点，而非简单将项目或者论文的摘要让人工智能进行压缩，放一堆没有任何信息量的空话上去。简历在某种程度上体现了你的表达能力，而老师在相关领域内也已经足够资深，并不是使用套话空话可以骗过去的类型，因此还是拿出真东西说话；另一个方面则是突出自己在项目中的贡献点，自己在其中参与了哪些部分，并且实现了什么核心内容，这些可以体现你的能力。其他奖项则仅列举名称，没有必要详细阐述。&lt;/p&gt;
&lt;p&gt;对于在此之前没有相关经历的同学，也就是完成了上述我讲述的这些内容，但是没有参加实际项目，事实上一封套瓷信也可以说明一切，无需附上简历。&lt;/p&gt;
&lt;p&gt;套瓷信主要分为几个部分。首先自我介绍，并且表达在老师课题组进行科研实习的意向，以及对于老师做研究领域的兴趣；接下来进行简短的自我介绍，例如之前的科研和项目经历，或者说自己的成绩排名；再之后表现自己对于领域的了解，例如阅读过哪些论文，或者假如已经进行过科研训练，是否完成了一些项目，并且有一些深入的理解，在这部分非常简洁的题，并且再次表达对于老师课题组的兴趣；最后客套收尾，很高兴认识老师，并且主动约会议，如老师假如有时间可以加微信并且约会议进行详谈，同时留下自己的联系方式。&lt;/p&gt;
&lt;h4&gt;面试相关&lt;/h4&gt;
&lt;p&gt;假如老师确实要和你进行面试，那么恭喜你进入了最后一个阶段。事实上大多数老师是很愿意要能力尚可的实习生的，面试多半也以聊天为主，而不是进行考核，但是还是要进行适当的准备，例如如何实现 attention 这种老生常谈的问题。&lt;/p&gt;
&lt;p&gt;在面试的过程中，投其所好是一个关键，对于已经有科研产出的读者来说，自己的产出与老师的方向是否 match，如果 match，如何让老师认可你的工作；假如没有产出或者不 match，如何同样向你陶瓷的老师阐述你的能力是关键。一个细节是在面试或者聊天的时候不要关注我的方法做到了 XX，除非你做出了绝对领先于领域内大多数人的工作，不然即使是顶会，老师们本质上更加关注的是你的能力。这是一个很有趣的现象，在简历上一般的建议是对于你的项目给出量化的体现，比如说超过 SOTA 4.2%，但是到了面试，老师更关注你的个人能力，即，能否在老师的课题组中继续达到上个项目的完成度甚至更好。&lt;/p&gt;
&lt;p&gt;完成了上述的步骤之后，读者就可以加入课题组，并且正式开始科研实习了。&lt;/p&gt;
&lt;h3&gt;完成第一篇论文&lt;/h3&gt;
&lt;p&gt;在加入课题组之后，下一件事情就是完成自己的第一篇论文了。&lt;/p&gt;
&lt;p&gt;一般来说我们认为，进组之后的状态存在两种，一种是老师分配了一名或者多名师兄带你（具体的形式可能是让师兄本人和你对接，或者安排你进入师兄的课题中，看看能否帮上一些忙），另一种则是老师希望你自己独立完成一个课题。值得一提的是，在这个过程中大多数时候实际上并不存在一个所谓的状态，是被你的老师放养了。课题组并不是某种贴心的论文/科研辅导机构，你需要主动联系老师以及师兄，并且寻找自己在其中可以做些什么事情，而并不是等待他们贴心地找上你。&lt;/p&gt;
&lt;p&gt;在这里回扣上一章节的内容，事实上在选择课题组的过程中，老师所进行的课题是否收敛也是很关键的一个评判指标。笔者的第一段实习很难说不坎坷，老师打算开展一个新的方向，并且让笔者探索，这在某一方面意味着整个课题组中实际上只有笔者一人有经验。哪些代码是好用的，有没有具体的调参技巧，哪些论文是有必要阅读的，这些经验往往只有同课题组中更加资深的老师或者前辈才可以给你指导，而假如老师研究的领域并不集中，这或许意味着你会看到不同领域的师兄彼此没有交集，所能获得的帮助也就更加分散，这并非有助于长远发展的。&lt;/p&gt;
&lt;p&gt;回到正题，在两种参与课题的情况中，一般来说第一种是更加合理且幸运的结果，也就是有一名前辈带领你进行第一次科研实践，并且以核心身份参与到他的课题中。在这里我们会先简要说明整个科研的流程，并且在之后强调剩余需要关注的重点。不过在此之前，你需要分辨前辈让你做的事情是否是核心参与者需要进行的，而不是亲切的 PUA 你帮助课题组标注一些数据，或者进行其他的打杂工作，这些内容让你不能参与到课题的核心迭代流程中，从而不仅因为没有参与核心工作而作者位次较低，也很难从中学到任何内容。&lt;/p&gt;
&lt;h4&gt;形成 Idea&lt;/h4&gt;
&lt;p&gt;一般来说，一个课题的诞生从 insight 开始，也就是对于领域中发现了某个关键问题，或者对于有的问题找到了更好的解决思路，或者基于某些理解可以进行一些针对性的探索，并且从探索的结果中寻找思路。&lt;/p&gt;
&lt;p&gt;在参与课题的过程中，读者有必要跟随前辈的思路，积极参与对于整体模型的迭代，并且在这个领域中尝试寻找其他有待解决的问题。在这里举例一个笔者所参与的领域的例子，也就是 VLA Manipulation 这个方向，即使用模型控制机器人（主要是机械臂）进行操作任务，例如分拣桌面上的物品。&lt;/p&gt;
&lt;p&gt;在对于之前的 VLA，在实验的过程中不难发现，对于抓取任务来说，很多时候机械臂确实已经到达了物体的上方，但是在向下抓取的时候，因为高度不够导致没有成功抓取，从而导致了非常大的失败率。这里就呼应了前面提到的找到一个关键问题，也就是当模型只有 RGB 输入的时候，缺乏对于深度的感知，从而使得任务失败率很高。在遇到问题之后就去分析导致问题的原因，并且可以思考最直接的方式去解决，在这个问题中，最好的方式当然是接入一个深度编码器（当然事实上这里只是一个例子，类似的方法已经有其他人做过，笔者才不会心疼去讲这个）。在往更加细致分析，VLA 目前常见的架构是 VLM+Diffusion 拼接在一起的模型，本身进行端到端训练，可以简单理解前面的 VLM 负责任务理解以及各种的语义理解，甚至包括 grounding，而后面的 Diffusion 则关注于动作本身。这时需要关注的重点就在于基于领域中的方法以及一些常识进行分析，应该如何更好地实现自己的 idea。对于这个问题来说，VLM 大多数输出的是语义信息，这意味着其失去了更加细致度的表达，将深度信息作为 VLM 的输入，并且希望 VLM 将这一信息传递到后面显然是更加不具有效率的，而直接将深度编码和 VLM 输出拼接在一起作为 Diffusion 的 condition 虽然是更加直接且有效的方法。这时候一个最基础的新方法就已经出现了，剩下的对于读者来说就只需要实现算法并且形成一篇论文了。&lt;/p&gt;
&lt;h4&gt;关于代码&lt;/h4&gt;
&lt;p&gt;在拥有了 idea 之后，之后就是要进行代码的实现了。在这里一般来说对于第一篇论文，还是建议基于以前的方法的代码进行改进，在他们的基础上加入自己的创新点。一个事实是，现如今绝大多数的论文的要求不只是算法有效，更需要超过当前领域中的全部论文，达到 SOTA 的性能，以往的论文除了自己的创新点，但也包括不少的调试技巧。直接在他们的代码基础上进行修改，可以确保你的方法带来的性能改进是在当前最好的基线上进行上下波动。而假如自己从零开始进行实现，极大的概率是忽略了一些实际影响深远的技巧，从而导致难以对齐性能。&lt;/p&gt;
&lt;p&gt;这里同样有必要提及此时此刻的代码能力，在此之前我们建议读者掌握 Python 以及 Linux 的使用方法，并且在学习深度学习的过程中了解 pytorch 的内容。假如读者处于一些主流领域，那么在拥有这些基本功，并且熟练了解 Git 以及 GitHub 之后，同样有必要了解 Hugging face 以及 Transformer 库，在一些比较传统的领域中，这些内容没有被推广，但是更多的领域内它们已经成为了绝大多数代码的基石。&lt;/p&gt;
&lt;p&gt;不同的领域包含了不同领域细分的需要掌握的技巧，这些内容就需要读者自己去练习了，假如参与其他人的课题，代码是一件工作量很大的事，而重点在于了解自己的能力边界，并且在能力边界之内尽力去参与，而不是揽下大多数自己似乎可以去做的事情，但是最后却发现没有兑现自己的承诺。&lt;/p&gt;
&lt;h4&gt;进行实验&lt;/h4&gt;
&lt;p&gt;完成了方法的实现之后，剩下的就是漫长的实验过程，其自然包括了对于性能的调试。事实上一个悲伤的事情是，排除方法确实很有效这一情况，在大多数时候对于略微无效的方法，可以通过调试技巧使得性能超越以前的方法，假如读者认为就是有必要的，或者说大多数时候这都是有必要的，那么在性能不是最好或者不够理想的情况下，不断地在实验中迭代是必须的。&lt;/p&gt;
&lt;p&gt;实验过程有两个重点需要注意。&lt;/p&gt;
&lt;p&gt;其一是需要详细的记录实验每一步的 setting。一个事实是在论文写作的过程中包含 ablation study 这个环节，你需要客观分析你的方法在整体的性能提升中带来的增益，一些更加细节的参数带来的影响，以及假如这是一个更加偏向于探索，而非制作一个模型的论文来说，阐述此时此刻模型内部的运作机制，使用一些定性或者定量的实验来证明你的观点，这些也是必要的。这意味着那些即使在调试过程中性能不理想的内容，既然可以作为实验结果放到正文之中。一个简单的例子是关于学习率的敏感性，在调试的过程中，按照数量级对于学习率进行缩放是常见的调参技巧，假如说进行了反复的调试，但是性能没有什么变化，那么这些实验结果就可以作为「模型对学习率不敏感，非常稳定」的佐证；假如说在调试中有了明显的变化，那么则可以阐述如何更好地选择学习率。&lt;/p&gt;
&lt;p&gt;其二是在实验过程中依然关注哪些问题是存在的，老实说之前的创新点确实可能不大，后续我们会阐述针对有限的创新应该如何形成一篇论文，在这里明显更好的结果是发现新的问题，并且对于方法进行更多的补充。&lt;/p&gt;
&lt;h4&gt;论文写作&lt;/h4&gt;
&lt;p&gt;在完成了主要实验的同时就可以进行论文的写作了，这也是形成一篇论文中最后的重要环节。写作主要从三个方面进行讲解，分别是故事线、创新点以及作图。&lt;/p&gt;
&lt;p&gt;一种调侃是当今的写论文就是在讲故事，这从某方面侧面反映了事实。所谓讲故事以及所谓论文的故事线，完全不是一个贬义词。本身论文是事关学术的，这意味着在论文中，你不是在讲述那些你费尽心思写下的代码机制是如何实现的，或者各种调参中你是多么的辛苦，你是如何使用一个 torch 的类做到了某个模块，而是需要赋予整个论文一条故事主线。继续下面对于深度的例子，这条主线可能是当前深度信息的缺失导致了模型抓取精度的丧失，从而引出如何将深度信息更合理的编码到模型中的问题，并且讲述自己论文的贡献点。&lt;/p&gt;
&lt;p&gt;所以这里就是第二点了，也就是如何阐述论文的贡献点，一般来说论文的贡献点都在三条左右，其中可能包括一点是在当前所有的测评基准中都获得了 SOTA 的性能，但是另外两件往往事关方法本身。如何从巧妙的角度找到切入点，并且贴合故事进行讲述，这是需要技巧的。&lt;/p&gt;
&lt;p&gt;对于上面提到的 idea 来说，我们之前讲到合理的深度输入位置，可以带来更好的改进，这里就已经是一个关键创新点了，而在此基础上需要衍生出下一个点。例如，如果使用目前已有的深度编码器，这意味着模型都是在通用数据上进行训练的，而我们其实希望我们的模型可以聚焦于机器人领域的深度信息，这些深度信息是具有特征的，比如说对于腕部视角来说，一因为相机固连在机械臂的夹爪上，夹爪部分的深度不变，而这些可以作为锚定的信息，可以为模型带来更加准确的深度估计，那么自然也可以提出一个类似于机器人域上的深度信息微调的贡献点；又或者就像上面提到的内容一样，同样出于让模型适配机器人数据，那是不是对于最后任务上的训练过程，依然可以不冻结深度编码器，从而自然引出了是否可以设计一个多阶段训练的策略。诸如此类的方法还有很多，可以充实你的框架。&lt;/p&gt;
&lt;p&gt;最后一个要点在于作图，作图实际上是论文呈现中非常重要的一个环节。一种说法是，好的作图可以让你不看论文就理解方法，甚至性能的图表也可以让你不看文字就理解他想表达什么，这是一个需要长期锻炼的能力，多看多积累审美。在这里推荐使用 PPT 进行作图。&lt;/p&gt;
&lt;p&gt;值得一提的是，读者在阅读论文的过程中，不少的领域中通常会有极其复杂的作图，若干的模块交织在一起，看上去十分有创新点。然而需要怯魅的是，事实上排除一些确实需要工程实现的论文，例如某些 Agent 论文，绝大多数这类论文实际上反而是没有核心的创新点以及故事线，而是在堆砌一些看上去不明觉厉的模块，最后也难以进行非常详细的分析，对于他们如何达成了最后的性能。&lt;/p&gt;
&lt;p&gt;在论文的写作过程中，一个重点在于反复向前辈以及老师寻求反馈，并且进行迭代。记录他人的指导，并且详细的落实，甚至在部分有分歧的地方咨询他们的指示，在大多数时候都是正确的，作为科研中新手，读者的第一次论文写作可能过于理想化，例如对于一些自己方法的 feature 进行了 overclaim，一个想法是这样讲才会更加抓人眼球或者令人印象深刻，而前辈的建议则不建议这么说。这实际上都是从以往的经验出发，过分的 overclaim 反而会以来审稿人的质疑，就像过分的遣词造句会使得文章晦涩难懂，适当的故事是有助于表达的，但却在于如何将你的实际方法串联起来，并且形成完整的表述，而并非去讲云里雾里的新名词。让别人能够立刻看懂论文才是第一要务，也是一篇好论文的核心素养，你需要在实验部分进行细致的分析，或者在方法部分展现有趣的设计，而非堆砌废话。&lt;/p&gt;
&lt;p&gt;与此同时，丁霄汉在 &lt;a href=&quot;https://github.com/hzwer/WritingAIPaper&quot;&gt;Writing AI Conference Papers: A Handbook for Beginners&lt;/a&gt; 中给出了一个更加聚焦到细节的论文写作流程，感兴趣的读者可以参考。&lt;/p&gt;
&lt;h4&gt;投稿流程&lt;/h4&gt;
&lt;p&gt;形成论文之后，下一个重点就在于投稿了，但事实上一般来说选择投稿的会议往往在正式论文形成之前，从而导致你需要在会议投稿截止之前进行经典的 rush，从而按时完成你的提交。&lt;/p&gt;
&lt;p&gt;在这里简单介绍人工智能领域的投稿概况，排除诸如 NSC 这种最顶级的刊物，在人工智能领域的一个普遍现象是会议的含金量大于期刊，这是因为人工智能领域的迭代速度过快，而期刊的审稿周期往往更长，因此周期更短的会议更受研究者的喜爱，从而形成了这样的格局。甚至到如今大家同样愿意将论文投稿到预印本平台，让论文可以第二天就发布，从而进行宣传。毕竟预印本平台也可以算引用，影响力即是远远大于会议中稿（毕业要求除外）。目前在国内存在的评价标准是 CCF 评级（SCI 分区并不在人工智能内被认可，假如有人和你强调他的论文是 SCI 某区，这往往意味着他的论文含金量较低），分为 ABC 三个等级，一般来说 A 才是所谓的顶会。当然在不同的领域中也有所例外，而 A 中也并非全都是好的。例如在机器人领域的 IROS 评级为 C，更加广泛的，ECCV 作为计算机视觉三大会，评级为 B；而 ICLR 作为顶级会议，甚至没有评级。当然也存在包括 TPAMI 在内的不少顶刊，认可度依然很高。&lt;/p&gt;
&lt;p&gt;论文投稿首先在注册，会议中通常需要在网上搜索会议的信息进入主页，然后前往对应的平台进行注册，注册过程往往是免费的，之后你需要在截止日期之前投稿的论文。在论文正文的提交同时还可以提交附录，包括文本形式的补充材料，以及甚至可以有视频以及网页，当然在其中需要注意的是双盲，也就是你不能向审稿人透露你的身份，即使是无意的泄露也会导致被直接拒稿，即 desk reject。一些会议的补充材料需要在正文截止之后的一周内提交，剩下的可能需要同时提交。&lt;/p&gt;
&lt;p&gt;在投稿完成之后就只需要等待结果了，第一轮审稿意见由审稿人给出，一般一篇论文会被分配三到四名审稿人，他们会在对论文给出意见的同时进行打分，之后你可以对他们的意见进行回复，解释他们认为的不足并且提供补充以及修改，也就是所谓的 rebuttal。不少会议要求回复在一页 pdf 之内，这意味着这一要求依然存在：保持表述的准确与简洁。&lt;/p&gt;
&lt;p&gt;本质上来说，审稿人的意见仅供参考，最后的是否接受的决定则由 meta reviewer 决定，但是一般来说依然有必要进行参考，一个基准是评分是否都在 borderline
以上，也就是没有负分，对于五分制的会议来说，意味着每一个分数都要大于等于三。&lt;/p&gt;
&lt;p&gt;一个好消息是审稿人在看到你的回复之后会再次修改分数，并且在此之后交给 meta reviewer 进行决定。&lt;/p&gt;
&lt;p&gt;大多数论文的整体流程就是如此，在过程中你当然需要决定何时继续课题，这个课题已经难以进行或者需要撤稿，在中稿之后你才需要正式进行注册，也就是进行缴费。一般来说大多数的会议都需要至少一名作者进行正式注册，而同时你不需要让全部作者都注册。注册之后就意味着你的论文可以被正式接收了，在此之后你需要提交一个正式版本，称之为 camera ready，然后你就不需要管了。线下参加会议并非必须的，但是确实是可以选择。&lt;/p&gt;
&lt;p&gt;以上就是一次完整科研的流程了。&lt;/p&gt;
&lt;p&gt;在这里再次回到一开始的命题，关于是否第一个课题就是自己独立完成，其实本身和参与别人的课题没有非常大的区别，但是确实会带来更多的坎坷，这意味着上面的事情内容大多数都需要你独立完成，花费更多时间思考以及试错，不过在结束之后也可以为你带来更多的成长。&lt;/p&gt;
&lt;p&gt;而在第一个课题结束之后，接下来就是在继续对于这个课题内你发现的问题开始下一段课题，或者重新在更广泛的领域中去寻找新的问题。&lt;/p&gt;
&lt;h3&gt;中稿之后&lt;/h3&gt;
&lt;p&gt;在论文的中稿一起投稿之后，实际上也并不意味着万事大吉，假如你确实心向科研，想要在其中做出一番事业，那么明显还有更多的事情是你需要关注的。&lt;/p&gt;
&lt;p&gt;在程序员界存在一个广泛的故事，关于程序员与产品经理的斗争，在科研中作为初学者，并且手动实现了不少的内容之后，读者不难更加关注于代码能力，而对于那些指使别人工作的人嗤之以鼻，然而项目管理合作能力同样非常重要。&lt;/p&gt;
&lt;p&gt;按照笔者的理解，学界存在从低到高不同的能力，这些能力做到极致都可以具有深远的影响力，当然之所以一些拥有所谓高级能力的人还会受到你的鄙夷，主要还是因为他们实际上哪一点都没有做好。&lt;/p&gt;
&lt;p&gt;首先就是工程能力，这包括代码以及其他的计算机基础知识在内的一系列能力，比如你是否可以快速用程序实现一些表述清晰但没有开源的算法，或者是否掌握在大规模集群上训练模型的方法。工程能力可以说是一切的基石，可以加速你的方法迭代速度，甚至并行不同的项目。&lt;/p&gt;
&lt;p&gt;其次就是对于整个领域的理解能力，这包括一些直觉，当前的领域中哪些问题是核心的，当前的问题是否可以通过某些方法解决，当前的参数应该向哪些方向调试，当前的模型中哪些组件是更加必要的。这些内容一些可以通过广泛阅读其他人的博客来进行领悟，而更多的则是建立在自己广泛的积累、大量的论文阅读以及大量的实验的基础之上的。&lt;/p&gt;
&lt;p&gt;最后则是项目管理能力。实际上在科研领域中的管理型人才简直少之又少，不少人可以在多人之间进行一次正常的合作，但是更广泛的并行则难以处理。一个正常的现象是，伴随着对于领域的理解深入，读者会越发发现领域实际上千疮百孔，存在若干有待解决的问题，而其中也有不少的问题是读者有 idea 可以去解决的。Idea 的形成速度会更快，而在 insight 的帮助下，一些有价值的 idea 可以被筛选出来，但是却没有时间实现。这时候如何 involve 更多的人力，说服他们在做有价值的事情，让他们加入你的项目，而非合作，管理更多并行的项目，充分利用每一个人的能力，这是一门很讲究的学问。&lt;/p&gt;
&lt;p&gt;在中稿之后，可以说你的科研之路才刚刚开始，在领域中不断探索，并且关注这三个能力，对于那些做的比你好的人，请教，或者观察他们是如何做到的，并且不断锻炼自己的能力，这就是一切的核心。&lt;/p&gt;
&lt;h3&gt;一条可能的时间线&lt;/h3&gt;
&lt;p&gt;既然给出了上述全部的学习内容，那么在这里同样可以给出一条可能的最卷的时间线来给读者进行参考，值得注意的是，这一时间线非常具有挑战性，对于大多数同学来说可以说是难以完成的，在这里仅作为参考，事实上读者可以与本时间线具有半年以内的误差。&lt;/p&gt;
&lt;p&gt;在当前的学术环境下，论文投稿的命中与否有相当的随机性，没有任何一篇论文能确保绝对被录用。因此，一篇稿件往往需要经历反复修改与数次投稿的“滚动（Rolling）”过程，从投出到获得结果，单次周期通常为三到四个月。一篇质量正常的论文，想要成功发表，经历一到三次投稿是常态。&lt;/p&gt;
&lt;p&gt;如果我们设定一个目标——在大二学年结束时，能拥有一篇被顶级会议录用的论文，那么便可以此为基准倒推出一条清晰的路径。要实现这一目标，按照两到三次的滚动投稿周期来保守计算，首次投稿的时间点最晚不能迟于大二上学期的期末。这意味着，你必须在整个大二上学期就已在课题组内进行实质性的科研工作。顺理成章地，你最晚需要在大一结束后的暑假申请并进入一个合适的课题组。&lt;/p&gt;
&lt;p&gt;当然，这里需要澄清一个事实：对于本校学生而言，校内科研经历并非申请外校顶尖课题组的绝对前提。本校的声誉在多数国内顶尖高校（如清北华五）中仍具备一定影响力，因此，部分学业排名顶尖的学生，即使没有任何科研经历，也可能仅凭优异的成绩在大一结束后直接申请到这些高校的优秀课题组进行远程实习。不过，这条捷径对个人能力的要求更高，它意味着你必须在短短一年内，不仅掌握所有必需的基础知识，还要在某个特定领域建立起超越同龄人的学术视野。&lt;/p&gt;
&lt;p&gt;具体来说，这趟旅程始于入学前的那个暑假。你需要在此时到大一上的前半学期，系统性地学完数理基础和Python编程。在学期的后半段，则应开始深入学习 CS231n等核心课程，将重心放在掌握扎实的理论与基本的编程实践能力上。进入大一下学期后，重心将全面转向实践探索。你需要花费几周到一个月的时间进行大量调研，以了解热点、找到兴趣，并在此后将主要精力投入到该领域的论文阅读中。与此同时，熟练使用Linux系统、租用服务器练习命令行、尝试拉取并跑通领域的开源项目代码，这些工程能力的培养同样至关重要，它们将为你进入课题组后提供显著的先发优势。&lt;/p&gt;
&lt;p&gt;在这一切紧锣密鼓的准备中，切不可忽视课内成绩。尽管它在长远发展中的权重或许不是最高的，但依然是各项申请中无法回避的考察项。你不必苛求自己始终维持年级第一，但将整体成绩稳定在年级前20% 是非常有必要的。&lt;/p&gt;
&lt;p&gt;当一切准备就绪，进入大二，你的科研之路便正式步入了正轨。在课题组中，加上初期的探索，你大概需要三到六个月来完成并投出自己的第一篇工作。投稿之后，切勿停下脚步等待结果，而应立即投身于后续的课题中，让研究的节奏持续下去。&lt;/p&gt;
&lt;p&gt;理想情况下，当大二结束时，你已经拥有了一篇录用论文。以此为敲门砖，你便可以开始申请自己真正心仪的、计划在博士期间长久投入的课题组，并争取在暑假前往进行一段线下实习。线下实习的体验是远程无法比拟的，它能让你更深入地融入团队，与导师和同门建立更紧密的关系。&lt;/p&gt;
&lt;p&gt;随之而来的大三，便是正常的科研产出阶段，你需要不断地产出新工作，并积极参加夏令营以获得正式的录取意向。最终，在大四进入推免季时，按部就班地填报系统，完成这趟漫长旅程的最后一步。&lt;/p&gt;
&lt;p&gt;最后必须重申，这终究只是一条仅供参考的时间线。每个人的旅程都会有顺境与波折，这都是正常的。你完全无需为此感到紧张，只需按照这样一个大致的节奏，不疾不徐，稳步前行，就一定能获得很好的结果。&lt;/p&gt;
&lt;h2&gt;保研二三事&lt;/h2&gt;
&lt;p&gt;假如读者事先了解过大学升学的激烈竞争，保研自然也就是一个绕不过去的话题了。不需要参加考试就可以获得研究生或者直博生的资格，并且可以直接升往外校，听上去如此的诱人。&lt;/p&gt;
&lt;p&gt;事实上，假如我们从功利主义的角度去划分升学，无非就是两种方向，一个是科研，一个是就业。这里的科研包括高校教职也包括企业的科研岗位，就业则是类似成为大厂的程序员。在绝大多数情况下，假如参加考研，那么走的路多半是偏向于就业而非科研，上述所讲述的漫长的学习道路自然也没有必要亲自体验一番，而是可以先享受快乐的三年大学时光，之后在最后的时间内准备考研一鸣惊人；而假如想要从事科研的方向，那么保研是一条必经之路。&lt;/p&gt;
&lt;p&gt;关于保研，计算机保研交流群（绿群）是一个公益组织，包含数千人以及大量建议与实时信息，读者可以 &lt;a href=&quot;https://csbaoyan.top/&quot;&gt;前去了解&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;保研流程&lt;/h3&gt;
&lt;p&gt;读者往往会听到很多名词，比如保研本校或者保研外校，或者是保研或者直博，但是假如粗略的拆解，一切保研流程，在中国大陆都可以被分为两个环节：获取保研资格以及获得意向学校的Offer。&lt;/p&gt;
&lt;p&gt;实际上无论保研还是直博，讲究的都是一个你情我愿，也是自己的学校愿意放人，而对面的学校愿意收人。所谓自己学校愿意放人，也就是学校每年从国家收到的保研名额，会按照一定比例分配给每个专业的同学，而这个分配过程通常按照成绩进行排序，适配西交的排序流程将在后续说明。在获得保研资格之后，本质上你是获得了进入国家推免系统的资格，并且可以在里面填报志愿。而所谓的对方愿意收人，则是在系统中是否被录取，这并不是因为你需要在系统中提交一些资料并被对方认可，而是你要参加对方组织的招生活动，一般称之为夏令营以及预推免，在线下进行。&lt;/p&gt;
&lt;p&gt;所谓夏令营或者预推免，可以理解为在先后两段时间组织的线下统一选拔考试，本质上除了举办时间之外没有任何区别。这些考试分为几个阶段，全部通过并且优胜之后会获得 Offer。首先你需要填报系统，之后对方学校的老师或者教务处就会进行一轮筛选，并且向合格的同学发送邮件邀请参加线下活动，也是所谓的入营。这意味着你在对方学校的填报系统中需要提交提供吸引人的材料，这里面成绩是最基础的，而科研或者别的项目以及竞赛则是重中之重。&lt;/p&gt;
&lt;p&gt;在此之后，根据不同学校的安排，可能会存在笔试、机试或者面试中的一种或者多种考核，必须按照一定的规则选拔，轻松脱颖而出的人家获得优秀营员以及 offer。&lt;/p&gt;
&lt;p&gt;当然，事实上这些活动也有客观主观之分，也就是所谓的强 com 以及弱 com，即招生委员会在整个选拔工程中的权重。com 越强，意味着招生委员会的权重越大，考核越依赖你的纸面实力以及笔头实力，事先联系老师并不能帮助你获得offer；com 越弱，意味这个考核中老师的权重越大，先联系老师并且聊得来之后后就可能能获得Offer，老师可以在考核的任何阶段捞你，也就越依赖你的综合能力，比如说科研。或者甚至你已经事先联系了老师，并且在他的课题组中进行了很长时间的科研实习，那么在老师明确给出口头Offer之后，夏令营基本上只是走一个流程了。&lt;/p&gt;
&lt;p&gt;至此读者也就不难理解专业保研的大致情况了，而说到底，学校给予保研名额依靠的是你的绝对成绩，而意向学校给予的Offer，则更多看的是自己的综合实力。&lt;/p&gt;
&lt;h3&gt;保研细则&lt;/h3&gt;
&lt;p&gt;对于人工智能专业来说，我们作为实验班具有50%的保研率，这个比例是相较于专业内普招的同学，少年班以及其他项目则是单独计算。保研排名中的成绩由两部分组成：智育分，也就是在大学前三年的必修课考试中获得的成绩，按照每个课程的学分进行加权平均后得到的数字；而德育分则每年有基础的七十分，每学年结算一次，包括一个详细的加分规则，例如在竞赛中获奖可以累计最高加十分，参加学校组织的集体活动可以累计最高加三分。&lt;/p&gt;
&lt;p&gt;对于比较卷德育分的专业来说，往往很多同学的成绩会超过九十分以上，而根据目前的观察来说，在本专业中成绩取得超过八十五分，就已经很不错了。在前三学年全部成绩都已经确定之后，学院的教务处会统计排名并且进行公示，其中智育分和德育分按照九比一加权。&lt;/p&gt;
&lt;p&gt;值得一提的是两个可以翻盘的例外。其中之一是参加学校认可的部分科技类竞赛，并且获得国家级奖项，根据团队中的贡献比例可以获得智育分的加分，根据奖项以及贡献排名分数也有不同；而出国交换也可以获得三分加分。由于这一加分是直接加在学分绩的总分上，不难想象每一分的权重其实很大，即使在其他科目上落后同学很多分数，关键的加分也可以直接翻盘。&lt;/p&gt;
&lt;h3&gt;两种流派&lt;/h3&gt;
&lt;p&gt;从此我们不难梳理两种主要的保研流派。&lt;/p&gt;
&lt;p&gt;第一种即主要参与强 com 的夏令营，你需要在前期的学习过程中保持成绩的优异，例如年级前五，同时适当参与一些竞赛来充实你的简历。同时你需要在平时经常联系代码能力，甚至参加 ICPC 在内的一系列算法竞赛来锻炼你的算法能力，在强 com 中，机试是最常见的考核内容。科研是可供选择的，可以作为加分项，而过于提前联系老师也是如此，因为强 com 重点还是在于硬实力能否通过层层选拔。这种类型适合就业向的选择，也就是大多数时候攻读硕士学位，不需要提前进组参与科研，而是关注老师是否可以放实习等内容。&lt;/p&gt;
&lt;p&gt;第二种即主要参与弱 com 的夏令营，甚至只参加一个夏令营，即你早已经提前进组的老师所在的课题组给你发的 offer 的夏令营。这方面的发展方式则如上面提及的发展路线一样，重点在于凭借科研能力提前进组，并且获得老师的 return offer。对于这一类来说，关键的重点完全在于科研，成绩甚至仅需要保持在可以获得预推免资格即可。在这里同样给出简历上各种内容的含金量评级，与老师 match 的顶会论文 &gt;&gt; 顶会论文 ≈ ICPC 金牌 &gt; 特定领域的特定比赛（如机器人领域的 RM/RC 对于对应领域的老师） &gt;&gt; 其他的水赛（包括各种互联网+/挑战杯/数学建模）。&lt;/p&gt;
&lt;p&gt;当然，在这里一个事实是，出于对于教务处的安排，人工智能专业的专业实习横穿各大高校夏令营，并且难以请假，同时专业内部也并没有算法竞赛的风气，导致强 com 的路线在最近更是少之又少。而放眼更加广泛的范围内，弱 com 也在逐渐占据主流，毕竟老师希望招收和自己更加 match 而且磨合了一段时间的学生，这也是本文出于笔者个人的推荐，即按照弱 com 路线进行发展。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;在经历了三个多月的思考以及迭代之后，从 2025 年的六月份到九月份，我终于完成了这篇漫长的博客。&lt;/p&gt;
&lt;p&gt;长久以来，我一直受到开源精神的影响，和非常多的同学交流学习感悟以及方法。信息是一个很有趣的东西，它具有极高的价值，但是却可以进行不断地复制，并且分享给更多的人，而在分享的过程中，我也可以受益。就像之前提问的智慧中提到的，我也喜欢一个好的问题，再教授别人一些细节知识的同时，他们的提问可以帮助我更好的思考，并且凝练出更加关键的信息。&lt;/p&gt;
&lt;p&gt;与此同时，在与众多同学进行若干交流之后，我发现大多数的内容都是具有一定共性的，那些通用的经验，为什么不以一种更加通用的形式进行传播？于是有了西安交大生存指南。再时候的故事就像前言中提到的那样，一个泛而全的指南某种程度上是不切实际的，而聚焦在我擅长的领域，帮助与我更加相关的那一批人，这篇博客随之诞生。&lt;/p&gt;
&lt;p&gt;三年时间说不上长，但在其中摸爬滚打也很难说不坎坷。不得不承认，即使掌握了一些方法，我在学习的过程中仍走了不少弯路，而后来者假如按照迭代出的最短路径继续前进，能否获得更辉煌的成就，这是我由衷好奇的。&lt;/p&gt;
&lt;p&gt;假如你刚刚读到这篇文章，并且有志于此，那就继续努力向前探索吧，毕竟时间尚早。&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Change Logs&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;2025-10-04.&lt;/strong&gt; 在朋友的建议下，在文章中添加了更多的外链，它们指向了同样优秀的博客或者文章。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2025-09-29.&lt;/strong&gt; 补全了剩余的内容，包括对于科研过程的详细展开论述。对于错别字等的修正。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2025-06-16.&lt;/strong&gt; 最初的文章，将西安交大生存指南中的内容，以及关于人工智能专业详细的学习路线进行了整合，并且添加了一些关于保研以及科研的内容。&lt;/li&gt;
&lt;/ol&gt;</content:encoded><h:img src="https://picr2.axi404.top/advise-zh.webp"/><enclosure url="https://picr2.axi404.top/advise-zh.webp"/></item><item><title>本科生代表致新生发言</title><link>https://axi404.github.io/blog/speech-undergraduate</link><guid isPermaLink="true">https://axi404.github.io/blog/speech-undergraduate</guid><description>在老师的邀请下，我在大一新生入学的典礼上作为本科生代表进行了发言，这里是发言稿。</description><pubDate>Thu, 25 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在老师的邀请下，我在大一新生入学的典礼上作为本科生代表进行了发言，如此想来还是会觉得有些不可思议，曾经大一如此迷茫，第一次考试一败涂地，到如今作为本科生代表进行发言，似乎时间并没有过去多久，但是日拱一卒，还是成为了现在的我。对于 Senior 一些的读者来说，部分的内容是稚嫩的，也欢迎在评论区指正~&lt;/p&gt;
&lt;h2&gt;正文&lt;/h2&gt;
&lt;p&gt;老师们同学们大家好，很高兴有机会作为本科生代表和大家在这里分享我的经历，以及一些见解。&lt;/p&gt;
&lt;p&gt;简单介绍一下我自己，我是来自人工智能 2202 的高宁，专业综合排名第四。在我前三年的大学时光中，我充分参与了竞赛与科研。在科研中，我发表过两篇顶会一作，并且以核心贡献者参与了上海人工智能实验室众多技术报告与开源项目。同时我还连续三年作为学校 RoboMaster 队伍的视觉组组长，带领队伍获得若干国奖省奖。同时还创建了 Lumina 具身智能社区，并且维护有数千 Star 的 Github 项目。这里是我的个人主页（&lt;a href=&quot;https://axi404.top&quot;&gt;https://axi404.top&lt;/a&gt;），我平时会有论文阅读以及技术和学习经验分享放在上面，欢迎大家关注。&lt;/p&gt;
&lt;p&gt;我今天来这里，主要是和大家，尤其是本科生的新生们来分享一下，如何去卷。卷，这个词汇听上去确实很糟糕，美好的大学生活，丰富的社团活动，充满激情的青春与友谊，大学确实有着无数的可能性，而人工智能专业的下限让你们在市场上仍然可以保持一定的竞争力，只要不因为挂科被淘汰。如何享受生活是每个人都会的，因此让我来讲讲另一个极端，也就是如何全身心投入在学习中并且有所产出。不过留给同学们的，不是如何去按照这些内容去执行，而是去思考，尝试在这两个端点之间找到自己的舒适区，找到你的那个平衡点。&lt;/p&gt;
&lt;p&gt;第一件事情在于前瞻性。在我刚刚入学的时候是 2022 年，那时候 ChatGPT 还没有横空出世，大多数人对于人工智能的了解还局限于 AlohaGo，只是在某个智力游戏中取得优胜的程序，而后在十一月份的末尾，ChatGPT 横空出世，一切都变了，一切都按下了快进键。细想起来这是一件很恐怖的事情，从 2022 年到现在，其实也就过去了区区三年，而有关人工智能，甚至计算机领域或者世界，居然已经发生了如此大的变化。你们现在才刚刚入学，而假如说从事科研，则需要至少直博，国内的直博一般以五年计算，那么你们面对的是未来九年之后的未知，说实话，这确实是难以想象的。面对这个飞速演进的世界，我们能做的，也就只有充足地规划，然后留足容错空间。以保研外校举例，当然，事实上西交本身已经是非常优秀的平台，可以托举你们进行非常不错的科研探索。每年的大四此时大约确定Offer，为了稳妥起见，那么提前半年进组，保持竞争力则要有一篇论文发表，论文的周期大约是四五个月，加上运气因素可能的 rolling，往多算可能要一年，而课题的完成，假如由自己主持，则少说半年，也就是大二开始进组科研。在此之前，你已经需要学习大量的基础知识并且广泛地阅读论文。乍一看，你们才大一，还有充分的时间探索这个世界，但是细看下来，时间确实十分紧张。前瞻性是一个很难获得的能力，而缓解前瞻性不足的重点在于规划好每一步，为未来留足容错空间。当然，说到提前规划，如何了解需要学习什么，需要面对什么，我写过一篇博客，叫做致新生的你（&lt;a href=&quot;https://axi404.top/blog/advise&quot;&gt;https://axi404.top/blog/advise&lt;/a&gt;），欢迎大家关注。&lt;/p&gt;
&lt;p&gt;第二件事在于第一性原理，找到什么事情是必须的。假如说你按照本科教学培养方案去看，你会发现这是何其长的一条路线，对于按部就班学习的同学，这套方案可以让你在毕业的时候全面认知人工智能以及周边的一系列领域，并具有一定的知识储备，但一旦你确定了某一方向，并且确切按照每一步必须的路线前进时，你会发现，你很轻易地将任何高校的培养方案都远远甩在了身后。伴随着技术的发展，人工智能的主要领域的入门门槛越来越低，如今你不需要学习 CUDA 编程，不需要手动实现反向传播，甚至在 hugging face 的加持下，不到 30 行代码就可以让你流畅运行大多数的主流大模型。君子生非异也，善假于物也，通往深度学习的大树曾经枝繁叶茂也布满荆棘，而在技术和开源社区的发展之后，这棵大树早已已经被剪枝干净，定睛看去，只剩下最核心的主线屹立在那里，通向你感兴趣的领域。当然，抛弃不必要的内容也是必须的，假如你心向科研，那就没有必要在美赛或者各种竞赛上去浪费时间，在德育分中反复挣扎，抓住最核心的事情，并且做到极致，这才是一切的关键。你需要进行的事情，就是充分的调查以及思考，从第一性原理出发，想清楚自己要学什么，再开始进一步的行动。&lt;/p&gt;
&lt;p&gt;第三件事在于保持激情。历届以来非常多的同学向我请教过不少的问题，我向来愿意向大家展示最短的路径，如何去学习，需要学什么，然而并不是每一位同学在听后都可以获得学业上的成功。这听上去是一件很正常的事情，不是每个人都可以取得成功，但是问题的关键是，为什么，在众多取得了更大进步的同学中有什么共性。我认为关键在于持续学习，始终对于知识保持热爱与激情，如果你对于科研有兴趣，那么每天阅读 Arxiv 全部的论文是一种乐趣；如果你对于软件工程有兴趣，那么在冗杂的工程文件中耗上一整天是一种享受。同时，量化一切也是有必要的，在长久的学习过程中，很少有人可以自己一直上进，而量化一切带来的好处在于获得正反馈。学习完一门学科或者了解一个领域需要漫长的时间，这期间的煎熬会带来持续的负反馈，一个好的方法是记笔记，不是为了在将来复习的时候使用到，而是在加固你的记忆的同时，随着笔记数量的积累，你会有一个直观的感受，自己这段时间究竟学到了多少东西。我现在每周都会记录周记，来提醒我自己，相较于上一周，我究竟取得了多少进步，而在大学的前两年，我维护了一个自己的列表，我每一天都会将今天学习的内容写到上面，七百三十天，日拱一卒，每一天都相较于前一天有新的进步。&lt;/p&gt;
&lt;p&gt;讲了这么多，有不少都是我引以为傲的心法，这些内容看上去是废话，但是实际做到，实际用心去思考并不是那么简单。当然，假如你没有听进去，至少我向你们传播了焦虑，而焦虑在将来的某一天也会作为动力，让你们在学习的道路上前进的更远。需要记住的是，时间是紧迫的，every second counts，而留给我们的时间并没有想象中的那么多。所以，从第一性原理出发，找到自己的需要什么，然后保持前瞻性，并且提前规划，留好容错，之后，在自己计划的道路上持续学习，保持激情。假如说你对于科研有兴趣，欢迎私下来找我聊聊，我也很好奇大家知道了相对更加本质的路径之后，会有怎样的规划，如果你不知道如何去学，也可以看看我的博客。以上就是这次分享的全部，感谢大家的聆听。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/1755457065338_107799122_p0.webp"/><enclosure url="https://picr2.axi404.top/1755457065338_107799122_p0.webp"/></item><item><title>周记 Week64</title><link>https://axi404.github.io/blog/week-64</link><guid isPermaLink="true">https://axi404.github.io/blog/week-64</guid><description>2025-09-15 ~ 2025-09-21.</description><pubDate>Thu, 25 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一周可以说是很多的终点，也是很多的起点。从上海回来之后，我确实开始经历严重的 burn out，难以集中注意力，但是在科研上以及学业上还是有着不少的东西需要去做，自然也就只能强打精神去做了。Burn out 的一个严重体现在于我几乎难以集中精力超过四个小时就需要进行一段时间的休息，而且其他时间就只能保持在注意力涣散的状态中，好在目前大多数事情也不需要动脑，不过想必事后我是需要一段时间的休息了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这周依然是在做技术报告，而且可以说已经做得相当不错了，至少将一段时间内，我们的努力都进行了一些体现，我这边还是惯例负责图、视频以及网页的打磨。&lt;/p&gt;
&lt;p&gt;此时此刻，我们的技术报告，&lt;a href=&quot;https://internrobotics.github.io/internvla-m1.github.io/&quot;&gt;InternVLA-M1&lt;/a&gt; 应该放出来了一段时间了，不过目前还没有挂在 arxiv 就是了，现如今还是在 GitHub 的仓库，因为有一些文字表述和其他东西需要进一步润色，不过至少内容就是你看到了这些。我一直会期望我去参与以及投入的事情是必须的，之前的 GenManip 为我提供了可观的数据工厂及闭环验证，尽管现在似乎在技能方面略有欠缺，但这其实本质上只是我是否愿意投入时间而已，而 M1 同样如此。&lt;/p&gt;
&lt;p&gt;就像我之前说过的一样，目前的 VLA 发展主要包含两个路线，一条路线更加看重泛化，而另外一条则看重动作能力，新款大家在设计模型的时候想要两者兼得，但从根本思路上来说肯定有所偏好，M1 在其中就更加偏向于泛化。事实上其实这主要是因为受到了设备以及数据的限制，我们并没有进行大量的跨本体实验，而是在最后补充了一个在 aloha 上面的实验，但也就点到为止了，虽是史上这个任务，在我看来完成的相当不错，我们的模型其实也具有在动作能力上进一步研究的潜力。&lt;/p&gt;
&lt;p&gt;说回到泛化，在当前的数据规模下，直接获得泛化的模型几乎是不可能的，如何 leverage VLM 的能力到 VLA，可以说是唯一的重要问题以及道路。在此之前的工作实际上已经讨论了不同的训练策略，以及是否对模型进行协同训练，即将 VLM 以及 VLA 的数据在一个阶段的共同作为模型的损失进行监督，我们在这方面用了简洁且有效的方法同样进行实现，而更关键的是，在 large scale 的测试中证明了显著的有效性（比如说 SimplerEnv 上提高了将近 10 个点），同时代码可复用、开源。&lt;/p&gt;
&lt;p&gt;当然，显然的事情是，尽管 tech report 正式发布了，但是还有漫长的 polish 需要去做，同时 GenManip 也需要和 M1 一起联动，将我们的 large scale 的 pick and place benchmark 放出去。这里也就是我之前所说的，在 benchmark 这部分，尤其是大家越来越意识到 sim2real gap，以及类似的很多问题之后，真机进行实验明显是更加高效且实用的方法，尽管有的时候公平性难以保证。公平性与可复现性绝非大家使用仿真的原因，那么可能的原因是什么，之前提及过我的思考，这里就是体现，如何测试 large scale 的 instruction following 问题。比如说我们的 benchmark 提供了 200 个 pick and place 任务，包括背景物体在内有 3000 个物体，这种规模是真机难以比拟的。测试自己的模型尚可，但是不同的模型都进行这样的测试，成本肯定太大了。&lt;/p&gt;
&lt;p&gt;当然，我们的 report 也会改成 ICLR paper，这部分又是另外的故事了，下一周又要扑在上面。毕竟大多数的 work 已经结束了，剩下的主要是 presentation 的部分，而这部分恰好是我需要更加专注负责的，尽力吧。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;至于生活，还是可以的，依然是正常的学习，以及和乐小姐的二人世界～&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-64-zh.webp"/><enclosure url="https://picr2.axi404.top/week-64-zh.webp"/></item><item><title>周记 Week63</title><link>https://axi404.github.io/blog/week-63</link><guid isPermaLink="true">https://axi404.github.io/blog/week-63</guid><description>2025-09-08 ~ 2025-09-14.</description><pubDate>Mon, 22 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一次总共也是过了很久，来记录一些内容。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这一周总体来说，貌似也没有做很多事情，主要还是继续参与技术报告的写作，整体因为很多东西都要更新以及修改，包括仿真部分的描述，以及许多的图和最后的视频，这一周的时间也就基本上在这上面了。&lt;/p&gt;
&lt;p&gt;主要还是因为远程的原因，加上最近社团这边正在招新，所以说网速很慢，因此远程连接也很卡，自然没有很多心情去进行代码的修缮工作。&lt;/p&gt;
&lt;p&gt;与此同时的话也是做了这次技术报告的主页，在这段时间的锻炼之下，可以说我也培养了一些审美，以及发现确实主页制作这方面，我还是具有一些技术栈的，这导致在大多数的论文主页都是按照固定的模板来创建的同时，我却可以更高效的进行自定义，并且给出更美观的呈现。&lt;/p&gt;
&lt;p&gt;因为下一周技术报告就要发布了，所以说这些内容虽然写出来不多，但是各种细节的修缮还是改良了非常多次，基本大概如此。&lt;/p&gt;
&lt;p&gt;当然，虽然说没有修缮很多程序，但是 GenManip 我还是按照 Google format 进行了更新，现在大多数程序看上去会更加顺眼一些，同时我发现适当的空行也会让程序的可读性更高。在此之前我的风格其实一直是连续的不加注释的代码，在我看来大量密集的程序十分的顺眼，然而对于其他的用户或许就并非如此了，因此这一习惯还是要改。不过对于日志应该如何优雅的进行添加，还不会导致程序的可读性变差，我现在还没有想出来好方法，因此目前还是使用尽量少的日志来记录必要内容，而不是事无巨细进行呈现。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;现在也是开始了正式的上课，说起来这一点也很有趣，虽然只是上完第一周课，但是不少课程只有四周，所以细想下来倒是上完了四分之一，可以预料的，未来的日子确实会轻松不少。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-63-zh.webp"/><enclosure url="https://picr2.axi404.top/week-63-zh.webp"/></item><item><title>周记 Week62</title><link>https://axi404.github.io/blog/week-62</link><guid isPermaLink="true">https://axi404.github.io/blog/week-62</guid><description>2025-09-01 ~ 2025-09-07.</description><pubDate>Sun, 14 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以又是比较久以后的一次周记，今天算是下一周的周日，这个习惯并不是很好，我也吐槽过自己很多次了，但是从目前的进度上来看，确实只能如此。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;开始的几天在上海继续苟活了之后，也是回到西安进行像以前一样的 remote 了，讲实话，这并不是一件简单的事情。因为远程导致卡顿，加上最近社团的招新正在如火如荼的进行，社团这边的同学很多，网络也不是很流畅，让我写程序的意愿也下降了很多。&lt;/p&gt;
&lt;p&gt;好在还是姑且做了不少事情，首先还是继续画图，包括各种各样的 teaser 以及柱状图，同时也将之前的 demo 视频改成了一个更加完整的版本。&lt;/p&gt;
&lt;p&gt;对于科研来说，基本上主要的能力无外乎几种，想 idea 的能力，实现 idea 即 coding 的能力，以及 presentation 的能力。&lt;/p&gt;
&lt;p&gt;虽然我也很难想象出来一个非常惊世骇俗的 idea，但是在这个主流仍是 scaling 的时代，architecture 的创新一方面是很难，一方面是很难流行，而除此之外的大多数 idea，我仍勉强能跟上思路，并且有自己提出的有趣角度。&lt;/p&gt;
&lt;p&gt;而在一年的仿真过程中，我的 coding 能力可以说也是被极大地历练了，包括对于整体结构的设计，快速适应新的开发规范，以及更加细节的代码习惯，现在只需要如闪电般归来到模型领域，就可以实现闭环。&lt;/p&gt;
&lt;p&gt;不过对于论文来说，在此之前可以说我参与并不是很多，也需要一些审美的积累，对于故事整体脉络的修缮，我尚且可以把握，但是在此之前我的画图水平实在是一言难尽。这一次技术报告可以说让我在画图和 demo 制作方面也极大地增长了能力，具体水平如何，读者们也可以等到工作放之后欣赏一二，个人感觉还是在技术报告的平均水平上。&lt;/p&gt;
&lt;p&gt;GenManip 的 cleanup 工作还在有条不紊的进行过程中，这里面包括格式规范以及相关的注释，包括更多的对程序进行封装和切分，来实现更加易懂的设计。只可惜本来认为我这边后面没什么很多事情，可以花费大量的时间在修缮 GenManip，从而在九月的上半个月完成这些内容，不过如今来看显然是天方夜谭了，不过无论如何也是要在九月份完成的，目前整体程序的 cleanup 已经基本形成，最外层的注释已经添加完毕，来解释清楚整体的工作流程，同时对应的使用则不需要深入代码内部，对于用户来说已经十分友好，无需担心。&lt;/p&gt;
&lt;p&gt;我大概会留给自己三天时间，来进行文档的撰写，并且确保易用性，比如说提供一键下载资产的脚本，同时解释清楚数据的格式，也就是说留给我的时间大概还有两周，我会加入一些新功能，并且保证注释更加清晰。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;回到西安，可以说最大的幸事就是又可以简单朝思暮想的乐小姐了，也是一起愉快地吃饭然后聊天。闲暇之余不禁感慨，劳累的日子究竟何时才能结束，何时才能迎来内心的宁静。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-62-zh.webp"/><enclosure url="https://picr2.axi404.top/week-62-zh.webp"/></item><item><title>Paper Reading: 3D Reconstruction</title><link>https://axi404.github.io/blog/paper-reading-3r1</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-3r1</guid><description>一些 3D Reconstruction 相关的论文阅读。</description><pubDate>Mon, 08 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;DUSt3R&lt;/h2&gt;
&lt;h2&gt;Fast3R&lt;/h2&gt;
&lt;h2&gt;MonST3R&lt;/h2&gt;
&lt;h2&gt;VGGT&lt;/h2&gt;
&lt;h2&gt;STream3R&lt;/h2&gt;
&lt;h2&gt;$\pi^3$&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/1755456716559_102194214_p0.webp"/><enclosure url="https://picr2.axi404.top/1755456716559_102194214_p0.webp"/></item><item><title>周记 Week61</title><link>https://axi404.github.io/blog/week-61</link><guid isPermaLink="true">https://axi404.github.io/blog/week-61</guid><description>2025-08-25 ~ 2025-08-31.</description><pubDate>Sun, 07 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;没有太晚的记录一次周记，不过实际上也没有什么好说的，基本上一些大概的想法上一周都已经说完了，主要还是讲一下科研的心路历程。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;实验室这边的光电中心发布了 EO-1 模型，本身做的中规中矩，但是确实在 presentation 上面乍一看还是比较有质感的，属于是一眼就很有 report 的味道，本身我们中心的全部技术报告还是要等到月中的开源周才可以发出来，以及我们自己的确实也还在不断打磨，因此可以说是一下子上来了很多的压力。&lt;/p&gt;
&lt;p&gt;除此之外，这周基本上主要在做技术报告的视频以及插图，前一周已经几乎把我需要负责的文字部分都弄完了，也就是以实验部分以及数据生成部分为主的章节，还有附录里面也讲了一下 GenManip 对于闭环测试的一些细节。&lt;/p&gt;
&lt;p&gt;这一周感觉在主要工作上科研能说的事情也是如此的少，当然这从某种程度上也是因为一直集中在做一件事情，相应的想必也是可以有所回报的，毕竟确实不可置否的是，在正常的项目组中，只要付出了更多的努力，最后都会体现在 credits 上面。&lt;/p&gt;
&lt;p&gt;除此之外，我这一周也在对之前写的博客以及一些其他内容进行一个完善的回顾，这里面应该主要还是以我之前的 paper reading 为主。毕竟确实注意到在和别的同学讨论论文细节的时候，有的时候还是因为自己之前阅读的疏漏导致对于一些结论造成了误判。&lt;/p&gt;
&lt;p&gt;另外目前还在筹划的就是 Isaac 101 的剩下两个基础章节，也就是 Camera 以及 Robot，完成了这些内容之后，基本上 Isaac 相关的第一段内容就可以告一段落了，之后大概会用一个单独的章节来讲解一下 curobo，再把一切内容串联起来。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;至于生活其实也依然是老样子，下一周就要从上海回到西安了，大概收拾了一下行李，发现带来的东西好多都没有用上，比如说本来还想再闲暇的时候用 Quest3 来玩一会 VRChat，但是很明显进度并没有给我这个喘息的时间。接下来直到九月末应该都要做一些 cleanup 的工作，也不会有很长的时间让我喘息，只能说还是希望一切顺利吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-61-zh.webp"/><enclosure url="https://picr2.axi404.top/week-61-zh.webp"/></item><item><title>周记 Week60</title><link>https://axi404.github.io/blog/week-60</link><guid isPermaLink="true">https://axi404.github.io/blog/week-60</guid><description>2025-08-18 ~ 2025-08-24.</description><pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说，又是很久以后的一次周记了，甚至这次隔了两周还要更多，本身当然是因为一些各种各样的事情耽误了，不过从侧面也体现出了事态并没有变得更加简单，而且变得更加复杂了。&lt;/p&gt;
&lt;p&gt;只能说历史频繁会给我这个教训，每次我总是认为过一段时间之后就可以闲下来，但是却总又会有其他事情需要去做，我认为这某种程度上是一个态度的问题，也就是当你闲下去的时候，你是不是会相对“游手好闲”想要加入到其他事情中，还是说呆在原地沉淀一段时间，并且开始下一段旅程。&lt;/p&gt;
&lt;p&gt;具身智能是一个要求工程能力很强的学科，从另一个角度来说，就意味着这个领域对于工程开发的量级较大，也就意味着一个 idea 转化成代码、项目或者论文，所需要花费的时间越长，就意味着具有相同思维活跃度的研究者，具身智能领域的人总是有更多做不完的 idea。因此在刚刚完成 IROS Benchmark 之后，不可避免的一件事情就是我会想要继续去做下一件我认为必要的事情，而依然没有再次闲下来。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;在上个年度结束之后，近期时间是即将迎来下一次的结构调整，具体内容出于保密不能透露，但是可以预见的是，我做的部分内容后续可能很难作为主要内容去推动，或者至少在一段时间之后才可以再次找到一线生机，使得我开始思考应该何去何从。&lt;/p&gt;
&lt;p&gt;仿真是有局限性的，这是在进行了大量的时间之后，我可以得出的一个结论，除非某些基于 RL 的范式可以根据很简单的奖励函数收集到稳定的数据，否则在 dense reward 仍未被解决的当下，使用强化学习来生成大量多样的高质量数据，基本上是不可能的。&lt;/p&gt;
&lt;p&gt;使用 rule based 的方法生成数据，我确信我已经找到了 scalable 的边界，在边界的里面，开发者可以使用一套精心设计的规则来适配足够大量的物体，其中主要以操作刚体以及一自由度articulation 物体为主，在 AIGC 的加持下，这些类型的数据可以源源不断地生成，并且这些技能已经被定义好后，可以让大模型通过函数调用的方式来生成更加 make sense 的数据，从而产生更多样的任务，这些是没有任何问题的。&lt;/p&gt;
&lt;p&gt;GenManip 在经过了漫长的开发之后，已经几乎达到了这一标准，并且我也在短时间内没有向流体以及软体仿真（倒水和叠衣服）发展的想法，假如没有对内的保密要求存在，那么进行完善的代码清理以及文档撰写之后，将其彻底开源是一个更好的选择，同时借助一些 PR 的渠道，整体代码会变得相当可用，也可能会具有一定的影响力。&lt;/p&gt;
&lt;p&gt;在大量生产数据的基础上，GenManip 对于闭环验证的优雅把控也是可圈可点的，通过完善的设置来让整个框架可以以低代码的形式让用户想要自己搭建的 Benchmark，不需要对整体代码逻辑有任何了解，这也是很不错的。&lt;/p&gt;
&lt;p&gt;当然还包括了一系列的思考，事实上伴随着真机数据越来越多，仿真数据的必要性可能会越来越低，甚至在最后因为和现实的 gap 而成为噪声一样的数据，包括目前的仿真也越来越难有人去刷，大家目前集中在 simpler env，尽管我非常欣赏这篇工作，但是本身在询问了同学们之后，感觉还是点刷不上去的原因集中在引入了 real2sim 的 gap，不然从任务难度上来说，不应该成功率如此之低。为什么没有一个新的仿生平台脱颖而出？一方面对于一些所谓现在流行的 reasoning 能力，GenManip IROS Benchmark 也从这个视角出发，但是在比赛中上尚且可以，假如是平时使用，为什么不用对于审稿人也更有说服力的真机实验来做呢？有什么事情是仿真能做到反而真机做不到，我目前有一些初步的想法，后续 GenManip 假如说谈妥可以完全开源，会以 tech report 的形式发布 1.5 版本，大家可以期待。&lt;/p&gt;
&lt;p&gt;最后就是最近开始写我们组的技术报告了，我基本上还是像之前一样提供了仿真平台以对应的数据，不过还是尽量能帮一些忙就多做一些，其他同学对于真机测试的压力力也很大，在论文写作以及绘图上，我就可以做一些努力了。只能说我的画图还在锻炼过程中，希望经过这一次之后可以获得更多的进步。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活上就几乎没有什么可以说的了，还是和乐小姐每天打电话聊天，然后每天都坚持上班，甚至因为要赶一些成果出来，这周末的两天都和 mentor 一起在实验室加班。周日下班之后出门，我们两个人都顺手打开了飞书，想要打卡下班，但是飞书却弹出了极速上班打卡的提示，原来已经四点了。&lt;/p&gt;
&lt;p&gt;应该再过两周就要回学校了，一些加分以及保研的材料我也在努力准备，希望一切顺利。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-60-zh.webp"/><enclosure url="https://picr2.axi404.top/week-60-zh.webp"/></item><item><title>Paper Reading: Embodied AI 3</title><link>https://axi404.github.io/blog/paper-reading-eai3</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-eai3</guid><description>从一些 Embodied AI 相关工作中扫过。</description><pubDate>Tue, 22 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Embodied AI Paper Reading&apos;,
items: [
{
title: &apos;Batch 1&apos;,
href: &apos;/blog/paper-reading-eai1&apos;,
order: &apos;1&apos;
},
{
title: &apos;Batch 2&apos;,
href: &apos;/blog/paper-reading-eai2&apos;,
order: &apos;2&apos;
},
{
title: &apos;Batch 3&apos;,
href: &apos;/blog/paper-reading-eai3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Batch 4&apos;,
href: &apos;/blog/paper-reading-eai4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Batch 5&apos;,
href: &apos;/blog/paper-reading-eai5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Batch 6&apos;,
href: &apos;/blog/paper-reading-eai6&apos;,
order: &apos;6&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;A_0&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/6a7896d0170d3138776360d91bdf74f2.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;A_0 某种程度上算是类似于 RoboPoint 类似的思路。在之前的工作中使用了 Point 以及 Bounding box 作为输出，而 A_0 则是使用了 2D 的离散轨迹点作为输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/qjcxnvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;整体的框架如图所示，使用了 Qwen 2.5 作为 Language Encoder，并且使用 SigLip 作为 Vision Encoder，同时取 $I_t$ 以及 $I_{t-1}$ 的编码后的 Token 来加强运动信息的 perception。&lt;/p&gt;
&lt;p&gt;之后就不难理解了，使用这些内容作为 condition，用 Diffusion 预测 2D 的稀疏轨迹，并且在大规模（~1M）的数据上进行预训练。在部署的时候，用 A_0 预测了 2D trace 之后，用 GraspNet 预测第一帧的 Grasp Pose，之后的用 VLM 判断高度，并且进行平滑，从而形成连续的轨迹，但是可想而知的是，limitations 可能在于执行一些更加灵巧操作的任务，还是需要下游是一个模型来进行训练，才可以表征。&lt;/p&gt;
&lt;h2&gt;OneTwoVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Wf30OvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;OneTwoVLA 算是比较有意思的 VLA 工作，可以自由切换 Reasoning 和 Action 的能力，本身就是根据输出 Begin Token，也就是 &lt;code&gt;BOR&lt;/code&gt; 和 &lt;code&gt;BOA&lt;/code&gt; 来决定处于哪个模式。本身假如说每一个 Action Chunk 执行完之后，还没有执行完 Action，VLM 那一部分会直接输出 BOA 然后继续输出 Action。&lt;/p&gt;
&lt;p&gt;整体的模型的流程就是用 Pi0 的方式，但是支持了动态切换 Reasoning 和 Action。本身 Reasoning 的内容也值得参考，包括四类：详细场景描述，即突出任务相关物体的位置；高层计划，即逐步列出完成任务的操作；历史总结，即保持上下文感知；下一步计划，即接下来需要执行的操作。&lt;/p&gt;
&lt;p&gt;本身方法还提供了一个生成 Reasoning 数据的 Pipeline，可以理解为就是 Gemini 负责 Reasoning，然后 Flux 1.x 负责图片生成。&lt;/p&gt;
&lt;h2&gt;Diffusion Policy&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ACCYnvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;经典之作。用 Diffusion 做 Action 预测，输入是 Noise，输出是 Action Chunk，支持 CNN 以及 DiT 的两种变体。&lt;/p&gt;
&lt;p&gt;鉴于 Diffusion 可以一次性输出一个 Action Chunk，在某种程度上推理速度更快，同时也支持 Condition，因此各种如 VLM 的 Hidden state 等可以被用作 Condition，从而在整个的 Manipulation 领域中被广泛应用。&lt;/p&gt;
&lt;h2&gt;RDT-1B&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/OGEZnvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RDT-1B 本质上就是在多本体的数据集上训练的超大 DP。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4c5603428d467b4b871ff640e7413b05.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于不同的本体，RDT 使用了叫做 Unify Action Space 的方案，也就是将不同本体的 Action 映射到一个很长的 Action Space 上的不同位置上。在此基础上，RDT-1B 依然是使用 DP 的范式，通过一个 Transformer 来预测 Action Chunk。&lt;/p&gt;
&lt;h2&gt;Octo&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/53a78d1b358d8fa0ff07f157196712df.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Octo 本身也是比较正常的设计。语言过 Text encoder，图像过 CNN，组成 OBS Chunk，可变长度，输出 Readout Token，类似于一种 hidden state，后面接一个 action Head。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/uw9dovK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在切换本体之类的时候，直接换 action head 就好，同时也因此可以在多本体的数据集上进行训练。&lt;/p&gt;
&lt;h2&gt;CogACT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/7TxeovK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;CogACT 本身算是在 TinyVLA 等一系列的 Pi-like 的工作中对于架构的另一次探索。本身 CogACT 遵循的思想，本质上还是希望尽可能 decouple VLM 和 DP 各自的职能。因此从架构上来看，CogACT 本身就是 VLM 之后输出包含一个 Cog Feature，说白了就是一个类似于 bert 的 cls token 一样的东西，然后和 noise 一起作为输入来让一个 DiT 的 DP 来生成 Action Chunk。&lt;/p&gt;
&lt;p&gt;CogACT 本身在 Simpler Env 的 Benchmark 上取得了非常 impressive 的性能，证明了其可以在 OXE 级别的较大的数据集上进行训练并且在多任务中取得很高的性能，在此之后的一系列模型都在 Simpler 上进行训练，并且遵循 Pi-like 的范式，很大程度上也是因 CogACT 而起的。&lt;/p&gt;
&lt;h2&gt;Helix&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/07CEovK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Helix 属于是双系统中较早的，两个模型分别推理，频率不同。从 demo 上，一个模型控制两个上身，比较 impressive，但是没有相关的 report，不评价。&lt;/p&gt;
&lt;h2&gt;FAST&lt;/h2&gt;
&lt;p&gt;因为离散 Action 进行量化的方法生成的 Action token 效果不好，尤其是采样率越高，越容易退化为复读机，因此提出了 FAST Tokenizer。流程是先归一化 Action，之后 DCT 变换，按低频优先顺序展平不同维度的数据，然后用 BPE 压缩为稠密，这些操作都是可逆的。在经历上述操作之后，收敛效率更高，性能不掉。&lt;/p&gt;
&lt;p&gt;FAST 本身在 Action token 的 embedding 上做出了新的范式，而并非简单的量化，其中的数学细节对于感兴趣的读者可以自行阅读，本身这一模块已经进行了封装，可以直接使用。&lt;/p&gt;
&lt;h2&gt;Magma&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/bf3c5d3f8109beb470286346e05bd525.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Magma 是较早提出 Generalist VLA 的模型。尽管在此之前，从 OpenVLA 开始，大量的模型就已经使用了 VLM 作为 pre-training 了，但是显式提出了模型需要在 General VQA 等任务上需要进行泛化，Magma 还是较早的。&lt;/p&gt;
&lt;p&gt;本身 Magma 包含三个不同的任务，也就是多模态理解、UI 操作以及机器人操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/oIG9PvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Magma 使用 LLaMa 3 作为 Backbone 进行训练，同时提出了 SoM 以及 ToM 的表征形式，也就是使用一系列的点来进行物体以及轨迹的表征，从而统一 UI 以及 Manipualtion 的接口，最后还是在全部的数据中进行训练，并且在 Manipulation 任务中再进行 finetune。对于 Manipulation 依然是 follow 的 OpenVLA 的量化 Action 范式。&lt;/p&gt;
&lt;h2&gt;ChatVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/UygKPvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;ChatVLA 本身依然是使用了 OpenVLA 的范式，加入了共享 Transformer 但是不共享 FFN 的 MOE，这里的 MOE 是静态的路由，在不同的数据时激活不同的 Expert。&lt;/p&gt;
&lt;p&gt;ChatVLA 同时使用了两阶段训练，第一阶段只使用机器人数据，第二阶段加入推理数据 co-train。整体来说也是试图 leverage VLM 的能力到 VLA，并且性能也算说得过去。&lt;/p&gt;
&lt;h2&gt;ChatVLA-2&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/0EikPvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;ChatVLA 系列的前作的 MOE 是静态的，也就是根据训练选择激活哪些，这里的 ChatVLA2 使用动态 MOE，也就是由路由网络来选择激活哪些 FFN，并且在后面接了一个 Action Expert。这种选择逐渐显示，学界逐渐都开始发现，不能让 VLM 直接输出 Action，而是应该是双层模型，不然会有很大的代价，即损失 VLM 的通用能力。&lt;/p&gt;
&lt;p&gt;ChatVLA2 同时把本来的推理信息作为 condition 嵌入了 Action Expert 中。这次的两个阶段，第一阶段是 co-train，第二阶段 freeze VLM 然后训练 Action Expert。&lt;/p&gt;
&lt;h2&gt;RoboBrain&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/U3aMPvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RoboBrain 本质上就是加入了 Planning, Grounding 以及 Trace 数据进行 finetune 的 Qwen，加入了两个 LoRA 分别处理 Grounding 和 Trace 的任务，本身 VLM 可以做 Planning，并且在大量的数据上进行了 Finetune。不过从结果上来看，RoboBrain 本身在 Point affordance 上进行了比较多的 overfit，在一些别的任务上并不是那么的 impressive。&lt;/p&gt;
&lt;h2&gt;CoT-VLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/2atuZvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;CoT-VLA 本身还是正常的 VLA 的配置。本身的思路是先生成图片之后再使用图片来作为 condition 来生成动作。&lt;/p&gt;
&lt;p&gt;CoT-VLA 的思路看似引入了 World model，可以让模型引入对于动态的认知，使得这个想法确实看上去十分的诱人，但是事实上并非如此。首先其本身使用的是 VILA-U 这一模型，虽然说我对于 Unified 领域并不是那么了解，本身对于语言与文本模态同时在理解与生成进行 alignment 貌似已经存在 trade off，况且还要加入动作，对齐上却没有进行比较多的研究，似乎有些草率了。&lt;/p&gt;
&lt;h2&gt;Interleave-VLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PwQXZvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Interleave-VLA 本身就是在 VLA 中沿用了之前 &lt;a href=&quot;/blog/paper-reading-eai1#vima&quot;&gt;VIMA&lt;/a&gt; 的思路，大概的意思就是改成输入为 put &amp;#x3C;image of apple&gt; on the &amp;#x3C;image of plate&gt; 的 VLA，从结果上有效果。同时一个有意思的现象也体现在 teaser 中，也可以使用不同形式的文本来作为 condition 输入，除了标准的使用如 SAM2 等流程来进行 crop，还可以以如草图等内容进行输入。&lt;/p&gt;
&lt;h2&gt;Knowledge Insulating Vision-Language-Action Models&lt;/h2&gt;
&lt;p&gt;Pi0.5-KI 本身是在 Pi0 的基础上进行了一系列的 study。Pi0 在 Pre training 之后的后训练过程中禁用了梯度回传，但是依然提升了性能。Frozen VLM 的做法其实不少的模型都有做过，但是其实很少有 Pi0.5-KI 的效果。这意味着，假如说我们认为 VLM 其中包括的是多任务中的泛化能力，而 DP 中是更多的动作能力，在经过了对于 VLM 使用 FAST 编码后的离散动作进行训练后，在相当恐怖的数量级的预训练之后，Pi0 确实已经具备了非常强大的泛化能力了，使得 VLM 中就已经具有了相当多的 Action 能力，而 Action 能力也开始逐渐像 VLM 汇聚。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/fbedaae9a2355096bda6767e40c655ff.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后简明说一下值得 Highlight 的策略，一方面使用了 FAST 编码器，一方面使用了 Co-training，使用离散 action 以及 VQA 数据训练 VLM，同时使用 DP 来训练 Action，并且禁止 DP 的梯度回传。具体细节推荐读者亲自品读。&lt;/p&gt;
&lt;h2&gt;Hi Robot&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/tSZZZvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Hi Robot 本身描述了一种双系统的图景，并且做出来了有效的 Demo，不过更多是 infra 方面的设计，并没有过多的训练。简单来说，Hi Robot 也是双系统，其中 System 2 是 VLM，system 1 是 VLA，并且是两个系统同时推理并且可以异步。&lt;/p&gt;
&lt;p&gt;论文中有意思的或者可以感兴趣的反而是一些任务的设置或者说认为 system 2 需要具备的能力，假如你也对 system 2 感兴趣，可以参考。同时还包括一些如何设置模型的可供参考，比如说对于 VLM 何时被触发，可以每秒触发一次以及在有语音唤醒的时候触发。在这些方面对于同样的架构来说，target 是什么，可以参考。&lt;/p&gt;
&lt;h2&gt;Scenethesis&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/SMlazvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;因为具身这及 3D 场景资产，所以 Scenethesis 也勉强算是具身的 scope。本身出自 Nvidia，基本的思路看图就好。&lt;/p&gt;
&lt;p&gt;本身的输入是需求，先使用 AIGC 根据需求生成一张图片，之后用 VLM 输出 Scene graph，以及 retrieve 一些 3D 资产，之后进行优化，得到合理的布局。不过因为我觉得这方面本身他们也是在用 Objaverse，甚至 demo 中还出现过我常用的模型，这说明 Objaverse 确实很脏，大家选出来的资产也都大差不差了，同时在资产这方面也没有本质的突破。同时 scene graph 对于 Top 之类的关系的处理也往往不会特别合理。不过从结果上来看，demo 展现的还可以。&lt;/p&gt;
&lt;h2&gt;LBM&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/rO7CzvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;LBM 从效果上来看确实还不错，但是论文实在是有点乱。本质上就是在 DP 的基础上进行了修改，并且评估了大量的 scaling 以及预训练的效果，使用了大量的数据等，从根本的形式上类似于 RDT-1B。建议读者看下他们 &lt;a href=&quot;https://toyotaresearchinstitute.github.io/lbm1/&quot;&gt;网站&lt;/a&gt; 里的 demo，非常 impressive。&lt;/p&gt;
&lt;h2&gt;UniVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1IN2zvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;UniVLA 使用 emu3 的 tokenizer 处理文本和图像，用 FAST 处理动作，使用离散 token 进行自回归训练。在输入的时候会使用 &lt;code&gt;boi&lt;/code&gt; &lt;code&gt;eoi&lt;/code&gt; &lt;code&gt;boa&lt;/code&gt; &lt;code&gt;eoa&lt;/code&gt; 来标记图像和动作的开始和结束。本身训练用 emu3 初始化之后，先用视频进行后训练，之后再用 Action 数据微调。Action 输出的过程中包括预测 image。将三个不同的模态进行联合训练，这种看上去更加符合 Uni 的思路。&lt;/p&gt;
&lt;h2&gt;GR-3&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/fheBEwK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GR-3 本身在 VLM 后面接 Flow Matching 的 Action Head，可以理解为基本上 follow 的 Pi 的架构，因为用了 Qwen，所以看上去效果也还不错。基本上这种 Pi-like 的架构，几乎算是一个主流思路了，毕竟 VLA 一定要借助 VLM 的通用能力，而且不能破坏输入的 distribution，也就只能在 VLM 之后做文章，也就是接入 Action Head。&lt;/p&gt;
&lt;h2&gt;SmolVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/hq52EwK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Huggingface 出品，用了社区的大量数据集，说明 LeRobot 的社区生态还是不错的。本身效果看上去也还可以的。细节在于 DP 里面的交错 Attention，交替使用 Cross/Self Attention，可以提升成功率并加速推理，特别是 SA 层有助于生成更加平滑的动作序列。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/pGLDEwK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;另外一个亮点在于使用了异步并行的方式，可以提升推理速度。大概意思就是，模型推理 n 步，但是执行 k 步之后就让模型推理下一次，可能说推理花了两步对应的时间，之后将新推理的从第二步开始以及旧的里面的从第 k + 2 步开始进行聚合，在旧的用完了之后就完全执行新的。这样子确实可以让 VLA 不再一顿一顿的。&lt;/p&gt;
&lt;h2&gt;ThinkAct&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/VANfEwK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;ThinkAct 本身依然是 Pi-like 的架构，VLM 是 Qwen 2.5 7B，Action head 经过 OXE pre-train，然后第一波先用 GRPO 训练 VLM，因为会要求 VLM 输出动作的 2D 轨迹，在此基础上可以计算 Reward。之后 frozen VLM，用 Action 训练 DP。&lt;/p&gt;
&lt;h2&gt;VIDAR&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/hkpSEwK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;VIDAR 本质上就是 pretrain 了一个 Video 生成模型，然后生成图片。同时另一个模型，也就是图中的 MIDM，大概流程就是先从 Image 学出来一种 hidden 编码，然后 hidden state 去生成 action。本身论文中并没有说很多的细节，因此也很难给出更加细节的理解，不过本身这种做法显然并不是处于第一性原理，使用 Image 作为某种中间表征，必然带来了额外的学习难度，并且表征本身也不能完全 focus 在 action 信息中。&lt;/p&gt;
&lt;h2&gt;DreamVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ssaVEwK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;DreamVLA 本身还是很有意思的，预测的是动态区域以及 Depth 和 Semantic Mask，可以说学习了更多的知识。比较神奇的是，居然本身的 Transformer 用的是 GPT-2，可以说基本上都是完全重头预训练了，居然可以训练出来，不知道是否是处于什么考量使用的。本身架构上有可学习的 Set of Dream Queries。对于动作，将包含预测未来动态、深度和语义的潜在嵌入聚合成一个紧凑的动作嵌入，作为 DP 的条件输入。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-14-zh.webp"/><enclosure url="https://picr2.axi404.top/template-14-zh.webp"/></item><item><title>Paper Reading: Embodied AI 2</title><link>https://axi404.github.io/blog/paper-reading-eai2</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-eai2</guid><description>从一些 Embodied AI 相关工作中扫过。</description><pubDate>Fri, 13 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Embodied AI Paper Reading&apos;,
items: [
{
title: &apos;Batch 1&apos;,
href: &apos;/blog/paper-reading-eai1&apos;,
order: &apos;1&apos;
},
{
title: &apos;Batch 2&apos;,
href: &apos;/blog/paper-reading-eai2&apos;,
order: &apos;2&apos;
},
{
title: &apos;Batch 3&apos;,
href: &apos;/blog/paper-reading-eai3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Batch 4&apos;,
href: &apos;/blog/paper-reading-eai4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Batch 5&apos;,
href: &apos;/blog/paper-reading-eai5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Batch 6&apos;,
href: &apos;/blog/paper-reading-eai6&apos;,
order: &apos;6&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;Embodied AI 是一个比较新的领域，而且可能横跨的任务也很多，在这方面做的事情来说，可能一些和具身智能具有比较高相关度的 perception 任务，也都会放在其中。&lt;/p&gt;
&lt;h2&gt;HPT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.60u8wwy8i3.webp&quot; alt=&quot;The pipeline of HPT&quot;&gt;&lt;/p&gt;
&lt;p&gt;HPT 是 Kaiming He 团队在具身领域的新作，可以说是很直接地解决了 cross-embodiment 任务的问题，也就是使用在多模态领域中一贯使用的 projector 的思想。&lt;/p&gt;
&lt;p&gt;HPT 的思想属于是看一眼 Pipeline 图就能看懂的，就是对于不同的机器人，使用不同的 stem 把他们投影到同一个空间中，可以理解为一种机器人任务空间，然后在里面进行 transformer，之后再训练不同的 MLP 来投影回 actions。假如在具有无限多数据的情况下，这种方法确实可以简单有效地进行 scaling up 并且很好地迁移到不同的机器人上，不过令我好奇的是，这种方法居然在此之前没有人提出过。&lt;/p&gt;
&lt;p&gt;在此之前的工作中本栏目提到过一篇 &lt;a href=&quot;https://arxiv.org/abs/2402.19432&quot;&gt;Extreme Cross-Embodiment&lt;/a&gt;，然而是将不同的模态统一到了动作空间中，类似于无论是移动还是抓取，本质上都是位移以及旋转。HPT 更多还是聚焦在 manipulation 的任务中，直接且本质地给出了这个架构，并且在很多数据上进行了训练。&lt;/p&gt;
&lt;p&gt;当然，问题同样存在，在预训练的语境下，使用如此多的 Action，也并没有使用 VQA 进行训练，仅仅是这些数据，依然仅能让模型被限制在预训练后在少量任务上具有不错的性能，而很难进行更高级语义的泛化，Stem 也很难决定将哪些信息 decouple 在 Transformer 中，而哪些会残留在 Stem 中，在更换 Stem 的时候被抛弃掉。&lt;/p&gt;
&lt;h2&gt;RoboDual&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4jo3v6aymf.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RoboDual 本质上就是使用两个模型的并行运行来代替了单一的模型，用了一个大的 OpenVLA 作为一个 high-level 的模型，之后用 DiT 去做 low-level 的 policy，其中 OpenVLA 的输出作为 conditioned input 给到 DiT 中。乍一看是一个双系统的设计，当然事实上并非如此，可能说顶多是一个快慢系统，毕竟 OpenVLA 本身的输出就已经是离散 Action 了，DiT 最多是在此基础上进行了额外的插值，所以并不是一个真正意义上的双系统。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4xujm1jr7w.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;从 pipeline 中也不难看出思想的核心其实就在于把 OpenVLA 的一个 action 扩成了若干的 action，而且 DiT 负责一种扩写，而 OpenVLA 则负责泛化。不过遗憾的是，事实上 DP 的推理速度与 OpenVLA 之间并没有明显的差距，因此可能甚至不成立快慢系统的故事，不过确实包含加速，这其中与 DiT 直接输出 Action Chunk 也是有所关联的。本质上模型的框架其实和后续笔者称之为 Pi-Like，由 TinyVLA 开始的框架是一致的，不过确实从故事线上，确实双系统或者说快慢系统，是一个不错的切入点。&lt;/p&gt;
&lt;p&gt;不过当将视线放到 Pi0.5 中，其在 VLM 中也添加了离散动作作为输出，并且下游依然是 Flow Matching 的 DP，可以说也是一种呼应了。&lt;/p&gt;
&lt;h2&gt;GR-2&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/520b14ee50b77a7a0fee4e8a05617520.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GR-2 本质上就是在之前的 GR-1 的基础上的一个拓展性的工作，在更多的数据上进行了预训练以及微调，具体的方法依然不变，还是在大量视频数据里面训练一个 World Model 之后在机器人的数据里面进行动作微调，让模型学会 Action。&lt;/p&gt;
&lt;h2&gt;Humanoid Manipulation&lt;/h2&gt;
&lt;p&gt;这篇文章思想本身很直接，就是一篇对于 DP3 的 Scaling up 的论文。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/b013e6d5f9c47162a4065eee776d51b3.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;文章里面提出了一种 iDP3，也就是一个 improved 的方法，但是其实就是一些 trick 的集合，在这里也进行一下介绍。第一个就是 camera centric 的 point cloud 输入，这个应该是利好数据预处理的，而且 scaling up 也比较简单；然后就是下采样少一些；以及把视觉编码器的 MLP 变成卷积，这个应该是经验之谈，可以让输出更加平滑，也可以得到更多的编码内容；以及预测更长时间，这个自然会更好。最后从结果来看，Scaling up 的结果很好，皆大欢喜。&lt;/p&gt;
&lt;h2&gt;Surfer&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Surfer.8adam0a9tq.webp&quot; alt=&quot;Surfer&quot;&gt;&lt;/p&gt;
&lt;p&gt;Surfer 主要提出了一个 World Model，本身就是拿了两个现成的 Encoder 把任务以及图像进行了编码，然后预测下一次的 action 以及 frame。&lt;/p&gt;
&lt;p&gt;不过从 pipeline 来看，我其实确实不是很确定这个 next frame prediction 是否有效果。隔了一个网络然后来传梯度给 Action prediction module，本身是串行的，等于说加了一个模块以增加额外约束，这种方法肯定符合直觉，但是肯定拓展性以及潜力有限。&lt;/p&gt;
&lt;p&gt;监督信号还是本身加在输出 action 的模型本身会好，这种可以说是一种增加监督的 trick，但是不一定在更大规模的 scaling 中好用，毕竟效率不高，增加了一个开销很大的模型。&lt;/p&gt;
&lt;h2&gt;ACT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.eskdeuqyp.webp&quot; alt=&quot;ACT&quot;&gt;&lt;/p&gt;
&lt;p&gt;ACT 是十分经典的论文了，使用了 CVAE 的结构来运行。按照说法，CVAE 也就是 conditional VAE，使用了 VAE，并且使用图像 + joint position 作为解码器的输入。由于我没有看过 CVAE，但是大概猜测 conditional 在这个里面其实指的是 image 以及 joint position 在解码器的输入，而至于任务本身，就是重建输入到编码器里面的 action。&lt;/p&gt;
&lt;p&gt;本身论文里面有很多的细节，包括说在 VAE 编码器的时候只使用 joint position 以及在训练的时候使用 L1 损失而非 L2，在这里就不进行展开了。&lt;/p&gt;
&lt;p&gt;解码器的结构神似 DETR，包含一个编码器以及一个解码器，不过有必要指出的是，这里其实等于说一共有两个编码器，VAE 本身还有一个。ACT 可以说是比较优雅的 VAE 范式的解决方法，但是不得不说的是，VAE 甚至是传统 diffusion 的策略在当下来看都已经过时了，这种策略可能难以作为一个可以大量 scaling up 的一个策略存在。以及其实 ACT 并不是 language-conditioned 的模型。&lt;/p&gt;
&lt;h2&gt;SceneVerse&lt;/h2&gt;
&lt;p&gt;这篇工作做了一个大型的数据集，里面的全部的 scene 应该以扫描出来的为主，然后使用了不同 level 的标注，这是比较有参考价值的。也就是 Scene Level 的标注，比如说「这是一个有床和衣柜的卧室」，以及一个 object level 的标注，比如说「床」，「衣柜」，以及一个 object ref 的标注，也就是物品之间的关系，比如说「床在衣柜的左边」。其里面提出的 Scene Graph 还是很有效地可以用于表示场景中的相对关系的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2rv87ruraz.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SceneVerse 同时提出了一个模型，这个图应该是画的比较好看，但是不太容易看懂。本身包括一个 PCD Encoder，以及一个 Text Encoder，之后使用一个 Transformer 进行编码，建立了四个损失，也都算是 MLLM 的比较经典设计。&lt;/p&gt;
&lt;p&gt;一个是 ALEBF 的损失，这里面叫 $\mathcal{L}&lt;em&gt;{obj}$，在输入到 Transformer 之前，使用一个 MLP 来预测 object 的 3D 坐标，对齐 PCD Encoder 以及 Text Encoder 的输出。然后这里面多了一个 Spatial Attention，其实就是把 Object 的空间信息编码了一下，之后才和 Text Encoder 的信息一起输入到 Transformer 中。然后就是一个 $\mathcal{L}&lt;/em&gt;{MLM}$，以及 Transformer 之后把 PCD 和 Text 进行对齐的 $\mathcal{L}&lt;em&gt;{ref}$。最后还有一个过了 Spatial Attention 的 $\mathcal{L}&lt;/em&gt;{scene}$。可以说各种地方都加了各种的损失。&lt;/p&gt;
&lt;p&gt;$\mathcal{L}&lt;em&gt;{obj}$ 是让 PCD Encoder 包含 Object 全部的表征，$\mathcal{L}&lt;/em&gt;{scene}$ 确保 Spatial Attention 的输出和 Scene Level 的标注一致，也就是 Spatial 之后真的编码出了这个场景，$\mathcal{L}&lt;em&gt;{ref}$ 和 $\mathcal{L}&lt;/em&gt;{MLM}$ 则是保证 Fusion 之后的信息的准确性。中规中矩，十分合理。&lt;/p&gt;
&lt;h2&gt;Robot See Robot Do&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.3yekisxcqv.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Robot See Robot Do 算是比较袖珍的 Real2Sim2Real 的流程，精致的工程的工作。简单来说，这篇的方法就是先生成 Mesh，然后进行拆分。根据人工的 Demonstration 中的动作来分析不同的部分的运动方向（比如说看到 Demonstration 中一直让一个盖子绕着某个轴旋转，那么就认为这个盖子是绕着这个轴旋转的），同时通过人工的手的位置来获得双臂机器人的爪子的放置位置，再然后就是常规的求解了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/8351ccee1155c89123085cda2df90da4.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这其中的细节还是比较多的，使用 DINO 以及 Depth 在整个视频中的输出，作为监督，约束 State 通过第一帧表征的部件。第一阶段最后的保存形式称之为 4D-可微分部件模型，这个名字比较直观，也就是将物品按照部件保存，以及对应的 Action 动作。&lt;/p&gt;
&lt;p&gt;在部署阶段，因为上述的数据是以物体为中心的，也就是所谓的 object centric，因此可以跨 embodiment 进行部署。本身就是解析物品的姿态，恢复到录制时候的姿态，然后执行动作即可。&lt;/p&gt;
&lt;p&gt;可以说这种论文看上去都是十分的优雅，给出了一种看上去逻辑自洽的解法，但是实在是并不具备泛化性，而且可以预见的是，其部署也不是十分的方便，仅作为参考还是可以的。不过搭建这种工程作为小的 research 也是有趣的体验，也是一种格外精致的优雅。&lt;/p&gt;
&lt;h2&gt;ReKep&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.32i3684dbh.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;ReKep 的思路其实很直接，或者说大多数的 Prompt-based 方法都很直接，本身就是建立了一系列的点约束。比如说我现在有两个点是在一个盘子的两端，那么我约束这两个点的 z 的差等于 0，也就等价于让这个盘子平放了。&lt;/p&gt;
&lt;p&gt;本身的 Pipeline 就是使用 VLM 标点，VLM 生成点约束，生成约束下的 Pose，生成约束下的不同 Pose 之间的 Path。通过这四个环节，就可以生成一个动作序列了。&lt;/p&gt;
&lt;p&gt;本身这篇其实其程度和 CoPa 比较像，属于一个比较具有细粒度而且比较自由的方法，相较于 CoPa 使用向量、面等约束，两个点毕竟也能表示向量，其可实现性还更强，也具有泛化性。看上去还是很不错的，加上其中使用的工具很多（其附录中的内容），代码应该具有参考性。&lt;/p&gt;
&lt;h2&gt;OmniManip&lt;/h2&gt;
&lt;p&gt;之前已经介绍过了 ViLA, CoPa 以及 ReKep，这一篇可以说是集百家之长。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2rv9d76fla.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;事实上从 Pipeline 的图也就可以看出来了，首先是一个大的 ViLA，包括拆分以及闭环（事实上这并非 ViLA 独有，方便理解这样说罢了），然后是一个 CoPa 的向量的约束生成，并且用这种约束的描述来生成动作，最后是一个 Optimized trajectory 的生成，这部分也和 ReKep 的思路比较像。&lt;/p&gt;
&lt;p&gt;尽管看上去有点缝合怪，但是还是用到了他们组拿手的 6D Pose Estimator，并且同时还引入了最近比较火的 3D AIGC，可以说是很全面了。在其他领域，为了显示 novelty，大多数工作都会刻意不使用那些有效但是并非基石的方法，而转而使用自己的模型，然而 Robotics 暂时还是需要在有效性以及鲁棒性上进行进一步的探索，具有强大的 Prompt-based 方法，具有强大的数据生成能力，从而具有大量的数据，才能再谈其他。这一篇可以说通过集百家之长，验证了之前种种方法的有效性，并且获得了相当不错的效果。&lt;/p&gt;
&lt;h2&gt;DiffuserLite&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.175ilwwul7.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这一篇的核心思想其实和 Diffusion 关系不大，主要还是 corse to fine 的思想，使用模型生成一个粗粒度的动作预测，然后将其中前两个动作作为下一次预测的首尾，进一步预测。&lt;/p&gt;
&lt;p&gt;举例来说，比如第一次预测 &lt;code&gt;1, 8, 64, 512&lt;/code&gt;，第二次可以预测 &lt;code&gt;1, 2, 4, 8&lt;/code&gt;，将第一次的 &lt;code&gt;1, 8&lt;/code&gt; 作为condition。这种 corse-to-fine 的方法，还是很容易被理解的，多轮的优化对于精细的操作还是有好处的。不过对于论文中提到的效率优化，我对此表示怀疑。&lt;/p&gt;
&lt;p&gt;本身其实这篇做的很早，但是和后续在 T2I 领域的 VAR 有着微弱的联系，在将来也可能被整合到更多的 VLA 之中。只是实验确实也并不 solid，只能说十分一般。&lt;/p&gt;
&lt;h2&gt;SOFAR&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.6m41683emh.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SOFAR 的工作量还是很大的。首先是提出了一个标注的数据集，叫做 OrienText300K，这个数据集是一个基于 Objaverse 的数据集，使用 VLM 进行了一些三维标注，然后使用这个数据集训练了一个叫做 PointSO 的方向标注模型，类似一种 Affordance 的标注模型，输入文本就可以输出向量。接下来用这个模型进行了一个类似于 CoPA 一样的 Prompt-based 的方法，也就是 SOFAR。&lt;/p&gt;
&lt;p&gt;&amp;#x3C;img src=&quot;https://picr2.axi404.top/image.1sf6a3qhei.webp&quot; alt=&quot;PointSO&quot; style={{width: &apos;50%&apos;}}/&gt;&lt;/p&gt;
&lt;p&gt;本质上 PointSO 的效果就是给你一组点云，并且说出来自己想要标注什么（比如说把手），然后 PointSO 会给你标注出这个内容的一个向量。而其结构本身就是比较经典的多模态的一个 Transformer 的结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.8hglyupay8.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SOFAR 的流程如图中所示，本身 affordance 由 GPT 分析，然后交给 PointSO 进行标注，之后回到 GPT 里面去进一步分析，给出始末位置，之后直接生成动作，感觉是可行的。&lt;/p&gt;
&lt;p&gt;同时很吸引人的一点在于，这个模型确实做了大量的实验，本身工作是 Galbot 的，只能说工业界的积累确实足，大量的在 OXE 训练过的模型，确实不一般。&lt;/p&gt;
&lt;h2&gt;PIVOT-R&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.361pe6izhr.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;PIVOT-R 的画图一眼就似曾相识了，果然是 Surfer 的后续工作。&lt;/p&gt;
&lt;p&gt;直接在宏观上 Diff 一下，一个是引入了异步的机制，本身的三个模块，任务拆解、场景预测、动作预测，都是异步的，可以用不同的速度来运行。可以想到的是任务拆解最慢，场景预测其次，动作预测最快，符合直觉。另一个不同点则在于相比起之前将 action 预测的 token 喂给场景预测，这一次将场景预测的输出喂给了动作预测，可以说是终于符合直觉了，毕竟这是一个动作预测，而不是一个图像生成模型。&lt;/p&gt;
&lt;p&gt;不过鉴于 PIVOT-R 依然是一个传统的 Policy 模型，整体的结构依然是比较复杂，三个异步的模型，确实没有什么可以参考的点。&lt;/p&gt;
&lt;h2&gt;ManipGen&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2yyhirfxla.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;ManipGen 基本上可以说是一图以蔽之。首先在仿真中训练了一堆的 RL Experts，然后都蒸馏到一个模型里面，这里面值得一提的是一个经典的 Sim2Real 方法，就是使用 SegMask/Depth 等作为中间表征。这种思想的本质上都是认为 Sim2Real 的最大 Gap 在于视觉区别，而当中间表征作为输入的时候，这种 Gap 自然也就降低了，因此也就可以更好地进行 Transfer 了。&lt;/p&gt;
&lt;p&gt;ManipGen 使用大量的 RL 进行整合，理论上还是 scalable 的，不过是在是仿真在 Manipulation 中依然是由 Limitations 的，生成更多的任务也是困难的。在这里简单为不知道的读者提一下 DAgger，也就是 Data Aggregation 的思路，说白了就是不止使用 Ground Truth 来进行监督，同时也通过一些专家的策略来生成数据，然后训练模型。这种思路也是比较常见的。&lt;/p&gt;
&lt;h2&gt;DemoGen&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.1e8s3acylt.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;DemoGen 也算是一篇比较直观的论文，通过 Object-centric 的思路进行数据生成，检测物体的位置，并且生成连续的点云数据。这里因为训练的是 DP3D，所以说只需要对于点云进行 crop 以及变换，不需要对于画面进行补全，也会更加简洁。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/769f9f7690b312796ed43bdbb35e6182.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本身的 Pipeline 在其中引入了一些 SAM 之类的来做 PCD 的 semantic 分割，然后用类似于 MimicGen 的思路，将不同的动作片段进行切分，然后可以进行 Layout 的 randomization，还是通过变换将不同的部分整合在一起，不连贯的头尾之间使用 Motion Planning 衔接在一起，Action 也用 Motion Planner 来获得就好。因此 DemoGen 直接合成数据，无需 replay，从效果上来看还挺不错的。&lt;/p&gt;
&lt;h2&gt;ArticuBot&lt;/h2&gt;
&lt;p&gt;ArticuBot 提了一个方法和一个数据生成流程，方法本身依然是常规的 Policy，在这里并不聚焦于这个点，而是主要看数据生成的流程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/59688057ed77a722d1f2e0521d553cfe.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Articulation object，比如说笔记本或者柜子其实是大多数的，这些物体都只有一个自由度，因此假如说你标注了一个可以抓握的 pose，你就可以用这个轴进行旋转，并且通过变换得到旋转后的 Pose，进而组成 action 序列，然后生成数据。这是一个简单有效的方法，我很早之前也提出过这个 idea，但是目前他们做出来了。抓住绝大多数 articulation 物体只有一个自由度，然后对这个自由度施加变换是一个很本质的事情，并且可以借此直接解决绝大多数 articulation 数据生成的问题。&lt;/p&gt;
&lt;h2&gt;LAPA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.32i50qt7z0.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;LAPA 可以作为 GR00T 的前置来看，分为了三个步骤，分别是 action 的 pretrain，VLA 的 pretrain 以及一个 finetune。本身最有意思的还是第一个环节，也就是用 VQ-VAE 来生成 latent action 的这个操作。&lt;/p&gt;
&lt;p&gt;如图中所示，$x_1$ 和 $x_2$ 是相隔了几帧的图片，输入到 encoder 里面求出来 z，之后用 $z$ 和 $x_1$ 恢复 $x_2$。这个流程有趣的地方是，我们可以获得这样一个比喻，本质上 encoder 做的事情就是求逆运动学，也就是已知当前和目标，求中间的动作；而 decoder 做的事情就是求正运动学，也就是已知当前和动作，求目标。这个流程之后，我们就已经给全部的 Video 数据凭空标注了 action 信息了，这种 action 称之为 latent action。需要注意的是，latent action 并不是某种类似于 action 经过 embedding 之后转化为了 latent，而是可以理解为描述了某种抽象的 embodiment 的 action。即，latent action 和诸如 franka action 等是一个 level 的。&lt;/p&gt;
&lt;p&gt;之后就是第二阶段，本身可以理解为在用 Video 的 V 以及 GPT 标注的 L 以及这里获得的 A 来将 VLM 预训练为 VLA，之后最后第三阶段，在真实的数据集上去 finetune。&lt;/p&gt;
&lt;h2&gt;GR00T&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.7zqluepb7d.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GR00T 的 pipeline 乍一看很像是某种双系统，具有一定的误导性，而且论文中也可以看到类似频率差异的表述，但是实际上只是一个快慢系统罢了。其中 VLM 只是一个 VL encoder，而下面的部分则是 action encoder，最后放到一起 fusion。&lt;/p&gt;
&lt;p&gt;很有趣的一点是，这篇使用了 LAPA 的方法来做 Web Video 数据的标注，从而将数据金字塔，即 Real-World Data + Synthetic Data + Web Data &amp;#x26; Human Videos 都统一为了相同的 VLA 格式。之后就是直接预训练了，没啥问题。&lt;/p&gt;
&lt;p&gt;GR00T 的 Codebase 基于 LeRobo，使用了 Modality 的设计，非常好用，后续我们的 InternManip 从中解耦了这一框架，并且做成了训测一体的框架。&lt;/p&gt;
&lt;h2&gt;RoboVerse&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/UYXwkvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RoboVerse 是一篇工作量很大的工作。比如说近期的一些巨型工作，GRUtopia 是纯基于 IsaacSim，更多的是资产以及平台；Genesis 基于 Taichi 的技术做了仿真平台；而 RoboVerse 则是直接 Base 别的仿真平台，将它们彼此串联在了一起。所以说其看上去有很多的 contribution，但是要是说真正的核心，其实在我看来就是做了这个所谓的 MetaSim 或者说 hybrid_sim 的东西。&lt;/p&gt;
&lt;p&gt;说白了就是在众多的仿真之间找了一种中间表征，比如说对于刚体仿真来说，就是全部的 world pose 以及 articulation joint position，这样可以将 physics 的仿真和 render decouple 开，比如可以将 mujoco 的 physics 和 IsaacSim 的 render 结合起来。但是事实上这个东西很难做 solid，毕竟不同的仿真的物理层面的内容都是不同的，比如说有的仿真支持软体，有的压根就不支持。本身的愿景很不错，但是诸如上面所说的，依然存在不少的 Limitation，更何况有谁需要比如说 Isaac 的物理 + Sapien 的渲染，确实是并未从第一性原理出发，但是依然是很不错的工作。&lt;/p&gt;
&lt;h2&gt;AgiBot World Colosseo&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.7p3uuaj1sg.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GO-1 本质上和 GR00T 还是比较类似的，本身的思想还是如何使用互联网的视频数据，而在这里也很直接的给出了解释，即将 VQVAE 的 encoder 和 decoder 解释为了前向/逆向运动学。这里面称之为 LAM，用来在 latent planner 中作为中间表征的输出。这里的 pipeline 让人感觉其实是三个不同的模型，然后在分别进行推理，之后整合成一个串行的流水线。然而事实上虽然最后实际上也是串行的，但是并非三个不同的模型。&lt;/p&gt;
&lt;p&gt;实际上 GO-1 使用的三个模型都是同一个模型，至少进行了 MOE 设计，如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4n7yt4eh76.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;不难看出就是使用了不同的 FFN 之类的，某种程度上确实算是端到端模型，也是优雅了不少。而同时作为一个大模型，在推理的过程中也十分的有趣，是在一次推理中先预测 latent action，之后再进行降噪，感觉是人为设置了 FFN 的切换操作。&lt;/p&gt;
&lt;p&gt;GO-1 其实相对于 GR00T 来说更加优雅。首先是使用了 MOE，可以用一个完整的模型来完成所有任务，优雅的切换不同的 FFN 也很 make sense。其次他们其实有做 latent action 的消融实验，证明了这一部分的重要性。虽然说目前感觉大家都没有 fully leverage 互联网数据，但是使用 latent 的这种表征形式来统一不同的机器人本体以及网络数据，目前来看是 promising 的，就像之前介绍的，这种使用前向以及逆向运动学来解释 VQVAE 的思路听上去就十分的有道理。同时他们也有一个巨大的开源数据集。&lt;/p&gt;
&lt;h2&gt;UniVLA&lt;/h2&gt;
&lt;p&gt;LAPA 里面很重要的问题就是并不对齐。按照这个故事来说，LAM 是一个逆运动学，但是只是通过图像做 VQVAE 来获得 Latent Action，自然里面就除了本体本身的 Action 之外引入了别的信息。其实我之前在看了 LAPA 之后提出了一个 idea，但是一直因为时间问题没有上手，把里面的理解搬过来。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;事实上导致画面变化的原因有很多，我们可以归纳为以下三点，即环境变化、Action 以及 Camera 变化。其中自然环境变化包含了环境自身的变化（其他的个体也算是环境的一部分）以及 Embodiment 与环境交互导致的环境变化。预测环境变化是一个不错的能力，但是属于 World Model，在 VLA 的 A 中属于冗余的信息，并且和正规 VLA 数据一起使用的时候，信息量的不统一可能导致 co-training 的结果不好。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 UniVLA 里面比较不错地解决了这个问题。首先用 T 和一些可学习的 Token 一起作为输入，并且这时候认为这些 Token 里面吸收了全部的与任务无关的东西，之后把这部分 frozen 住，再学任务有关的 token。在这样子处理之后，也就得到了更好的 Action Token 了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/z50SkvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后其实就是 OpenVLA 的范式，输出的也是 Latent Action，再过一个 Action decoder，从而输出动作。
&lt;/p&gt;
&lt;h2&gt;Pi0&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/23aa8977f1fd2a98f676c8b78a30148e.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;经典之作。简单理解一下 Pi-like 的范式，也就是图像/语言/状态编码成 token → 输入 VLM → 得到上下文 token；之后动作 token（随机初始化）在 action expert 中作为「预测目标」，这里面上下文 Token 来作为 cross attention 来当作 condition；使用 flow matching loss，训练模型将噪声 token 转化为真实动作轨迹。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/KngxkvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Pi0 的 Flow Matching 使用的实际上是 MoT 的方案，直观上来说，就是 VLM 先 Forward，但是会进行 KV cache，然后 Action model 本身是用 VLM + 自己的 VQ 一起进行 Self-attention 的，并且对输出来进行 Flow Matching Loss 的迭代。&lt;/p&gt;
&lt;p&gt;Pi0 用了巨量的真机数据（10,000 小时），两秒一个长度的语言标注，产出的是目前几乎是最好的 VLA 模型。&lt;/p&gt;
&lt;h2&gt;Pi0.5&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/5ab2c3cd0b8d8acba72141aeb12cdcbc.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Pi0.5 的 Pipeline 在总体上和 Pi0 还是比较相似的，依然是采用了 MoT 的设计，以及 VLM + DP 的结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/GILCLvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;主要的改动点包括两个。&lt;/p&gt;
&lt;p&gt;其中之一是在 VLM 上进行大量的 Pre training。包括使用 VQA, discretized actions, sub tasks 以及 bounding boxes。这一改动使得在 inference 的过程中，可以让 VLM 使用两次，第一次是直接使用 VLM 来拆解 Subtasks，第二次才是类似于 Pi0 的正常 MoT 推理流程。&lt;/p&gt;
&lt;p&gt;另一个则是使用了 FAST 编码器，并且让 VLM 已经可以输出离散动作了，这里可以看出 Pi 系列工作的一个趋势或者说认知，他们正在让 Action 能力本身从「尾部」的 DP 迁移到「头部」的 VLM 中，从而更加让 VLM 将之前先验以及后续 co training 进来的 VLM 能力和 Action 能力耦合在一起，而不是显式 decouple 开，从而增强模型的泛化能力。&lt;/p&gt;
&lt;h2&gt;GraspVLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/GPZDLvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本身模型不难理解，VLM 被训练去预测 2D bounding box；同时对于合成数据，VLM 进一步预测抓取位姿（在机器人 base frame 中），以及对于合成数据，Action Expert 会根据VLM 的 KV cache 以及输入图像 &amp;#x26; 文本和中间推理的 box 和 grasp tokens，来生成一个连续的动作段（action chunk）。所以说模型在 bbox 阶段还是利用好了大量的互联网数据，以及本身也有依然是 VLM + DP 的范式。&lt;/p&gt;
&lt;p&gt;这个事情的 Highlight 其实在于利用了大量的 Grasp 的仿真数据。实际上这件事实现上并不是那么困难，但是能做 work，还是证明了这条道路的可行性，出于个人的私心来看，确实非常喜欢。&lt;/p&gt;
&lt;h2&gt;GRAPE&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4JyUnvK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GRAPE 大致上还是比较有趣的，流程大概是先训练一个 SFT 的模型出来，之后用这个模型生成一批数据，这一批数据可以用大模型生成的 Goal（比如说平滑度或者接近物体之类的）进行打分（这里面先拆分 stages，之后分别打分等，有一套自动流程），和模型自己评估的分数以及是否成功结合起来成为总分，之后按照分数进行排序，用一个他们写的 TPO 公式来训练。&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}&lt;em&gt;{\text{TPO}} = - \mathbb{E}&lt;/em&gt;{(\zeta_w, \zeta_l) \sim \mathcal{D}} \left[ \log \sigma\left( \beta \left( \log \frac{\pi_\theta(\zeta_w)}{\pi_{\text{ref}}(\zeta_w)} - \log \frac{\pi_\theta(\zeta_l)}{\pi_{\text{ref}}(\zeta_l)} \right) \right) \right]
$$&lt;/p&gt;
&lt;p&gt;公式很好理解，就是偏向 w 不偏向 l，这里面 w 和 l 都是轨迹，w 是好的，l 是差的，组成了 pair。&lt;/p&gt;
&lt;p&gt;总的来说还算是有意思，这样子的训练某种程度上也算是经典的左脚踩右脚了（或者说其实就是以经典强化学习 offline 的算法外面套了一层 [X]PO 的故事），不过本身不断积累成功，而且多训练了不少的 steps，本来就可以带来性能提升，本身依然不 scalable。&lt;/p&gt;
&lt;p&gt;这里面有意思的反而是他说的故事，人类采集的数据里面包含了大量的偏好，然而这些偏好没有明显标注，因此有必要加入额外约束。奈何我现在更加看重泛化，到了更加追求成功率的阶段，RLHF 或者 RL Goal F 的范式确实是很有意思的。比方说在随机生成数据的时候，会发现某些完成方式的成功率更高，那么我不会选择保持多样性，而是把成功率低的完成方式的数据 filter 掉，追求模型的 awareness of 更好的完成方式。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Paper-reading-EAI2-zh.webp"/><enclosure url="https://picr2.axi404.top/Paper-reading-EAI2-zh.webp"/></item><item><title>Paper Reading: Embodied AI 1</title><link>https://axi404.github.io/blog/paper-reading-eai1</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-eai1</guid><description>从一些 Embodied AI 相关工作中扫过。</description><pubDate>Sat, 26 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Embodied AI Paper Reading&apos;,
items: [
{
title: &apos;Batch 1&apos;,
href: &apos;/blog/paper-reading-eai1&apos;,
order: &apos;1&apos;
},
{
title: &apos;Batch 2&apos;,
href: &apos;/blog/paper-reading-eai2&apos;,
order: &apos;2&apos;
},
{
title: &apos;Batch 3&apos;,
href: &apos;/blog/paper-reading-eai3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Batch 4&apos;,
href: &apos;/blog/paper-reading-eai4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Batch 5&apos;,
href: &apos;/blog/paper-reading-eai5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Batch 6&apos;,
href: &apos;/blog/paper-reading-eai6&apos;,
order: &apos;6&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;RT-1&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/RT-1.4uav067pyb.webp&quot; alt=&quot;The pipeline of RT-1&quot;&gt;&lt;/p&gt;
&lt;p&gt;RT-1 由 Google 的具身团队推出，是比较传统的 VLA 模型。所谓传统 VLA，也就是并未使用 LLM Transformer 作为 Fusion 的模型这种现代的结构，而是完全 from scratch 搭建的 VLA 模型，毕竟对于 VLA 的定义，也就只是需要输入 Vision 和 Language 作为条件，输出 Action 作为结果。&lt;/p&gt;
&lt;p&gt;Scaling up 的 deep learning 本质上是在学习多峰分布，并且在分布之外追求模型的外拓能力，从目前的视角上来看，使用 LLM pre-train 的 Transformer 作为 Fusion 的模型，是更优的。不过这篇论文上传于 22 年，彼时这种范式远未被探索，因此倒是合理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/26a9be12bee09ac7a0706340fa9d2101.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;RT-1 属于是彻底贯彻了 Scaling up 的理念，收集了 ~130K episode 的数据，以及超过 700 个不同的任务。事实上在实验的过程中，实际上貌似依然是在有限的任务上进行训练，而并没有一下子用上全部的数据，或者说用上了之后并没有 work，所谓的 unseen task 也仅限于交换物体（比如说见过捡起可乐瓶和推动水杯，未见则可以是捡起水杯）。不过无论如何，依然是初步 work 了的，可以做到交换级别的外拓。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/448a822763525011761d5342d70d5cf5.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后简单从结构上观察一下，如图所示，文本被 text encoder 编码之后，而 Image 取六个时间步的图片，输入到 efficient net 中。编码后的文本和图片的特征被 FiLM 调制，然后输入到 transformer 中。之后获得 Tokens 再过 TokenLearner，输入进一个 transformer 里面，获得最后的 Action 信息，格式是 End-effector Pose。本身使用 TokenLearner 而非直接端到端是出于效率的考量，相似的考量的另一个细节是保留 Image Token 的历史并且进行复用。&lt;/p&gt;
&lt;p&gt;最后简单提及一下感想，RT-1 可以说是提出了一个 work 的 VLA 范式，并且做的一定程度上可以泛化。不过 RT-1 还是具有一些局限性，不能引入 VLM 来强化模型的泛化能力使得模型几乎永远无法真正泛化到未见物体，而且减少了 co-training 的纯粹拟合 action 数据，使得很难利用到 VLM 等更加广泛的数据，而只能使用 VLA 的昂贵数据。后续的问题在 RT-2 解决了不少。&lt;/p&gt;
&lt;h2&gt;RT-2&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/RT-2.4xugxw0so5.webp&quot; alt=&quot;The pipeline of RT-2&quot;&gt;&lt;/p&gt;
&lt;p&gt;RT-2 是 Google 时隔一年之后的又一力作，从结构上来看就已经合理了不少了。因为 LLM 以及 VLM 的兴起，在过去的一年中出现了不少的新工作，包括了两个被 RT-2 采用的模型，也就是分别是 PaLI-X 以及 PaLM-E。&lt;/p&gt;
&lt;p&gt;RT-2 的结构可以说奠定了之后的一种流派。总的来说，RT-2 完全遵循使用 Transformer 进行 Fusion 的思路，并且替换 256 个 Token，改为离散化的动作表征 Token，并且 Token 可以解码为 End-effector Pose。&lt;/p&gt;
&lt;p&gt;模型的训练使用了 Web Data 进行 VLM 的预训练，并且在此之后，依然是进行了多任务的训练，效果也很不错，并且支持了 reasoning 环节。实验部分使用了 6K 的轨迹进行训练，体现了新物体、背景和环境上的泛化，并且可以进行符号理解。&lt;/p&gt;
&lt;p&gt;RT-2 作为初步使用 VLM 来作为 VLA 模型预训练权重的模型，通过符号理解等能力，体现了 VLM 的能力迁移到 VLA 的潜力，虽然事实上的性能可能还是来自于后训练。这一架构后续被包括 OpenVLA 在内的诸多模型沿用效仿，成为了一段时间内的主流方法。然而使用离散化 Token 的方法也存在其局限性，因为使用离散化，因此模型的精度有限，这一点在使用绝对位姿控制的模型上尤其显著，伴随模型的 Action Space 扩大，模型每一个 Step 的精度会进一步下降，因此难以进行一些精细操作，而带来更多的离散化数量在本质上也依然是治标不治本的。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RT-2 - &lt;a href=&quot;https://zhuanlan.zhihu.com/p/651670131&quot;&gt;https://zhuanlan.zhihu.com/p/651670131&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;VIMA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/df63b55442b368b62d8e37bb9a43577f.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;VIMA 是比较早期的工作，由 Feifei Li 的团队推出，也是很不错的实现，本身里面一些有意思的设置。&lt;/p&gt;
&lt;p&gt;VIMA 本质上并没有完全使用 LLM 或者 VLM，而是使用了 T5 的 Encoder 来进行 Token 之间的 Fusion。从输入开始说起，VIMA 一共包括 Object Token, Scene Token 以及 Text Token 三种不同的类型。其中 Scene Token 或者说 Image Token，就是非常传统的图片输入，Text Token 同理，而 Object Token 则是将需要关联的物体在使用 Mask R-CNN 之后，对于 Crop image 使用 ViT 编码，bounding box 使用对应的 encoder 编码，可以说同时包含了物体和位置信息。&lt;/p&gt;
&lt;p&gt;VIMA 还支持历史信息，可以进行长任务。虽然说 RT-2 也可以上下文理解，但是 VIMA 直接使用原本的信息，肯定表征更多一些，也就是所谓的 Visual centric 的表征方案。一个 insight 是使用 object token，常规的多模态输入都是先图像后文本，object token 将两个交叉在一起，interleave 的方案可以提供更加直接的 format，并且可能带来更好的效果，也更加将图像融入了文本的体系里面。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/VIMA.pf9o2ajvu.webp&quot; alt=&quot;The pipeline of VIMA&quot;&gt;&lt;/p&gt;
&lt;p&gt;VIMA 的问题在于使用 RCNN 还是太过于草率了，但是即使使用 Grounding DINO 或者更好的方案，似乎也并不会有本质的改变。在使用 Object token 的方案的情况下，相较于如今 VLM 直接力大砖飞的方式，在结果上来看并没有过多的性能差距，而 VLM 带来了更多的可拓展性以及泛化能力。&lt;/p&gt;
&lt;p&gt;留给 VIMA 的问题是显然的，是否有更加优雅的方式来进行 object token 的生成或许会是一个问题，以及如何进行泛化的大规模学习。而同时，目前随着通用能力的提升，模型能否在泛化以及更加复杂的 Skill 上处于一个良好的 trade off，也是一个关键问题，不过 VIMA 在 Input 上有了不少的探索，但是输出似乎还是直接使用 Transfomer 来整合 History 和 Prompt，直接进行 from scratch 的训练，泛化能力也是有限的。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VIMA - &lt;a href=&quot;https://zhuanlan.zhihu.com/p/659016759&quot;&gt;https://zhuanlan.zhihu.com/p/659016759&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;SayCan&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/daafb20bafee09e8a1695b38782ada53.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SayCan 可以说是在做这种规划任务里面比较早的了，所谓规划任务，或者说 System2，其实就是通过任务拆解以及分析，来为下游真正的 low level 的模型提供一个更清晰的指引，进而扩大整个体系的泛化能力，显而易见的就是可以外拓到一些 reasoning 任务来。&lt;/p&gt;
&lt;p&gt;SayCan 本质解决的问题可以理解为是 LLM 或者 VLM 的幻觉问题以及 LLM 缺乏对于机器人相关的先验知识的问题，如图中所示，模型可能会指示下游的模型去进行一些不切实际的任务，而这些任务实际上是不可执行的，SayCan 为了就是增加一个权重，纠正这种内容，把需求和可行性结合起来。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/SayCan.7p3j5ymwpy.webp&quot; alt=&quot;The pipeline of SayCan&quot;&gt;&lt;/p&gt;
&lt;p&gt;大概的流程就是，对模型输出本身的任务目标，而模型本身存在一个动作列表，里面是定义好的 Skill（可能是提前训练好的 low level policy），那么 LLM 就可以从这个动作空间里面给出不同的推荐。如上所述，一个问题在于，由于 LLM 不清楚当前的情况，所以说可能无法很好地给出能够执行的结果，这个时候可以使用另一个模型，或者说是一个价值函数，来去评判在当前情况下这些动作的价值。那么这个价值函数是使用了环境信息的，并且在 RL 上进行了学习，最后价值和大模型的推荐结合在一起，就生成了一个布置合理，而且可以完成的动作。&lt;/p&gt;
&lt;p&gt;这里面的 insight 其实不多，或者说显而易见，想要让 LLM 去参与到动作的生成，固然其本来就具有一定的规划能力，但是这种能力在没有现场情况的了解下是施展不开的，于是可以简单地使用价值函数来作为一种当前情况的引入，本身需要训练的东西也很少，可以说是十分的轻量化。不过伴随着模型的能力增强，这方面的规划不再是主要问题，而是在于更加 open vocabulary 的规划，以及复杂任务的理解能力，甚至说一些 grounding 能力，而这些能力甚至在 VLM 中都没有很强，在 VLA 中更是难以实现的难题。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SayCan - &lt;a href=&quot;https://zhuanlan.zhihu.com/p/655418399&quot;&gt;https://zhuanlan.zhihu.com/p/655418399&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Language Models as Zero-Shot Planners&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/lmzsp.syvls3mlc.webp&quot; alt=&quot;The pipeline of Language Models as Zero-Shot Planners&quot;&gt;&lt;/p&gt;
&lt;p&gt;这篇文章也是在 planning 领域的内容，某种程度上也可以说是 low hanging fruit，甚至说不需要任何的训练，就是纯粹的 prompt。在这篇文章中的设置下，系统会设置一个 Action Set，这一点与诸如 SayCan 等模型一致，假设存在需要的 low level policy。&lt;/p&gt;
&lt;p&gt;对于任务，这篇文章的方法是先让一个模型给出一些任务拆解，然后这些计划通过另一个模型翻译成在 action set 里面的最接近的内容，最后交给下游的 policy 来执行。从本质上，Zero-shot Planner 甚至感觉是 SayCan 的简化版，不检测可行性，相信 LLM 的能力，而是只确保任务在 action set 里面，而最后理论可以达成的效果与 SayCan 一致。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Language Models as Zero-Shot Planners - &lt;a href=&quot;https://zhuanlan.zhihu.com/p/656399047&quot;&gt;https://zhuanlan.zhihu.com/p/656399047&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PaLM-E&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PaLM-E.4cktbl6cde.webp&quot; alt=&quot;The pipeline of PaLM-E&quot;&gt;&lt;/p&gt;
&lt;p&gt;彼时谷歌的 PaLM 模型刚刚推出了一年，而 PaLM-E 则在其基础之上，将 PaLM 添加一个 ViT，组成 VLM 模型，在大量的常规 Language 任务, VQA 数据以及包括机器人场景的数据上进行训练，以得到一个通用的 VLM 模型，同时这个模型在机器人场景中也被设置为一个 specialist 的角色，可以在 Planning 等任务上更加擅长。&lt;/p&gt;
&lt;p&gt;因为 PaLM-E 是在届时最为先进的 VLM 模型之一，同时在具身任务上进行了训练，所以说看上去确实十分具有吸引力，也可以超过当时同样 Google 提出的 SayCan。不过从当今的视角来看，PaLM-E 却不太能带来额外的 insight。在 2023 年，VLA 的概念依然没有完全成型，端到端训练动作模型的方案也依然并不是主流，从 CVPR 2022 的 Best Paper 给自动驾驶的系统性论文就可以看出，当时包括 PaLM-E 在内的 VLM 作为 System 2，并且在此基础上使用 low-level policy 来提供诸多能力，似乎才是合理。当今使用相似的结构的包括 RoboBrain 系列等，依然是 follow 的 System2 的故事。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PaLM-E - &lt;a href=&quot;https://zhuanlan.zhihu.com/p/662935514&quot;&gt;https://zhuanlan.zhihu.com/p/662935514&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ViLA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ViLA.60u68rwmjs.webp&quot; alt=&quot;The pipeline of ViLA&quot;&gt;&lt;/p&gt;
&lt;p&gt;ViLA 提出了一个相当直接的 Idea，但是却在 Prompt 类型的工作中相当本质。笔者并没有了解 Agent 相关工作，使用 LLM 或者 VLM，并且使用闭环让 LLM 或者 VLM 持续获得 in-context 信息，并且可以对于反馈进行新的规划，应当并非新的 Idea，但是在 Manipulation 中确实是首次。&lt;/p&gt;
&lt;p&gt;ViLA 的思路非常简单，如图中所示，将任务以及 Obs 直接输入给 VLM，让 VLM 输出 CoT 以及 Task Planning，然后将 Task Planning 作为一个 TODO List 进行维护，将 List 的第一项交给 low-level policy 来执行，并且将执行的结果（新的 Obs）以及已经完成的任务列表给 VLM，让 VLM 进行新的 Task Planning。&lt;/p&gt;
&lt;p&gt;伴随着 VLM 的性能提升，如何 leverage VLM 的能力到 Long horizon 的规划中，并且利用 VLM 的优势，这是一个十分值得探索的问题，或者换句话来说，有什么时期是 VLM 可以做到，但是 low-level policy 难以做到的，这就是 System 2 的体系作为 System 1 的补充的存在必要性。ViLA 敏锐的把握住了其中的两个关键要点，一个是 Reasoning，在框架中体现为 CoT 的输出，可以将需要推理的任务拆解为更加直接的任务表示；另一个则是 Long horizon 的规划，在框架中体现为 Task Planning 的输出，同时还顺便支持了闭环的纠错。&lt;/p&gt;
&lt;p&gt;ViLA 尽管看上去十分草率，甚至说本质只是几句简单的 Prompt，但是却十分具有启发性，敏锐地把握了 VLM 的优势，整体故事十分完整。&lt;/p&gt;
&lt;h2&gt;CoPa&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/CoPa.5fkimh268l.webp&quot; alt=&quot;The pipeline of CoPa&quot;&gt;&lt;/p&gt;
&lt;p&gt;CoPa 是在 ViLA 之后进行的后续探索，虽然说本身还是以 VLM 驱动，但是却并非 System 2 的思路（即通过搭建 workflow 来为 low-level policy 稳定提供任务规划），而是引入了更多的 foundation model，并且直接用整体的体系直接输出 Action，并且可以直接完成任务。&lt;/p&gt;
&lt;p&gt;整体来说，CoPa 如图所示，由三个组件组成，分别是 Grounding, Grasp Proposal 以及 Motion Planning。Grounding 以及 Grasp Proposal 包括了 Grasp 的生成以及筛选，生成的思路十分简单，可以直接使用 GraspNet 这种 foundation model，通过 depth 和 rgb 输入来生成 Grasp Pose；而对于筛选来说，则是使用多组 SoM 来选择需要抓取的物体的部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/45cd406095c69f6295b7c217f8e4d599.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;SoM 即使用 SAM 进行分割之后使用 GPT 来选择对应的 Mask，来达到物体分割的效果。CoPa 在此基础上使用了多轮的 SoM，可以先选择粗略的物体，然后细致地选择物体的某个部分，如 &lt;code&gt;桌面-&gt;锤子-&gt;锤柄&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;接下来是一个路径规划，这里先识别了各种物体的位姿，然后将这些内容画在图上，估计这种选择是因为不信任大模型的数学能力，反而是图像比较直观，容易理解。之后通过这种细粒度的指示，大模型就可以给出更加合理的建议，类似于之前是将锤子放在钉子上，现在可以是将锤子和钉子对齐，而且根据识别的位姿，或许可以精确到距离。然后交给执行器。&lt;/p&gt;
&lt;p&gt;CoPa 以及之后提及的 MOKA 是使用 foundation model 来完成任务的典型工作，到之后学界开始发现使用端到端的 VLA 也可以达到相似的效果之后，这些范式就逐渐淡出了人们的视野。然而事实上，在当下，对于调试一个 Fancy 的 Demo 来说，得益于 foundation model 的稳定性以及显式拼接的可解释性，使用这种 modular framework 进行调参并且完成任务依然是十分高效的。&lt;/p&gt;
&lt;h2&gt;PointLLM&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PointLLM.13lpexj2yz.webp&quot; alt=&quot;The pipeline of PointLLM&quot;&gt;&lt;/p&gt;
&lt;p&gt;PointLLM 可以是说十分标志的工作了，属于是非常符合直接的工作，本身也得到了很高的认可度。PointLLM 本身的目的就是建立像是 VLM 对图片理解能力一样的对点云的理解能力。因此不难理解，PointLLM 的实现方法也就像是正常的 VLM 一样，但是只不过是将图像的模态输入换成了点云，然后使用 point encoder。总体来说改变并不算多。可以说这篇工作的诞生是符合直觉的，点云模态也可以对齐到语言的空间上进行建模并理解。
&lt;/p&gt;
&lt;h2&gt;EmbodiedGPT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/EmbodiedGPT.9kg3ykzkji.webp&quot; alt=&quot;The pipeline of EmbodiedGPT&quot;&gt;&lt;/p&gt;
&lt;p&gt;EmbodiedGPT 是一篇有些复杂的工作。简单来说，EmbodiedGPT 用 ViT 进行 encode 之后，和 embodied token 在 Q-former 中进行融合，然后和 Text 信息一起输入给 LLaMA3 的 decoder，输出 Planning 的信息。纯 Text 的 Planning 信息输出之后，可以再次 tokenize 之后和 ViT token 在 Q-former 中融合，之后输出一个 instance information。同时一个 CNN 处理图像输出一个 global information，两个 information 进行 concat 之后作为 low-level policy 的输入。&lt;/p&gt;
&lt;p&gt;本质上，EmbodiedGPT 依然是受到时代局限性的影响，受到 BLIP 2 的启发引入 Q-former，但是并未采用后续 VLA 通用的 VLM Fusion 方案。然而 EmbodiedGPT 依然是具有启发性的，Planning 如何在 VLA 系统中被显式引入，通过重复的输入来重复利用一些组件，并且将显式的 Planning 引入监督中，并且让后续的流程中可以作为输入，依然是值得借鉴的。&lt;/p&gt;
&lt;h2&gt;RT-Trajectory&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/RT-Trajectory.7w6r1e9adp.webp&quot; alt=&quot;The pipeline of RT-Trajectory&quot;&gt;&lt;/p&gt;
&lt;p&gt;从图中不难看出，RT-Trajectory 本质上是在 RT-1 的基础上进行了调整的模型，本身依然是使用 RT-1 的框架进行动作输出的 low-level policy。在此之前的模型，主要依然以 VLA 模型以及 VA 模型为主，然而 RT-Trajectory 则打破了这一范式，尝试使用 Trajectory 而非 Language 作为 Condition。&lt;/p&gt;
&lt;p&gt;模型的输入包括之前和当前帧的 Obs 以及一个 trajectory 的轨迹，轨迹使用图片进行表征，通过 R 和 G 两个通道表征了时间顺序以及高度信息，还可以加入交互标记，最后和图像一起输入。因为从文字 prompt 改为了图像（轨迹），所以本质上具有更高的细粒度，然而也会带来别的问题。首先是轨迹的来源，尽管论文中给出了多种可以产生轨迹的方法，但是具有高保真度的轨迹生成依然是一个问题，同时，对于轨迹来说，单一的轨迹即使包含深度信息，在对应机械臂的末端执行器在三维空间中的运动时依然具有歧义，在对于一些需要机械臂进行旋转的操作的时候，二维轨迹的表征能力依然不足。&lt;/p&gt;
&lt;p&gt;不过总的来说，RT-Trajectory 提供了一个新的视角，即，作为 low-level policy，并不一定需要以语言作为中间表征，恰恰相反的是，可以通过更加细粒度的信息，为模型带来更多的指引。&lt;/p&gt;
&lt;h2&gt;Im2Flow2Act&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Im2Flow2Act.3nrjrkj1kx.webp&quot; alt=&quot;The pipeline of Im2Flow2Act&quot;&gt;&lt;/p&gt;
&lt;p&gt;Im2Flow2Act 的核心思想在于，首先根据任务生成对象流，也就是以操作物体为中心的光流一样的轨迹簇，对象流具有很高的细粒度，并且甚至可以包括物体内部的指引，比如说折叠毛巾，之后对象流通过模仿学习来获得动作规划。&lt;/p&gt;
&lt;p&gt;Im2Flow2Act 使用了 Diffusion 里面的动作生成（视频生成）作为流生成的方法。首先先框出来一个物体，在物体上面可以采样若干的关键点，这些点就组成了一个 $H\times W$ 的图片，但是这个图片不是正常的图片，和RT-Trajectory 里面的轨迹图片一样，是通过像素表征了别的信息，这里面就是图像系下的坐标和可见度。那么根据条件输入，就可以生成视频了，而这个视频本质上表征的是这个物体在不同时刻的空间信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Im2Flow2Act-2.2krugon7pf.webp&quot; alt=&quot;Flow generation&quot;&gt;&lt;/p&gt;
&lt;p&gt;流生成了之后，基本上是直接使用模仿学习进行的运动规划，用了 Transformer 去编码当前帧的状态，再用 Transformer 去和任务流做融合，来生成剩余的流，最后交给 Diffusion Policy 去生成动作。&lt;/p&gt;
&lt;p&gt;Im2Flow2Act 的创新性在于使用生成式的方法生成高细粒度的物体流，这种过程在表征能力上显然是优于 RT-Trajectory 的，同时第二阶段的时候使用当前的状态和任务流做融合，有一种 nav 中全局规划和局部规划的意味，也可以说是十分 make sense 的 motivation，而并非一拍脑门的想法。总的来说是一篇 based 轨迹的动作规划的很不错的工作，而且相较于 RT-Trajectory 更有细粒度。&lt;/p&gt;
&lt;h2&gt;LLARVA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/LLARVA.175bcnc5ok.webp&quot; alt=&quot;The pipeline of LLARVA&quot;&gt;&lt;/p&gt;
&lt;p&gt;LLARVA 相较于之前的工作，可以说也是一个比较符合直觉的工作，使用指令调优（IT）的方法进行训练，处理了 OXE 这个数据集，让模型学习 2D Visual Trace。从 Pipeline 也不难看出，LLARVA 是一个比较经典的架构，基本上也是 LLAVA 的框架，训练一个 projection layer 以及后面的 Transformer 做对齐以及模态的融合。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/LLARVA-2.7p3j5yn4xw.webp&quot; alt=&quot;The instruction of LLARVA&quot;&gt;&lt;/p&gt;
&lt;p&gt;本身 LLARVA 的思路还是十分清晰的，使用 co-training 的方式来为模型带来更多的约束，以学到更多的知识，进而再类似于 OpenVLA 的范式下得到更好的动作能力。从本质上来说，作为使用 next token prediction 来输出 action 的模型，先输出轨迹再输出动作，也是一种 CoT 的体现。&lt;/p&gt;
&lt;h2&gt;ATM&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ATM.wihjhwxip.webp&quot; alt=&quot;The pipeline of ATM&quot;&gt;&lt;/p&gt;
&lt;p&gt;ATM 可以说是在提出光流式的轨迹，并且作为指引进一步生成 action 的框架中相对较早的，本身的做法也是比较清晰，也是得到了 RSS 的满分。&lt;/p&gt;
&lt;p&gt;本身的话，ATM 没有采取像是 Im2Flow2Act 一样的物体轨迹的预测，这也比较好理解，全局的点一方面或许可以具有全局的动作视野，而另一方面，全局的点也会比较好获取一些。本身的方法就是使用点跟踪的技术对图像里的点进行跟踪来生成数据集，然后让一个 track transformer 来预测点的轨迹。接下来就是一个正常的 Trajectory Conditional Policy，本身的实现，论文里也说了，也是使用 cls token 去做全局表征（ViT like），然后用了 track prediction 去作为额外的 condition 进行 fusion。&lt;/p&gt;
&lt;p&gt;从创新点来说，这篇算是开山之作之一了，引入了 Track 作为中间的表征以及条件，并且可以通过数据集的一些生成的技术进行标准的损失计算，因此在监督下训练提升的很好也是意料之中了。一方面增加了更具细粒度的输入，一方面这种细粒度也体现在任务的难度上（hard task），二者共同导致模型的简单易用。
&lt;/p&gt;
&lt;h2&gt;Track2Act&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Track2Act.231ss3lu4u.webp&quot; alt=&quot;DiT of Track2Act&quot;&gt;&lt;/p&gt;
&lt;p&gt;Track2Act 和 ATM 之间其实并未具有较大的差异，二者的方法实际上是近似的，也就是先预测轨迹，之后将轨迹作为动作生成的条件。模型首先还是进行点的预测，在这里使用的是 DiT，随机 sample 一些点和轨迹，然后就可以进行生成了，将当前状态、目标以及迭代次数都作为 adaptive conditioning 输入。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Track2Act-2.3k7xtupyvk.webp&quot; alt=&quot;Residual policy of Track2Act&quot;&gt;&lt;/p&gt;
&lt;p&gt;有了这些点之后，就不难给出一个刚性变化来描述 Action 了。然而刚性变化注定不太靠谱，于是乎加入了一个残差策略，再用另一个模型的预测来修正之前的结果。按照文章的表述，残差控制可以增加准确度并非首创，不过确实是一个纠正偏差的好方法，前面的轨迹生成并求刚性变化，获得一个变化之后加上残差，这本质上其实和 ATM 直接通过一个模型进行 action 的求解是等价的，毕竟刚性变化同样可以用模型来进行表征。&lt;/p&gt;
&lt;h2&gt;Extreme Cross-Embodiment&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Extreme-Cross-Embodiment.70a9lxzlxc.webp&quot; alt=&quot;The pipeline of Extreme Cross-Embodiment&quot;&gt;&lt;/p&gt;
&lt;p&gt;这篇文章基本的故事是说要实现一种跨不同机器人模态的表征学习，但是实际上只是视觉导航以及抓取这两种任务。本身的想法就是说，移动和抓取的本质上都是让EE-Pose 或者相机坐标系发生了坐标系变换，实际上是等价的，所以说可以统一，然后就开始直接训练一个模型，输入是 state 和 goal，之后直接融合，获得两个目标，一个是机械臂的位姿（DiT），一个是距离的预测（MLP），也算是将这两个任务统一了一点。&lt;/p&gt;
&lt;p&gt;之前的任务，绝大多数都在处理单一的机器人下的任务，一般为机械臂，这篇的创新点也就止步于同时使用两种训练数据了，最后从效果上来看并非是十分理想。&lt;/p&gt;
&lt;h2&gt;ECoT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ECoT.45m1rgbst.webp&quot; alt=&quot;The pipeline of ECoT&quot;&gt;&lt;/p&gt;
&lt;p&gt;ECoT 的文章可以说是 CoT 在 Embodied AI 领域里面一次较为全面的运用，或者说这其中包括让模型显式输出不同的中间表征，包括说输出 Task，SubTask，Planning，Move 以及各种的包括说 Gripper Pos 以及 Objects 的 Grounding。&lt;/p&gt;
&lt;p&gt;本身的模型依然是 OpenVLA，也就是在进行了上述的 Reasoning 之后输出离散化的 Action。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/3dfa93c1b16ed3376d0a0094ab072d55.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;ECoT 在 OXE 上收集了不少的 CoT 输出，建立了一套标注中间表征的方法，对于不同的内容使用不同的方法，如上图所示。&lt;/p&gt;
&lt;p&gt;ECoT 在 Reasoning 和 Action 相结合这方面还是做的很早的，但是依然有一些 limitation。本身 OpenVLA 的输出已经是 next token prediction，加上如此如此多的中间表征，可以见得模型的执行频率必然不高。如何让模型在执行的过程中自行控制何时开始 reasoning，甚至如何进行隐式推理，这些依然是一个值得思考的问题。&lt;/p&gt;
&lt;h2&gt;VoxPoser&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/VoxPoser.8ad6s9hl8y.webp&quot; alt=&quot;The pipeline of VoxPoser&quot;&gt;&lt;/p&gt;
&lt;p&gt;VoxPoser 本身是通过 LLM 以及 VLM 获取图像以及任务的表征，并且想要输出两张价值图，其中 VLM 是传统的 VLM，类似于开集检测器，可以获得物体的位置，之后 LLM 来去处理这些位置，获得两张价值图，这两张价值图进一步引导模型进行轨迹规划。&lt;/p&gt;
&lt;p&gt;使用价值图进行轨迹规划还是一个比较有趣的话题的，但是事实上从 VLM 输出的 Code 来表征价值图，还是会显得缺乏一些细粒度，只能说有一定的可行性。&lt;/p&gt;
&lt;h2&gt;MOO&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/MOO.175bcnc5op.webp&quot; alt=&quot;The pipeline of MOO&quot;&gt;&lt;/p&gt;
&lt;p&gt;MOO 的 pipeline 很简单，本身使用了 RT-1 的架构，大致的流程就像 pipeline 里面描述的一样，其可以将 Mask 作为一个通道融到图像里面，然后将动词提取出来，来作为 text token，在 FiLM 里面进行调制。MOO 本身是在尝试一件事情，能否使用 Mask 作为中间表征，来更好地建立一种通用的抓取模型，从实验来说，确实也是进了大量了 Unseen 的实验，从效果上来说认证了这一点。&lt;/p&gt;
&lt;p&gt;不过疑点可能在于，从结果上来看，模型的表现并没有比 RT-1 好太多，而从如今的视角来说，RT-1 并没有那么好的性能，尤其是接近于 zero-shot 等内容的时候。不过从思想上来说，往小了说，是 Mask 版本的 RT-Trajectory，而往大了说，可以说是某种 Visual centric 的 Guidance，到了 VLA 时代之后，这种思路似乎可以继续被采用。&lt;/p&gt;
&lt;h2&gt;ChatGPT for Robotics&lt;/h2&gt;
&lt;p&gt;本身可以理解为使用 ChatGPT 去做机器人的一个发散性的思考，同时提出了诸如 PromptCraft 之类的工具。&lt;/p&gt;
&lt;h2&gt;PIVOT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PIVOT.2krugon7pm.webp&quot; alt=&quot;The pipeline of PIVOT&quot;&gt;&lt;/p&gt;
&lt;p&gt;PIVOT 的思想还是比较有趣的，也算是充分利用的 MLLM 的 VLM 能力。本身的思路其实在于，让大模型在具身智能的任务中进行生成式不太靠谱，同时进行空间自由度的生成更加有问题了，毕竟自由度是具有差距的，但是去做平面的选择题还是可以的。&lt;/p&gt;
&lt;p&gt;于是设计了一套 Pipeline，可以先随机 sample 一些动作或者轨迹，之后将这些内容 annotate 到图片上，毕竟本身模型在图片上进行方位的理解会更好。在此之后就是让模型选择，然后一次次的选择即可。&lt;/p&gt;
&lt;p&gt;在这里除了明面上的这些内容，有意思的一点在于，正如在 Extreme Cross-Embodiment 中所说的一样，本身 Navigation 以及 Manipulation 都是坐标系的变化，因此 PIVOT 还可以做 Navigation 相关的任务。&lt;/p&gt;
&lt;h2&gt;Code As Policies&lt;/h2&gt;
&lt;p&gt;Code as Policy 这篇文章的思路很简单，就是可以使用代码来控制机器人，这等于可以让 LLM 与环境进行持续且合理的交互。大模型可以通过调用 API 来获取环境信息，比如说调用视觉 API 来获取物体位置，同时也支持了使用一些比如 for 之类的操作，毕竟代码肯定比一次次的生成式更加有条理。&lt;/p&gt;
&lt;p&gt;在后续的工作中，Code as Policy 作为一种思想也得到了很多的运用。这其中的核心在于，代码允许整体的体系使用 Loop 进行更多的事情，保持频率以及稳定性，并且在达到某一条件的时候立刻跳出循环或者转折。Code as Policy 某种程度上位于线段的一个中点，一端是 System 2，假如说模型直接输出 Planning 等，一是 Guidance 不明确，而是按照较慢的频率推理，feedback 不实时；另一端则是 System 1，本身需要学习 Action 的表征，并不 Training free，难以 leverage VLM 的能力，并且缺乏泛化能力。因此还是十分巧妙的。&lt;/p&gt;
&lt;h2&gt;MOKA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/MOKA.86tkujoiiw.webp&quot; alt=&quot;The pipeline of MOKA&quot;&gt;&lt;/p&gt;
&lt;p&gt;MOKA 的思路其实本质上和 CoPa 以及 PIVOT 是十分类似的，都是使用 Prompt-based 的 VLM，通过将不同的选择 annotate 到图像上，并且让模型进行选择，从而进行路径的规划。&lt;/p&gt;
&lt;p&gt;MOKA 等于希望通过若干的点标注，让模型学会如何去完成动作。所以流程上也是首先先找到需要操作的物体，然后再采样抓握点以及路径点之类的，最后结束。甚至说虽然 MOKA 里面没有明说，但是实际上其对于抓握点进行 filter，并且通过 filter 获得抓握姿态，这个流程实际上和 CoPa 可以说是一模一样，只是说 MOKA 希望通过路径点来完成动作，而 CoPa 则希望通过向量来完成动作。&lt;/p&gt;
&lt;h2&gt;RoboPoint&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.7egpxqs4ur.webp&quot; alt=&quot;The pipeline of RoboPoint&quot;&gt;&lt;/p&gt;
&lt;p&gt;RoboPoint 本身也算是一个十分直接的论文了，基于 Vicunal1.5 13B 进行训练，去训练模型对于 point 的 grounding 模型，也是按照 System 2 的路数。&lt;/p&gt;
&lt;p&gt;本身论文并没有很多值得更多 highlight 的点，不过在这里可以依然延伸一下，在这个时期，包括甚至说现在，单独去做 System 2 的模型还是有很多，但是很多时候，事实上学界并不清楚 System 1 需要 System 2 提供什么。整体双系统，借助 System 2 的 VLM 的能力来实现泛化的故事，在现在看来，还是需要两个系统的相互迭代，才可以得出结论，届时需要 Box 还是 Point，才可以有一个继续推进的方向。&lt;/p&gt;
&lt;h2&gt;GR-1&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.26lhazlhda.webp&quot; alt=&quot;The pipeline of GR-1&quot;&gt;&lt;/p&gt;
&lt;p&gt;GR-1 可以说是一个很不错的经典工作了，用了十分直接的方法，效果也很不错。具体来说先在人类数据上训练，然后放到机器人数据里面进行 fine-tune。执行的 Task 有两个，一个是预测图片（多张图片，也可以说是视频），一个是预测动作，见下图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.6f0oktcu0x.webp&quot; alt=&quot;The pipeline of GR-1&quot;&gt;&lt;/p&gt;
&lt;p&gt;本身因为 GR-1 是 GPT 架构的模型，所以说输出的 Token 数量是可以根据不同的训练阶段进行调整的，并且不同的阶段，不同位置的 token 可以拥有不同的含义。GR-1 运用了 world model 的思想，通过预测视频来预测未来，从而认为模型可以 train 出来对于世界如何运作的理解，然后在这个基础上进行微调。&lt;/p&gt;
&lt;p&gt;本身模型分为两个阶段的训练，第一个阶段是在视频上进行预训练，这里面输入图片以及 Language，输出的 Token 可以被解码为预测的未来图像，称之为 &lt;code&gt;[OBS]&lt;/code&gt;。同时，在后训练的时候，额外预测可以被 MLP 解码为动作的 token，称之为 &lt;code&gt;[ACT]&lt;/code&gt;。GR-1 预测未来的若干帧。&lt;/p&gt;
&lt;p&gt;从数据的角度来看，这种使用视频预测的策略确实很不错，因为只要存在一个文字视频对（应该不少），那么就可以大量地进行 scaling up。&lt;/p&gt;
&lt;p&gt;说到 Scaling up，总的来说，依然是需要体现出从 Scaling up 的 messy 数据中 transfer 出来的，几乎可以 zero-shot 到 post-training 之后的能力。例如在人类示教视频中进行了一个动作，而这个动作是并不在机器人的数据里面，但是在实验中机器人可以执行，那么就很能体现 scaling up 的意义了。因为加入的大量人类视频数据里面学习到的 skill 可以 transfer 到机器人的动作能力中。毕竟人类数据很多，而且录制起来也很简单，这就会成为一种未来。而这种验证，在未来的 VLA 中有望被实现。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Paper-reading-EAI1-zh.webp"/><enclosure url="https://picr2.axi404.top/Paper-reading-EAI1-zh.webp"/></item><item><title>具身十日谈：VLA 为什么需要 VLM</title><link>https://axi404.github.io/blog/embodied-talk-2</link><guid isPermaLink="true">https://axi404.github.io/blog/embodied-talk-2</guid><description>VLA 相较于其他 Manipulation 模型，最显著的区别便是引入了 L(anguage) 作为 condition，主流的范式至今为止仍然 VLM 作为核心组件，而我们为什么需要 VLM，如何利用 VLM，这对应着一系列的思考。</description><pubDate>Fri, 22 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;诸如 ACT[^act] 以及 DP[^dp] 等早期 Manipulation 模型，本质上均保持着 Vision-Action 的输入输出模态，也就是通常所说的 VA Model，缺少 Language Condition，这意味着本质上此类模型的目的并非训练一个通用的模型并且进行任务间的泛化，而模型本身的改进也聚焦于如何更快进行单一任务的拟合以及对于 Position 等内容的泛化。&lt;/p&gt;
&lt;p&gt;[^act]: ACT: https://arxiv.org/abs/2304.13705
[^dp]: Diffusioni Policy: https://arxiv.org/abs/2303.04137&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/53d7ea6be3a1d99052a205dd0cd767e6.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;伴随着 LLM 的兴起，Scaling law 展现了其可能性，谷歌的 Robotics 团队以及 Everyday Robots 采集了大量的真机数据，并且训练了鼎鼎大名的 RT-1[^rt1]，尽管 RT-1 出于时代的局限性并未采取 VLM 的多 Encoder + Transformer 作为 Fusion 的架构，但是依然展现了跨语言的泛化能力。&lt;/p&gt;
&lt;p&gt;[^rt1]: RT-1: https://arxiv.org/abs/2212.06817&lt;/p&gt;
&lt;p&gt;RT-2[^rt2] 依然由谷歌团队提出，并且使用了标准的 VLM 架构，而后的 OpenVLA[^openvla] 将这一架构的开源实现提供给了整个学界，自此将 VLM 作为核心组件的范式开始流行。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/f7bcdbdcfb7fd0a11b0805cb0bf25523.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;[^rt2]: RT-2: https://arxiv.org/abs/2307.15818
[^openvla]: OpenVLA: https://arxiv.org/abs/2406.09246&lt;/p&gt;
&lt;h2&gt;为什么我们需要 VLM&lt;/h2&gt;
&lt;p&gt;正如 &lt;a href=&quot;/blog/embodied-talk-1&quot;&gt;上一篇博客&lt;/a&gt; 所提及的，数据是具身智能的基石，而基于各种方案生成的数据质量，决定了模型可以走多远，但是一个残酷的事实是，在 LLM 以及 VLM 领域中发展的漫长时间中，最为庞大且体现 Scaling law 以及泛化的模型，都是基于数十 Million 起步，乃至数十 Billion 数据训练的庞大模型，而其中每一份数据都是在彼此之间具有多样性的。&lt;/p&gt;
&lt;p&gt;我们希望某个具身大模型，可以具备和 VLM  同等，甚至在具身相关的任务（比如说 Planning/Grounding/Trajectory/etc.）中获得更强的理解以及泛化能力，这毫无疑问意味着在语言作为任务描述的情况下，我们需要数以百万的 diversity 的 Tasks 才可以获得一个自己定义架构的 pre-trained VLA。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/27809993d25cdf4fa67bfa8d2dbabf8a.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;更何况这一点事实上现在对于 VLM 的数据体量尚且不现实，以至于 VLM 需要基于 LLM 的预训练权重进行训练，更何况 VLA，因此我们一定要引入 VLM 在整体的框架中，这是毫无疑问的。毕竟不说百万 Diversity 的 Tasks 了，诸如 OXE[^oxe] 等提供了百万量级的数据，但是绝大多数数据依然是短程的单一 Tasks 上重复采集的数据。尽管学习动作信息对于 VLA 同样至关重要，但是仅仅从 VLA 数据中获得 VLM 级别的理解以及泛化，这并不现实。&lt;/p&gt;
&lt;p&gt;[^oxe]: OXE: https://arxiv.org/abs/2310.08864&lt;/p&gt;
&lt;p&gt;引入 VLM 的方案是一种 VLA 的设计思路，而显然另一种范式的兴起也不可忽视，也就是 World Model 的范式，或者说，使用 Video Prediction Model 作为 VLA 的主干部分，并且辅佐以 DP 或者 Detokenizer 进行 Action 的输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4770b81c9c457a8dc1a4cfcbf858fc67.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;相关领域的代表工作包括 Unified Video Action Model[^unifiedvam] 以及 Genie Envisioner[^genieenvisioner] 等，而正如上一篇 &lt;a href=&quot;/blog/embodied-talk-1&quot;&gt;博客&lt;/a&gt; 所提及的，Video Gen 的泛化性以及质量，对于 VLA 来说依然是一个有待解决的问题，更何况作为一个 VLA 模型，需要进行推理并且执行复杂指令。&lt;/p&gt;
&lt;p&gt;[^unifiedvam]: Unified Video Action Model: https://arxiv.org/abs/2503.00200
[^genieenvisioner]: Genie Envisioner: https://arxiv.org/abs/2508.05635&lt;/p&gt;
&lt;p&gt;综上来看，不难看出，在 VLM 中引入一个 VLM 作为主干，是短时间内的刚需。&lt;/p&gt;
&lt;h2&gt;VLM 的两种范式&lt;/h2&gt;
&lt;p&gt;对于将 VLM 加入 VLA 的范式，主要包含两种流派，依照整个领域中最有影响力的相关工作，我将其概括为，OpenVLA-like 的架构以及 Pi-like 的架构，并且在此基础上，包含了大量的变种。&lt;/p&gt;
&lt;p&gt;两种 VLA 范式，其主要区别便在于，在 VLA 输入 Image, Language 以及 State（可选，当前部分模型无需输入当前 State）后，VLM 是否直接输出 Action。&lt;/p&gt;
&lt;h3&gt;OpenVLA-like&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/c20a2b1d383550dc6cfb063592d599c2.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;OpenVLA 的结构简洁明了，如图所示，这种结构受到诸如 RT-2 等模型的启发。图像同时经过 DinoV2 以及 SigLIP 作为 Vision Encoder，之后 Concat 进入 Projector，作为模型的视觉 token，而语言部分则经过 Llama2 的 tokenizer，并且二者 concat 之后输入 Llama2 的 backbone。这一 Setting Follow 了 Prismatic VLMs[^prismaticvlm] 的 Setting。&lt;/p&gt;
&lt;p&gt;[^prismaticvlm]: Prismatic VLMs: https://arxiv.org/abs/2402.07865&lt;/p&gt;
&lt;p&gt;OpenVLA 的核心在于让模型直接输出 Action token，这一过程实际上是将动作维度离散化为 256 个区间，并且使用 Llama 分词器中使用频率最低的 256 个 token 表示。&lt;/p&gt;
&lt;p&gt;事实上在笔者看来，直接使用 Llama 输出 Action token 是十分有风险的，毕竟本身的 Action token 并不是和语言 token 在统一语义空间，直接使用 action token 会导致模型本来输出的 logits 里面概率最低的 256 个词汇如今概率需要是最高的一批，这导致模型在 Finetune 的过程中几乎要推翻自己之前学到的分布。而同时，在 co-training VQA 以及 VLA 数据的时候，此时一方面 VQA 中这些 Token 概率需要接近 0，一方面 VLA 中需要接近 1，这使得模型在强行学习双峰分布。&lt;/p&gt;
&lt;h3&gt;Pi-like&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/20810604f8991d9d72e3498ca5313f95.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Pi-0[^pi0] 以及 Pi-0.5[^pi05] 的范式则同样本质，且某种程度上与 Unified Model 当前的范式基本一致，VLM 在训练 VLA 数据的过程中输出 hidden state，之后使用 hidden state 作为 condition，并且使用 DP/Flow Matching 进行 Action 的输出。&lt;/p&gt;
&lt;p&gt;[^pi0]: Pi-0: https://arxiv.org/abs/2410.24164
[^pi05]: Pi-0.5: https://arxiv.org/abs/2504.16054&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/69dec27db398864810d7624794308a07.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;而向前追溯，则可以溯源到 2024 年的 TinyVLA[^tinyvla] 以及 CogACT[^cogact] 等，均是使用相同的思路。相似范式的模型还包括 GR-3[^gr3] 以及 InstrucVLA[^instrucvla] 等，均使用 VLM 作为 backbone，并且使用 DP/Flow Matching 进行 Action 的输出。&lt;/p&gt;
&lt;p&gt;[^gr3]: GR-3: https://arxiv.org/abs/2507.15493
[^instrucvla]: InstrucVLA: https://arxiv.org/abs/2507.17520&lt;/p&gt;
&lt;p&gt;[^tinyvla]: TinyVLA: https://arxiv.org/abs/2409.12514
[^cogact]: CogACT: https://arxiv.org/abs/2411.19650&lt;/p&gt;
&lt;p&gt;这套框架之所以本质，包括两个特点。&lt;/p&gt;
&lt;p&gt;其一，完整的 VLM 保留了 VLM 完整的能力，使得模型可以在多个训练 Stage 中对于具身相关的 VQA 任务先进行预训练（在这些数据集上进行 Finetune），再之后进行 VLA 的训练，使用 Hidden state 进行 Action 的输出。一方面这可以让模型在 VLA 相关任务上具有更好的性能，另一方面不存在 OpenVLA 类似的多峰问题，可以进行 co-training。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/44ea689a2683b9368765b7473f3e2b59.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;其二，使用 DP 或者 Flow Matching 进行 Action 的输出，这使得模型可以更快输出更大的 Action chunk，增大了模型的推理速度。同时对于模型来说，使用 Condition 为后续的特征融合提供了更丰富的信息。诸如最近 DinoV3[^dinv3] 横空出世，自然就可以将 Vision 信息 concat 在 hidden state 上，而诸如三维感知等内容，如果在 VLM 的前端输入，对应的细粒度信息难以从 hidden state 中传达，而是主要剩余 semantic 信息，但是对于可以将 3D encoder 的输出 concat 在 hidden state 上，这使得模型可以获得更丰富的空间感知能力，如 FiS-VLA[^fisvla] 便是采用了此方案。&lt;/p&gt;
&lt;p&gt;[^dinv3]: DinoV3: https://arxiv.org/abs/2508.10104
[^fisvla]: FiS-VLA: https://arxiv.org/abs/2506.01953&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/3e99ba638984d6644960996574982260.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在此基础上，实际上 Pi-like 的范式在 VLM 与 DP 的连接方式，以及整体的训练流程上均进行了大量的讨论。早期的 TinyVLA 在一阶段的 VQA 预训练之后用 LoRA 进行 finetune，将 VLM 的 embedding 直接作为 DP 的 Condition；而之后 CogACT 则将连接的方式换为了 Cognition Token，也就是类似于 Bert 的 CLS Token；Pi-0 以及 Pi-0.5 使用 MoT 作为连接方式，也就是对于 VLM 的推理保留其 KV Cache，并且将 KV 和 Diffusion 的 KV 进行 Concat，一起进行 denoise 的学习。&lt;/p&gt;
&lt;p&gt;假如说使用 Pi-like 的范式搭建模型是更加本质的，那么剩下的事情其实就很明显了，对于模型来说，留给我们的无非是几个关键问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何引入 VLM 的先验知识
&lt;ul&gt;
&lt;li&gt;我们是否需要保留全部的先验知识，还是选择性地保留，如果需要选择，保留哪些&lt;/li&gt;
&lt;li&gt;我们需要以何种方式将 VLM 的信息传递给 Action Expert，例如，什么结构或者传递哪些信息&lt;/li&gt;
&lt;li&gt;我们如何设计一套训练流程，比如 KI，比如 Co-training，比如 Multi-stage training&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如何具有更好的具身操作能力
&lt;ul&gt;
&lt;li&gt;我们是否需要用 VLA 数据进行预训练，如果是，如何训练，哪些数据是关键的&lt;/li&gt;
&lt;li&gt;我们是否应该引入其他的表征信息，如果是，引入哪些，比如深度或者触觉&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;以及一些别的问题
&lt;ul&gt;
&lt;li&gt;我们是否需要进行强化学习，这是否是必经之路，如何设计通用的奖励模型或者奖励函数&lt;/li&gt;
&lt;li&gt;一些特殊的形式，比如说 interleave, reasoning, 等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以遇见的是，在未来客观的一段时间内，全部的主流论文都会在这些内容上进行探索。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;回顾 VLA 的发展史，其实一切才刚刚起步，但是范式已经开始快速迭代并且收敛。由于 VLA 需要的泛化能力，仅凭大量的真机数据，难以达到 VLM 级别的泛化，因此不得已借助 VLM 的能力，而如何利用 VLM 的能力，则是 VLA 范式中最为核心的问题。&lt;/p&gt;
&lt;p&gt;OpenVLA-like 以及 Pi-like 的范式是 VLM 在 VLA 中的两种主流范式，而暂时在笔者看来，Pi-like 的范式更具有潜力，并且可以进行更深层次的探索，至于 World Model，则尚且并未完全成熟，但是伴随着视频模型作为基模的不断发展，依然具有不可小觑的潜力。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-10-zh.webp"/><enclosure url="https://picr2.axi404.top/template-10-zh.webp"/></item><item><title>傻瓜也可以部署的 RustDesk 教程</title><link>https://axi404.github.io/blog/sealos-rustdesk</link><guid isPermaLink="true">https://axi404.github.io/blog/sealos-rustdesk</guid><description>使用 Sealos 部署 RustDesk 服务器</description><pubDate>Wed, 20 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;远程工作是一个刚需，因为笔者不可能一直待在实验室中，但是远程工作的远程桌面又通常不太稳定。尽管比如说在 BIOS 中开启开机自启动等功能可以避免停电等情况，但是如 ToDesk 常见的出错问题，使得我需要两个远程桌面来保证工作的稳定性。另一个选择自然是 RustDesk，所以问题就是如何自建一个 RustDesk 服务。&lt;/p&gt;
&lt;h2&gt;部署&lt;/h2&gt;
&lt;p&gt;这里使用 Sealos 来部署 RustDesk 服务，Sealos 是一个 SaaS 平台，可以方便的部署各种服务。点击下方按钮，直接部署：&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ENCRYPTED_ONLY&lt;/code&gt; 设置为 &lt;code&gt;1&lt;/code&gt;，这样就可以保证只有加密的流量可以访问到服务。&lt;/p&gt;
&lt;p&gt;之后按照应用界面的 README 来找到需要的域名和 Key，并且在 &lt;code&gt;我的应用&lt;/code&gt; 中找到 &lt;code&gt;21116:UDP&lt;/code&gt; 以及 &lt;code&gt;21117:TCP&lt;/code&gt; 的端口即可。&lt;/p&gt;
&lt;p&gt;之后下载 RustDesk，在设置中写入 ID Server 为 &lt;code&gt;域名 + 21116 对应的端口&lt;/code&gt;，Relay Server 为 &lt;code&gt;域名 + 21117 对应的端口&lt;/code&gt;，以及 Key 就好，Key 是在 Logs 中的输出中包含的。&lt;/p&gt;
&lt;h2&gt;自部署&lt;/h2&gt;
&lt;p&gt;使用 Sealos 部署，大概一天需要花大几毛钱，一个月下来也有 20 块钱，已经堪比买一个会员了，因此假如说你有别的用服务器的地方，可以整体租一个服务器，然后自己部署 RustDesk。比如说 Vultr 一个月也就 $3.46，算下来其实差不多，我还可以部署 Arxiv Reader 的邮件服务，以及 Blog 之类的东西。&lt;/p&gt;
&lt;p&gt;整体参照 RustDesk 的 &lt;a href=&quot;https://rustdesk.com/docs/zh-cn/self-host/rustdesk-server-oss/install/&quot;&gt;官方文档&lt;/a&gt; 来部署即可，直接运行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 允许 TCP 和 UDP 的流量
ufw allow 21114:21119/tcp
ufw allow 21116/udp
sudo ufw enable

# 下载并运行一键安装脚本
wget https://raw.githubusercontent.com/techahold/rustdeskinstall/master/install.sh
chmod +x install.sh
./install.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 ID Server 设置为域名，在 Key 中填上命令行中输出的 Key，之后就好了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-9-zh.webp"/><enclosure url="https://picr2.axi404.top/template-9-zh.webp"/></item><item><title>InternManip 踩坑记录</title><link>https://axi404.github.io/blog/internmanip-code</link><guid isPermaLink="true">https://axi404.github.io/blog/internmanip-code</guid><description>准备使用 InternManip 来构建我的模型，记录一下从搭建环境到训练模型过程中的问题</description><pubDate>Mon, 18 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;正如 &lt;a href=&quot;/blog/week-59&quot;&gt;上回书说到&lt;/a&gt;，我准备使用 InternManip 来构建我的模型，因为 InternManip 的代码支持训测一体，伴随着后续的支持变多，可以更加快速地进行模型的迭代，这正是我需要的功能。&lt;/p&gt;
&lt;p&gt;为了给 InternManip 团队部分的反馈，以及为了后续在其他环境上的可迁移性，在这里记录一下使用以及踩坑的过程。&lt;/p&gt;
&lt;h2&gt;环境搭建&lt;/h2&gt;
&lt;p&gt;InternManip 的代码提供了一键安装的脚本，不过在此之前还是需要安装一些基本的依赖。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 安装 gcc g++ 以及 ninja
sudo apt update
sudo apt install build-essential
sudo apt install ninja-build

# 安装 git 以及 git-lfs
sudo apt install git
sudo apt install git-lfs
git lfs install

# 安装 miniconda
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
source ~/miniconda3/bin/activate
conda init --all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然，假如说没有 sudo 权限，也可以使用 conda 来安装。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda install -c conda-forge ninja
conda install -c conda-forge gcc=11 gxx=11
conda install -c conda-forge git git-lfs
git lfs install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后安装基础包即可，全量包需要安装 InternUtopia 以及 GenManip，二者基于 Isaac，因此只能在 RTX 系列的卡上运行，在 H/A100 等卡上没必要安装。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Git Clone
git clone https://github.com/InternRobotics/InternManip.git
cd InternManip
git submodule update --init --recursive

# 安装 uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# 启动一键安装脚本
chmod +x install.sh
./install.sh --beginner
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;本身 InternManip 是基于 uv 来管理依赖的，在安装的过程中，在 model 环境中会安装 GR00T 的环境，这个过程中可能会报错：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;uv pip install &quot;git+https://github.com/NVIDIA/Isaac-GR00T.git#egg=isaac-gr00t[base]&quot;
Using Python 3.10.12 environment at: .venv/model Resolved 162 packages in 1.11s 
× Failed to download and build gr00t @ git+https://github.com/NVIDIA/Isaac-GR00T.git@ae7d46f02cdca5d9c80efc446fe41fe2b58e94c7
 ├─▶ Git operation failed
 ╰─▶ process didn&apos;t exit successfully: /usr/bin/git reset --hard ae7d46f02cdca5d9c80efc446fe41fe2b58e94c7 (exit status: 128) 
 --- stderr Downloading media/robot-demo.gif (6.6 MB) 
Error downloading object: media/robot-demo.gif (164861f): Smudge error: Error downloading media/robot-demo.gif (164861fc0eee1291db00364d449f4920e54f5132881f74526a636377f6bc11e5): error transferring &quot;164861fc0eee1291db00364d449f4920e54f5132881f74526a636377f6bc11e5&quot;: [0] remote missing object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个问题本质上是因为在 GR00T 中的 &lt;code&gt;media/robot-demo.gif&lt;/code&gt; 这个文件没有被下载下来，而这个文件本身是一个 git-lfs 存储的文件，虽然安装了 &lt;code&gt;git-lfs&lt;/code&gt;，但是貌似还是难以下载，这里直接修改 &lt;code&gt;install.sh&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;##############################################################
# Install model dependencies
##############################################################
install_model() {
    ...
    uv pip install --upgrade setuptools
    uv pip install &quot;git+https://github.com/NVIDIA/Isaac-GR00T.git#egg=isaac-gr00t[base]&quot; || { echo -e &quot;⚠️  \033[1;33mWarning: Failed to install GR00T dependencies due to network issues\033[0m&quot; &gt;&amp;#x26;2; } # [!code --]
    GIT_LFS_SKIP_SMUDGE=1 uv pip install &quot;git+https://github.com/NVIDIA/Isaac-GR00T.git#egg=isaac-gr00t[base]&quot; || { echo -e &quot;⚠️  \033[1;33mWarning: Failed to install GR00T dependencies due to network issues\033[0m&quot; &gt;&amp;#x26;2; } # [!code ++]
    echo -e &quot;📦 Installing flash-attn module...&quot;
    uv pip install --no-build-isolation flash-attn==2.7.1.post4 || { echo -e &quot;⚠️  \033[1;33mWarning: Failed to install flash-attn due to network issues\033[0m&quot; &gt;&amp;#x26;2; }
    uv pip install draccus
    uv pip install transforms3d

    install_base_requirements

    deactivate
    echo -e &quot;✅ \033[1;32mModel dependencies installed\033[0m&quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后正常安装 beginner 即可。&lt;/p&gt;
&lt;p&gt;当然，理论来说在安装的过程中也可能出现其他的问题，目前 InternManip 的文档中有一个 &lt;a href=&quot;https://internrobotics.github.io/user_guide/internmanip/quick_start/installation.html#troubleshooting&quot;&gt;Troubleshooting&lt;/a&gt; 的章节，可以参考一下。&lt;/p&gt;
&lt;h2&gt;训练模型&lt;/h2&gt;
&lt;p&gt;InternManip 的 Dataloader 和 Trainer 本质上都是基于 GR00T 的，也就是使用 Modality 来管理 LeRobot Dataset 格式中的各种 key，使得整体更加清晰且灵活。&lt;/p&gt;
&lt;p&gt;GR00T 的代码本身由和我合作的同学已经经过了反复的验证，确认是没有问题的，但是 InternManip 的代码按理来说为了加入到自己的框架中，必然需要从 GR00T 中解耦部分的内容，因此我并不确定在这个过程中是否有错误的修改，因此需要进行 Diff 的检查，同时也进行一下 code reading。&lt;/p&gt;
&lt;h3&gt;模型加载&lt;/h3&gt;
&lt;p&gt;在这里还是从训练的入口来看，也就是 &lt;code&gt;scripts/train/train.py&lt;/code&gt;，在开始的部分，首先通过 Training Cfg，从里面读取模型的信息，并且准备好模型：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def main(config: TrainCfg):
    &quot;&quot;&quot;Main training function.&quot;&quot;&quot;
    # ------------ load model ------------
    kwargs = config.model_dump()
    kwargs.pop(&apos;model_type&apos;)
    if config.base_model_path == &apos;&apos;:
        config.base_model_path = POLICY_NAME_TO_ID[config.model_type]
    if config.base_model_path is None:
        model_cfg = AutoConfig.for_model(config.model_type, **kwargs)
        model = AutoModel.from_config(model_cfg, **kwargs)
    else:
        # must ensure that if the path is a huggingface model, it should be a repo that has only one model weight
        model = AutoModel.from_pretrained(config.base_model_path, **kwargs)
    model.compute_dtype = config.compute_dtype
    model.config.compute_dtype = config.compute_dtype
    data_collator = DataCollatorRegistry.get_collator(config.model_type)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假如说没有通过 &lt;code&gt;config.model_type&lt;/code&gt; 来获取模型的类型，那么会通过 &lt;code&gt;AutoConfig.for_model&lt;/code&gt; 来获取模型的配置，之后通过 &lt;code&gt;AutoModel.from_config&lt;/code&gt; 来获取模型。否则就会通过 &lt;code&gt;AutoModel.from_pretrained&lt;/code&gt; 来获取模型。&lt;/p&gt;
&lt;p&gt;对于不了解 Transformer 框架的读者简单解释一下，这里是 Transformer 框架的模型加载方式，&lt;code&gt;AutoConfig.for_model&lt;/code&gt; 和 &lt;code&gt;AutoModel.from_config&lt;/code&gt; 用于从配置中创建模型，而框架之所以可以加载自己定义的模型，是因为在代码的编写过程中，通过 API 对于模型以及 Config 进行了注册。同时 &lt;code&gt;AutoModel.from_pretrained&lt;/code&gt; 会自动从 Hugging Face 中下载模型权重，并加载到本地。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# register
AutoConfig.register(&apos;gr00t_n1_5&apos;, GR00T_N1_5_Config)
AutoModel.register(GR00T_N1_5_Config, GR00T_N1_5)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;数据加载&lt;/h3&gt;
&lt;p&gt;在数据加载的部分，首先通过 &lt;code&gt;EmbodimentTag&lt;/code&gt; 来获取数据集的标签，之后通过 &lt;code&gt;DATA_CONFIG_MAP&lt;/code&gt; 来获取数据集的配置，之后通过 &lt;code&gt;LeRobotSingleDataset&lt;/code&gt; 来获取数据集或者 &lt;code&gt;LeRobotMixtureDataset&lt;/code&gt; 来获取多个数据集。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def main(config: TrainCfg):
    &quot;&quot;&quot;Main training function.&quot;&quot;&quot;
    ...
    # ------------ load dataset ------------
    embodiment_tag = EmbodimentTag(config.embodiment_tag)

    # modality configs and transforms
    data_config_cls = DATA_CONFIG_MAP[config.data_config]
    model_transform, observation_indices, action_indices = model.config.transform()
    modality_configs = data_config_cls.modality_config(observation_indices, action_indices)
    transforms = data_config_cls.transform()

    if config.pad_center_crop:
        transforms = [tr if not isinstance(tr, VideoCrop) else VideoPadSquareCrop(apply_to=tr.apply_to, scale=0.95) for tr in transforms]
    if model_transform is not None:
        transforms.append(model_transform)

    transforms = ComposedModalityTransform(transforms=transforms)
    # data_loader
    if isinstance(config.dataset_path, str):
        train_dataset = LeRobotSingleDataset(
            dataset_path=config.dataset_path,
            modality_configs=modality_configs,
            transforms=transforms,
            embodiment_tag=embodiment_tag,  # This will override the dataset&apos;s embodiment tag to &quot;new_embodiment&quot;
            video_backend=config.video_backend,
            cache_dir=config.HF_cache_dir,
            skip_unlabeled=config.skip_unlabeled
        )
    else:
        print(&apos;\n&apos; + &apos;=&apos;*30)
        print(&apos;⚠️  WARNING: MULTIPLE DATASETS DETECTED&apos;)
        print(&apos;=&apos;*30)
        print(&apos;You are about to train on multiple datasets simultaneously.&apos;)
        print(&apos;Please ensure that:&apos;)
        print(&apos;  1. All datasets have compatible and consistent modality configurations&apos;)
        print(&apos;  2. The datasets are from the same embodiment or compatible embodiments&apos;)
        print(&apos;  3. The datasets have similar data distributions and task objectives&apos;)
        print(&apos;=&apos;*30 + &apos;\n&apos;)
        single_datasets = []
        for p in config.dataset_path:
            assert os.path.exists(p), f&apos;Dataset path {p} does not exist&apos;
            # We use the same transforms, modality configs, and embodiment tag for all datasets here
            dataset = LeRobotSingleDataset(
                dataset_path=p,
                modality_configs=modality_configs,
                transforms=transforms,
                embodiment_tag=embodiment_tag,
                video_backend=config.video_backend,
                cache_dir=config.HF_cache_dir,
                skip_unlabeled=config.skip_unlabeled
            )
            single_datasets.append(dataset)

        train_dataset = LeRobotMixtureDataset(
            data_mixture=[
                (dataset, 1.0)  # we will use equal weights for all datasets
                for dataset in single_datasets
            ],
            mode=&apos;train&apos;,
            balance_dataset_weights=config.balance_dataset_weights,
            balance_trajectory_weights=config.balance_trajectory_weights,
            seed=42,
            metadata_config={
                &apos;percentile_mixing_method&apos;: &apos;weighted_average&apos;,
            },
        )
        print(f&apos;Loaded {len(single_datasets)} datasets, with {config.dataset_path} &apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;本身这部分并不难理解，就是从 Config 里面获得到数据集的配置，之后通过 &lt;code&gt;LeRobotSingleDataset&lt;/code&gt; 或者 &lt;code&gt;LeRobotMixtureDataset&lt;/code&gt; 来获取数据集。&lt;/p&gt;
&lt;p&gt;在这里最核心的操作还是从 &lt;code&gt;DATA_CONFIG_MAP&lt;/code&gt; 获取到的 &lt;code&gt;data_config_cls&lt;/code&gt;，这里也是 GR00T 的精髓所在，通过这里的 &lt;code&gt;data_config&lt;/code&gt; 以及数据中的 &lt;code&gt;modality.json&lt;/code&gt; 来设置 Key 以及对应的截取变换。&lt;/p&gt;
&lt;p&gt;以一个较为完整的数据集举例，如 &lt;code&gt;AlohaDataConfig&lt;/code&gt;，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class AlohaDataConfig(BaseDataConfig):
    #==========================key list==========================#
    video_keys = [&apos;video.left_view&apos;, &apos;video.right_view&apos;, &apos;video.top_view&apos;]
    state_keys = [&apos;state.arm_qpos&apos;]
    action_keys = [&apos;action.left_arm_delta_qpos&apos;, &apos;action.right_arm_delta_qpos&apos;, &apos;action.left_gripper_close&apos;, &apos;action.right_gripper_close&apos;]
    language_keys = [&apos;annotation.human.action.task_description&apos;]

    #==========================modality config==========================#
    def modality_config(self, observation_indices, action_indices) -&gt; dict[str, ModalityConfig]:
        self.action_indices = action_indices
        self.observation_indices = observation_indices
        video_modality = ModalityConfig(
            delta_indices=self.observation_indices,
            modality_keys=self.video_keys,
        )

        state_modality = ModalityConfig(
            delta_indices=self.observation_indices,
            modality_keys=self.state_keys,
        )

        action_modality = ModalityConfig(
            delta_indices=self.action_indices,
            modality_keys=self.action_keys,
        )

        language_modality = ModalityConfig(
            delta_indices=self.observation_indices,
            modality_keys=self.language_keys,
        )


        modality_configs = {
            &apos;video&apos;: video_modality,
            &apos;state&apos;: state_modality,
            &apos;action&apos;: action_modality,
            &apos;language&apos;: language_modality,
        }

        return modality_configs

    #==========================transform==========================#
    def transform(self):
        transforms = [
            VideoToTensor(apply_to=self.video_keys),
            VideoCrop(apply_to=self.video_keys, scale=0.95),
            VideoResize(apply_to=self.video_keys, height=224, width=224, interpolation=&apos;linear&apos;),
            VideoColorJitter(
                apply_to=self.video_keys,
                brightness=0.3,
                contrast=0.4,
                saturation=0.5,
                hue=0.08,
            ),
            # state transforms
            StateActionToTensor(apply_to=self.state_keys),
            StateActionTransform(
                apply_to=self.state_keys,
                normalization_modes={
                    &apos;state.arm_qpos&apos;: &apos;mean_std&apos;
                },
            ),
            # action transforms
            StateActionToTensor(apply_to=self.action_keys),
            StateActionTransform(
                apply_to=self.action_keys,
                normalization_modes={
                    &apos;action.left_arm_delta_qpos&apos;: &apos;mean_std&apos;,
                    &apos;action.right_arm_delta_qpos&apos;: &apos;mean_std&apos;,
                    &apos;action.left_gripper_close&apos;: &apos;binary&apos;,
                    &apos;action.right_gripper_close&apos;: &apos;binary&apos;
                }
            ),
            # concat transforms
            ConcatTransform(
                video_concat_order=self.video_keys,
                state_concat_order=self.state_keys,
                action_concat_order=self.action_keys,
            )
        ]
        return transforms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过代码中的注释，我们将每一个 data config 都分为了三部分，当读者要引入自己的数据集时，只需要按照这个格式来写即可。&lt;/p&gt;
&lt;p&gt;第一部分是 Key 的列表，一般来说数据中包括图像、状态、动作、语言四种内容，也就是 &lt;code&gt;video_keys&lt;/code&gt;、&lt;code&gt;state_keys&lt;/code&gt;、&lt;code&gt;action_keys&lt;/code&gt;、&lt;code&gt;language_keys&lt;/code&gt;，对于 &lt;code&gt;video_keys&lt;/code&gt;，其可能来自于不同的相机的不同图片，而对于 action 来说，也可能是不同的关节的数据，这些内容在 LeRobot 数据集中按照 dict 的格式保存，如对于 &lt;code&gt;video.left_view&lt;/code&gt; 来说，实际上就是数据集的 &lt;code&gt;data[&quot;video&quot;][&quot;left_view&quot;]&lt;/code&gt;，而 &lt;code&gt;state.arm_qpos&lt;/code&gt; 则是 &lt;code&gt;data[&quot;state&quot;][&quot;arm_qpos&quot;]&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;以源代码中的实例数据 &lt;code&gt;internmanip/demo_data/robot_sim.PickNPlace/meta/modality.json&lt;/code&gt; 为例，其 modality 决定了数据集的 Key 以及对应的截取变换，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
    &quot;state&quot;: {
        &quot;left_arm&quot;: {
            &quot;start&quot;: 0,
            &quot;end&quot;: 7
        },
        &quot;left_hand&quot;: {
            &quot;start&quot;: 7,
            &quot;end&quot;: 13
        },
        &quot;left_leg&quot;: {
            &quot;start&quot;: 13,
            &quot;end&quot;: 19
        },
        &quot;neck&quot;: {
            &quot;start&quot;: 19,
            &quot;end&quot;: 22
        },
        &quot;right_arm&quot;: {
            &quot;start&quot;: 22,
            &quot;end&quot;: 29
        },
        &quot;right_hand&quot;: {
            &quot;start&quot;: 29,
            &quot;end&quot;: 35
        },
        &quot;right_leg&quot;: {
            &quot;start&quot;: 35,
            &quot;end&quot;: 41
        },
        &quot;waist&quot;: {
            &quot;start&quot;: 41,
            &quot;end&quot;: 44
        }
    },
    &quot;action&quot;: {
        &quot;left_arm&quot;: {
            &quot;start&quot;: 0,
            &quot;end&quot;: 7
        },
        &quot;left_hand&quot;: {
            &quot;start&quot;: 7,
            &quot;end&quot;: 13
        },
        &quot;left_leg&quot;: {
            &quot;start&quot;: 13,
            &quot;end&quot;: 19
        },
        &quot;neck&quot;: {
            &quot;start&quot;: 19,
            &quot;end&quot;: 22
        },
        &quot;right_arm&quot;: {
            &quot;start&quot;: 22,
            &quot;end&quot;: 29
        },
        &quot;right_hand&quot;: {
            &quot;start&quot;: 29,
            &quot;end&quot;: 35
        },
        &quot;right_leg&quot;: {
            &quot;start&quot;: 35,
            &quot;end&quot;: 41
        },
        &quot;waist&quot;: {
            &quot;start&quot;: 41,
            &quot;end&quot;: 44
        }
    },
    &quot;video&quot;: {
        &quot;ego_view&quot;: {
            &quot;original_key&quot;: &quot;observation.images.ego_view&quot;
        }
    },
    &quot;annotation&quot;: {
        &quot;human.action.task_description&quot;: {},
        &quot;human.validity&quot;: {}
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第二部分是 Modality 的配置，这里相较于原本的 GR00T 的配置，改成了通过传参输入 &lt;code&gt;observation_indices&lt;/code&gt; 以及 &lt;code&gt;action_indices&lt;/code&gt;。假如说目光回到 main 的代码中，会发现这两个数值来自于 &lt;code&gt;model.config.transform()&lt;/code&gt;，而 对应的代码在 &lt;code&gt;internmanip/configs/model/gr00t_cfg.py&lt;/code&gt; 中，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# config
@dataclass
class GR00T_N1_5_Config(PretrainedConfig):
    model_type = &apos;gr00t_n1_5&apos;
    backbone_cfg: dict = field(init=False, metadata={&apos;help&apos;: &apos;Backbone configuration.&apos;})

    action_head_cfg: dict = field(init=False, metadata={&apos;help&apos;: &apos;Action head configuration.&apos;})

    action_horizon: int = field(init=False, metadata={&apos;help&apos;: &apos;Action horizon.&apos;})

    action_dim: int = field(init=False, metadata={&apos;help&apos;: &apos;Action dimension.&apos;})
    compute_dtype: str = field(default=&apos;float32&apos;, metadata={&apos;help&apos;: &apos;Compute dtype.&apos;})
    observation_indices = [0]

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

    def transform(self):
        transforms = GR00TTransform_15(
                state_horizon=len(self.observation_indices),
                action_horizon=len(list(range(self.action_horizon))),
                max_state_dim=self.action_head_cfg[&apos;max_state_dim&apos;],
                max_action_dim=self.action_head_cfg[&apos;max_action_dim&apos;],
            )
        return transforms, self.observation_indices, list(range(self.action_horizon))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以注意到这些参数实际上并没有默认值。实际上在运行 GR00T 的训练过程中，例如运行代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nnodes 1 --nproc_per_node 1 scripts/train/train.py --config run_configs/train/gr00t_n1_5_genmanip_v1.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时会包括 &lt;code&gt;model_type&lt;/code&gt; 为 &lt;code&gt;gr00t_n1_5&lt;/code&gt; 的参数，而后加载 &lt;code&gt;nvidia/GR00T-N1.5-3B&lt;/code&gt; 的 &lt;code&gt;model_base_path&lt;/code&gt;，此时会自动从 Hugging Face 中下载模型权重，并加载到本地，而在这个过程中，会自动加载 Huggingface 上的配置信息，也就是从 &lt;code&gt;nvidia/GR00T-N1.5-3B&lt;/code&gt; 的 &lt;code&gt;config.json&lt;/code&gt; 中加载 &lt;a href=&quot;https://huggingface.co/nvidia/GR00T-N1.5-3B/blob/main/config.json&quot;&gt;配置信息&lt;/a&gt;，并设置到 &lt;code&gt;GR00T_N1_5_Config&lt;/code&gt; 中。&lt;/p&gt;
&lt;p&gt;在拥有了 &lt;code&gt;observation_indices&lt;/code&gt; 以及 &lt;code&gt;action_horizon&lt;/code&gt; 之后，就会对应的指定 &lt;code&gt;ModalityConfig&lt;/code&gt;。这里面 key 的顺序决定了 dataloader 中返回的顺序，而 &lt;code&gt;observation_indices&lt;/code&gt; 以及 &lt;code&gt;action_indices&lt;/code&gt; 则决定了获得的内容长度。也就是假如说 &lt;code&gt;observation_indices&lt;/code&gt; 为 &lt;code&gt;[-2, -1, 0]&lt;/code&gt;，那么返回过去到现在的 3 帧的数据，而 &lt;code&gt;action_indices&lt;/code&gt; 为 &lt;code&gt;list(range(16))&lt;/code&gt;，那么返回现在到未来的 16 帧的动作。&lt;/p&gt;
&lt;p&gt;第三部分是 Transform 的配置，这部分本质上没什么好说的，就是各种各样的变换，值得注意的是其中 &lt;code&gt;apply_to&lt;/code&gt; 的参数，其决定了这个变换应用到哪些 Key 上。&lt;/p&gt;
&lt;p&gt;在配置好之后，通过 &lt;code&gt;ComposedModalityTransform&lt;/code&gt; 来将所有的 Transform 进行组合，之后通过 &lt;code&gt;LeRobotSingleDataset&lt;/code&gt; 或者 &lt;code&gt;LeRobotMixtureDataset&lt;/code&gt; 来获取数据集。&lt;/p&gt;
&lt;h3&gt;训练&lt;/h3&gt;
&lt;p&gt;在拥有了数据集以及模型之后，就可以开始训练了，也就是这个程序的最后一段。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def main(config: TrainCfg):
    ...
    # ------------ load model ------------
    ...
    # ------------ load dataset ------------
    ...
    # ------------ load trainer ------------
    if config.lora_rank &gt; 0:
        model = get_lora_model(
            model,
            rank=config.lora_rank,
            lora_alpha=config.lora_alpha,
            lora_dropout=config.lora_dropout,
        )
    # modify training args
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        run_name=None,
        remove_unused_columns=False,
        deepspeed=&apos;&apos;,
        gradient_checkpointing=False,
        bf16=True,
        tf32=True,
        per_device_train_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        dataloader_num_workers=config.dataloader_num_workers,
        dataloader_pin_memory=False,
        dataloader_persistent_workers=True,
        optim=&apos;adamw_torch&apos;,
        adam_beta1=0.95,
        adam_beta2=0.999,
        adam_epsilon=1e-8,
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        warmup_ratio=config.warmup_ratio,
        lr_scheduler_type=&apos;cosine&apos;,
        logging_steps=10.0,
        num_train_epochs=300,
        max_steps=config.max_steps,
        save_strategy=&apos;steps&apos;,
        save_steps=config.save_steps,
        eval_strategy=&apos;no&apos;,
        save_total_limit=8,
        report_to=config.report_to,
        seed=42,
        do_eval=False,
        ddp_find_unused_parameters=False,
        ddp_bucket_cap_mb=100,
        torch_compile_mode=None,
    )
    # Create the trainer
    trainer = BaseTrainerWrapper(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )

    # Add checkpoint format callback to ensure experiment_cfg is copied to each checkpoint
    ckpt_format_callback = CheckpointFormatCallback(
        train_dataset=train_dataset, exp_cfg_dir=training_args.output_dir
    )
    trainer.add_callback(ckpt_format_callback)
    # ------------ print model info ------------
    ...
    # ------------ train ------------
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
    trainer.save_state()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;逻辑也很简单，也就是通过 &lt;code&gt;TrainingArguments&lt;/code&gt; 来配置训练的参数，之后通过 &lt;code&gt;BaseTrainerWrapper&lt;/code&gt; 来创建 Trainer，之后通过 &lt;code&gt;trainer.train&lt;/code&gt; 来训练模型。这套逻辑基本上就是 Huggingface 风格的 Trainer 的逻辑，不过为了不了解这套风格的读者，还是需要深入 Trainer 来看一下具体的实现。&lt;/p&gt;
&lt;h3&gt;Huggingface Style Trainer&lt;/h3&gt;
&lt;p&gt;让我们深入 Trainer 看一下里面具体发生了什么，通过 Link 进入实现：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class BaseTrainerWrapper(BaseTrainer):
    import torch.nn as nn

    def prediction_step(self, model: nn.Module,
        inputs: Dict[str, Union[torch.Tensor, Any]]):

        actions = model.inference(inputs)[&apos;action_pred&apos;]
        valid_action = inputs[&apos;action_mask&apos;]
        gt_action = inputs[&apos;action&apos;][valid_action]
        pred_action = actions[valid_action].cpu()

        diff = pred_action - gt_action
        import matplotlib.pyplot as plt
        import numpy as np

        mean_diff = diff.abs().mean(dim=0)

        x = np.arange(mean_diff.shape[0])


        num_channels = mean_diff.shape[1]
        fig, axes = plt.subplots(num_channels, 1, figsize=(10, 3 * num_channels), sharex=True)

        x = list(range(pred_action.shape[1]))

        for i in range(num_channels):
            ax = axes[i]
            ax.plot(x, pred_action[0, :, i].cpu().numpy(), label=&apos;Predicted&apos;, color=&apos;blue&apos;)
            ax.plot(x, gt_action[0, :, i].cpu().numpy(), label=&apos;Ground Truth&apos;, color=&apos;orange&apos;)
            ax.set_title(f&apos;Channel {i}&apos;)
            ax.set_ylabel(&apos;Mean Value&apos;)
            ax.grid(True)
            if i == num_channels - 1:
                ax.set_xlabel(&apos;Index (0~15)&apos;)
            if i == 0:
                ax.legend()

        plt.tight_layout()
        plt.savefig(&apos;debug_value_{}.png&apos;.format(0))
        plt.close()

        return mean_diff
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不难发现本身支持包裹了&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-8-zh.webp"/><enclosure url="https://picr2.axi404.top/template-8-zh.webp"/></item><item><title>周记 Week59</title><link>https://axi404.github.io/blog/week-59</link><guid isPermaLink="true">https://axi404.github.io/blog/week-59</guid><description>2025-08-11 ~ 2025-08-17.</description><pubDate>Sun, 17 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { GithubCard } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;继上周把全部的 IROS Challenge 相关的内容都做完了之后，这周可以说基本上都恢复了摆烂的节奏，重启了大量的论文阅读，以及 Blog 的构建，同时也正在思考下一个计划，一个能让我做下一个一年的事情。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;所以说还是说回到科研的事情，基本上 IROS 这边已经彻底没有我需要负责的事情了，也就是假如说还有一些 Bug 的修复，或者是一些小功能的添加，我或许可以帮上忙。&lt;/p&gt;
&lt;p&gt;这一周重启了大量的论文阅读，读者应该不难看到我的 Blog 里面如今又已经充满了各种的 Paper Reading 的内容，当然，本阶段依然是以 Embodied AI 的领域内容为主，而且主要是 Manipulation，后面应该会逐渐多读一些其他的内容，比如说 MLLM 和 Unified Model 的相关内容，当然还有 GenAI 相关的。当下来看似乎 World Model 很火，应该后面也会有一些想法。&lt;/p&gt;
&lt;p&gt;为了加快我的 Paper 阅读的速度，以及浏览的市面上大多数的如 &lt;a href=&quot;https://papers.cool&quot;&gt;papers.cool&lt;/a&gt; 这样的网站之后，我沮丧地发现了目前貌似大家对于 CS.RO 分区的内容并不是很关注，大家都会推送诸如 CS.AI 以及 CS.CV 的内容，也就能覆盖我需要阅读的 MLLM 的内容，但是除此之外，Robotics 的内容却没有人做。因为之前做过一些 Arxiv 相关统计的内容，加上 Cursor 大显神威，因此我也是再次重启 Vibe Coding，花了几天写好了 Arxiv Reader 这个项目，也就是一个可以使用 GPT API 以及 Gmail 的 SMTP 去每日推送 Paper 邮件的项目，顺带一提，也已经开源，欢迎大家使用。我现在订阅了 CS.AI, CS.CV, CS.RO 三个分区，假如有领域兴趣相同的读者，也可以把自己的 email 发给我，我把大家加入到推送列表中。&lt;/p&gt;
&lt;p&gt;当然，顺带一提，因为写 Paper Reading，我一般喜欢附上一些插图，为了保证我本身 Blog 仓库的轻量化，这自然就不可避免需要一个图床，之前基于 Vercel 和 Github 的图床基于 PicX 搭建，但是因为不想滥用公共资源，因此也已经废弃。&lt;/p&gt;
&lt;p&gt;于是也是基于 Cloudflare 的 R2 服务搭建了一个自己的图床，毕竟 Cloudflare 的 R2 几乎就是免费，10 GB 的存储加上不限流量而是按照访问次数计算，免费额度巨大，一个月根本用不完。这个项目叫做 astro-r2，我也进行了开源。&lt;/p&gt;
&lt;p&gt;同时，Paper reading 的内容目前我感觉还是有一些流水线了，有的东西写的太过于简略，以至于基本上是完全给我自己看的，按照费曼学习法的原理来说，还是更详细地写出来会好一些。&lt;/p&gt;
&lt;p&gt;同时我也在写一些更加严肃的博客内容，也就是成主题的自己的思考，目前一个系列应该叫做具身十日谈，第一期讲解了 &lt;a href=&quot;/blog/embodied-talk-1&quot;&gt;数据与仿真器&lt;/a&gt;，已经写好了，欢迎大家去看。同时下一个选题大概率是 VLM 融入 VLA 的路线之争。这些内容应该也还是会反复打磨。&lt;/p&gt;
&lt;p&gt;以及说起来之前的 Isaac 101 系列，现在发现制作起来非常的麻烦，因为有的时候需要引入一些动图，但是本身 Ubuntu 如 &lt;a href=&quot;https://pixpin.cn&quot;&gt;PixPin&lt;/a&gt; 一样好用的截图工具并不多，而目前笔记本的小屏幕来回切屏又格外折磨，应该会等到回到西交之后，用更好的设备去制作。&lt;/p&gt;
&lt;p&gt;目前算是已经是有了一些初步的 VLA 相关的 Idea，对于仿真中的旅程，主线可能也会暂时停止，相关的看法也可以看我的第一期具身十日谈。简单来说，仿真本身确实很强大，能做到很多的东西，但是我目前所作的，生成泛化的 Pick and Place 数据，已经是能为打通 VLA 的 VLM 与 A 的 Gap 的最好性价比了，进一步的 1 DOF Articulation 数据的泛化生成本身也会完善，但是诸如仿真也可以做软体做叠衣服做流体，但是本身这些内容看上去变得复杂了，不过本身作为 researcher 在编程的时候，本身还是基于诸如 Isaac 的库去搭建，本身并不能帮助我的个人能力更进一步，而是一个庞大的 incremental 工作，应该会借助实验室的资源来慢慢推进了。&lt;/p&gt;
&lt;p&gt;现在的问题变成了，我有高质量的数十万 rich annotation 的大量 diverse 的仿真数据，应该如何去利用这些数据，来构建一个更好的 VLA 模型，这应该是一个非常有趣的问题，也是我接下来一段时间的思考方向，甚至说证明 sim2sim 的有效性，而将 real2real 的问题推给业界，在成果足够 solid 的情况下也是可行的，至于自己 sim2real 甚至 (sim+real)2real，也就只能看运气了。目前的 Idea 相当有趣，但是能否妙手偶得之，还未可知也。&lt;/p&gt;
&lt;p&gt;发散的思维说回，在工作量上，这周还是挂了一些数据的，并且做好了 grasp pose proposal 更优雅的解决方案，来自同实验室非常厉害的师兄。相较于目前市面上很多使用 GraspNet 或者 AnyGrasp，甚至比如说 NV 的 GraspGen，基本上都是使用模型，而这些模型的 GT，都来自于几个 GraspPose 的数据集，那些 ground truth 的生成方式被复现，并且用于管线，一方面不需要实时运算，一方面 Pose 也可以更加精准。&lt;/p&gt;
&lt;p&gt;至于上文提到的 Idea，在和 mentor 聊过之后，打算搭建一套自己的 VLA 框架，而因为 Lab 这边的 InternManip 本身就包含了训练框架，从 GR00T 的代码中解耦而来，因此准备 Diff 一波，确保没有问题之后就开始在 InternManip 里面搭建自己的模型。毕竟 InternManip 在一段时间内还会是中心主推的框架，因此比如说一键测试或者类似的东西，还是可以让我的工作流程加速。假如可以顺手多 contribute 一些代码到 InternManip 里面也是何乐而不为。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;无事发生，依然和乐小姐煲电话粥，我大概还有三周回到学校，完成最后未竟的事业，完成保研流程，想她。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-59-zh.webp"/><enclosure url="https://picr2.axi404.top/week-59-zh.webp"/></item><item><title>Isaac Sim 一百讲（6）：Collision 进阶</title><link>https://axi404.github.io/blog/isaac-6</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-6</guid><description>从零开始的 Isaac Sim 之路，第一季开始！</description><pubDate>Sun, 17 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Isaac 101&apos;,
items: [
{
title: &apos;安装&apos;,
href: &apos;/blog/isaac-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;万物皆 Prim&apos;,
href: &apos;/blog/isaac-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;USD&apos;,
href: &apos;/blog/isaac-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Transformation&apos;,
href: &apos;/blog/isaac-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Rigid and Collision&apos;,
href: &apos;/blog/isaac-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Collision 进阶&apos;,
href: &apos;/blog/isaac-6&apos;,
order: &apos;6&apos;
},
{
title: &apos;Camera&apos;,
href: &apos;/blog/isaac-7&apos;,
order: &apos;7&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在此之前，我们已经了解了如何导入一个 Mesh 的 USD 资产，并且通过 Collsion 和 RigidBody 的 API 来让这些物体具有一定的物理属性，可以互相碰撞，并且可以被施加一个外力。然而事实上如你所见，假如你将大量 messy 的资产，如来自 Objaverse 的资产加入到仿真中，因为物理不真实等情况，导致物体依然在碰撞的过程中出现穿模或者弹飞的现象。&lt;/p&gt;
&lt;p&gt;这一现象事实上是一切物理引擎的通病，在 Mujoco 中，读者依然需要调节物体的 stiffness 与 damping，来确保两个物体在接触之后，一个物体可以尽快弹出另一个物体，并且保持稳定，从而实现稳定的碰撞，而对于 Isaac，自然也不例外。本章节我们会介绍在大量调参之后得出的直接结论，这一系列的 API 以及经验可以帮到你，直接地尽可能避免这一现象。&lt;/p&gt;
&lt;h2&gt;现象描述&lt;/h2&gt;
&lt;p&gt;在此处挑选了 Isaac Sim 中的一对资产，这对资产在 &lt;code&gt;.glb&lt;/code&gt; 阶段之后经过了 coacd 的凸分解，并且将 Visual Mesh 与 Collsion Mesh 分离，其中 Collsion mesh 即 coacd 处理之后的 mesh，设置为 Invisible，并且 Collsion Type 设置为 ConvexHull。&lt;/p&gt;
&lt;p&gt;资产的链接，读者可以点击 &lt;a href=&quot;https://picr2.axi404.top/isaac6assets.zip&quot;&gt;这里&lt;/a&gt; 下载，并且拖动到 Isaac 中使用。注意 scale 调节为 0.001，这是由于比例尺的问题。&lt;/p&gt;
&lt;p&gt;在给二者添加默认的 Rigid Body，将盘子翻转，并且拖动到一定的高度之后 Play，可以观察到如下现象：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PixPin_2025-08-17_23-11-01.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;而这一切都是因为穿模以及关联的一系列原因引起的。&lt;/p&gt;
&lt;h2&gt;解决方案&lt;/h2&gt;
&lt;p&gt;解决这一系列的问题，在大量的尝试之后，发现两个参数的同时修改可以显著缓解，其中之一是 Contact offset 参数，按照 &lt;a href=&quot;https://docs.isaacsim.omniverse.nvidia.com/5.0.0/physics/simulation_fundamentals.html#contact-and-rest-offset&quot;&gt;Isaac 的文档&lt;/a&gt; 中的描述，其为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Contact Offset dictates how far from the collision geometry, irrespective of Rest Offset, the simulation engine starts generating contact constraints. The tradeoff for tuning the contact offset is performance vs. collision fidelity: A larger Contact Offset results in many contact constraints being generated which is more computationally expensive; a smaller offset can result in issues with contacts being detected too late, and symptoms include jittering or missed contacts or even tunneling (see notes on CCD above).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;说白了就是隔多远就开始计算。&lt;/p&gt;
&lt;p&gt;不过值得一提的是本段落中提到的 CCD，即 Continuous Collision Detection，文档中描述其为最有效的解决物理接触的方法，但是事实上在笔者大量的实践之后发现，在开启全局 CCD，并且某一物体的 RigidBody 的 Enable CCD 为 True 时，穿模极易导致机械臂的夹爪（接触位置）与本体分离，如视频中所示。&lt;/p&gt;
&lt;p&gt;因此除非在确保 Isaac 5.0.0 版本已经修复本 Bug 之后，不建议使用 CCD 解决任何的物理问题，本 Bug 在 4.1.0/4.2.0/4.5.0 中均可以进行复现。&lt;/p&gt;
&lt;p&gt;回到正题，于是可以使用 contact offset 来部分解决这一问题，设置到 0.02 即可。这是一个足够小，但是有效的数字，并且不会对性能产生太大的影响。同时读者在后续自己的实践中也需要注意，伴随着如 contact offset 等参数被设置，物理仿真的耗时会随着场景中的物体增多而增多，在物体密集的场景中，甚至或许是指数级的，因此在设置的过程中需要区分前景与背景物体，并且灵活地设置参数。&lt;/p&gt;
&lt;p&gt;设置的代码如下，同时包括对于第五讲中代码在迭代后的升级版本：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from omni.isaac.core.utils.prims import get_prim_at_path  # type: ignore
from pxr import UsdPhysics  # type: ignore
from pxr import PhysxSchema  # type: ignore
from pxr import Sdf  # type: ignore

def remove_colliders(prim_path):
    prim = get_prim_at_path(prim_path)
    schema_list = prim.GetAppliedSchemas()
    if &quot;PhysicsCollisionAPI&quot; in schema_list:
        prim.RemoveAPI(UsdPhysics.CollisionAPI)
    if &quot;PhysicsMeshCollisionAPI&quot; in schema_list:
        prim.RemoveAPI(UsdPhysics.MeshCollisionAPI)
    if &quot;PhysxConvexHullCollisionAPI&quot; in schema_list:
        prim.RemoveAPI(PhysxSchema.PhysxConvexHullCollisionAPI)
    if &quot;PhysxConvexDecompositionCollisionAPI&quot; in schema_list:
        prim.RemoveAPI(PhysxSchema.PhysxConvexDecompositionCollisionAPI)
    if &quot;PhysxSDFMeshCollisionAPI&quot; in schema_list:
        prim.RemoveAPI(PhysxSchema.PhysxSDFMeshCollisionAPI)
    if &quot;PhysxTriangleMeshCollisionAPI&quot; in schema_list:
        prim.RemoveAPI(PhysxSchema.PhysxTriangleMeshCollisionAPI)
    for child in prim.GetAllChildren():
        remove_colliders(str(child.GetPath()))


def set_colliders_by_prim_path(
    prim_path, collision_approximation=&quot;convexDecomposition&quot;, convex_hulls=None
):
    prim = get_prim_at_path(prim_path)
    for child in prim.GetAllChildren():
        set_colliders_by_prim_path(
            str(child.GetPath()), collision_approximation, convex_hulls
        )
    if prim.GetTypeName() == &quot;Mesh&quot;:
        collider = UsdPhysics.CollisionAPI.Apply(prim)
        mesh_collider = UsdPhysics.MeshCollisionAPI.Apply(prim)
        mesh_collider.CreateApproximationAttr().Set(collision_approximation)
        collider.GetCollisionEnabledAttr().Set(True)
        if collision_approximation == &quot;convexDecomposition&quot;:
            collision_api = PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(prim)
            collision_api.CreateHullVertexLimitAttr().Set(64)
            if convex_hulls is not None:
                collision_api.CreateMaxConvexHullsAttr().Set(convex_hulls)
            else:
                collision_api.CreateMaxConvexHullsAttr().Set(16)
            collision_api.CreateMinThicknessAttr().Set(0.001)
            collision_api.CreateShrinkWrapAttr().Set(True)
            collision_api.CreateErrorPercentageAttr().Set(0.1)
            collision_api = PhysxSchema.PhysxCollisionAPI.Apply(prim)
            collision_api.CreateContactOffsetAttr().Set(0.02)
        elif collision_approximation == &quot;convexHull&quot;:
            collision_api = PhysxSchema.PhysxConvexHullCollisionAPI.Apply(prim)
            collision_api.CreateHullVertexLimitAttr().Set(64)
            collision_api.CreateMinThicknessAttr().Set(0.00001)
        elif collision_approximation == &quot;sdf&quot;:
            collision_api = PhysxSchema.PhysxSDFMeshCollisionAPI.Apply(prim)
            collision_api.CreateSdfResolutionAttr().Set(1024)
    return prim

def set_colliders(
    prim_path, collision_approximation=&quot;convexDecomposition&quot;, convex_hulls=None
):
    remove_colliders(prim_path)
    prim = set_colliders_by_prim_path(prim_path, collision_approximation, convex_hulls)
    return prim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;直接调用 &lt;code&gt;set_colliders&lt;/code&gt; 即可。&lt;/p&gt;
&lt;p&gt;然而本参数依然不能本质上解决这一问题，在设置了远一些就计算还不能解决的时候，一个想当然的补救措施自然而生，那就同时多算几次不就好了。控制 Position 的迭代次数的参数为 Rigid Body 的 position solver iterations。本参数设置到 32 即可。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def set_rigid_body_solver_position_iteration_count(prim_path, count):
    prim = get_prim_at_path(prim_path)
    rigid_body = PhysxSchema.PhysxRigidBodyAPI.Apply(prim)
    rigid_body.CreateSolverPositionIterationCountAttr().Set(count)
    return prim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时再次播放，可以观察到如下现象：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PixPin_2025-08-17_23-12-17.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;One More Thing&lt;/h2&gt;
&lt;p&gt;至此我们得到了一个几乎可以稍微一劳永逸的方案，可以解决大部分的穿模问题，并且不会对性能产生太大的影响。同时，摩擦力等内容不会单独花费一章节进行讲解，在这里进行简单的提及。&lt;/p&gt;
&lt;p&gt;本质上如摩擦力以及弹力等内容，实际上是通过 Collsion 才可以传达的，因此在被设置了 Collison 的物体上，可以设置名为 Physics Material 的内容，即物理材质。Physics Material 在 Prim Path 下存在实体，而在物体的参数中则是标记关联的 Material 的 Prim Path，这意味着不同的 Prims 可以共用同一 Physics Material。当然，尽管如此，为了方便管理，一般来是每个物体设置一个 Physics Material。创建 Physics Material 的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pxr import PhysxSchema  # type: ignore
from omni.isaac.core.prims import GeometryPrim  # type: ignore
from omni.isaac.core.materials import PhysicsMaterial  # type: ignore


def add_physics_material(prim_path, static_friction=1.0, dynamic_friction=1.0):
    prim = str(prim_path)
    prim = GeometryPrim(prim_path)
    try:
        physics_material = PhysicsMaterial(
            prim_path=prim_path + &quot;/physics_material&quot;,
            static_friction=static_friction,
            dynamic_friction=dynamic_friction,
            restitution=0.1,
        )
        prim.apply_physics_material(physics_material)
    except Exception as e:
        print(&quot;Physics material already exists&quot;)
    return prim
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;读者经过本章节，不难自己学习到如何调节合理的参数，并且设置 Physics Material。通过上述提供的 cup and plate 的素材，读者不难进行一个实践，使用代码，在 Isaac Sim 中导入二者调节初始位置，设置物理参数以及 Physics Material，并且进行仿真。任何问题欢迎留言或者致信 &lt;code&gt;axihelloworld@gmail.com&lt;/code&gt;，愿你开心。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-6-zh.webp"/><enclosure url="https://picr2.axi404.top/template-6-zh.webp"/></item><item><title>周记 Week58</title><link>https://axi404.github.io/blog/week-58</link><guid isPermaLink="true">https://axi404.github.io/blog/week-58</guid><description>2025-08-04 ~ 2025-08-10.</description><pubDate>Sat, 16 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { GithubCard } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;又是很久之后的一次周记了，现在总是感慨，有的时候忙忙碌碌，但是回头一看又没有做什么事情，尤其是现在长期的项目结束了，有一点懒惰而不想开启新的忙碌，周记也不是很记了，这次一定要这个晚上写完这一篇和下一篇，简单记录下经历了什么。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这周的主基调依然是科研，IROS Challenge 已经全部交付完毕了，在完成了包括搭建十个任务，批量生成训练数据以及大量的 scaling up 的数据，并且给下游交付 &lt;code&gt;.usd&lt;/code&gt; 文件以及 &lt;code&gt;meta_info.pkl&lt;/code&gt; 之后，我基本上完全的主线任务也也就结束了。当然，在这里也包括了因为相机之类的问题在里面重新生成一些数据。&lt;/p&gt;
&lt;p&gt;在长期的认证之后，基本上可以确认目前代码中生成的数据都已经没有任何的 bugs 存在了，因此在后续假如说训练过程中出现了问题，也应该主要都是模型训练不稳定在内的一系列问题。在这一周还是搭建了非常有趣的 task 的，也就是一个叠放三明治的任务，确实看上去非常的棒，大家关注我们的 &lt;a href=&quot;https://internrobotics.shlab.org.cn/challenge/2025/&quot;&gt;IROS Website&lt;/a&gt;，里面给出了训练数据，欢迎下载~&lt;/p&gt;
&lt;p&gt;当然，值得一提的是，Website 里面其实有 contributors 的界面，只要一路拉到最下面，就可以在 Manipulation Track 的名单里看到本人了，虽然说这个字确实有点小就是了，但是毕竟我也确实在里面出了很多的力。&lt;/p&gt;
&lt;p&gt;同时，因为 IROS Challenge 是基于 InternManip 制作的，而 InternManip 的结构其实本质上类似于 OpenCompass，也就是用 Client+Server 的模式方便用户不需要完全接入，同时直接非侵入式在各路 Benchmark 里面接入 InternManip 的接口。&lt;/p&gt;
&lt;p&gt;目前在里面接入了 Calvin 以及 simpler env 两个别家的 Benchmark，以及 &lt;a href=&quot;https://genmanip.axi404.top/&quot;&gt;GenManip&lt;/a&gt;，也就是我之前的工作，算是自家的，也就是原生支持了。GenManip 本身包括数据生成以及测评的整体闭环，本质上 InternManip 基于 GenManip 测评部分的一份最简实现，在 InternUtopia 的 Agent 框架下进行了重构。&lt;/p&gt;
&lt;p&gt;不过因为历史原因，之前的 GenManip 并不支持 cross embodiment，这一弊端也就被继承到了 InternManip 中，而在 GenManip 的若干更新之后支持的 Cross Embodiment feature，我也就直接帮忙 merge 进去了。包括对于不同异构机械臂，直接使用继承的方法，调用 cuRobo 去接双臂 IK，这个设计可以让之后的机械臂接入过程也可以很轻量化，在保持原有的架构的基础上，我也都尽力接入了。&lt;/p&gt;
&lt;p&gt;私以为感觉算上之前的若干贡献点，姑且也算是 InternManip 的 Contributor 了。然后就是继续在 GenManip 的框架下去 scaling up 数据，度过这一周。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活上依然没有什么好说的，主要还是这段时间依然 workload 比较大，所以没有很多玩的地方，也就是和乐小姐两个人煲电话粥，还算是开心。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-58-zh.webp"/><enclosure url="https://picr2.axi404.top/week-58-zh.webp"/></item><item><title>具身十日谈：数据与仿真器</title><link>https://axi404.github.io/blog/embodied-talk-1</link><guid isPermaLink="true">https://axi404.github.io/blog/embodied-talk-1</guid><description>数据是具身智能的基石，而基于各种方案生成的数据质量，决定了模型可以走多远。从数据入手，一窥仿真以及数据合成的体系，并探讨模型在其中的变化。</description><pubDate>Wed, 04 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Aside } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;伴随着具身智能的兴起，人们逐渐发现 Block 模型性能的，往往来自于数据的羸弱。早期 RT-1[^rt1] 以及 RT-2[^rt2] 的模型，均使用了大量的现实中采集的数据进行训练，但是这种方法终归效率是有限的。&lt;/p&gt;
&lt;p&gt;在早期的探索结束之后，处于对于端到端架构的憧憬，以及大量学界业界资源的涌入，人们开始尝试使用不同的方案进行数据的 Scaling Up。事实上大家都心知肚明，尽管架构的改进可以带来性能的提升以及更高的效率，但是已有的在 LLM 和 VLM 上的经验告诉我们，数据才是泛化的关键，而一些极端人士则会这样说：「泛化即智能」。&lt;/p&gt;
&lt;p&gt;[^rt1]: RT-1: https://arxiv.org/abs/2212.06817
[^rt2]: RT-2: https://arxiv.org/abs/2307.15818&lt;/p&gt;
&lt;p&gt;本篇博客从数据的视角出发，简单看下目前各种类型的数据方案，学界与业界如何生产并且利用这些数据，以及笔者所看好的仿真合成数据方案的优势与 Limitations。&lt;/p&gt;
&lt;h2&gt;Web Video&lt;/h2&gt;
&lt;p&gt;使用广泛的互联网数据是第一种路线。研究者们相信，从广大的 Web Video 数据中可以学习到动作信息，尤其比如说 EGO-4D[^ego4d] 等数据集，包含了人类在各种场景下的动作信息，而他们相信这些以人类为「本体」的数据，可以为具身大模型赋予抽象的动作理解能力。&lt;/p&gt;
&lt;p&gt;包括 LAPA[^lapa], UniVLA[^univla], GR00T[^gr00t] 以及 GO-1[^go1] 等模型，在架构涉及中都包括一个 IDM，即 inverse dynamic model，通过学习 video prediction 来从视频数据中提取使用了 Web Video 数据进行训练。当然，另一类利用 Web Video 的方式则是训练 World Model (Video Generation Model) 来生成视频，这部分在后面会展开描述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755199339545_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;[^ego4d]: EGO-4D: https://ego4d-data.org/
[^lapa]: LAPA: https://arxiv.org/abs/2410.11758
[^univla]: UniVLA: https://arxiv.org/abs/2505.06111
[^gr00t]: GR00T: https://arxiv.org/abs/2503.14734
[^go1]: GO-1: https://arxiv.org/abs/2503.06669&lt;/p&gt;
&lt;p&gt;如图中所示，这类模型的本质上是通过类似 VQ-VAE 的架构，将 $x_t$ 和 $x_{t+T}$ 的图像进行编码，这个编码之后的结果称之为 latent action，之后通过 $x_t$ 和 latent action 来预测 $x_{t+T}$。&lt;/p&gt;
&lt;p&gt;这个过程其实和 Robotics 里面的 FK[^fk] 以及 IK[^ik] 很类似。前者类似与 IK，从当前状态和目标求解动作，而后者类似与 FK，从当前状态和动作求解目标。因此训练处理的 Encoder 被称为 IDM，而此类型工作则使用 IDM 将 Video 数据转换为 VLA 数据。&lt;/p&gt;
&lt;p&gt;在目前仅有 Video 的情况下，也就是一个 Image 的 Chunk，此时尚且需要 Language 以及 Action 的标注，并且这部分需要是自动化的，否则难以 scalable。Language 部分的处理相对直观，使用视频理解的 VLM 为 Video 添加语言标注即可，而对于 Action，则使用之前在 Video 数据上进行 pre-train 的 IDM。通过将视频数据输入给 IDM，IDM 可以输出 latent action 作为 A 的标注，并认为这种 A 的表征描述了前后两帧之间的变化，其中自然也含有本体的变化。因此共同组成了 VLA Data。&lt;/p&gt;
&lt;p&gt;[^fk]: FK: https://en.wikipedia.org/wiki/Forward_kinematics
[^ik]: IK: https://en.wikipedia.org/wiki/Inverse_kinematics&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755204433219_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;而在此之后，大多数模型本质上也都使用了常规 VLA 的架构进行了训练，本身 latent action 并没有什么特殊，这是一种和 Franka action 类似的概念等价的。&lt;/p&gt;
&lt;p&gt;Latent action 最大的问题其实不难理解，就是训练 latent action 的过程中本质上是在描述图像之间的变化，而这种变化事实上可能并非本体（如人体）的变化导致的。不相关的变化可以简单枚举出两种，一种是环境变化，其中也包括其他的本体的变化，对环境操作之后的惯性，以及本身就是动态的环境等；另一种则是相机视角的变化，虽然说相机本身在本体上，但是相机视角的变化会带来大幅的图像特征变化，而这并不能在任何程度上代表本体本身的运动。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755205051873_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;诸如 UniVLA[^univla] 等方法本质上就是在通过 Task-irrelevant 和 Task-centric 的 learnable token 来更好地提取属于本体的 latent action 部分。&lt;/p&gt;
&lt;h2&gt;数据采集&lt;/h2&gt;
&lt;p&gt;雇佣大量的数据标注人员，进行数据采集，也是一条可行的路线，并且对于被资本青睐的具身智能这一新兴领域，未尝不是最好上手且 scalable 的路线。诸如上述提到的 RT-1 的数据集，以及后续 AgiBot 开源的基于他们机器人采集的 AgiBot-World 数据集[^agibotworld]，都是这方面的典范。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755199723815_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;[^agibotworld]: AgiBot World: https://www.bilibili.com/video/BV1Qw6gYXEyh/&lt;/p&gt;
&lt;p&gt;大量的公司生产自己的机器人本体，并且雇佣大量的数据采集人员，在自己搭建的数据工厂中 scaling 大量的数据，这些数据伴随着数据采集的流程变得规范，质量也会逐渐提高。然而对于大多数公司来说，数据几乎是核心资产，在我的认知中，排除 AgiBot 之外貌似也并不存在大量开源。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755205598090_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;尽管 LeRobot[^lerobot] 提供的 format 提供了很好的数据格式，并且在一定程度上统一了机器人领域的数据格式，但是目前各个工作可能开源的零星数据集本身依然难以汇聚成大规模训练的数据集，而 LeRobot 格式更大的作用还是在于大家在对齐了数据格式之后，可以快速将自己采集的数据用于公开模型的训练。&lt;/p&gt;
&lt;p&gt;[^lerobot]: LeRobot: https://huggingface.co/docs/lerobot/index&lt;/p&gt;
&lt;p&gt;这条道路在一定的时间内似乎只对于大型公司有效，而在学界中难以普及。这主要是因为对于学界来说，整合网络上大量杂乱的数据，不只面临着数据质量的问题，同时不同视角下的异构机器人本体也会为学习带来额外的困难。目前的 VLA 学习仍然主要集中在同本体下的多任务泛化，而非跨本体的操作，因而如何筛选出合适的数据作为预训练仍然是一个很大的问题。&lt;/p&gt;
&lt;p&gt;上述原因也进一步导致了基于真机数据采集的 VLA 模型，在学界中仍然大多数在单一任务上进行 post-training，一方面自然是算力等问题，但更多的也包括学界需要一个数据集，其中包括大量的同视角同构本体在不同任务上采集的数据。这些数据需要尽可能的多样，并且这款机器人也要尽可能地普及，而这一盛况在当今来看是难以实现的。&lt;/p&gt;
&lt;p&gt;所以简单来说，尽管使用大量真机采集的。VLA数据对于模型进行直接的预训练是最简洁且本质的方法，而且数据工厂在业界开始逐渐普及。不过那些具有自己研发的本体的公司在数据工厂中大量采集数据之后，由于其不开源的特性，对于学界来说依然难以拥有统一的大量同本体预训练数据，而只能依赖公司开放的预训练权重，或者通过其他方式生成数据。这也为后续的内容带来了存在的必要性。&lt;/p&gt;
&lt;h2&gt;Video Gen 与 World Model&lt;/h2&gt;
&lt;p&gt;伴随着 Sora[^sora] 的火爆，Video Gen 逐渐进入了人们的视野，而随着近期视频模型的进一步升级，如 Veo-3[^veo3] 等生成模型可以生成超高质量的视频，至于 Genie-3[^genie3] 的横空出世，可交互的 3D 视频生成也逐渐展现着可能性。同时，通过类似的范式，人们也可以用来生成机器人的数据。&lt;/p&gt;
&lt;p&gt;[^sora]: Sora: https://openai.com/sora/
[^veo3]: Veo-3: https://deepmind.google/models/veo/
[^genie3]: Genie-3: https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755206459850_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;诸如 DreamGen[^dreamgen] 等工作通过 Cosmos 等管线生产合成的视频，并且将其用于模型训练的数据，也可以达到不错的性能。&lt;/p&gt;
&lt;p&gt;[^dreamgen]: DreamGen: https://arxiv.org/abs/2505.12705&lt;/p&gt;
&lt;p&gt;然而，当前基于世界模型的数据生成管线依然具有大量的不确定性，其主要就来自于机械臂本体动作与视觉上一致性的保持问题，也就是机器人的动作是否与视觉上的变化一致。而更进一步，对于深度估计等内容本身，单目深度估计已经存在一定的难度，因此一致性仍然难以保持。&lt;/p&gt;
&lt;p&gt;因此视频生成的管线生成的质量不高的数据是否可以在大规模的预训练中带来 VLA 的泛化性也是一个问题，更何况当前学界大多数的视频生成模型依然在泛化上具有一定的局限性，而引入业界的视频生成模型，依然是一件有待完成的事项。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755231670579_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;利用视频生成或者说是 world model 的另一种方式则是将其直接作为 VLA 的一部分，GR-1[^gr1] 以及 GR-2[^gr2] 在很早期就进行了相关的探索，而直到近期如 Genie Envisioner[^genieenvisioner] 等方法，构建一个 VLA 所需要的实时推理的模型，同时还具有较高的视频生成质量，暂时来看仍然是一个 trade-off。&lt;/p&gt;
&lt;p&gt;[^gr1]: GR-1: https://arxiv.org/abs/2312.13139
[^gr2]: GR-2: https://arxiv.org/abs/2410.06158
[^genieenvisioner]: Genie Envisioner: https://arxiv.org/abs/2508.05635&lt;/p&gt;
&lt;p&gt;至于对于学界对于 VLA 起始的期望，一个端到端的可以理解任务并且推理，然后输出动作完成任务的模型，对于 World model 来说似乎依然有些强人所难。无论是图像还是视频生成，因为其中多半并不包括隐式推理的过程，而是直接进入类似 DiT 的流程中，因此仍然无法生成诸如 「1 + 1 个苹果」，更何况拆解或者分析更加复杂的指令并且进行泛化。&lt;/p&gt;
&lt;h2&gt;仿真平台与数据合成&lt;/h2&gt;
&lt;p&gt;得益于计算机图形学在内的发展，物理仿真引擎以及配套的基于光线追踪等技术的渲染引擎构成的仿真平台应运而生，并且提供了仿真数据的生成方式。使用仿真来生成数据是最广泛的数据生成路线。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755207420798_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Isaac Sim[^isaac] 作为 Nvidia 推出的强大仿真器，在渲染方面具有着得天独厚的优势，在后续被广泛应用于具身智能的仿真数据生成中；而另一方面，MuJoCo[^mujoco] 的仿真器则在刚体仿真上具有着更好的表现。除此之外诸如 Genesis[^genesis] 等仿真器，尽管当前对于渲染仍然会带来巨大的性能开销，但是其对于流体软体的仿真以及其运行速度。作为新生代也展现了一定的前景，并在 RL 上已经可以进行了应用。&lt;/p&gt;
&lt;p&gt;[^isaac]: Isaac Sim: https://docs.isaacsim.omniverse.nvidia.com/latest/index.html
[^mujoco]: MuJoCo: https://mujoco.readthedocs.io/en/stable/overview.html
[^genesis]: Genesis: https://genesis-embodied-ai.github.io/&lt;/p&gt;
&lt;p&gt;2025 年上半年 Nvidia 宣传的 Newton[^newton] 基于 Mujoco Warp 以及 USD 的体系，继承了 Isaac Sim 优质的渲染质量，也展现了更多的可能性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755199258996_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;[^newton]: News about Newton: https://developer.nvidia.com/blog/announcing-newton-an-open-source-physics-engine-for-robotics-simulation/&lt;/p&gt;
&lt;p&gt;我们不得不承认的是，使用仿真框架生成数据是不可能生成大量无穷任务的数据的。对于流体或者软体的仿真在仿真数据生成中仍然具有较大的局限性，乃至于生成叠衣服的数据可以成为一个独立的赛道，也诞生了诸如 DexGarmentLab[^dexgarmentab] 等工作。而另一方面，使用仿真意味着需要积累大量的 3D 资产，这样才能构建尽可能多样的场景，并且生成尽可能多样的数据，这也为相关内容的积累提出了挑战。&lt;/p&gt;
&lt;p&gt;[^dexgarmentab]: DexGarmentLab: https://wayrise.github.io/DexGarmentLab/&lt;/p&gt;
&lt;h3&gt;Rule-based 与 RL&lt;/h3&gt;
&lt;p&gt;一般来说，仿真数据的生成分为两条主要的方式。一种是通过手写规则的 rule based 方法，而另外一种则是基于 RL 实现。&lt;/p&gt;
&lt;p&gt;所谓 rule base，也就是使用大量的手写规则来进行数据的生成，在早期学界对于每个任务进行 hard-code 的规则编写，这意味着并不 scalable，并且需要耗费大量的人力进行调参。&lt;/p&gt;
&lt;p&gt;不过在后续流程的优化中，对于 pick and place 这一类型的任务，本质上已经可以使用一种 general 的规则生成 diverse grasp pose 的数据，并且在大规模的资产上进行 scaling up。如 GenManip[^genmanip] 使用 scene graph 作为 goal 的表示，可以使用简短的语句来描述任务，并且快速生成 long-horizon 的 pick and place scaling up 数据而无需任何参数调试，并且使用 mimicgen[^mimicgen] 来进行 articulation 操作，在跨 GPU 的集群中并行。&lt;/p&gt;
&lt;p&gt;[^genmanip]: GenManip: https://genmanip.axi404.top/
[^mimicgen]: MimicGen: https://arxiv.org/abs/2310.17596&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1755209460936_image.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;同时如 ArticuBot[^articubot] 等方法也展示对对于 1 自由度的 articulation object，同样可以使用 rule base 的方法进行泛化的数据生成。而笔者也了解到相关的工作也在类似的方面进行了探索，事实上 SHAILAB 的 InternData[^interndata] 之中的 M1 来自于 GenManip Framework，而 A1 则来自于在 articulation 以及 digital task 方面探索更多的框架。数十万高质量对齐的 scaling up 数据毫无疑问是学界所需要的。&lt;/p&gt;
&lt;p&gt;[^interndata]: InternData-M1: https://huggingface.co/datasets/InternRobotics/InternData-M1, InternData-A1: https://huggingface.co/datasets/InternRobotics/InternData-A1, InternData-N1: https://huggingface.co/datasets/InternRobotics/InternData-N1
[^articubot]: ArticuBot: https://arxiv.org/abs/2503.03045&lt;/p&gt;
&lt;p&gt;而另一方面，RL 则是一种更加灵活的方式，通过强化学习的方式，理论来说只通过设置 Goal 就可以生成更加多样化的数据。然而目前来看，相较于已经可以做到泛化 rule 的 rule based 方案，RL 的优势并不多，甚至体现了部分的 limitations。&lt;/p&gt;
&lt;p&gt;例如 RL 难以泛化到多任务，因此在采集数据的过程中可能需要反复训练模型，才可以收集更加 diverse 的数据，相较于 rule-based 的 training free，伴随着资产量级的增加，这一限制会显得越发明显；同时对于 Manipulation 的 dense reward 对于 RL 来说也难以进行泛化的设置，因此 RL 收集到的数据往往在关键动作上稳定，而在不同动作之间的衔接上因为没有 reward 的变化而乱动，这一点在诸如 &lt;a href=&quot;https://robogen-ai.github.io/&quot;&gt;RoboGen&lt;/a&gt; 中展示的 demo case 中有一个直观的展现。&lt;/p&gt;
&lt;h3&gt;仿真的多样性&lt;/h3&gt;
&lt;p&gt;无论如何，读者不难预见的是，伴随着 rule base 的方法在更多的技能上的应用以及泛化的增强，仿真数据可以逐渐覆盖更多的技能种类以及任务类型。此时的问题忽然变成了，当仿真可以做到真实世界中相应的任务，那么为什么我不使用真实世界，而是要使用仿真数据？毕竟众所周知，sim2real 一直以来都是仿真中的一个重大难题，尽管使用诸如 GS 等方案的方法已经可以进行缓解，但是 GS 又带来了更多的 workload 以及资产的稀缺。&lt;/p&gt;
&lt;p&gt;事实上，几个简单的例子可以说明仿真数据在一定程度上的优越性。比如说对物品多样性的覆盖，scaling up 数据的一个经典案例是在杂乱桌面上对于不同的物体进行 pick and place 操作。在仿真平台中，可以对于桌面物体进行快速的替换，并且覆盖上百乃至数千种物体，从而产生大量多样性的组合。而对于真机数据采集来说则更加麻烦，需要重新布置桌面，从库房或者物资管理获得新的物品，之后进行漫长的摆放以及布置，才可以采集一条数据。&lt;/p&gt;
&lt;p&gt;出于这种特性，读者不难发现大多数真机数据集中任务种类较少（假如认为 pick A to B 当 A 与 B 不同的时候是不同的任务），但是可以涉及更加灵活的操作以及任务种类。而仿真中则任务数量较多，而技能种类则相对有限。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PixPin_2025-08-15_06-14-59.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;再包括仿真中可以灵活替换的贴图以及光照和机器人本体的各种设置，以及仿真可以输出如 depth、affordance 以及 semantic mask 等信息的 GT，仿真平台以及合成数据似乎是天生为预训练而生的，在大量仿真数据中将 VLM 与 A 之间的 bridge 构建，并且在真机数据中进一步强化模型对于技能种类的掌握，这或许是对于 VLA 的一条可行路线。&lt;/p&gt;
&lt;h2&gt;小憩&lt;/h2&gt;
&lt;p&gt;一口气看了这么多，不妨歇息一下，喘口气。本博客主要介绍了当前不同的数据方案，以及对应的相关工作、模型如何利用这些数据以及limitations。其中也包括了本人自己的一家之言，略有见解，多有爆论，还望见谅。也欢迎读者在评论区进行积极讨论。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/embodied-talk-1-zh.webp"/><enclosure url="https://picr2.axi404.top/embodied-talk-1-zh.webp"/></item><item><title>周记 Week57</title><link>https://axi404.github.io/blog/week-57</link><guid isPermaLink="true">https://axi404.github.io/blog/week-57</guid><description>2025-07-28 ~ 2025-08-03.</description><pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说这其实又是很久之后的一次周记了，不过上一周确实没有忙碌很多，只是可能是一些琐事，以及懒惰，让我没有及时动笔，这件事话并不是一个好习惯。以及总是感觉看东西和做有意义的事情比写周记更有意思，然而周记的记录其实也恰好是我很需要每日反省的点，只是被我悄然遗忘了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这大概是最后一周我去高强度进行这个 IROS Challenge 的内容，还是把 aloha 进行了更好的整理，支持了不同的相机 setting，以及现在也支持了 Isaac Sim 的 aloha 双臂 scaling 的数据，这部分是很关键的内容，在将来感觉也是派得上用场的。&lt;/p&gt;
&lt;p&gt;最近也在读很多的 paper，读者不难看到我把很多的 paper reading 的 blog 都翻出来了，现在里面加了不少的内容，虽然都还没有写读后感，但是 paper list 确实已经形成出来了，感觉后面可以慢慢去读，以及可以有一些感想了。&lt;/p&gt;
&lt;p&gt;我一直在思考仿真这件事情的重要性，加上其他的同学也在和我交流，是否仿真是有极限的，并不能做出来我在现实里可以做到的一切内容。从某种程度上，我在仿真这方面已经算是相当 senior 了，我知道如何写一个 general 的数据生成范式，处理过大量的 physics issue，并且对于 articulation 操作也同样如此。事实上假如我去自己做 articulation skill 或者 deformable object manipulation，这件事情固然是可以继续做下去的，但是事实上这些内容并非 rigid body object 的一个扩充，而只是一个平行。从 Isaac 的角度来看，他们都是对于 prim 设置不同的 api，然后 pre define 一个 skill，从资产中 parse 你需要的表征，并且执行动作，这套逻辑依然是这样的，对于 articulation 或者 deformable，甚至自由度在降低，柔性甚至会遇到更少的 physics 问题。继续在一个不会增长我的本质能力，但是会进一步增加我的知识，却耗费大量时间的地方努力，这件事情是否值得，还是说实验室这边的工程师可以承接部分的内容，而我去探索全新的内容，这是一个斟酌的事情。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活上倒是没什么事情，我的 blog 更新了不少，包括现在提供了一个更好的学术主页，可能看上去稍微正经一些，而不是之前一样，只有 publication，看上去比较草率。我也特意加上了 open source project 这个类别，毕竟我在其中有不少有意义的内容。&lt;/p&gt;
&lt;p&gt;和乐小姐在一起的生活依然照旧，打打电话，每日平安顺遂，开开心心。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-57-zh.webp"/><enclosure url="https://picr2.axi404.top/week-57-zh.webp"/></item><item><title>Paper Reading: Unify MLLM 1</title><link>https://axi404.github.io/blog/paper-reading-uni1</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-uni1</guid><description>浅浅尝试读一下Unify MLLM。</description><pubDate>Sun, 04 May 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;因为朋友们都在做 Unified MLLM，所以我也浅浅尝试读一下，这样可以有一些参与感。假如理解 Unify Model 做的事情是 Multi-in Multi-out 的大模型，那么按照这种定义，其实 VLA 也是 Unify Model 的一种，说不定可以触类旁通呢~&lt;/p&gt;
&lt;h2&gt;SEED&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.8l0aq23pvn.webp&quot; alt=&quot;The pipeline of SEED&quot;&gt;&lt;/p&gt;
&lt;p&gt;按照朋友的 Paper List，这一篇应该是较早的 Paper 了，或许是第一个提出这个想法的 Paper。大概的思路其实和正常的 MLLM 差不多，本来的模型支持 Visual token 的输入，并且和 Text token concat 到一起。这里因为用 Reconstruct，因此假如说 LLM 可以输出对应的 Token，那么就可以进行 VQ 的解码，也就自然可以生成图片了。过程中很多讲究，比如说使用 1D 的 token 而非 2D，兼容单项注意力，同时用高层语义。本身的结构还是经典的一些 Q-Former 和 Unet，这里不展开。&lt;/p&gt;
&lt;h2&gt;LaVIT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.969ycdo8n9.webp&quot; alt=&quot;The pipeline of LaVIT&quot;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，实际上这个模型长得十分的标准，使用 Vision Encoder 以及 Text Encoder 来分别处理图片和文本，用 LLM 进行 Fusion 之后再用 Decoder 生成图片或者文本。一个设计是 Vision Encoder 中使用的 token selector 和 merger，selector 选择重要的 token，merger 用 cross attention 把选中和没选中的 token 进行融合。这一步本质上是为了动态选择 token，进行信息压缩，以及可以生成动态长度的 visual token。如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.83a91hw0h2.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;所以说这篇也是比较符合直觉的。只是一个疑问是这个 token selector 和 merger 的结构是否真的有用，难道本来的 token 输出真的很 sparse 吗？假如是的话，不是应该训练一个 encoder 吗，为什么执着于这个设计。&lt;/p&gt;
&lt;p&gt;以及后续和朋友聊了一下，这个架构的本质问题可能在于 interleave 的设计，也就是一段文字一段图片这种结构交错在一起。理论来说这样子是可行的，但是事实上可以思考一下，有点像是让模型将两个任务交叉在一起，其实很难学到两个的并集，从而学不好。真正涉及所谓图片编辑的场景，都较少需要语言作为上下文，或者说可以被解耦为多次推理。一次推理中涉及多轮所谓图片以及语言的交互，其实乍一想可能是解数学几何题，但是这种明显使用 symbolic 的方式来解决更好。模型的“大脑”很难在两个模式之间切换，这是一个本质问题吧。&lt;/p&gt;
&lt;h2&gt;SEED-X&lt;/h2&gt;
&lt;p&gt;这篇感觉就是已经是比较成熟的设计了。标准的 MLLM 架构作为输入，支持对于图片的各种分辨率的适应。本身 Visual tokenizer 就是 ViT，之后输入到 MLLM 中。然后在输出的时候，如果说有必要输出 Visual token，那么输出之后就作为 UNet 的 conditional 输入进 SD 中，实现图片编辑。这篇是多个 Stage 的训练，感觉或许继续看下去之后回头看这里或许可以有一些感悟。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.7w718rln3u.webp&quot; alt=&quot;The pipeline of SEED-X&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先是训练了 Visual tokenizer，这一部分分为两个 Stage，第一个 Stage 就是简单的训练，直接 Noise 图片以及 Zero Padding 来 concat 进行输入，ViT 的输出作为 condition 代替本来的 language condition，然后做 reconstruction；第二个 Stage 的输入变为一个 Noise + Conditional Image 的输入，这一部分的说明是可以恢复原始图像的细粒度细节，可以理解为使得 SD 这一部分可以兼容正常的图像生成以及图像编辑。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4xur59czou.webp&quot; alt=&quot;Two stage training of the visual tokenizer&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后也就是标准的 two stage MLLM 训练，第一个 stage 用 LoRA 训练了 Llama2-chat-13B，包括了 image-captions pairs, grounded image-texts, interleaved image-text data, OCR data and pure texts。第二个 stage 就是根据具体的任务再进行 instruction tuning，也就是说本质上 SEED-X 是一气化三清，而不是真的一个模型。不过总的来说这个工作确实足够简洁且优雅，应该是很标准也很符合想象的设计了。&lt;/p&gt;
&lt;h2&gt;Emu&lt;/h2&gt;
&lt;h2&gt;Emu2&lt;/h2&gt;
&lt;h2&gt;Chameleon&lt;/h2&gt;
&lt;h2&gt;Transfusion&lt;/h2&gt;
&lt;h2&gt;Emu3&lt;/h2&gt;
&lt;h2&gt;MMAR&lt;/h2&gt;
&lt;h2&gt;Janus&lt;/h2&gt;
&lt;h2&gt;PUMA&lt;/h2&gt;
&lt;h2&gt;Show-o&lt;/h2&gt;
&lt;h2&gt;VILA-U&lt;/h2&gt;
&lt;h2&gt;MIO&lt;/h2&gt;
&lt;h2&gt;JanusFlow&lt;/h2&gt;
&lt;h2&gt;Orthus&lt;/h2&gt;
&lt;h2&gt;TokenFlow&lt;/h2&gt;
&lt;h2&gt;Liquid&lt;/h2&gt;
&lt;h2&gt;MUSE-VL&lt;/h2&gt;
&lt;h2&gt;SILMM&lt;/h2&gt;
&lt;h2&gt;ILLUME&lt;/h2&gt;
&lt;h2&gt;Visual Lexicon&lt;/h2&gt;
&lt;h2&gt;LatentLM&lt;/h2&gt;
&lt;h2&gt;SynerGen-VL&lt;/h2&gt;
&lt;h2&gt;MetaMorph&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/Paper-reading-Uni-zh.webp"/><enclosure url="https://picr2.axi404.top/Paper-reading-Uni-zh.webp"/></item><item><title>Paper Reading: Unify MLLM 2</title><link>https://axi404.github.io/blog/paper-reading-uni2</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-uni2</guid><description>浅浅尝试读一下Unify MLLM。</description><pubDate>Sun, 04 May 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;LMFusion&lt;/h2&gt;
&lt;h2&gt;UniToken&lt;/h2&gt;
&lt;h2&gt;VARGPT&lt;/h2&gt;
&lt;h2&gt;Janus-Pro&lt;/h2&gt;
&lt;h2&gt;BLIP3-o&lt;/h2&gt;
&lt;h2&gt;Show-o2&lt;/h2&gt;
&lt;h2&gt;Qwen-Image&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-5-zh.webp"/><enclosure url="https://picr2.axi404.top/template-5-zh.webp"/></item><item><title>Paper Reading: LLM 1</title><link>https://axi404.github.io/blog/paper-reading-llm1</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-llm1</guid><description>从 BOW 到 GPT，一些 Paper Reading。</description><pubDate>Mon, 09 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;BOW&lt;/h2&gt;
&lt;p&gt;BOW，也就是 Bag of Words，是一种十分简单的模型，简答来说就是将一句话使用词的形式进行分割，然后用键值对的形式进行储存。这样做的一个显然的结果就是，词袋模型并不能很好的建模语言的顺序，但是作为一种最为初级的 tokenizer 来说也已经很不错了。&lt;/p&gt;
&lt;p&gt;所以很显然，词袋模型的第一个通病，就是处在无法对于语序进行建模这个问题上，而且同时，可以理解为这个模型是使用一种表格来进行表示的，这种表格是 one-hot 且离散的，本质上也没有很好的建模语言。&lt;/p&gt;
&lt;p&gt;词袋模型的一个 trick 在于处理过大的词表，可以使用 hash 的方法，更好的利用空间。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;词袋模型 - &lt;a href=&quot;https://en.wikipedia.org/wiki/Bag-of-words_model&quot;&gt;https://en.wikipedia.org/wiki/Bag-of-words_model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Feature Hash - &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;https://en.wikipedia.org/wiki/Feature_hashing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;TF-IDF&lt;/h2&gt;
&lt;p&gt;TF-IDF 可以理解为是一种对于知识库中的文档中的词汇的重要性的建模方法。这个思想十分简单，也是由两个因素组成，TF 和 IDF，前者用来形容一个词汇在文档中出现的次数，后者则是使用了这个词汇的文档的次数。但事实上其中使用了 log 与乘法等内容进行数学形式的计算，不过这里只讨论 insight。&lt;/p&gt;
&lt;p&gt;这种方法很好地体现了一个真正的关键词汇，在文档中所需要包含的特征。首先，这个词汇一定会被反复提起，因此这个词汇与文档的关联性才高；同时，这个词汇不会被太多的文档所提及，假如被被提及太多，意味着这个词汇丧失了独特性，诸如人称代词等一系列内容，均符合 TF 的描述，因此需要 IDF 来进行 filter。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TF-IDF - &lt;a href=&quot;https://www.cnblogs.com/L-shuai/p/13817978.html&quot;&gt;https://www.cnblogs.com/L-shuai/p/13817978.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Word 2 Vec&lt;/h2&gt;
&lt;p&gt;Word 2 Vec 是一种用于生成词向量的技术，它通过将词语映射到一个高维向量空间中，使得语义相似的词在向量空间中距离较近。其中比较常见的是 skip-gram 和 CBOW 两种模型，前者是使用词预测上下文，后者是使用上下文预测词。简单理解一下方法的话，CBOW 是输入一个词（one-hot 向量），然后经过编码，再解码为一个向量，最大化上下文的概率；CBOW 则是输入上下文，最大化词的概率。这两种方法显然都可以很好的训练编码器，也就使得词汇被编码到了一个连续的高维空间中。&lt;/p&gt;
&lt;p&gt;Word 2 Vec 的一个 insight 是，它将词映射到了一个高维空间中，而高维空间中，距离较近的词，语义上更相似。因此，这种思想可以拓展到其他领域，例如图像，声音等等，将不同模态的信息映射到同一个高维空间中，然后进行相似度的计算。&lt;/p&gt;
&lt;h2&gt;Transformer&lt;/h2&gt;
&lt;h2&gt;GPT 1.0&lt;/h2&gt;
&lt;h2&gt;BERT&lt;/h2&gt;
&lt;h2&gt;GPT 2.0&lt;/h2&gt;
&lt;h2&gt;Megatron-LM&lt;/h2&gt;
&lt;h2&gt;T5&lt;/h2&gt;
&lt;h2&gt;ZeRO&lt;/h2&gt;
&lt;h2&gt;Scaling Law&lt;/h2&gt;
&lt;h2&gt;GPT 3.0&lt;/h2&gt;
&lt;h2&gt;Switch Transformers&lt;/h2&gt;
&lt;h2&gt;Codex&lt;/h2&gt;
&lt;h2&gt;COT&lt;/h2&gt;
&lt;h2&gt;InstructGPT&lt;/h2&gt;
&lt;h2&gt;PaLM&lt;/h2&gt;
&lt;h2&gt;LLaMA&lt;/h2&gt;
&lt;h2&gt;GPT 4&lt;/h2&gt;
&lt;h2&gt;DPO&lt;/h2&gt;
&lt;h2&gt;ToT&lt;/h2&gt;
&lt;h2&gt;LLaMA2&lt;/h2&gt;
&lt;h2&gt;Mistral 7B&lt;/h2&gt;
&lt;h2&gt;Mamba&lt;/h2&gt;
&lt;h2&gt;Mamba2&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/Paper-reading-LLM-zh.webp"/><enclosure url="https://picr2.axi404.top/Paper-reading-LLM-zh.webp"/></item><item><title>Paper Reading: LLM 2</title><link>https://axi404.github.io/blog/paper-reading-llm2</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-llm2</guid><description>从 BOW 到 GPT，一些 Paper Reading。</description><pubDate>Mon, 09 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;Qwen2.5&lt;/h2&gt;
&lt;h2&gt;DeepSeek-V3&lt;/h2&gt;
&lt;h2&gt;DeepSeek-R1&lt;/h2&gt;
&lt;h2&gt;Kimi K2&lt;/h2&gt;
&lt;h2&gt;DFT&lt;/h2&gt;
&lt;h2&gt;MMaDA&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-4-zh.webp"/><enclosure url="https://picr2.axi404.top/template-4-zh.webp"/></item><item><title>Paper Reading: MLLM</title><link>https://axi404.github.io/blog/paper-reading-mllm</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-mllm</guid><description>一些和多模态相关的论文阅读，从 CLIP 开始，向后延伸。</description><pubDate>Mon, 09 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ArxivRating, RatingCriteria } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本文主要是关于一些 MLLM 相关的论文的阅读工作，一些浅显的 insight 分享，以及，阅读的可能大多数是领域中的主脉络，对于刚刚入门的小白来说，或许这些论文也是值得推荐的。&lt;/p&gt;
&lt;p&gt;Noting 的是，全部的内容都是直接基于论文阅读的，参考资料中提及的内容指，这些内容或许能够帮助读者进一步理解论文里说的内容。大的基石还是论文。&lt;/p&gt;
&lt;h2&gt;CLIP&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/CLIP.7ljx88ttzm.webp&quot; alt=&quot;The pipeline of CLIP&quot;&gt;&lt;/p&gt;
&lt;p&gt;CLIP 在某种程度上也可以说是一个开山之作，虽然说对多模态的探索早在它之前就已经开始了，然而不只是数据量很大，本身对于内容处理的范式也使得 CLIP 极具拓展性，可以在很多任务中泛化。&lt;/p&gt;
&lt;p&gt;简单理解一下 CLIP，也就是使用一个图像编码器和一个文本编码器，对于一组图像文本对进行编码，然后获得输出。接下来就是对比学习类型的工作了，需要清楚的是，相匹配的图像文本对一定是在编码之后相似度很高的，那么直接对大量输出之间的余弦相似度进行优化，是一个显然的答案。&lt;/p&gt;
&lt;p&gt;这里面激动人心的事情，一是在进行混合，或者说再进行多模态的相似度求解的时候，可以直接使用余弦相似度这种这种方法，这证明这些编码器在经过大量数据的训练之后，确实可以将不同模态的输入投射到一个通用的 high-level 空间中。事实上由于大多数的论文都是从故事说起，因此可能会忽略，尽管在人类的概念上图像和文本可以统一于一个高层的思维中的概念，然而这种表示，在使用数学或者计算机形式的信息时是否成立，这依然是一个问号。不过从目前的实验结果来看，答案是肯定的，而后续的一系列工作也证明了，不只是图像与文本，不同的模态之间确实可以具有一种数学意义上的高维空间中的统一。&lt;/p&gt;
&lt;p&gt;当然同时，CLIP 的 prompt template 进行 zero shot 分类的技巧也同样令人印象深刻，这本质上是对于 bert 范式在多模态领域对一种拓展。后续的工作中也涌现了一系列的对于 prompt 的应用，然而这是后话了。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CLIP - &lt;a href=&quot;https://www.bilibili.com/video/BV1SL4y1s7LQ/&quot;&gt;https://www.bilibili.com/video/BV1SL4y1s7LQ/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ViLT&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ViLT.67xe47irzd.webp&quot; alt=&quot;The pipeline of ViLT&quot;&gt;&lt;/p&gt;
&lt;p&gt;ViLT 也算是比较经典的多模态领域的工作了，这里面需要说的东西其实不多。首先需要先理清一些常规的内容，也就是 ViT 和 Transformer 在形式上究竟有什么区别。假如说我们不去关注这两个模型的输出，一个显而易见的事情是，他们的不同点仅仅在于模型的输入部分，当然对于输入的处理也有所不同。具体来说，在文本的部分使用了 tokenizer，还在图像的部分分 patch 变成 token 之后进行了一次简单的编码。借用一下后期的 insight，假如不去在意这种简单的编码的性能，已经可以理解为，视觉信息本身就是一种语言。&lt;/p&gt;
&lt;p&gt;这篇论文首先总结了之前的工作，然后给出了一个双塔的模型的对比。具体来说，双塔的多模态模型有三个组件组成，分别是文本编码器、图像编码器和多模态编码器，这其中，这三个编码器的大小也就成了一个问题。首先需要考虑的是，当我们有固定的算力的情况下，我们应该如何分配算力给三个模型。一种最为常见的做法，是把多一些的算力分配给图像，这是由于图像本身就具有更难的编码难度，然后将两个编码器在多模态上进行简单的融合 ；之后也就是 CLIP，属于是用了一个文本和图像都很大，之后在多模态进行一个简单的编码。但是一个直觉显然是，作为多模态的任务，我们需要将多模态的进行更好地处理，给足算力，因为真正的多模态的理解，不是像 CLIP 一样进行简单的高维表征的融合，而是直接从低维信息中直接获得高维的多模态理解。所以说显而易见的，可以直接将多模态的部分变成一个 Transformer，然后将不同模态的数据进行简单的 tokenize 之后就 concat 作为输入。&lt;/p&gt;
&lt;p&gt;在这里提供了几个 insight，其中之一是，尽管我们认为 ViLT 的这种做法比较符合直觉，但是很明显它缺乏一种泛化能力。在已经训练好的模型的基础上，假如新加入一种新模态，例如语音，ViLT 就需要重新进行一次训练，而 CLIP 将新的编码器 align 到之前的空间中即可，原来的编码器可以 frozen。虽然说这种方法并不优雅（因为三个模态同时进行训练，所获得的图像文本编码器的权重，肯定和他们两个进行训练的时候不一样，这也是因为对于三模态的输入来说，最后获得的那个高维空间，本身也会具有新模态的含义，但是尽管如此强行的对齐依然是可以的），但也能反映出来泛化能力上的不同。&lt;/p&gt;
&lt;p&gt;另一方面的几个小技巧，包括说对于图像使用数据增强（因为没有繁重的图像编码器，所以不同于之前的方法将编码后的特征储存起来使用，ViLT 作为端到端的模型，可以直接使用图像，那么图像增强就有必要了），同时避免使用 cut 以及 color 类型的增强。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ViLT - &lt;a href=&quot;https://www.bilibili.com/video/BV14r4y1j74y/&quot;&gt;https://www.bilibili.com/video/BV14r4y1j74y/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ALBEF&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ALBEF.1ovd18db0t.webp&quot; alt=&quot;The pipeline of ALBEF&quot;&gt;&lt;/p&gt;
&lt;p&gt;介绍一下 ALBEF，这份工作可以说也是很经典的内容了，基本来说，符合了前人工作的几个共识。首先就是，一般来说，图像编码器需要大于文本编码器，同时的话，多模态的编码器也要尽可能的大，于是使用了 12 层 Transformer 作为图像编码器，6 层文本以及 6 层多模态。同时也是用了 ITC/ITM/MLM，这几种经典的任务。&lt;/p&gt;
&lt;p&gt;其中一个创新点在于 hard negative，也就是从 ITC 中选择最相似的难样本作为 ITM 的 negative；同时还有一个，也可以理解为是自学习或者自蒸馏，反正就是加入了一个 MT 来获得稳定表征。这里面需要注意的是，事实上在训练的过程中，数据的噪声巨大无比，而且不一定准确，因此加入一个 MT，已经不是在单模态里面的那种简单平均了，而是甚至可以生成质量远高于当前 GT 的标签，这一点在后续的 BLIP 里面也有体现，也可以说是对于数据的处理。&lt;/p&gt;
&lt;p&gt;但是进行一个简单的拓展，之所以使用动量的方法，本质上还是因为它是 one- stage 的，假如说使用 noisy student 那种，每训练完一个模型再作为 Teacher，肯定也是没有问题的，在这里，BLIP 似乎更加出色，后续去说。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多模态串讲 - &lt;a href=&quot;https://www.bilibili.com/video/BV1Vd4y1v77v/&quot;&gt;https://www.bilibili.com/video/BV1Vd4y1v77v/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;VLMo&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/VLMo.6t71qid8a3.webp&quot; alt=&quot;The pipeline of VLMo&quot;&gt;&lt;/p&gt;
&lt;p&gt;VLMo 也可以说是一个比较经典的工作，其中提出的主要就是 MoME，但是这里面，MoE 的experts 是模型自己去选择的，而在这个里面则是手动的进行切换。&lt;/p&gt;
&lt;p&gt;大概的结构就是一个 L 层的 Transformer，但是其中的 FFN 都被换成了多个 FFN 的形式，然后在训练的过程中决定使用哪一个。&lt;/p&gt;
&lt;p&gt;这里面的一个 insight 在于无需使用多个 attention block，而是说确实一个 attention 就可以处理完全部内容了，而且不同的 FFN 也可以接收同样的输出，并根据自己的模态进行理解。&lt;/p&gt;
&lt;p&gt;那么对于这三个经典的 loss，ITC 可以分别激活图像和文本，最后算损失；ITM 先分别激活图像和文本若干层，之后再全交给多模态；MLM 同 ITM，从图上看起来还是十分优雅的。&lt;/p&gt;
&lt;p&gt;最后，这个预训练的策略也比较有意思，属于是采用了分阶段训练，首先用图像数据训练图像 FFN，之后是文本，在经过了一定量的预训练之后，才是多模态。在这个里面需要注意的是，图像和文本的顺序不能换，不知道具体是因为什么。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多模态串讲 - &lt;a href=&quot;https://www.bilibili.com/video/BV1Vd4y1v77v/&quot;&gt;https://www.bilibili.com/video/BV1Vd4y1v77v/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BLIP&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/BLIP.3d4pyf3l73.webp&quot; alt=&quot;The pipeline of BLIP&quot;&gt;&lt;/p&gt;
&lt;p&gt;BLIP 可以说是我比较喜欢的一篇工作了，当然，基础的模型结构并没有很大的创新，本身还是 VLMo 的框架，贡献了 attention block 的参数，但是把 MLM 换成了 LM，所以这里的参数不能共享，换成了一个 casual attention。&lt;/p&gt;
&lt;p&gt;这里面我非常喜欢的一个设计，就是它的 caption-filter 框架。这种设计其实在 ALBEF 里面已经体现出来了一些，也就是我前面说的使用 MT 的方法。但是事实上，这种方法并不完全的优雅，尽管是 one-stage，但是或许效果并不如 two-stage，更何况本身还是完全的套用之前的范式，属于是意识到了 noisy 和 pseudo label 的潜力，但是并没有完全发挥。&lt;/p&gt;
&lt;p&gt;那么，BLIP 的这个框架就不一样了。首先是一个 two-stage，这一点无伤大雅，正如我所说的，one 和 two 的区别并不是很大，甚至说 EMA 唯一的意义在于维护一个 bank，其他情况下完全可以想象，性能应该不如 two-stage。&lt;/p&gt;
&lt;p&gt;BLIP 的重点在于，ALBEF 只关注到了 MLM 生成的高质量，然后就直接融合进去了，这种粗糙的融合固然是可行的，但是效果不一定特别好，只能说是缓解了 noisy 的情况，因为 noisy 依然存在，只是因为 MT 的权重而被稀释了。那么一个更彻底的方案就是进行 filter，BLIP 巧妙的注意到了这种 filter 的需求和 ITM 的任务惊人的相似，于是使用 LM 进行 caption，把 caption 和 GT 一起交给 ITM 去二选一，这样最后的结果就会很好了。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多模态串讲 - &lt;a href=&quot;https://www.bilibili.com/video/BV1fA411Z772/&quot;&gt;https://www.bilibili.com/video/BV1fA411Z772/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;CoCa&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/CoCa.41xzifr47n.webp&quot; alt=&quot;The pipeline of CoCa&quot;&gt;&lt;/p&gt;
&lt;p&gt;CoCa 可以说和 ALBEF 十分的相似，基本上就是和 ALBEF 一模一样，但是 CoCa 的关注点在于，之前的工作，虽然看上去从 pipeline 里面都是同时进行的输入，但是实际上在一个 iteration 里面都是经过了很多次的 forward，而 CoCa 则是希望，在同一个 iteration 里面，所有的 forward 都只进行一次，也就是所谓的 one-pass。&lt;/p&gt;
&lt;p&gt;方法也十分简单，既然 one-pass 了，那么 scale 上去很多数据就会方便很多，毕竟计算快了很多，于是直接对文本输入直接采取 casual-attention，也不需要管数据的损失，算就完事了，于是任务也变成了一个 Co 和一个 Ca，也就是 contrast 和 caption。&lt;/p&gt;
&lt;p&gt;所以说白了其实带来的 insight 不算多，一方面 ITC 确实有效，一方面 LM 也是一个难任务，但是在诸多 trick 之上，CoCa 的 large model 以及 scale up 的 data 显然为其性能带来的更大的影响。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多模态串讲 - &lt;a href=&quot;https://www.bilibili.com/video/BV1fA411Z772/&quot;&gt;https://www.bilibili.com/video/BV1fA411Z772/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BEiT V3&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/BEiT_V3.8l00lewl5a.webp&quot; alt=&quot;The pipeline of BEiT V3&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以说 BEiT V3 本质上和之前的 VLMo 是十分类似的，但是区别在于，其只采用了一种任务，也就是 LM 任务，这自然也增加了运算的效率。之后就是通过大量的数据，以及不同 FFN 的激活，来在不同的的任务里面训练，可以说是十分的简洁。&lt;/p&gt;
&lt;p&gt;这篇说白了也就是一个 insight，也就是阐述了 MoME 在 LM 任务下 scale up 之后确实很强，同时当然，这些 MoME 依然可以组合，再去 transfer 到不同的下游任务里。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多模态串讲 - &lt;a href=&quot;https://www.bilibili.com/video/BV1fA411Z772/&quot;&gt;https://www.bilibili.com/video/BV1fA411Z772/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;BLIP2&lt;/h2&gt;
&lt;p&gt;虽然说名字叫做 BLIP2，但是实际上感觉模型的结构上区别还是很大的，只是说任务比较类似而已。&lt;/p&gt;
&lt;p&gt;BLIP2 的主要贡献，以及 motivation 在于，之前的模型，都是全部由自己训练的，无论是效率还是算力之类的，开销都很大，而目前领域内已经有了很多的性能很好的模型，于是直接 frozen 之后拿过来用就好。于是提出了一个 Q-former，可以对于 frozen 的图像 encoder 以及 LLM 起到桥梁的作用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/BLIP2-1.1ovd18db0w.webp&quot; alt=&quot;Stage 1 for BLIP2&quot;&gt;&lt;/p&gt;
&lt;p&gt;训练还是一个 two-stage，这里面 stage-1 和 stage-2 的图画的其实很迷惑，因为 Q-former 里面本质上是有两个 Transformer 的，那么后面在 stage-2 的输出，是两个 Transformer 的 concat 还是什么，就很神秘。这里一篇 &lt;a href=&quot;https://blog.csdn.net/LoseInVain/article/details/136013909&quot;&gt;csdn 的博客&lt;/a&gt; 的图很不错，事实上拿的是 queries 输入的那个 transformer 的输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/BLIP2-2.8s38guiqkx.webp&quot; alt=&quot;Stage 2 for BLIP2&quot;&gt;&lt;/p&gt;
&lt;p&gt;Stage-1 和正常的 ALBEF 区别不大，之后 stage-2 把输出过 MLP 送给 LLM，再进行训练。本质上假如没有 Stage-2，那么就是一个 ALBEF，而假如没有 stage-1，则是一种新的范式。那么能否抛开 stage-1 呢？毕竟 stage-2 也是一个完整的训练流程，而且也是多模态的，但是实验表明不行。一种理解是，在 Q-former 里面之所以要引入一个文本编码器，目的就是通过 stage-1 的各种任务，让图像端的 Q-former 和文本对齐，换句话说，这个 token 输入给后面的 LLM 的时候，模型说的是人话，而不是图像话，毕竟后面跟的 MLP 只是为了统一维度，本身与文本类似的语言表征，还是在 Q-former 里面进行建模的。比起来能够将两个模型拼起来，我觉得还是这个 align 的启发更大一些。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BLIP2 - &lt;a href=&quot;https://blog.csdn.net/LoseInVain/article/details/136013909&quot;&gt;https://blog.csdn.net/LoseInVain/article/details/136013909&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;LLava&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/LLava.9rjbu0lhr3.webp&quot; alt=&quot;The pipeline of LLava&quot;&gt;&lt;/p&gt;
&lt;p&gt;LLava 比较简单，主要是提出了一种只使用 GPT 的文字功能，就可以生成高质量 caption 的方法，简单来说，对于具有 captions 和 bounding boxes 的内容来说，其实际上具有更多的信息量可以挖掘，所以可以生成一些高质量的 hard task。&lt;/p&gt;
&lt;p&gt;模型的结构就是一个 image encoder 之后跟一个 MLP 来映射，然后一起输入到 LLM 里面。依然训练是 two-stage 的，首先只训练 MLP 来对齐，之后训练 MLP 和 LLM 来适应具体任务。&lt;/p&gt;
&lt;p&gt;本身的 insight 一方面对齐不需要很强的表征能力，MLP 已经足矣；另一方面高质量的数据很重要。同时 LLava 用的各种 prompt 自然也很有参考价值。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLava - &lt;a href=&quot;https://blog.csdn.net/qq_35812205/article/details/136586853&quot;&gt;https://blog.csdn.net/qq_35812205/article/details/136586853&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Flamingo&lt;/h2&gt;
&lt;h2&gt;Visual Instruction Tuning&lt;/h2&gt;
&lt;h2&gt;Qwen-VL&lt;/h2&gt;
&lt;h2&gt;Gemini&lt;/h2&gt;
&lt;h2&gt;GPT-4V&lt;/h2&gt;
&lt;h2&gt;LISA&lt;/h2&gt;
&lt;h2&gt;Cambrian-1&lt;/h2&gt;
&lt;h2&gt;Qwen2-VL&lt;/h2&gt;
&lt;h2&gt;Qwen2.5-VL&lt;/h2&gt;
&lt;h2&gt;InstructBLIP&lt;/h2&gt;
&lt;h2&gt;LLaMA-Adapter&lt;/h2&gt;
&lt;h2&gt;VisionLLM&lt;/h2&gt;
&lt;h2&gt;SigLIP&lt;/h2&gt;
&lt;h2&gt;SigLIP 2&lt;/h2&gt;
&lt;h2&gt;LLaVA-OneVision&lt;/h2&gt;
&lt;h2&gt;GPT 5&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/Paper-reading-MLLM-zh.webp"/><enclosure url="https://picr2.axi404.top/Paper-reading-MLLM-zh.webp"/></item><item><title>RoboMaster 视觉组第三次培训</title><link>https://axi404.github.io/blog/rm-tutorial-section-3</link><guid isPermaLink="true">https://axi404.github.io/blog/rm-tutorial-section-3</guid><description>关于 OpenCV 的安装以及使用</description><pubDate>Thu, 07 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本教程为 RoboMaster 视觉组第三次培训，主要内容为 OpenCV 的安装以及使用。&lt;/p&gt;
&lt;h2&gt;安装 OpenCV&lt;/h2&gt;
&lt;p&gt;在安装 OpenCV 之前需要安装部分的依赖：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install libgtk2.0-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一般来说，可以直接使用源代码对于 OpenCV 进行编译安装，代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget https://github.com/opencv/opencv/archive/4.8.0.zip opencv-4.8.0.zip
unzip opencv-4.8.0.zip
cd opencv-4.8.0
mkdir build &amp;#x26;&amp;#x26; cd build
cmake ..
make -j$(nproc)
sudo make install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OpenCV 是 C++ 的库，因此可以使用 CMakeList 来使用。&lt;/p&gt;
&lt;p&gt;此时的 CMakeLists.txt 在依次的地方添加：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;find_package(OpenCV REQUIRED)
include_directories(
    ${OpenCV_INCLUDE_DIRS}
)
target_link_libraries(test
    ${OpenCV_LIBS}
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时在 main.cpp 中添加语句 &lt;code&gt;#include &amp;#x3C;opencv2/core.hpp&gt;&lt;/code&gt; 或者 &lt;code&gt;#include &amp;#x3C;opencv4/opencv2/core.hpp&gt;&lt;/code&gt; 并且进行编译，假如通过就没有问题。&lt;/p&gt;
&lt;h2&gt;OpenCV 基础&lt;/h2&gt;
&lt;h3&gt;图像的本质&lt;/h3&gt;
&lt;p&gt;在计算机中，图像是以矩阵的形式存储的。彩色图片通常由三个通道（如 BGR）组成，而灰度图只有一个通道。OpenCV 使用 &lt;code&gt;cv::Mat&lt;/code&gt; 数据类型来表示图像，它包含两部分数据：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;矩阵头 (Matrix Header)&lt;/strong&gt;：包含矩阵的大小、存储方法、存储位置等信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;指向像素值的指针&lt;/strong&gt;：一个指向实际存储像素值的矩阵的指针。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cv::Mat&lt;/code&gt; 的常用构造与操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;构造函数&lt;/strong&gt;：&lt;code&gt;cv::Mat(rows, cols, type, Scalar(value))&lt;/code&gt; 是其经典构造函数，可以指定行、列、数据类型（如 &lt;code&gt;CV_8UC3&lt;/code&gt; 代表 8位无符号3通道图像）和初始值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;赋值与拷贝&lt;/strong&gt;：&lt;code&gt;cv::Mat&lt;/code&gt; 支持直接通过等号赋值或拷贝构造。&lt;code&gt;clone()&lt;/code&gt; 和 &lt;code&gt;copyTo()&lt;/code&gt; 函数可以实现底层矩阵的深度拷贝。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;访问成员&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.rows&lt;/code&gt; 和 &lt;code&gt;.cols&lt;/code&gt;：获取矩阵的行数和列数。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.channels()&lt;/code&gt;：获取图像的通道数。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.empty()&lt;/code&gt;：返回一个布尔值，判断矩阵是否为空。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.at&amp;#x3C;Vec3b&gt;(i, j)[t]&lt;/code&gt;：访问坐标为 (i, j) 的像素点的第 t 个通道的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;OpenCV 基本操作&lt;/h3&gt;
&lt;h4&gt;读取与显示&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;读取图片&lt;/strong&gt;：&lt;code&gt;cv::Mat src = imread(&quot;你的图片路径&quot;);&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;显示图片&lt;/strong&gt;：&lt;code&gt;cv::imshow(&quot;窗口名&quot;, src);&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;等待按键&lt;/strong&gt;：&lt;code&gt;cv::waitKey(0);&lt;/code&gt;，如果参数为0，则无限等待直到有按键按下。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;读取视频&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;创建 &lt;code&gt;cv::VideoCapture&lt;/code&gt; 对象：&lt;code&gt;cv::VideoCapture capture;&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;打开视频文件：&lt;code&gt;capture.open(&quot;你的视频路径&quot;);&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;逐帧读取：在一个循环中使用 &lt;code&gt;capture &gt;&gt; pic;&lt;/code&gt; 来读取下一帧。&lt;/li&gt;
&lt;li&gt;循环播放：可以通过 &lt;code&gt;capture.get(cv::CAP_PROP_FRAME_COUNT)&lt;/code&gt; 获取总帧数，判断播放结束后使用 &lt;code&gt;capture.set(cv::CAP_PROP_POS_FRAMES, 0)&lt;/code&gt; 回到视频开头。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;绘制图案&lt;/h4&gt;
&lt;p&gt;在图像上绘制点、线、形状和文字对于调试和结果可视化至关重要。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据结构&lt;/strong&gt;：&lt;code&gt;cv::Point&lt;/code&gt; 是一个核心数据类型，用于表示点的坐标。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;绘制圆形&lt;/strong&gt;：&lt;code&gt;void circle(img, center, radius, color, thickness);&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;绘制直线&lt;/strong&gt;：&lt;code&gt;void line(img, pt1, pt2, color, thickness);&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;绘制文字&lt;/strong&gt;：&lt;code&gt;void putText(img, text, org, fontFace, fontScale, color, thickness);&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;绘制折线/多边形&lt;/strong&gt;：&lt;code&gt;void polylines(img, pts, isClosed, color, thickness);&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;图像预处理&lt;/h3&gt;
&lt;p&gt;预处理旨在对原始图像进行初步筛选和优化，以便后续的特征提取。&lt;/p&gt;
&lt;h4&gt;滤波 (Filtering)&lt;/h4&gt;
&lt;p&gt;滤波是一种平滑处理，主要用于去除图像中的噪声。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;均值滤波 (&lt;code&gt;blur&lt;/code&gt;)&lt;/strong&gt;：使用一个所有元素值都相等的卷积核（如 3x3 大小的核，每个元素为 1/9）对图像进行卷积，将每个像素替换为其邻域像素的平均值。这会使图像变得模糊。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高斯滤波 (&lt;code&gt;GaussianBlur&lt;/code&gt;)&lt;/strong&gt;：使用基于高斯函数生成的卷积核，中心点的权重最大，离中心越远的像素权重越小。这种方法在去除噪声的同时，能更好地保留图像的边缘信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;二值化 (Binarization)&lt;/h4&gt;
&lt;p&gt;二值化根据设定的阈值将图像转换为只有黑白两色的图像，以突出感兴趣的区域。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;色彩空间转换&lt;/strong&gt;：在二值化之前，通常需要将图像从 BGR 色彩空间转换到更符合人类视觉感知的 HSV 空间 (&lt;code&gt;cv::cvtColor(src, dst, cv::COLOR_BGR2HSV);&lt;/code&gt;)。HSV 将颜色分为色相 (H)、饱和度 (S) 和亮度 (V)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;彩色二值化 (&lt;code&gt;inRange&lt;/code&gt;)&lt;/strong&gt;：在 HSV 空间下，可以设定一个颜色范围的上限和下限，使用 &lt;code&gt;inRange&lt;/code&gt; 函数提取出这个范围内的所有像素，生成二值图。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灰度二值化 (&lt;code&gt;threshold&lt;/code&gt;)&lt;/strong&gt;：首先将图像转为灰度图 (&lt;code&gt;cv::cvtColor(img, gray_img, cv::COLOR_BGR2GRAY);&lt;/code&gt;)，然后使用 &lt;code&gt;threshold&lt;/code&gt; 函数，将灰度值在指定阈值范围内的像素设为白色，其余设为黑色。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;形态学操作 (Morphological Operations)&lt;/h4&gt;
&lt;p&gt;膨胀和腐蚀是常用于二值图像的形态学操作，用以优化形状或减少噪声。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;膨胀 (&lt;code&gt;dilate&lt;/code&gt;)&lt;/strong&gt;：扩展图像中的白色区域，可以用于填充小孔洞或连接断开的目标。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;腐蚀 (&lt;code&gt;erode&lt;/code&gt;)&lt;/strong&gt;：收缩图像中的白色区域，可以用于消除小的噪声点或分离粘连的物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开操作 (Opening)&lt;/strong&gt;：先进行腐蚀操作，然后进行膨胀操作。主要作用是去除小的噪声斑点，并分离连接的物体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;闭操作 (Closing)&lt;/strong&gt;：先进行膨胀操作，然后进行腐蚀操作。主要作用是填充物体内部的小孔洞，并连接邻近的物体。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;图像特征获取&lt;/h3&gt;
&lt;p&gt;在预处理后的二值图上，我们可以提取轮廓等形状特征。&lt;/p&gt;
&lt;h4&gt;寻找轮廓&lt;/h4&gt;
&lt;p&gt;使用 &lt;code&gt;findContours&lt;/code&gt; 函数可以检测出二值图像中所有物体的轮廓。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;函数原型&lt;/strong&gt;: &lt;code&gt;cv::findContours(image, contours, hierarchy, mode, method);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;参数说明&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;image&lt;/code&gt;: 输入的必须是二值图像。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contours&lt;/code&gt;: 用于存储所有找到的轮廓，通常是 &lt;code&gt;vector&amp;#x3C;vector&amp;#x3C;Point&gt;&gt;&lt;/code&gt; 类型。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hierarchy&lt;/code&gt;: 存储轮廓之间的层级关系。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: 轮廓的检索模式，如 &lt;code&gt;RETR_EXTERNAL&lt;/code&gt;（只检索最外层轮廓）或 &lt;code&gt;RETR_TREE&lt;/code&gt;（检索所有轮廓并重建嵌套结构）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;method&lt;/code&gt;: 轮廓的逼近方法，如 &lt;code&gt;CHAIN_APPROX_SIMPLE&lt;/code&gt;（压缩水平、垂直和对角线段，只保留端点）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;分析轮廓特征&lt;/h4&gt;
&lt;p&gt;找到轮廓后，可以通过计算其几何特征来进行筛选和识别。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;面积 (&lt;code&gt;contourArea&lt;/code&gt;)&lt;/strong&gt;：计算轮廓的面积。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;外接矩形 (&lt;code&gt;boundingRect&lt;/code&gt;)&lt;/strong&gt;：找到包围轮廓的最小正立矩形，返回一个 &lt;code&gt;cv::Rect&lt;/code&gt; 对象，包含矩形的坐标和宽高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;长宽比&lt;/strong&gt;：通过外接矩形的 &lt;code&gt;width&lt;/code&gt; 和 &lt;code&gt;height&lt;/code&gt; 计算物体的长宽比 (&lt;code&gt;aspectRatio = width / height&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;面积比&lt;/strong&gt;：通过计算轮廓面积与外接矩形面积的比值，可以用来区分不同形状的物体 (&lt;code&gt;areaRatio = contourArea / boundingRectArea&lt;/code&gt;)。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-3-zh.webp"/><enclosure url="https://picr2.axi404.top/template-3-zh.webp"/></item><item><title>RM 教程列表</title><link>https://axi404.github.io/blog/rm-list</link><guid isPermaLink="true">https://axi404.github.io/blog/rm-list</guid><description>伴随着事务的繁忙以及时间的紧张，RoboMaster 视觉组的教程已经再难亲力亲为，在这里汇总一条正确的路线，充分引用、复用已有内容，并给出一些建议。</description><pubDate>Wed, 06 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本教程作为西安交通大学 RoboMaster 视觉组教程的个人列表，旨在为视觉组的新生提供一个正确的学习路线，并给出一些建议。&lt;/p&gt;
&lt;p&gt;本教程面向完全零基础的视觉组新生，如果你已经对计算机视觉以及教程中提及的计算机知识具有部分的了解，那么可以跳过对应的部分。尽管计算机视觉并非计算机知识的初级内容，然而 RoboMaster 作为具有一定深度但是同样面向 Junior 的竞赛，其内容对于初学者来说保持在具有一定的难度但并非不可接受的程度，在经过系统的若干月的学习之后，你将能够掌握视觉组所需要的知识。&lt;/p&gt;
&lt;p&gt;不过有必要提及的是，RoboMaster 比赛的性质绝非其他正常「水赛」，需要你花费你学期中的大量时间，甚至耗费你的假期以及课内时间，尽管带来的成长也是毋庸置疑的，但是还是要做好心理准备。&lt;/p&gt;
&lt;p&gt;本教程仅给出最小完全掌握知识所需要的内容。&lt;/p&gt;
&lt;h2&gt;基础知识&lt;/h2&gt;
&lt;p&gt;基础知识部分可以 Follow &lt;a href=&quot;/blog/advise&quot;&gt;致新生的你&lt;/a&gt; 博客，读者需要掌握基本的技能，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基础计算机知识（参考 &lt;a href=&quot;https://www.criwits.top/missing/&quot;&gt;你缺失的那门计算机课&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;基础数学知识，包括高等数学与线性代数，可以使用课内教材自学&lt;/li&gt;
&lt;li&gt;Markdown 语法，参考 &lt;a href=&quot;https://www.markdownguide.org/basic-syntax/&quot;&gt;Markdown 语法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;C++，参考视觉组相关教程，文字版于 &lt;a href=&quot;/blog/rm-tutorial-section-1&quot;&gt;C++ 教程博客&lt;/a&gt;，视频版见 &lt;a href=&quot;https://www.bilibili.com/video/BV1uz4y1u7SW/&quot;&gt;Bilibili&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些内容不会在线下的培训中涉及，但是是视觉组学习的基础，需要读者自行学习。&lt;/p&gt;
&lt;h2&gt;Linux&lt;/h2&gt;
&lt;p&gt;视觉组需要基于 Linux 系统进行开发，因此你需要掌握基本的 Linux 知识，可以参考 &lt;a href=&quot;/blog/install-ubuntu&quot;&gt;安装 Ubuntu 双系统&lt;/a&gt; 安装双系统，或者在 Windows 之中一键安装 WSL。&lt;/p&gt;
&lt;p&gt;更多的 Linux 以及进阶的计算机相关知识可以参考 &lt;a href=&quot;https://missing-semester-cn.github.io/&quot;&gt;The Missing Semester of Your CS Education&lt;/a&gt;，同时 &lt;a href=&quot;https://101.lug.ustc.edu.cn/&quot;&gt;Linux 101&lt;/a&gt; 可以作为拓展阅读。&lt;/p&gt;
&lt;p&gt;在安装了 Linux 之后，读者可以尝试配置 CMake 以及对应的 CPP 开发环境，可以参考 &lt;a href=&quot;/blog/rm-tutorial-section-2&quot;&gt;CMake 教程&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;ChatGPT&lt;/h2&gt;
&lt;p&gt;使用 GPT 在内的大模型服务可以帮助读者快速学习新知识，并且完成视觉组相关任务。读者如果可以直接订阅海外服务，建议直接使用 Gemini 或者 ChatGPT 的服务，同时在访问或者支付受限的情况下，可以选择 &lt;a href=&quot;/blog/gpt_collection&quot;&gt;使用转发站&lt;/a&gt;，国内模型更次之。&lt;/p&gt;
&lt;h2&gt;计算机视觉&lt;/h2&gt;
&lt;p&gt;计算机视觉是视觉组的核心内容，需要读者掌握基本的计算机视觉知识，主要为 OpenCV 相关内容，并且完成视觉组相关任务。OpenCV 的绝大多数内容均可以使用 ChatGPT 进行辅导。&lt;/p&gt;
&lt;h2&gt;ROS&lt;/h2&gt;
&lt;p&gt;ROS，即 Robot Operating System，是机器人领域的通用框架之一，主要提供了 TF、rviz 以及 ROS 通信等链路，可以通用接口，教程后续补上。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-2-zh.webp"/><enclosure url="https://picr2.axi404.top/template-2-zh.webp"/></item><item><title>RoboMaster 视觉组第二次培训</title><link>https://axi404.github.io/blog/rm-tutorial-section-2</link><guid isPermaLink="true">https://axi404.github.io/blog/rm-tutorial-section-2</guid><description>关于 SSH 以及 CMake 的基础讲解。</description><pubDate>Wed, 17 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本部分的博客是 RoboMaster 机甲大师视觉组培训的第二期内容，主要讲解一些计算机的基本技能，包括使用 Markdown/Linux/SSH/CMake，其中主要讲解的是包括 SSH 以及 CMake 在内的内容，这些内容是将来使用 Linux 进行编程的重要组件。&lt;/p&gt;
&lt;h2&gt;Markdown&lt;/h2&gt;
&lt;p&gt;对于没有使用过 Markdown 的同学来说，大多数时候，我们均使用 Word 来进行文档的编辑工作，但是 Word 往往具有一定的缺点，这包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要使用 Word 软件进行打开，而 Word 软件是闭源的。&lt;/li&gt;
&lt;li&gt;无法进行实时渲染。&lt;/li&gt;
&lt;li&gt;打开的过程中过于耗时。&lt;/li&gt;
&lt;li&gt;排版并不直观（对于 Geek 来说，在生成更富文本内容时，一般选择使用 $\LaTeX$ 以替代 Markdown）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些内容对于正常的办公人士来说是可以忍受的，但是对于追求性能的人来说，可以说是弊端十足，此时 Markdown 是满足这些需求的最佳选项。&lt;/p&gt;
&lt;p&gt;一方面，Markdown 可以很快捷的编译为 html，而同时又不同于 txt，其本身具备一定的排版系统，可以实现对于大多数文档的必要编写需求。&lt;/p&gt;
&lt;p&gt;Markdown 的文件格式为 &lt;code&gt;.md&lt;/code&gt;，使用此后缀名便可以较为轻松的将内容标记为 Markdown，并在大多数的代码工具中被直接渲染。而专业的 Markdown 编辑器，如 typora/obsidian/VSCode Markdown 插件，读者均可以自行进行探索。&lt;/p&gt;
&lt;p&gt;Markdown 的详细语法见 &lt;a href=&quot;https://markdown.com.cn/&quot;&gt;Markdown 官网文档&lt;/a&gt;，在这里不进行重复的说明，因为在占用篇幅的同时，这是多余的。&lt;/p&gt;
&lt;h2&gt;安装 VSCode&lt;/h2&gt;
&lt;p&gt;通过Ubuntu自带的火狐浏览器找到熟悉的 VSCode 的&lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;官网&lt;/a&gt;，并且进行安装。&lt;/p&gt;
&lt;p&gt;选择 &lt;code&gt;.deb&lt;/code&gt; 进行下载，下载完毕之后进入下载文件夹，应该可以看到下载的 deb 包，右键在终端中打开，输入：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo dpkg -i code_your_version.deb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;code_your_version.deb&lt;/code&gt; 为你的 deb 包的名字，在命令行中可以使用 TAB 进行自动补全，这样你就只需要输入一个 code，之后进行自动补全即可。&lt;/p&gt;
&lt;p&gt;输入密码，其中密码的输入是不可见的，输入之后终端没有反应并非你没有输入，输入之后按下回车即可。&lt;/p&gt;
&lt;p&gt;稍等片刻，等命令行又一次可以输入的时候，在命令行中输入 code，回车，进入 VSCode。&lt;/p&gt;
&lt;h2&gt;SSH&lt;/h2&gt;
&lt;p&gt;在这里简短的介绍 SSH 语法，一般来说 SSH 安装在每一个系统中，无需额外的安装，在这里推荐使用 VSCode 的 SSH 插件&lt;/p&gt;
&lt;p&gt;通过在 VSCode 的拓展栏进行搜索，可以很轻易地找到 VSCode 的 SSH 插件：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/SSH_1.4cktbl78ng.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;启用插件之后，点击左下角的打开远程窗口，选择连接到主机即可：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/SSH_2.3k7xtuqmx9.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在服务器的租用界面中，一般会提供 SSH 的指令，其格式为 &lt;code&gt;ssh -p port user@address&lt;/code&gt;，之后按照提升输入密码即可。&lt;/p&gt;
&lt;h2&gt;Linux&lt;/h2&gt;
&lt;p&gt;使用 SSH 后，我们会进入正式的 Linux 系统中，同时，由于使用 SSH，此时的 Linux 并没有提供图形化界面（这也是 Linux 最原始的形态），因此在本章节中，我们会首先讲解一些基础的 Linux 指令，以便读者可以进行接下来的操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ls&lt;/code&gt;：可以展示当前目录下的文件内容，其中显示隐藏内容需要使用 &lt;code&gt;ls -a&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd&lt;/code&gt;：用法为 &lt;code&gt;cd folder&lt;/code&gt;，可以前往指定的文件夹中，需要注明的是，&lt;code&gt;..&lt;/code&gt; 为上级目录，如想要前往上级，使用 &lt;code&gt;cd ..&lt;/code&gt;，上级的上级，以此类推 &lt;code&gt;cd ../..&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;文档的编辑操作需要使用 vim，这一技巧具备一定的难度，读者请勿尝试指令 &lt;code&gt;vim filename&lt;/code&gt;，若无法退出，请狂点 &lt;code&gt;esc&lt;/code&gt; 之后依次按下 &lt;code&gt;:&lt;/code&gt;, &lt;code&gt;w&lt;/code&gt;, &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;!&lt;/code&gt;, &lt;code&gt;Enter&lt;/code&gt; 以保存并退出，若不希望保存，无需按下 &lt;code&gt;w&lt;/code&gt;。&lt;/p&gt;
&lt;h2&gt;配置 C++ 开发环境&lt;/h2&gt;
&lt;p&gt;安装 build-essential 包，其中包括 gcc, g++, cmake 在内所需要的大多数包：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install build-essential
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果希望使用 clang 而非 gcc，可以安装：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install clang-15 clangd-15 clang-format-15
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clang 可以简单理解为等价于 gcc 的 C++ 编译器，但是可以提供一些更好的开发体验，不过从结果上并无本质区别。&lt;/p&gt;
&lt;p&gt;之后建立软链接：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo rm /usr/bin/clang /usr/bin/clang++ /usr/bin/clangd /usr/bin/clang-format
sudo ln -s /usr/bin/clang-15 /usr/bin/clang
sudo ln -s /usr/bin/clang++-15 /usr/bin/clang++
sudo ln -s /usr/bin/clangd-15 /usr/bin/clangd
sudo ln -s /usr/bin/clang-format-15 /usr/bin/clang-format
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于 Ubuntu20.04 的读者来说，以上全部的 15 改为 12，输出没有找到xxx是正常现象，因为 Ubuntu 20 并不支持 15 的 clang。&lt;/p&gt;
&lt;p&gt;在终端分别输入clang以及clangd得到以下输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ clang                                                                        
clang: error: no input files

$ clangd
clangd is a language server that provides IDE-like features to editors.

It should be used via an editor plugin rather than invoked directly. For more information, see:
	https://clangd.llvm.org/
	https://microsoft.github.io/language-server-protocol/

clangd accepts flags on the commandline, and in the CLANGD_FLAGS environment variable.

I[18:10:45.783] Ubuntu clangd version 15.0.7
I[18:10:45.783] Features: linux+grpc
I[18:10:45.783] PID: 3507769
I[18:10:45.783] Working directory: /home/gaoning
I[18:10:45.783] argv[0]: clangd
I[18:10:45.783] Starting LSP over stdin/stdout
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;则安装成功。&lt;/p&gt;
&lt;p&gt;之后安装 VSCode 插件，包括 CMake、CMake Tools 以及 C/C++。&lt;/p&gt;
&lt;p&gt;新建文件夹并且进入，&lt;code&gt;ctrl shift p&lt;/code&gt;，选择 CMake 快速入门，之后按照提示进行：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输入项目名称，如 mytest。&lt;/li&gt;
&lt;li&gt;创建 C++ 项目&lt;/li&gt;
&lt;li&gt;创建可执行文件&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;之后依然是点击下方栏的生成（编译），或者直接点击三角形按钮（编译并运行），得到了喜闻乐见的 Hello。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/FBVG3wK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3&gt;配置 Clang 环境&lt;/h3&gt;
&lt;p&gt;在VSCode安装 clangd 以及 Clang-Format，其中 Clang-Format 如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/OkTg3wK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;点击齿轮图表-拓展设置，找到 &lt;code&gt;Clang-format: Executable&lt;/code&gt; 设置为 &lt;code&gt;/usr/bin/clang-format&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;打开终端（ctrl alt t），创建 &lt;code&gt;.clang-format&lt;/code&gt; 作为格式化代码的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;touch .clang-format
gedit .clang-format
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;并且向其中输入配置内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{ BasedOnStyle: LLVM, UseTab: Never, IndentWidth: 4, TabWidth: 4, BreakBeforeBraces: Allman,
AllowShortIfStatementsOnASingleLine: false, IndentCaseLabels: false, ColumnLimit: 0,
AccessModifierOffset: -4, NamespaceIndentation: All, FixNamespaceComments: false }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;保存并退出。&lt;/p&gt;
&lt;p&gt;打开某一代码文件，右键选择格式化文档（快捷键为ctrl shift i），并且在弹窗中选择clang-format，可以发现代码变得十分的工整。但是有的时候一些代码会用红线，说一些内容未找到（如 &lt;code&gt;iostream&lt;/code&gt;），但是编译可以正常进行，输入以下命令解决这一问题。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install libstdc++-12-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;CMake &amp;#x26; CMakeList&lt;/h2&gt;
&lt;p&gt;在 C++ 的编译过程中，我们面临这样一个需求：我们有一个 C++ 编译器，一些 C++ 程序文件（它们彼此之间有依赖关系），一些 C++ 库（它们被正常的安装），并希望生成一个 C++ 编译的二进制文件，如何进行？&lt;/p&gt;
&lt;p&gt;一个基础的想法是，通过某些语法，声明他们之间的关联，并通过某种工具通知编译器，进行编译，CMake 和 CMakeList.txt 可以很好的完成这一内容。&lt;/p&gt;
&lt;p&gt;你需要进行的只是在一个程序的文件夹中的根目录下创建一个名为 &lt;code&gt;CMakeList.txt&lt;/code&gt; 的文件，并且在其中按照一定的语法，关联你的项目，之后在当前根目录下运行下述程序即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir build
cd build
cmake ..
make -j8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;-j8&lt;/code&gt; 为调用八个核进行编译工作，这个数字是可调节的，或者直接 &lt;code&gt;-j&lt;/code&gt; 进行自动调节也可以。&lt;/p&gt;
&lt;p&gt;一般来说项目具有两种不同的结构可以选择，一种是直接将 include 和 src 文件夹分开放置在项目的根目录下，之后主程序在根目录下。一种则是将功能包放在项目的根目录下，功能包中包含 include 和 src 文件夹。在这里推荐并且讲解后者，因为可以便于项目的管理，比如说一位同学写了一个功能包，想要加入整个项目之中，只需要把功能包拷贝进来并且稍加改变 CMakeLists.txt 就可以直接使用，十分的方便。&lt;/p&gt;
&lt;p&gt;对于功能包中的 CMakeLists.txt 写法如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;cmake_minimum_required(VERSION 3.0.0)
project(test VERSION 0.1.0)
SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++14 -pthread&quot;)

aux_source_directory(./src ALL_SRCS)
add_library(test STATIC ${ALL_SRCS})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此处即声明了一个名为 test 的功能包。&lt;/p&gt;
&lt;p&gt;同时在主&lt;code&gt;CMakeLists.txt&lt;/code&gt;中各项中添加：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;include_directories(
    test/include
    )
add_subdirectory(test)
target_link_libraries(infantry_new
    test
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即可完成 CMakeList.txt 的更新。&lt;/p&gt;
&lt;p&gt;此时的结构如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt; . 
 ├── test
 │    ├── CMakeLists.txt
 │    ├── include 
 │    │    ├── test1.hpp
 │    │    └── test2.hpp
 │    └── src
 │         ├── test1.cpp
 │         └── test2.cpp
 ├── build
 ├── CMakeLists.txt
 └── main.cpp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一个基础的 CMakeList.txt 仅包括以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;# 声明 CMake 版本需求
cmake_minimum_required(VERSION 3.0.0)
# 声明项目与版本/语言等信息
project(cpp VERSION 0.1.0 LANGUAGES C CXX)
# 将 main.cpp 编译为名为 cpp 的二进制文件 
add_executable(cpp main.cpp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下给出一个健全的 CMakeList.txt：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;# 声明 CMake 版本
cmake_minimum_required(VERSION 3.0.0)
# 声明 C++ 版本
set(CMAKE_CXX_STANDARD 17)
# 设置 TARGET_NAME 变量的值为 infantry_new
set(TARGET_NAME infantry_new)
# 设置项目为 TARGET_NAME 变量的值
project(${TARGET_NAME})
# 开启 CMAKE_EXPORT_COMPILE_COMMANDS
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
# 开启多线程
SET(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++17 -pthread&quot;)
SET(CMAKE_CXX_FLAGS_RELEASE &quot;-std=c++17 -pthread&quot;)
# 设置 OpenCV 路径，当系统中存在多个 OpenCV 时尤为重要
set(OpenCV_DIR /usr/local/lib/cmake/opencv4)
# 找库的依赖
find_package(OpenVINO REQUIRED COMPONENTS Runtime)
find_package(Ceres REQUIRED)
find_package(OpenCV REQUIRED)
# 设置宏定义
add_definitions(-DDEBUGMODE)
# 引用库与功能包
include_directories(
    /opt/MVS/include
    armor/include
    ${CERES_INCLUDE_DIRS}
    )
# 链接一些库
link_directories(
    /opt/MVS/lib/64
    /opt/MVS/lib/32
    /usr/local/lib
    )
# 添加子路径，为功能包
add_subdirectory(armor)
# 编译 main.cpp 为 infantry_new
add_executable(infantry_new main.cpp)
# 设置动态链接库
target_link_libraries(infantry_new
    armor
    ${CERES_LIBRARIES}
    )
# 一些常规设置
set(CPACK_PROJECT_NAME ${PROJECT_NAME})
set(CPACK_PROJECT_VERSION ${PROJECT_VERSION})
include(CPack)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不难发现，十分的简单，不懂的地方可以咨询 ChatGPT。&lt;/p&gt;
&lt;p&gt;同时需要额外教学的是，对于一些需要人工编译安装的 C++ 库来说，同样需要使用 CMake，其特征为根目录下有 CMakeList.txt，语法为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir build
cd build
cmake ..
make -j
sudo make install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以上，全部内容，多谢惠顾。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/RM-Tutorial-Section-2-zh.webp"/><enclosure url="https://picr2.axi404.top/RM-Tutorial-Section-2-zh.webp"/></item><item><title>在 Astro 中使用 Waline</title><link>https://axi404.github.io/blog/waline-install</link><guid isPermaLink="true">https://axi404.github.io/blog/waline-install</guid><description>Waline 是一个博客评论系统，本文将介绍如何在 Astro 中使用 Waline。</description><pubDate>Wed, 16 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在搭建博客的时候，往往一个评论系统会是一个刚需，这样可以和读者进行互动，而 Waline 是一个不错的评论系统。&lt;/p&gt;
&lt;p&gt;在此之前笔者尝试过使用 Giscus 作为评论系统，但是一方面这要求你的仓库需要开源，而我的博客主题还没有完全重构好；另一方面，Giscus 的评论需要通过 GitHub 的 API 获取（本身是基于 Discussion 建立的），这意味着要接受 Github 在大陆被 DNS 污染导致的降速（虽然可以 lazy load）以及必须要注册 Github 账号并且登录，这毫无疑问将评论的受众限制到了编程人员为主的群体，而不是更加广泛。假如读者希望使用 Giscus 搭建评论，见 &lt;a href=&quot;https://vitepress.yiov.top/plugin.html#%E8%AF%84%E8%AE%BA&quot;&gt;这个教程&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Waline 可以基于 Leancloud 以及 Vercel 搭建，并且以组件的形式嵌入到 Astro 中，虽然需要后端，但是项目本身做了很好的封装，用户不需要了解任何内容，只需要跟着流程走下来就好。其安装教程为 &lt;a href=&quot;https://waline.js.org/guide/get-started/&quot;&gt;官方文档&lt;/a&gt;，全部手把手进行即可。&lt;/p&gt;
&lt;p&gt;值得注意的是，第一个在 &lt;code&gt;https://your-waline-server-url/ui/&lt;/code&gt; 中注册的账号是管理员，务必在部署后直接先行登录。&lt;/p&gt;
&lt;h2&gt;在 Astro 中集成 Waline&lt;/h2&gt;
&lt;p&gt;为了在 Astro 博客中集成 Waline，我们通常会创建一个 Astro 组件来封装 Waline 的所有逻辑和样式。这样可以保持代码的整洁和可维护性。&lt;/p&gt;
&lt;h3&gt;核心原理&lt;/h3&gt;
&lt;p&gt;集成 Waline 的基本原理是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引入 Waline 客户端库： 在前端页面中加载 Waline 的 JavaScript 和 CSS 文件。&lt;/li&gt;
&lt;li&gt;提供挂载点： 在 HTML 中创建一个特定的 div 元素作为 Waline 评论界面的容器。&lt;/li&gt;
&lt;li&gt;初始化 Waline： 通过调用 Waline 客户端库提供的 init 方法，将评论系统挂载到指定容器，并传入必要的配置参数（如后端服务地址、表情包等）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;说白了直接先运行 &lt;code&gt;npm install @waline/client&lt;/code&gt; 安装客户端库，然后直接在组件中引入即可。&lt;/p&gt;
&lt;h3&gt;实现&lt;/h3&gt;
&lt;p&gt;以下是我的一个实现，可供参考。同时在当下，假如说你嵌入 Waline，实际上 Cursor 之类的 AI 工具也可以帮你生成。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-astro&quot;&gt;---
import &apos;@waline/client/style&apos;
const { class: className } = Astro.props
---

&amp;#x3C;comment-component&gt;
  &amp;#x3C;div id=&apos;waline&apos; class={`not-prose ${className}`}&gt;
    Comment seems to stuck. Try to refresh?✨
  &amp;#x3C;/div&gt;
&amp;#x3C;/comment-component&gt;

&amp;#x3C;script&gt;
  import { init as walineInit } from &apos;@waline/client&apos;
  const walineConfig = {
    server: &apos;https://your-waline-server-url/&apos;,
    emoji: [
      &apos;weibo&apos;,
      &apos;alus&apos;,
      &apos;bilibili&apos;,
      &apos;qq&apos;
    ],
    additionalConfigs: {
      locale: {
        placeholder: &apos;欢迎留言~(支持Markdown语法)&apos;
      }
    }
  }

  class Comment extends HTMLElement {
    constructor() {
      super()
    }

    connectedCallback() {
      const walineInstance = walineInit({
        el: &apos;#waline&apos;,
        serverURL: walineConfig.server,
        reaction: [],
        dark: &apos;html.dark&apos;,
        pageview: true,
        comment: true,
        ...walineConfig.additionalConfigs
      })
      walineInstance?.update()
    }
  }

  customElements.define(&apos;comment-component&apos;, Comment)
&amp;#x3C;/script&gt;

&amp;#x3C;style&gt;
  :global(.dark) #waline {
    --waline-bg-color: var(--card-bg);
    --waline-bg-color-light: var(--btn-plain-bg-hover);
  }

  :global(.wl-count) {
    color: var(--waline-dark-grey);
  }

  :global(.wl-editor) {
    padding: 0.35em 0.5em;
    width: calc(100% - 2em);
  }

  :global(.wl-edit) {
    color: var(--waline-dark-grey);
  }

  :global(#wl-edit)::placeholder {
    color: var(--waline-dark-grey);
  }

  :global(.wl-panel) {
    margin: .5em 0;
    border: none;
  }
&amp;#x3C;/style&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;评论支持邮件提醒&lt;/h2&gt;
&lt;p&gt;Waline 同样支持邮件提醒，在这里使用我的 Gmail 小号作为我的提醒邮箱。&lt;/p&gt;
&lt;p&gt;首先使用读者需要前往 &lt;a href=&quot;https://myaccount.google.com/apppasswords&quot;&gt;Google 应用密码&lt;/a&gt; 生成一个应用密码，假如说你没有开启 2FA，这个界面可能不存在，需要先开启 2FA。&lt;/p&gt;
&lt;p&gt;之后在 Vercel 的项目中增加环境变量，参考 &lt;a href=&quot;https://waline.js.org/guide/features/notification.html#%E9%82%AE%E4%BB%B6%E9%80%9A%E7%9F%A5&quot;&gt;Waline 文档&lt;/a&gt; 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SMTP_USER=your-gmail-account@gmail.com
SMTP_PASS=your-gmail-app-password
SMTP_SERVICE=Gmail
SMTP_SECURE=TRUE
SITE_NAME=your-site-name
SITE_URL=your-site-url
AUTHOR_EMAIL=your-author-email@gmail.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以及可选的配置，你可以选择你的发件人名称，以及发件人邮箱：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SENDER_NAME=your-name
SENDER_EMAIL=your-author-email@gmail.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后重新部署 Vercel 项目，即可完成配置。&lt;/p&gt;
&lt;h2&gt;后记&lt;/h2&gt;
&lt;p&gt;大概就是这样，因为想写点 Blog，所以写了这个，希望对你有帮助，假如需要协助，欢迎评论区评论。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-16-zh.webp"/><enclosure url="https://picr2.axi404.top/template-16-zh.webp"/></item><item><title>使用 PM2 持久化你的 Node 服务</title><link>https://axi404.github.io/blog/pm2</link><guid isPermaLink="true">https://axi404.github.io/blog/pm2</guid><description>使用 PM2 持久化你的 Node 服务</description><pubDate>Mon, 04 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;因为搞了一个服务器，所以自然而然也要折腾一些东西，主要就是我目前在我的服务器上面部署了 &lt;a href=&quot;/blog/folo&quot;&gt;RSSHub&lt;/a&gt; 以及一个可以重新拉取并且部署我的 Blog 的 &lt;a href=&quot;/blog/astro-multi-pages&quot;&gt;Webhook&lt;/a&gt;，当然还有我的博客本身。我的博客是静态博客，只需要 Nginx 反向代理就可以了，不需要什么开机自启，但是别的还是需要的。&lt;/p&gt;
&lt;h2&gt;PM2&lt;/h2&gt;
&lt;p&gt;PM2 是一款先进的、开源的 Node.js 应用进程管理器，专为生产环境设计。它能够帮助开发者以零停机（Zero-Downtime）的方式在服务器上高效运行和管理应用。人话来说，就是可以让你的 Node 服务在服务器上一直运行，并且可以自动重启。&lt;/p&gt;
&lt;p&gt;直接使用命令行安装：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm install pm2 -g
pm2 startup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后使用 &lt;code&gt;pm2&lt;/code&gt; 命令来启动你的服务，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pm2 start &amp;#x3C;入口文件&gt; --name &amp;#x3C;进程名称&gt;
# by npm
pm2 start npm --name my-app -- start
# by pnpm 
pm2 start pnpm --name my-app -- start
pm2 save
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后每次重启之后就可以自动启动了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-1-zh.webp"/><enclosure url="https://picr2.axi404.top/template-1-zh.webp"/></item><item><title>周记 Week56</title><link>https://axi404.github.io/blog/week-56</link><guid isPermaLink="true">https://axi404.github.io/blog/week-56</guid><description>2025-07-21 ~ 2025-07-27.</description><pubDate>Mon, 04 Aug 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这次也算是很久之后的一次周记吧，这周略有奔波，处理了不少事情，但是周记上倒是没什么可以写的。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;首先还是大概说下科研，一件让我心中的石头落地的事情发生了，GR00T 在我们内部迭代的某个版本的 Benchmark 上面训出了性能，而且很高，而后我们也从代码中找到了数据生成的一些 bug，这些 bug 会导致模型在之前不能很好的学到如何关闭夹爪，而恰好和我们之后的预处理方法抵消掉。如今 bug 改正了，同时性能也有了，可以说一切都好起来了。&lt;/p&gt;
&lt;p&gt;接下来的问题到了模型学习的难度上了。我们内部初步迭代的版本一直是 RoboTiq 的机械臂，而非 Aloha，单臂的模型在我们初步的 benchmark 上很轻松就把几个点跑满了，尽管我们已经在强调一些 novel object 以及 reasoning 的元素（之前的 GenManip 也是最早强调这些元素的 Benchmark 之一）。因此决定加大难度（事实上之前设计的难度也比这个刷满的高，但是因为担心模型学不出来大家都没有点，紧急降低了难度），发挥 GenManip 框架最擅长的 long horizon 生成。&lt;/p&gt;
&lt;p&gt;相较于 AGIBot 那边的 benchmark，数据主要来自于遥操作，GenManip 自动生成数据的好处就在于伴随着 horizon 的变长，不会带来更大的数据采集负担，而且可以多次迭代。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;这周主要还是生活上的事情，到处奔波。首先要回到学校参加一下专业实习的答辩，反正是随便聊了点，没啥东西，但是要线下，所以说坐火车回到了西安，也和乐小姐一起吃了饭什么的。然后就是回北京拔牙，因为我有两颗智齿完全是横向生长的，因此需要拔掉，可以预见的是未来的一段时间内都不能自由地大快朵颐了。&lt;/p&gt;
&lt;p&gt;这周大概就是这样，风平浪静。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-56-zh.webp"/><enclosure url="https://picr2.axi404.top/week-56-zh.webp"/></item><item><title>Hello, Claude Code. Goodbye, Cursor</title><link>https://axi404.github.io/blog/claude-code</link><guid isPermaLink="true">https://axi404.github.io/blog/claude-code</guid><description>关于为什么不使用 Cursor 以及转用 Claude Code 的一些考量</description><pubDate>Sun, 20 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;更新&lt;/h2&gt;
&lt;p&gt;近期 Cursor 又一次提供了大陆的服务，尽管 Claude code 功能强大，但是使用体感上，虽然说之前嘴硬说没必要 &lt;code&gt;Tab Tab Tab&lt;/code&gt;，但是发现离开之后是真的不行，还是有 Tab 好一些。在这里同样推荐读者使用 Cursor，并且在仅能使用一个服务时，以 Cursor 优先。Cursor 同时支持支付宝订阅，链接为 &lt;a href=&quot;https://cursor.com/?from=home&quot;&gt;https://cursor.com/?from=home&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;我一直是 Cursor 的老用户，尤其是 Claude 4 刚出的那一会，当时 Cursor 可以连续帮助我直接重构一个网页级别的项目，尽管之后略有降智，但是还算是可以接受的。事实上我是很享受 Vibe Coding 的，因此一直以来使用 Cursor 的体验感也还不错。不过一个悲伤的事实是，Cursor 很快就出现了地区的限制问题，导致绕过去又要花很大的功夫。&lt;/p&gt;
&lt;p&gt;同时，伴随着个人编程水平的提高，代码补全这种大多数时候不一定正确，尽管可以提高编程速度，但是意义有的时候也没有那么大。进一步来说，像是 Claude Code 的更强大的整体的程序生成的能力，才是我更加需要的。因此从主观以及客观角度出发，我都不得不进行转向。&lt;/p&gt;
&lt;h2&gt;使用 Claude Code&lt;/h2&gt;
&lt;p&gt;因为放弃了 Cursor，所以也就干脆转回到了 VSCode，而不是用 Cursor 的界面，不过令人惊讶的是，之前 VSCode 的工具栏在最左侧的设计我十分适应，但是如今必须要改成 Cursor 一样的工具栏在左侧的上方才用起来顺手，只能说长时间使用确实可以改变人的使用习惯。&lt;/p&gt;
&lt;p&gt;回到 Claude Code 的使用，其实就是在命令行中进行需要的提出，这个使用起来和 Agent Mode 的 Cursor 其实很像，只是有的反馈没有那么直观而已，但是也无伤大雅。所以开始上手吧，从装包开始：&lt;/p&gt;
&lt;p&gt;首先对于没有安装过 nodejs 的读者需要先安装 nodejs：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Download and install nvm:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
# in lieu of restarting the shell
\. &quot;$HOME/.nvm/nvm.sh&quot;
# Download and install Node.js:
nvm install 22
# Verify the Node.js version:
node -v # Should print &quot;v22.17.1&quot;.
nvm current # Should print &quot;v22.17.1&quot;.
# Verify npm version:
npm -v # Should print &quot;10.9.2&quot;.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后安装 Claude Code 的包：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm install -g @anthropic-ai/claude-code
# 验证安装
claude --version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假如安装过程中有权限问题，指令前面加上 &lt;code&gt;sudo&lt;/code&gt; 即可。&lt;/p&gt;
&lt;p&gt;之后设置环境变量，覆盖 Claude 默认的，图省事可以写到 &lt;code&gt;~/.zshrc&lt;/code&gt; 中：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export ANTHROPIC_AUTH_TOKEN=你的SK码
export ANTHROPIC_BASE_URL=https://中转站网址
export CLAUDE_CODE_MAX_OUTPUT_TOKENS=32000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里你需要找一个提供 Claude 4 api 服务的中转站。&lt;/p&gt;
&lt;p&gt;之后 &lt;code&gt;cd&lt;/code&gt; 到你的项目中，输入 &lt;code&gt;claude&lt;/code&gt; 即可开始使用~&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-15-zh.webp"/><enclosure url="https://picr2.axi404.top/template-15-zh.webp"/></item><item><title>Hello, Folo and RSSHub!</title><link>https://axi404.github.io/blog/folo</link><guid isPermaLink="true">https://axi404.github.io/blog/folo</guid><description>Folo 是一个 RSS 订阅器，可以使用 Folo 订阅一切你的 RSS。而 RSSHub 则是一个 RSS 生成器，可以将任何网页的内容变为 RSS。</description><pubDate>Sat, 26 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;RSS 是什么，很多读者可能会感觉比较迷惑，简单来说，其全称是 Really Simple Syndication，也就是简易内容聚合。一般来说，支持 RSS 的网页会将自己更新的内容同步到网页中的一个 &lt;code&gt;rss.xml&lt;/code&gt; 文件中，而其他人订阅了 RSS 之后，则可以使用这个较小的文件来快捷 fetch 更新的内容。&lt;/p&gt;
&lt;p&gt;一般来说，只有支持了 RSS 的网页才可以获得 RSS，因此我们经常浏览的内容平台，如小红书、bilibili、X 等，一般来说都是不支持原生的 RSS 的。不过不难理解的是，假如有人人为爬取这些网页，并且将内容的更新变为 RSS 的格式，那么也可以使用 RSS 进行订阅，&lt;a href=&quot;https://docs.rsshub.app/&quot;&gt;RSSHub&lt;/a&gt; 就做了这件事情，比如说你可以使用 &lt;code&gt;https://rsshub.app/bilibili/user/video/508452265&lt;/code&gt; 来订阅我非常喜欢的博主，硅谷 101。当然，事实上因为 B 站反扒取的政策，这个链接 99% 的时候都是失效的，可以自己部署来解决。&lt;/p&gt;
&lt;h2&gt;使用 Folo&lt;/h2&gt;
&lt;p&gt;Folo 的使用非常简单，首先其具有开源的仓库，同时可以直接前往 &lt;a href=&quot;https://folo.is/&quot;&gt;官网&lt;/a&gt;，在 &lt;a href=&quot;https://app.folo.is/&quot;&gt;Web&lt;/a&gt; 上面使用或者使用 &lt;a href=&quot;https://github.com/RSSNext/Folo/releases&quot;&gt;软件端&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;登录之后界面大致如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Mgy9dWK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;也比较好理解，直接点击加号添加 RSS 订阅即可，比如说本站是 &lt;code&gt;https://axi404.top/rss.xml&lt;/code&gt;，直接输入添加即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/7H58hWK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;同时，Folo 专门支持 RSSHub 的链接，此时使用的链接可以支持 bilibili 的订阅，应该是 Folo 内部做了优化，选择 RSSHub，并且比如说输入 &lt;code&gt;rsshub://bilibili/user/video/508452265&lt;/code&gt; 即可。&lt;/p&gt;
&lt;h2&gt;部署 RSSHub&lt;/h2&gt;
&lt;p&gt;当然，想要在其他地方也订阅 RSSHub，那还是要进行一下自建，在这里假设读者已经拥有了服务器。&lt;/p&gt;
&lt;p&gt;按照 RSSHub 的文档的 &lt;a href=&quot;https://docs.rsshub.app/zh/deploy/#%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2&quot;&gt;手动部署章节&lt;/a&gt; 进行部署。&lt;/p&gt;
&lt;p&gt;首先安装相关的依赖：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Download and install nvm:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash

# in lieu of restarting the shell
\. &quot;$HOME/.nvm/nvm.sh&quot;

# Download and install Node.js:
nvm install 22

# Verify the Node.js version:
node -v # Should print &quot;v22.17.1&quot;.
nvm current # Should print &quot;v22.17.1&quot;.

# Verify npm version:
npm -v # Should print &quot;10.9.2&quot;.

npm install -g pnpm
npm install -g bun
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后 &lt;code&gt;git clone&lt;/code&gt; 并且部署即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/DIYgod/RSSHub.git
cd RSSHub
pnpm i
pnpm build
nohup pnpm start &gt; rsshub.log 2&gt;&amp;#x26;1 &amp;#x26;
disown %1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后就会启动在 &lt;code&gt;1200&lt;/code&gt; 端口上。&lt;/p&gt;
&lt;p&gt;此时在 Cloudflare 中配置 &lt;code&gt;A&lt;/code&gt; 记录，指向服务器 IP，名称如 &lt;code&gt;rsshub&lt;/code&gt;，即，对于笔者来说，可以希望可以通过 &lt;code&gt;https://rsshub.axi404.top&lt;/code&gt; 访问。&lt;/p&gt;
&lt;p&gt;然后配置反向代理，首先安装依赖：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt install -y nginx certbot python3-certbot-nginx
sudo tee /etc/nginx/sites-available/rsshub &amp;#x3C;&amp;#x3C;EOF
server {
    listen 80;
    server_name rsshub.axi404.top;

    location / {
        proxy_pass http://localhost:1200;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
EOF
sudo ln -s /etc/nginx/sites-available/rsshub /etc/nginx/sites-enabled/
sudo nginx -t &amp;#x26;&amp;#x26; sudo systemctl reload nginx
sudo certbot --nginx -d rsshub.axi404.top --non-interactive --agree-tos -m your@email.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后一行需要为你自己的域名以及邮箱。&lt;/p&gt;
&lt;p&gt;即可。&lt;/p&gt;
&lt;p&gt;同时，对于 B 站的 RSS，需要配置 Cookie，见 &lt;a href=&quot;https://docs.rsshub.app/zh/deploy/config#bilibili&quot;&gt;官方文档&lt;/a&gt;，即需要配置 &lt;code&gt;BILIBILI_COOKIE_12345678&lt;/code&gt; 的环境变量，值为你自己的 Cookie，通过文档中指引来找到自己的 Cookie。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;完结撒花&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-13-zh.webp"/><enclosure url="https://picr2.axi404.top/template-13-zh.webp"/></item><item><title>为 Astro 博客添加多部署站点</title><link>https://axi404.github.io/blog/astro-multi-pages</link><guid isPermaLink="true">https://axi404.github.io/blog/astro-multi-pages</guid><description>为 Astro 博客添加多部署站点</description><pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;最近还是在折腾博客，之前一直用的是 Vercel 部署，本身还是很优雅的，但是还是想要玩一些别的。具体来说，使用 Vercel 部署，我使用的就是 Vercel 的 Pages 功能，使用的自然是 Vercel 自己的服务器。同时，网络上几大其他的赛博菩萨，比如说 Cloudflare Pages 以及 GitHub Pages 等，都支持免费的 Pages 功能，多几个服务器来跑我的网页，而且不是同一个服务器，这样子可以避免单点故障。&lt;/p&gt;
&lt;p&gt;Astro 本身其实支持了不同的 adapter，也可以很便捷的部署到不同的站点，所以就简单配置了一下，然后记录一下这个过程。&lt;/p&gt;
&lt;h2&gt;编辑配置文件&lt;/h2&gt;
&lt;p&gt;首先，编辑 &lt;code&gt;astro.config.mjs&lt;/code&gt; 文件，添加不同的 adapter 配置。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-js&quot;&gt;const platform = process.env.DEPLOYMENT_PLATFORM || &apos;vercel&apos;
const isCloudflare = platform === &apos;cloudflare&apos;
const isGithubPages = platform === &apos;github&apos;

export default defineConfig({
  site: isGithubPages ? &apos;https://axi404.github.io/&apos; : (isCloudflare ? &apos;https://axi-blog.pages.dev/&apos; : &apos;https://axi404.top/&apos;),
  trailingSlash: &apos;never&apos;,
  adapter: isGithubPages ? undefined : (isCloudflare ? cloudflare() : vercel()),
  output: isGithubPages ? &apos;static&apos; : (isCloudflare ? &apos;static&apos; : &apos;server&apos;),
})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;整体的思路就是根据环境变量来判断当前的部署平台，然后根据不同的平台来选择不同的 adapter，然后默认的是使用 Vercel。在这里因为 Github 和 Cloudflare 都是在薅羊毛，没有进行氪金，所以说都是使用的静态站点，服务端渲染这种特性毕竟本来也不是必须得。&lt;/p&gt;
&lt;p&gt;关于如何使用 Vercel 部署站点在 &lt;a href=&quot;/blog/website-vercel&quot;&gt;之前的文章&lt;/a&gt; 中已经介绍过了，这里就不再赘述了。主要介绍剩下的两种的过程。&lt;/p&gt;
&lt;p&gt;Cloudflare Pages 的部署过程其实也有类似的博客，也就是我部署 &lt;a href=&quot;/blog/r2-image-host&quot;&gt;R2 图床的博客&lt;/a&gt; 的那一篇，本身也就是直接绑定之后部署就好，之后每一次 Push 之后就会自动部署。&lt;/p&gt;
&lt;p&gt;至于对于 Github Pages，其实就是需要写一个 Github Action 的文件，放到 &lt;code&gt;.github/workflows/deploy.yml&lt;/code&gt; 中，然后每一次 Push 之后就会通过运行这个 Action 来部署。这一步其实直接交给 GPT 来做就好了，我的一个特殊的点在于我博客的仓库是一个 private 仓库，并且不是 &lt;code&gt;axi404.github.io&lt;/code&gt; 的名称，因此需要在 Axi-Blog 的仓库中运行 Github Action，进行 build，并且将 build 的结果推送到 &lt;code&gt;axi404.github.io&lt;/code&gt; 的仓库中。&lt;/p&gt;
&lt;p&gt;简单介绍下这个方案，大概就是需要先创建一个 Token，打开 Github 的创建 Token 的 &lt;a href=&quot;https://github.com/settings/personal-access-tokens/new&quot;&gt;页面&lt;/a&gt;，然后在 Repository access 中选择 &lt;code&gt;axi404.github.io&lt;/code&gt; 的仓库，仓库权限中给 Content 的 read and write，之后在 &lt;code&gt;Axi-Blog&lt;/code&gt; 的仓库的 settings 中选择 Secrets and variables 中的 Actions 中添加一个 Secret，名字为 &lt;code&gt;PERSONAL_TOKEN&lt;/code&gt;，值为刚刚创建的 Token。&lt;/p&gt;
&lt;p&gt;文件内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yml&quot;&gt;name: Deploy to Axi404.github.io

on:
  push:
    branches:
      - main

permissions:
  contents: write

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout source
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.bun/install/cache
          key: ${{ runner.os }}-bun-${{ hashFiles(&apos;**/bun.lockb&apos;) }}
          restore-keys: |
            ${{ runner.os }}-bun-

      - name: Install dependencies
        run: bun i

      - name: Set environment variables
        run: |
          echo &quot;DEPLOYMENT_PLATFORM=github&quot; &gt;&gt; $GITHUB_ENV

      - name: Build site
        run: bun run build:github

      - name: Deploy to Axi404.github.io
        uses: peaceiris/actions-gh-pages@v3
        with:
          personal_token: ${{ secrets.PERSONAL_TOKEN }}
          external_repository: Axi404/axi404.github.io
          publish_branch: main
          publish_dir: ./dist
          force_orphan: true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即可。&lt;/p&gt;
&lt;h2&gt;服务器部署&lt;/h2&gt;
&lt;p&gt;因为折腾了服务器，所以说也搞了个服务器部署，包括设置 Hook，以及设置反向代理。&lt;/p&gt;
&lt;p&gt;首先基础配置 Git 跳过，之后安装 nginx 并且开放防火墙：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt install -y nginx certbot python3-certbot-nginx
sudo ufw allow 80
sudo ufw allow 443
sudo ufw reload
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后 &lt;code&gt;git clone&lt;/code&gt; 你的仓库，并且安装相关依赖。&lt;/p&gt;
&lt;p&gt;之后是关键，首先配置反向代理：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-nginx&quot;&gt;server {
    listen 80;
    server_name blog.axi404.top;

    root /var/www/blog;
    index index.html;

    location / {
        try_files $uri $uri/ /index.html;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 Cloudflare 添加一个 A，内容是 blog，指向服务器 IP，关闭代理。&lt;/p&gt;
&lt;p&gt;之后获得 https 证书：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo certbot --nginx -d blog.axi404.top --non-interactive --agree-tos -m your@email.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后配置 Hook，在服务器上创建一个文件 &lt;code&gt;deploy.sh&lt;/code&gt;，并且创建 log 文件夹，&lt;code&gt;mkdir -p /var/log/blog&lt;/code&gt;，脚本内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
LOG_DIR=&quot;/var/log/blog&quot;
LOG_FILE=&quot;$LOG_DIR/deploy_$(date &apos;+%Y-%m-%d_%H-%M-%S&apos;).log&quot;

echo &quot;$(date &apos;+%Y-%m-%d %H:%M:%S&apos;) - deploy.sh started&quot; | tee -a &quot;$LOG_FILE&quot;

cd /root/Axi-Blog || {
  echo &quot;Failed to cd to /root/Axi-Blog&quot; | tee -a &quot;$LOG_FILE&quot;
  exit 1
}

echo &quot;git pull origin main ...&quot; | tee -a &quot;$LOG_FILE&quot;
git pull origin main &gt;&gt; &quot;$LOG_FILE&quot; 2&gt;&amp;#x26;1

echo &quot;Start build (try 1) ...&quot; | tee -a &quot;$LOG_FILE&quot;
if ! bun run build &gt;&gt; &quot;$LOG_FILE&quot; 2&gt;&amp;#x26;1; then
  echo &quot;Build failed, retrying (try 2) ...&quot; | tee -a &quot;$LOG_FILE&quot;
  if ! bun run build &gt;&gt; &quot;$LOG_FILE&quot; 2&gt;&amp;#x26;1; then
    echo &quot;Build failed twice, aborting.&quot; | tee -a &quot;$LOG_FILE&quot;
    exit 1
  fi
fi

echo &quot;Copying files ...&quot; | tee -a &quot;$LOG_FILE&quot;
rm -rf /var/www/blog/* &gt;&gt; &quot;$LOG_FILE&quot; 2&gt;&amp;#x26;1
cp -r dist/* /var/www/blog/ &gt;&gt; &quot;$LOG_FILE&quot; 2&gt;&amp;#x26;1
chown -R www-data:www-data /var/www/blog &gt;&gt; &quot;$LOG_FILE&quot; 2&gt;&amp;#x26;1

echo &quot;$(date &apos;+%Y-%m-%d %H:%M:%S&apos;) - deploy.sh finished&quot; | tee -a &quot;$LOG_FILE&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后写一个 hook，效果是每次 push 之后，只要 fetch，就执行部署。&lt;/p&gt;
&lt;p&gt;首先创建：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir ~/deploy-hook
cd ~/deploy-hook
npm init -y
npm install express
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后创建 Hook：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-js&quot;&gt;// hook.js
const express = require(&apos;express&apos;);
const { exec } = require(&apos;child_process&apos;);
const app = express();

app.post(&apos;/hook&apos;, (req, res) =&gt; {
    res.send(&apos;Deploying...&apos;);

    exec(&apos;bash /root/deploy.sh&apos;, (err, stdout, stderr) =&gt; {
        if (err) {
            console.error(&apos;❌ Error:&apos;, stderr);
        } else {
            console.log(&apos;✅ Output:\n&apos;, stdout);
        }
    });
});

app.listen(XXXX, () =&gt; {
    console.log(&apos;🚀 Webhook listening on http://localhost:XXXX/hook&apos;);
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将上述 &lt;code&gt;XXXX&lt;/code&gt; 替换为你的端口，然后运行 &lt;code&gt;node hook.js &amp;#x26;&lt;/code&gt; 即可。别忘了开放这个端口的防火墙。&lt;/p&gt;
&lt;p&gt;接下来在 github 的 workflows 里面加一行，&lt;code&gt;curl -X POST http://XXX.XXX.XXX.XXX:XXXX/hook&lt;/code&gt;，即可。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;内容反正大概就是这样，也不是很难，最后喜提两个新域名，一个是 &lt;a href=&quot;https://axi-blog.pages.dev&quot;&gt;axi-blog.pages.dev&lt;/a&gt;，一个是 &lt;a href=&quot;https://axi404.github.io&quot;&gt;axi404.github.io&lt;/a&gt;，前者是 Cloudflare Pages 的，后者是 Github Pages 的。&lt;/p&gt;
&lt;p&gt;最后加了一个服务器，可以用 &lt;a href=&quot;https://blog.axi404.top&quot;&gt;blog.axi404.top&lt;/a&gt; 访问。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/template-17-zh.webp"/><enclosure url="https://picr2.axi404.top/template-17-zh.webp"/></item><item><title>一些常用的指令</title><link>https://axi404.github.io/blog/common-commands</link><guid isPermaLink="true">https://axi404.github.io/blog/common-commands</guid><description>因为一些指令比较常见，所以写一篇博客记录一下，以在将来方便同步。</description><pubDate>Sat, 01 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;ZSH 配置&lt;/h2&gt;
&lt;p&gt;首先可以安装 zsh 和 oh-my-zsh&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt install zsh
sh -c &quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;为了兼容主题，接下来安装字体&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget https://github.com/ryanoasis/nerd-fonts/releases/download/v3.3.0/JetBrainsMono.zip -O JetBrainsMono.zip
unzip JetBrainsMono.zip -d JetBrainsMono
mkdir -p ~/.local/share/fonts/JetBrainsMono
cp JetBrainsMono/*.ttf ~/.local/share/fonts/JetBrainsMono/
fc-cache -fv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完插件之后就可以修改命令行中的字体选项了，找到安装的字体并设定。&lt;/p&gt;
&lt;p&gt;接下来安装插件：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions
git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting
git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot;
plugins=(z git zsh-autosuggestions zsh-syntax-highlighting)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结束。&lt;/p&gt;
&lt;h2&gt;Docker 初始化&lt;/h2&gt;
&lt;p&gt;首先先安装 docker&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -fsSL get.docker.com -o get-docker.sh
sudo sh get-docker.sh --mirror Aliyun
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成之后，可以初始化 docker 服务并且添加用户组：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo systemctl enable docker
sudo systemctl start docker
sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结束。&lt;/p&gt;
&lt;h2&gt;Windows 安装 Torch&lt;/h2&gt;
&lt;p&gt;直接使用 pip 安装即可，需要指定 CUDA 版本：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结束。&lt;/p&gt;
&lt;h2&gt;Miniconda 安装&lt;/h2&gt;
&lt;p&gt;参考 &lt;a href=&quot;https://www.anaconda.com/docs/getting-started/miniconda/install&quot;&gt;Miniconda 文档&lt;/a&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后激活环境并且进行初始化：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;source ~/miniconda3/bin/activate
conda init --all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结束。&lt;/p&gt;
&lt;h2&gt;安装 Starship&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://starship.rs/&quot;&gt;Starship&lt;/a&gt; 是一个命令行美化工具，提供了多种预设。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-zsh&quot;&gt;curl -sS https://starship.rs/install.sh | sh
echo &apos;eval &quot;$(starship init zsh)&quot;&apos; &gt;&gt; .zshrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后重启终端即可。&lt;/p&gt;
&lt;p&gt;一个我喜欢的预设为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-zsh&quot;&gt;starship preset gruvbox-rainbow -o ~/.config/starship.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;安装 NodeJS&lt;/h2&gt;
&lt;p&gt;参考 &lt;a href=&quot;https://nodejs.org/en/download&quot;&gt;官网指令&lt;/a&gt;，对于 Ubuntu/Linux 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Download and install nvm:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash

# in lieu of restarting the shell
\. &quot;$HOME/.nvm/nvm.sh&quot;

# Download and install Node.js:
nvm install 22

# Verify the Node.js version:
node -v # Should print &quot;v22.17.1&quot;.
nvm current # Should print &quot;v22.17.1&quot;.

# Verify npm version:
npm -v # Should print &quot;10.9.2&quot;.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后可以安装 &lt;code&gt;pnpm&lt;/code&gt; 和 &lt;code&gt;bun&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm install -g pnpm
npm install -g bun
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;配置 Python 清华源&lt;/h2&gt;
&lt;p&gt;直接执行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt;</content:encoded><h:img src="https://picr2.axi404.top/common-commands-zh.webp"/><enclosure url="https://picr2.axi404.top/common-commands-zh.webp"/></item><item><title>周记 Week55</title><link>https://axi404.github.io/blog/week-55</link><guid isPermaLink="true">https://axi404.github.io/blog/week-55</guid><description>2025-07-14 ~ 2025-07-20.</description><pubDate>Fri, 25 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一次周记也不算很久之后吧，甚至可以说是越来越早了，某种程度上也是自己逐渐闲下来的一个侧面的体现，不过虽然说每天的负载还是纯粹的拉满，只是相对好了一些。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;首先依然是科研，这部分基本上还是一心扑在了 IROS Benchmark 上面，这部分里面包括了对于之前造好的 RoboTiq 场景的进一步修缮，并且提供数据给负责模型的同学进行训练，同时还有接入最终的机械臂，也就是 Piper on Aloha Split。&lt;/p&gt;
&lt;p&gt;从 Sapien 切换回到 Isaac，紧接着传来了一个不知道算好还是算坏的消息。因为同学在 Sapien 上进行了一定量的测试，结果在一个上面获得了百分百的成功率，从复现上来说自然是好事。我们的任务需要模型具有同时理解多个任务，甚至泛化的能力，在有的测试场景中，用有限的数据可以达到较高的成功率，一方面意味着我们的训练数据不存在显著的问题，另一方面也意味着模型可以进一步的在更难的挑战中继续优化。&lt;/p&gt;
&lt;p&gt;不过坏事也就在于说我们打算切换回到 Isaac，最后的结果也并不是特别理想，导致陷入了一种复现漩涡，在某个环境下的结果，迁移到另一个环境之后并不作效，那么可能整个团队就会围绕复现这个效果而浪费大量的时间，毕竟假如无法探究性能损失的原因，长久来看肯定是一个隐患。&lt;/p&gt;
&lt;p&gt;当然是事实上对于具身智能这个领域，大量的模型对数据敏感，并且本身在谈到预训练的时候，尽管大多数模型都包含一定的初始权重，但是并没有在动作空间上进行大规模的训练，因此这种训练是一个包含大量技巧与巧合的流程，而并非如在已有的大语言模型上面进行微调一样，在大多数情况下是稳定的。所谓是否会调一个 VLA 模型，有的时候就已经是一种技巧，看出数据或者模型训练时的问题，并且进行细微调整，作用在结果上可能是天差地别的，这就是没有本质上的大量预训练导致的。&lt;/p&gt;
&lt;p&gt;毕竟目前的数据多样性其实也并不是很大，有的时候对于某一本体的某几个动作，模型已经充分掌握，但对于场景的视觉特征的学习却也已经使模型开始退化，很难说将一个这样的模型迁移到自己布置的场景下是否是一种合理的利用预训练的方式。综上所述，无论如何，我们在这上面又要开始进行大量的调试了。&lt;/p&gt;
&lt;p&gt;与此同时，基本上可以理解为，当场景资产已经准备就绪，并且确保了正常数据和测试没有问题，在当前的流水线中就一部分内容就被交付给了后续的同学，自然我也就开始接入最终的机械臂，也就是松灵的 Aloha。&lt;/p&gt;
&lt;p&gt;上周已经吐槽过 Piper 的动作空间，在这里也就不再赘述了，不过相关的看法还比较有意思，可以再说一遍。事实上对于双臂任务来说，就像之前曾经讲过的一样，我在短期研究中可能更加看重模型的能力，而不是执行某个特定的 primitive skill 的成功率。&lt;/p&gt;
&lt;p&gt;在单臂任务中，通常使用的机械臂如 Franka，为模型提供了庞大的动作空间，因此在任务的设计上可能要更加偏向于如何让模型在这不太受到约束的动作空间中进行展现自己的泛化能力。&lt;/p&gt;
&lt;p&gt;假如我们将 VLA 的能力从 VLM 延伸过来，为了证实 VLM 的能力可以成功迁移，一种理想的任务是 pick and place。模型不需要学到太多的动作能力或者动作特征，但是却需要具备这样的基本能力，同时我们几乎可以认为 VLA 需要继承的就是来自于 VLM 的 grounding 能力，一旦这种继承可以充分的体现，距离一个真正可以 zero shot pick anything to anyplace 的模型也就不远了，还有希望在这些模型中看到一些推理的可能性。&lt;/p&gt;
&lt;p&gt;那么话题回到任务的设计中，我们其实只需要设计尽可能的刁钻的问题，比如更加丰富的背景扰动以及干扰物体的增加，或者对于物品描述的复杂化，比如说将「可口可乐」描述为「偶然间从止咳糖浆演化而来的美国汽水」或者「包装主题色为红色的黑色汽水」。&lt;/p&gt;
&lt;p&gt;对于单臂任务来说，这些内容可以被这样设计得益于机械臂庞大的动作空间，而我们想要验证的也只是 VLM 到 VLA 的能力迁移，但是反观双臂，变化却出现了。&lt;/p&gt;
&lt;p&gt;事实上，出于便携性以及一些设计的考量，不难发现大多数的双臂机器人的双臂各自的动作空间会明显小于单臂，这种动作空间变小带来的约束使得另一设计成为可能。&lt;/p&gt;
&lt;p&gt;当我们在谈论具身智能的时候，除了想让一个可以输出动作的模型理解场景和指令之外，自然也想让他了解有关空间以及机器人本体的信息，这是很有意思的。在实际中自然也可以让机器人去理解什么叫做它能碰到的物体或者无法碰到的物体，但是对于 Benchmark 设计来说，一个前提是，「任务必须是可完成的」，同时只有一个夹爪和一个机械臂，这意味着当设计出「给我拿一瓶唯一我能碰到的饮料」这样的任务时，本质上会被退化为给我拿一瓶离我最近的饮料，或者说在画面上最大的饮料（假如饮料的尺寸相似），而这本质上依然是简单的 grounding 任务，毕竟还是那个前提，任务一定是可以完成的。&lt;/p&gt;
&lt;p&gt;但是当视角放到双臂之后，事情变得有意思起来了，事实上某些任务可以用左臂或者右臂完成，但是另外一些任务只能用某一个机械臂，这迫使模型需要更加理解自己所处于的本体的动作空间这一概念。尽管这一设计依然可以被退化为，首先选择你的目标物体，接下来选择距离你目标物体最近的机械臂，让它动起来去抓取，但是伴随着抉择的变多，模型需要对距离、空间以及机械臂的存在的越发的敏锐。&lt;/p&gt;
&lt;p&gt;另外一个有趣的任务设计是，有一个任务是把某个物体放到一个托盘上，托盘仅位于左臂的动作空间中，物体同时位于左右臂的动作空间，但是离右臂更近，这个时候完成任务存在两种可能性。第一种是遵循上文的规则，找到抓取的物体后，选择离他更近的右臂，但这也意味着想要将物体放到托盘中，需要把物体从右臂转移至左臂，也就是进行一个 handover 操作，而这一操作具有一定的难度，从目前来看，也意味着一定的失败率。另一选择则违背了上述原则，直接使用左臂进行抓取，这样自然带来了更高的成功率，但也意味着模型需要跳出某种固定的思维方式，而开始真正的理解机械臂之间的配合关系和动作空间之间的相互制约和整合。&lt;/p&gt;
&lt;p&gt;当然，事实上现在我们的 Benchmark 中并没有太过于刻意的相关设计，但是确实说起来也比较有趣，因此聊一聊，不过本身对于让模型抉择用哪只手，这个设计确实广泛存在，而现有的大多数双臂 Benchmark，由于并不存在诸如大量的候选物体或者经过更好调整的动作空间，因此本质上依然是把双臂当作两个单臂来使用，最多加入一个双手拿起一个物体这样的事情，但是无论如何也并不本质。&lt;/p&gt;
&lt;p&gt;这里值得一提的是，即使试图在背景中加入干扰物体，干扰物体的加入也有讲究。假如说加入的干扰物体位于模型的动作空间之外，尽管本质上存在这样一个干扰选项，但是在模型推理时因为无法触及那个区域，在进行部分训练之后，自然也就不太会选择过远的物体，这使得干扰物体本质上和将桌面贴图以及环境光照进行随机化没有本质的区别。而在动作空间中加入可以交互的干扰物体，则对于动作生成提出了挑战。&lt;/p&gt;
&lt;p&gt;这周总的来说就是这样，还有一些对于 Isaac Sim 的细节理解，后续会在专门的博客中说一下。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活依然是照常生活，似乎也没有什么好说的，平时和同学们一起吃饭，然后晚上下班之后回到公寓，躺在床上写一会儿代码，逛一逛 Github，心有余力也会刷一刷论文，再和我最爱的乐小姐电话聊上一会，大概就是这样的生活。&lt;/p&gt;
&lt;p&gt;下周因为分散实习要结束了，所以还要回学校进行答辩，也就顺便再回一趟北京去拔个智齿，顺便陪陪家里人。想到差旅费，又是让人头疼，但也没有办法，继续生活吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-55-zh.webp"/><enclosure url="https://picr2.axi404.top/week-55-zh.webp"/></item><item><title>周记 Week54</title><link>https://axi404.github.io/blog/week-54</link><guid isPermaLink="true">https://axi404.github.io/blog/week-54</guid><description>2025-07-07 ~ 2025-07-13.</description><pubDate>Sat, 19 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说又是很久之后的一次周记了，其实本来这周有很多次可以稍微记录一下上周发生的事情，不过好在，无论如何这次还是在周内记录下来了，没有再一次到下一周去，所以多少还算是及格吧。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;上回书说到我找到了 Sapien 如何切换回到 Isaac Sim 的一种方式，也就是解决了关键的 Gripper 分离问题，即在开启了 Physics Scene 中开启了 &lt;code&gt;enableCCD&lt;/code&gt; 之后，对于被抓取的物体（需要是 Rigid Prim）也开启了 CCD 之后，在穿模的情况下就有很大的概率出现 Gripper 的分离问题。不过既然解决了这个问题，就需要回到开启 CCD 的初衷，也就是为什么我们需要 CCD，本来就是为了解决物体碰撞的不稳定问题，也就是容易穿模之后弹飞。这部分在这周迭代出来了一系列的参数，同时还有一些周边的参数，在调试过程中尝试过，并且摸清楚了真正在仿真中的效果，但是在最后的参数组合中没有被应用，这些全部的内容不出意外的话都会被记录在下一次的 Isaac 101 中，虽然说下一次应该是在八月份更新（毕竟七月份的一系列的交付节点实在是让人应接不暇）。&lt;/p&gt;
&lt;p&gt;调节物理参数是一件非常麻烦的事情，所以也花费了一些时间，做了很多的尝试。同时之前在内部分支代号为 &lt;code&gt;feature/sapien&lt;/code&gt; 的分支上新添加的一系列的 features 迁回了 debug 的新分支，并且将之前的 IROS Bench 的全部 Configs 迁移回到了 Isaac Sim 的框架中。虽然说开始的时候的迁移比较浪费时间，但是实际上在开始了后续的开发中，Isaac Sim 的开发流程在一些方面就简单了很多了。虽然本身编程难度上远高于 Sapien，但是从编辑场景这一点中，由于 Isaac Sim 提供了强大的 GUI，因此可以非常快捷地修改场景。目前一共提供了十个 Tasks，都已经可以批量生成数据，实际上一晚上可以生成数万条，但是为了节约参赛者的训练模型迭代时间，我们应该会找到一个恰当的数量，不然给了太多数据也不一定是好事。&lt;/p&gt;
&lt;p&gt;目前整体的流程是在 Franka with RoboTiq hand 上面进行了验证，后续接入 Aloha piper 又是大量的工作量，因为 piper 过于小的 action space，全部的桌面布局可能都要调整。举例来说，读者不难发现，在目前已有的全部的双臂任务中，很少有包含多个 container 的任务，也就是选择把某个物体放到多个 container 中的某一个中。因为 container 需要有一定的尺寸，而 action space 里面放不下那么多的东西。&lt;/p&gt;
&lt;p&gt;这周还解决了一些其他的琐碎的内容，但是主要就是在做这些事情。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;说到生活，这周我的弟弟来到上海来玩，主要就是去参加 BW。我一是因为工作繁忙，二也是因为没抢到票，反正是没去。不过周末的时候，还是疯狂打工省出了一天的时间，陪他去逛了一下 ZX 创趣场，大概是上海的二次元商场。不过事实上是，在我看来西安和上海之间还是差了不少，但是西安的银泰城也没有比 ZX 小那么多。不过确实是非常的热闹，而且很多店铺都是比较正规的，因此还是逛的十分开心的。有的时候感觉逛这种谷子店，一是人要多，人挤人才能走得慢，才能仔细看这些商品；二是要和别人一起来，才能讨论起来。毕竟说到底，为啥不去逛淘宝呢，主要还是一个氛围感。&lt;/p&gt;
&lt;p&gt;最后去死亡搁浅的快闪店买了一点周边，感觉还不错。&lt;/p&gt;
&lt;p&gt;总的来说这周大概就是这样，下周的主线应该还是 IROS Benchmark，继续努力吧~&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-54-zh.webp"/><enclosure url="https://picr2.axi404.top/week-54-zh.webp"/></item><item><title>周记 Week53</title><link>https://axi404.github.io/blog/week-53</link><guid isPermaLink="true">https://axi404.github.io/blog/week-53</guid><description>2025-06-30 ~ 2025-07-06.</description><pubDate>Sun, 13 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说这其实又是很久之后的一次周期，甚至说这一周发生了什么我都不太记得清了，但隐约还有一些回忆，所以大概说一说。总的来说，这周就是基本在上海呆着了，而且也没有发生很多的事情，就是在做按部就班的科研，以及在平时休息的时候写一些博客。&lt;/p&gt;
&lt;h2&gt;工作与科研&lt;/h2&gt;
&lt;p&gt;目前的作息大概来说还算正常，不过也很难说，因为事实上有点像是作息绕了一圈又绕回来了。目前是八九点去上班之后，之后上班一直到晚上七八点的时候就会有些困了，这个时候坚持把一些工作做完，或者去找一个睡眠舱休息一会儿，大概工作到晚上 12:00 左右就回去酒店休息。基本上现在感觉是沾床就睡了，也不存在什么通宵到三四点。&lt;/p&gt;
&lt;p&gt;不过也是实在还是放不下自己的博客以及这些可以捣鼓的技术，所以通常可能四五点就定了闹钟起床，然后继续搞这些研究。目前来说博客也是更新了不少的新功能，虽然大多数其实你可能注意不到。包括一些对于 Bug 的修复。&lt;/p&gt;
&lt;p&gt;对于科研来说这一周主要还是在进行 IROS 的 Benchmark 的搭建，也就是整个我们中心打算出的一个 challenge，然后我们这边承接了 Manipulation Benchmark 这一部分。说是我们，其实仿真框架以及场景这部分几乎只有我及另一位同学，我负责几乎仿真的绝大多数，包括数据生成以及场景，他负责上下游的一些整合，在之前帮忙搭了一些场景并且帮忙训模型。不过这位同学感觉还是和我合作起来相当顺畅，希望可以长期继续合作下去。&lt;/p&gt;
&lt;p&gt;现在接下来的 workload 可能就在于我们的 challenge 和松灵机器人有合作，也就是他们那边会为我们的线下比赛提供一些机器，自然我们的仿真场景中也要加入他们的机器人，也就是双臂机器人。事实上对于双臂操作这件事情虽然说并不难做，但是我们的框架也一直没有支持，感觉主要是因为我现在并没有关注这些细节，比如说加入双臂或者灵巧手或者触觉。&lt;/p&gt;
&lt;p&gt;之前和这边一位非常非常厉害的学长聊天，我提出了一个问题，「你是想要一个模型在全部的任务上都可以有 10% 的成功率，还是一个模型可以在单独一个任务上有 100% 的成功率？」我可能选择全部都是 10%。在包括之后和别人的讨论中，大家其实都赞同一个观点 ，也就是目前全部具身智能产品的模型或者应用都会在最后加入后训练过程，也就是在某个特定的任务下继续去加入大量的数据训练模型。那么换句话说只要有一个稍微经过预训练可能学到了动作表征这些事情的模型，并且在后续的时候在学习任务本身，这样其实对于整个领域就已经足够了。&lt;/p&gt;
&lt;p&gt;因此我们不难看到一个相对反直觉的事情，在大模型或者其他领域中对于泛化这件事情往往是大厂去做的，而实验室或者课题组主要则关注于那些更加细粒度的任务下。而在具身这边恰好相反，因为全部的落地应用都需要后训练，因此如何构建一个泛化的模型这件事情实际上并没有那么大的收益。&lt;/p&gt;
&lt;p&gt;回到框架，因此我对于诸如双臂或者灵巧手或者触觉这些内容并没有那么感兴趣，而是认为我的框架在第一时间应该尽可能向通用抓取或者生成更加多样化的数据这一点去靠拢，并且切实地提供这么多的数据，而这件事情实际上没有那么简单。假如读者看我之前的周记就会发现大量的时间其实被浪费在了处理仿真框架的物理问题上，而这件事情基本只在我这里出现的原因，自然是我引入了更加多样但相应的也更脏的数据。当然在这周的最后，我也发现了之前导致夹爪分离这个现象的关键问题，也是我为什么要换到 sapien 进行双端渲染的原因，因此下周也就要陆续切换回到纯 Isaac 了。&lt;/p&gt;
&lt;h2&gt;其他&lt;/h2&gt;
&lt;p&gt;这些内容后续应该会单独写一个模客来说，或者换句话说我的 Isaac 101 的博客之所以卡住了这么久，就是因为下一章节要讲解更多的物理内容，但是实际上物理相关的问题已经将我困扰住了。不过这个问题解决之后就会有更多的话题可以探讨，并且教给更多的人。&lt;/p&gt;
&lt;p&gt;尽管我其实并没有从开源社区中获得太多的收获，但是我依然热衷于帮助别人，希望最后能有一个属于自己的圈子或者群体，大家可以一起做这些有意义的事情。另外就是这周我把给新生的话这个博客以及再次把西安交大生存指南 PR 给了年级群以及新生群，后续可能我也会用了一部分的时间在西交的新生群里面水群，看看能不能拉到一些比较有潜力的学弟学妹。不过事实上，做这些事情一部分是想要帮助他人，另一部分也算是一种不甘。虽然说现在我的科研以及进步已经相当可观，但是事实上还是走了非常多的弯路，并且浪费了不少的时间，那么假如把这些正确的道理都提前告诉一个人，大学的三年或者四年时间能发展到什么程度，我还是比较期待的。不过目前的观察来看，我目前下面两级的学弟学妹貌似还没有到达我目前的程度，而是可能大多数还处于比较初级的阶段。&lt;/p&gt;
&lt;p&gt;过两天应该会继续写下一周的内容，希望不会隔太远，因为事实上你会在下一周的周期中看到一个巨大的工作量。不过事后来说，因为写作这篇内容的时候已经是下下周了，这件事情也已经被处理掉了，剩下的大概也就只是一点点零星的小事了吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-53-zh.webp"/><enclosure url="https://picr2.axi404.top/week-53-zh.webp"/></item><item><title>博客上手指南</title><link>https://axi404.github.io/blog/website-vercel</link><guid isPermaLink="true">https://axi404.github.io/blog/website-vercel</guid><description>一个简单的博客部署方案</description><pubDate>Tue, 15 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Aside } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在这个数字时代，我们习惯于在社交媒体上分享生活，在简历上罗列技能。但这些平台终究是别人的空间，充满了格式化的限制和信息的喧嚣。你是否想过，拥有一个完全由你掌控的、独特的线上空间？&lt;/p&gt;
&lt;p&gt;很多人可能会觉得，搭建一个看起来很炫酷的网站是件非常复杂和昂贵的事情，需要学习 HTML/CSS/JS 全家桶，还要懂后端、数据库和服务器运维。但事实是，&lt;strong&gt;借助现代化的工具，这一切比你想象的要简单得多，甚至可以完全免费&lt;/strong&gt;。本教程将作为一个完整的指南，带你从零开始，一步步搭建并部署一个高性能、易于维护的个人网站。即使你没有任何编程基础，只要跟着步骤走，也能拥有一个属于自己的主页。&lt;/p&gt;
&lt;h2&gt;Part 1: 现代网站搭建的基本逻辑&lt;/h2&gt;
&lt;p&gt;在动手之前，我们先花几分钟了解一些核心概念，这会让你在后续操作中更加得心应手。&lt;/p&gt;
&lt;h3&gt;什么是静态网站？&lt;/h3&gt;
&lt;p&gt;你可能听过前端、后端、数据库这些词。简单来说，后端和数据库负责处理动态的、需要实时交互的数据（比如用户登录、评论提交）。而很多个人主页和博客的核心内容——文章、图片、个人介绍——并不会频繁变动。&lt;/p&gt;
&lt;p&gt;静态网站（Static Website）的核心思想就是，把这些内容提前生成为标准的 &lt;strong&gt;HTML、CSS 和 JavaScript&lt;/strong&gt; 文件。当用户访问时，服务器直接把这些现成的文件发送给用户的浏览器，无需任何服务器端的动态计算。&lt;/p&gt;
&lt;h3&gt;为什么选择静态网站搭建个人主页？&lt;/h3&gt;
&lt;p&gt;对于个人项目，静态网站是近乎完美的选择，因为它：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;极致的速度&lt;/strong&gt;：用户直接下载成品文件，没有服务器处理和数据库查询的等待，加载速度飞快。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更高的安全性&lt;/strong&gt;：没有后端和数据库，大大减少了被攻击的风险。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;极低的成本&lt;/strong&gt;：静态文件托管的费用非常低，甚至有大量优秀的平台提供&lt;strong&gt;免费&lt;/strong&gt;托管服务（比如我们后面要用的 Vercel）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;简单的部署&lt;/strong&gt;：你只需要把生成好的文件上传到任何一个能放文件的地方，网站就能访问了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的诸如 Github Pages 或者 Vercel 等平台，都是支持部署静态网页的，因此我们只需要将网页的构建产物上传到这些平台，就可以实现网页的部署。&lt;/p&gt;
&lt;h3&gt;拥抱现代前端框架&lt;/h3&gt;
&lt;p&gt;现代前端框架（如 &lt;strong&gt;Astro&lt;/strong&gt;, &lt;strong&gt;Vue&lt;/strong&gt;, &lt;strong&gt;React&lt;/strong&gt;）允许开发者用更高效、更模块化的方式来组织代码（比如写 Markdown 文章，或者用组件拼装页面）。虽然你在开发时写的是 &lt;code&gt;.vue&lt;/code&gt;、&lt;code&gt;.jsx&lt;/code&gt;、&lt;code&gt;.astro&lt;/code&gt; 等文件，但这些框架最终会通过构建工具（如 &lt;strong&gt;Vite&lt;/strong&gt;、&lt;strong&gt;Webpack&lt;/strong&gt;、&lt;strong&gt;Rollup&lt;/strong&gt; 等）将你的代码转换为浏览器可以识别的 &lt;strong&gt;HTML、CSS 和 JavaScript&lt;/strong&gt; 文件。这个构建过程就像“翻译”一样，把开发者写的高层代码翻译成浏览器能“听懂”的语言。&lt;/p&gt;
&lt;p&gt;而更进一步，无数的开发者为了便利博客等网站的构建，在这些现代框架的基础上构建了模板。这意味着你无需从零开始，只需要找到一个你喜欢的&lt;strong&gt;模板&lt;/strong&gt;，在作者预设好的框架里，像填写个人信息、写文章一样去填充内容即可。模板已经帮你处理了 99% 的复杂技术细节。&lt;/p&gt;
&lt;h2&gt;Part 2: 动手！从零到一的建站实战&lt;/h2&gt;
&lt;p&gt;理论讲完了，让我们卷起袖子开始实战！整个过程分为七个步骤。&lt;/p&gt;
&lt;p&gt;顺着这些步骤，你可以搭建一个自己的网页，在本地进行编辑并且可以上传到云上，自动部署，同时绑定自己的域名（或者使用服务商提供的免费的域名）。&lt;/p&gt;
&lt;p&gt;对于完全的萌新来说，下面的步骤可能因为涉及了你没有接触的知识而显得有些晦涩。不用担心，先不要理解，而是跟着步骤走，先搭建起来，等到步骤三之后，你已经可以获得一个可以访问的网站了，收获一些正反馈，再沉下心继续走。&lt;/p&gt;
&lt;h3&gt;步骤一：准备本地开发环境 (Node.js)&lt;/h3&gt;
&lt;p&gt;要运行现代前端模板，你的电脑需要一个叫做 &lt;strong&gt;Node.js&lt;/strong&gt; 的环境，它是整个 JavaScript 工具链的基础。虽然其实在本地直接写文本并且上传也不是一种办法，但是假如想要实时获得你修改后的内容的反馈，还是需要一个本地环境。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;安装 Node.js&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: 前往 &lt;a href=&quot;https://nodejs.org/zh-cn/download/prebuilt-installer&quot;&gt;Node.js 官网&lt;/a&gt;，下载预构建的 &lt;code&gt;.msi&lt;/code&gt; 安装程序，像安装普通软件一样双击运行并完成安装即可。
&lt;img src=&quot;https://picr2.axi404.top/image.3rbi7ru3lc.webp&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;:
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt install -y nodejs npm
# 推荐使用 n 来管理 Node.js 版本，可以轻松切换到最新稳定版
sudo npm install -g n
sudo n stable
hash -r # 刷新 shell 缓存，让新版本生效
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;验证安装并安装 pnpm&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;打开你的终端（Windows 用户可以使用 PowerShell 或 CMD），输入以下命令检查是否安装成功：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;node -v
npm -v
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果能看到版本号，说明 Node.js 和它的包管理器 &lt;code&gt;npm&lt;/code&gt; 已经就绪。我们推荐使用一个更现代、更快速的包管理器 &lt;code&gt;pnpm&lt;/code&gt;。同时，对于一些框架，也推荐使用 &lt;code&gt;bun&lt;/code&gt; 来安装依赖。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;npm install -g pnpm
npm install -g bun
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;步骤二：选择并下载一个模板&lt;/h3&gt;
&lt;p&gt;你不需要从零开始设计。网上有海量的优秀开源模板。你可以从我的 &lt;a href=&quot;https://github.com/stars/Axi404/lists/theme&quot;&gt;GitHub Stars 收藏列表&lt;/a&gt; 中挑选一个你喜欢的，或者在 GitHub 上搜索“astro theme”, “vue portfolio”等关键词。&lt;/p&gt;
&lt;p&gt;找到喜欢的模板之后可以直接 Fork 到你的 Github 账号中，之后在本地克隆到你的电脑上并且进行编辑。（关于 Git 的教程以及 Github 的教程将来会单独写 Blog，暂时本博客面向有 Git 以及 Github 基础的读者）。&lt;/p&gt;
&lt;h3&gt;步骤三：在本地运行你的网站&lt;/h3&gt;
&lt;p&gt;现在，你的网站代码已经在你的电脑里了。让我们用几个简单的命令让它“活”起来。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;进入项目目录&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在终端里，使用 &lt;code&gt;cd&lt;/code&gt; 命令进入你刚刚下载的模板文件夹。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd path/to/your/project
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;安装依赖 (&lt;code&gt;install&lt;/code&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个项目都依赖一些第三方库来工作。项目根目录下的 &lt;code&gt;package.json&lt;/code&gt; 文件定义了所有必需的依赖。这个过程和 Python 用户熟悉的 &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; 完全一样。&lt;/p&gt;
&lt;p&gt;如果项目推荐使用 &lt;code&gt;pnpm&lt;/code&gt; (根目录有 &lt;code&gt;pnpm-lock.yaml&lt;/code&gt; 文件)，执行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pnpm install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果项目推荐使用 &lt;code&gt;bun&lt;/code&gt; (根目录有 &lt;code&gt;bun.lockb&lt;/code&gt; 文件)，执行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bun install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果都没有，使用 &lt;code&gt;npm&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个命令会自动下载所有依赖项到 &lt;code&gt;node_modules&lt;/code&gt; 文件夹中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;启动开发服务器 (&lt;code&gt;dev&lt;/code&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;依赖安装好后，执行以下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pnpm dev  # 或者 npm run dev, bun dev
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;终端会显示一个本地网址，通常是 &lt;code&gt;http://localhost:4321&lt;/code&gt; 或类似的地址。在浏览器中打开它，恭喜你，你的网站已经在本地成功运行了！&lt;/p&gt;
&lt;p&gt;更棒的是，它支持&lt;strong&gt;热更新（HMR）&lt;/strong&gt;。现在你修改任何源文件（比如一篇文章），浏览器里的页面都会自动刷新，实时展示你的改动。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;步骤四：个性化你的内容&lt;/h3&gt;
&lt;p&gt;本地网站跑起来了，现在是把它变成你自己的东西的时候了。作为模板使用者，你的工作非常简单，主要集中在两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;修改配置文件&lt;/strong&gt;：在项目根目录找到 &lt;code&gt;astro.config.mjs&lt;/code&gt;, &lt;code&gt;_config.yml&lt;/code&gt; 或类似名字的配置文件。打开它，把里面的网站标题、作者名、社交链接等改成你自己的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;管理内容文件&lt;/strong&gt;：对于博客，通常会有一个 &lt;code&gt;src/content/blog/&lt;/code&gt; 或 &lt;code&gt;posts/&lt;/code&gt; 目录。你只需要在这个目录里添加、修改或删除 Markdown (&lt;code&gt;.md&lt;/code&gt; 或 &lt;code&gt;.mdx&lt;/code&gt;) 文件，网站的文章列表和页面就会自动更新。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;步骤五：将你的成果推送到 GitHub&lt;/h3&gt;
&lt;p&gt;在本地修改满意后，我们需要把代码上传到 GitHub。这不仅是备份的好习惯，更是我们实现自动化部署的关键一步。&lt;/p&gt;
&lt;p&gt;假如说你直接 &lt;code&gt;git clone&lt;/code&gt; 了源码，可以直接删掉 &lt;code&gt;.git&lt;/code&gt; 文件夹，然后执行 &lt;code&gt;git init&lt;/code&gt; 初始化一个新的仓库。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt; 官网上创建一个新的空仓库（New Repository）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据 GitHub 页面的提示，在你本地的项目文件夹中，通过终端执行以下命令，将代码推送到你的新仓库：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git init -b main
git add .
git commit -m &quot;Initial commit: My personal website setup&quot;
git remote add origin [你的仓库HTTPS或SSH地址]
git push -u origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假如说本来就是从自己的仓库中克隆的，则可以直接执行 &lt;code&gt;git push&lt;/code&gt; 将代码推送到远程仓库。&lt;/p&gt;
&lt;h3&gt;步骤六：使用 Vercel 一键部署&lt;/h3&gt;
&lt;p&gt;代码已在 GitHub，接下来是最激动人心的一步：让全世界都能访问你的网站。我们将使用 Vercel 这个强大的免费平台。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么不用 GitHub Pages？&lt;/strong&gt; 虽然免费，但它需要你手动配置构建流程（GitHub Actions），且自定义域名等操作较为繁琐。Vercel 真正做到了“零配置”自动化部署。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;用 GitHub 登录 Vercel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;前往 &lt;a href=&quot;https://vercel.com&quot;&gt;Vercel&lt;/a&gt;，点击右上角的“Login”，选择使用 GitHub 账号授权登录。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.8z6qg420y0.webp&quot; alt=&quot;Vercel 登录页面&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;导入你的项目&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;登录后，在主面板点击 &lt;code&gt;Add New…&lt;/code&gt; → &lt;code&gt;Project&lt;/code&gt;。之后选择你自己的账号，直接 instal vercel。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.ma7vlv0g.webp&quot; alt=&quot;Vercel 添加新项目&quot;&gt;&lt;/p&gt;
&lt;p&gt;Vercel 会列出你的 GitHub 仓库，找到你刚刚创建的网站仓库，点击旁边的 &lt;code&gt;Import&lt;/code&gt; 按钮。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.wirnbxpwa.webp&quot; alt=&quot;从 GitHub 导入仓库&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;部署！&lt;/strong&gt;
Vercel 会自动识别你的项目是什么框架（Astro, Next.js, etc.），并帮你填好所有构建设置。你什么都不用改，直接点击 &lt;code&gt;Deploy&lt;/code&gt; 按钮。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.58hkuvhhew.webp&quot; alt=&quot;Vercel 部署设置&quot;&gt;&lt;/p&gt;
&lt;p&gt;稍等片刻，Vercel 就会完成构建和部署。当看到庆祝的动画时，你的网站就已经上线了！Vercel 会提供一个 &lt;code&gt;.vercel.app&lt;/code&gt; 结尾的免费域名供你访问。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.39le4jdbr7.webp&quot; alt=&quot;Vercel 部署成功&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个时候你已经可以把这个链接分享给你的朋友了。当然，你也可以选择绑定自己的域名，之后在 Vercel 的设置中进行配置。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;从此以后，你只需要在本地修改代码，然后 &lt;code&gt;git push&lt;/code&gt; 到 GitHub，Vercel 就会自动拉取最新代码，重新构建和部署你的网站。完全自动化！&lt;/p&gt;
&lt;h3&gt;步骤七（可选）：绑定你的专属域名&lt;/h3&gt;
&lt;p&gt;拥有一个 &lt;code&gt;yourname.com&lt;/code&gt; 这样的域名才算真正完整。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;购买域名&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;前往 &lt;a href=&quot;https://www.namesilo.com/&quot;&gt;NameSilo&lt;/a&gt;、&lt;a href=&quot;https://godaddy.com/&quot;&gt;GoDaddy&lt;/a&gt; 等域名注册商，购买一个你喜欢的域名。过程就像网购一样简单。&lt;/p&gt;
&lt;p&gt;以 NameSilo 为例，可以搜索之后直接购买，并且用支付宝付款。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.77dtzvqmp0.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.361ulhm288.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.3k8accwfq4.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;使用 Cloudflare 管理 DNS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;虽然域名商也提供 DNS 解析，但 Cloudflare 提供免费的全球 CDN 加速和更强大的安全防护。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;注册并登录 &lt;a href=&quot;https://dash.cloudflare.com/&quot;&gt;Cloudflare&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;点击 &lt;code&gt;Add a domain&lt;/code&gt;，输入你购买的域名，选择免费（Free）套餐。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.5tqyhumx92.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;接下来输入你的域名，这里以一个不存在的域名为例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-1.7zqd3mel07.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;遗憾选择 Free 方案，完全已经够用：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-2.7p3jagzcuw.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;接下来 DNS 记录先跳过，之后再添加。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cloudflare 会扫描你现有的 DNS 记录（如果是新域名，这里是空的），然后提示你&lt;strong&gt;更改 NameServer&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-3.6pnfxawlp5.webp&quot; alt=&quot;Cloudflare 提示更改 NameServer&quot;&gt;&lt;/p&gt;
&lt;p&gt;它会提供两个 NameServer 地址。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-5.4uav4ok63f.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;回到你的域名注册商（如 NameSilo）的域名管理后台，找到 NameServer/DNS服务器 设置，删除原来的，&lt;strong&gt;替换为 Cloudflare 提供的那两个地址&lt;/strong&gt;。例如对于 NameSilo，在 My Account -&gt; Domain Manager -&gt; axi404.top -&gt; NameServers 删掉原来的东西，并且添加这些。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;等待几分钟到几小时，时常刷新一下，让更改生效。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;在 Vercel 和 Cloudflare 中配置&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Vercel 端&lt;/strong&gt;：进入 Vercel 的项目设置（Settings → Domains），输入你的域名并添加。
&lt;img src=&quot;https://picr2.axi404.top/image.2rvcfycyz1.webp&quot; alt=&quot;在 Vercel 添加域名&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cloudflare 端&lt;/strong&gt;：在你的域名管理页面，进入 &lt;code&gt;DNS&lt;/code&gt; → &lt;code&gt;Records&lt;/code&gt;，添加一条 &lt;code&gt;CNAME&lt;/code&gt; 记录：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: &lt;code&gt;CNAME&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: &lt;code&gt;@&lt;/code&gt; (代表你的根域名)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt;: &lt;code&gt;cname.vercel-dns.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proxy status&lt;/strong&gt;: Proxied (保持橙色云朵亮起)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-4.839z1c7nq2.webp&quot; alt=&quot;在 Cloudflare 添加 CNAME 记录&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最后，在 Cloudflare 的 &lt;code&gt;SSL/TLS&lt;/code&gt; 页面，将加密模式设置为 &lt;strong&gt;Full (strict)&lt;/strong&gt;，以确保端到端的安全连接。
&lt;img src=&quot;https://picr2.axi404.top/image-7.70a9qgbtul.webp&quot; alt=&quot;设置 Cloudflare SSL 模式&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;完成这些，稍等片刻，你就可以通过自己的专属域名访问你的网站了！&lt;/p&gt;
&lt;h2&gt;小结&lt;/h2&gt;
&lt;p&gt;恭喜你！从一个想法开始，到现在拥有一个部署在全球 CDN 网络、自动化更新、绑定了专属域名的个人网站，你已经走完了现代 Web 开发最主流、最高效的一条路径。&lt;/p&gt;
&lt;p&gt;你会发现，这个流程的核心是“关注内容，而非工具”。你未来的大部分时间，都将花在写文章、更新作品这些创造性的工作上，而不是和服务器配置作斗争。&lt;/p&gt;
&lt;p&gt;更重要的是，今天你所学的这套流程是高度可扩展的。它不仅仅适用于个人博客，任何拥有类似构建方式的静态网站模板——无论是产品展示页、在线简历还是文档网站——都可以用完全相同的方法进行部署。&lt;/p&gt;
&lt;p&gt;从这里开始。尽情创造吧！&lt;/p&gt;
&lt;p&gt;以及，假如教程帮到了你，你也成功搭建了自己的网站，非常欢迎和我交换&lt;a href=&quot;/links&quot;&gt;友链&lt;/a&gt;！&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/website-vercel-zh.webp"/><enclosure url="https://picr2.axi404.top/website-vercel-zh.webp"/></item><item><title>周记 Week52</title><link>https://axi404.github.io/blog/week-52</link><guid isPermaLink="true">https://axi404.github.io/blog/week-52</guid><description>2025-06-23 ~ 2025-06-29.</description><pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这已经是很久之后才写的一篇周记了，本来以为这次会很快就记录下来。现实却没想象中那么顺利。&lt;/p&gt;
&lt;h2&gt;夏令营&lt;/h2&gt;
&lt;p&gt;我记录的这周的前一个周末，我参加了 AILAB 的夏令营，活动一直持续到了这周的周一。周一是面试，面试的老师基本上都是我平时就认识的老师，加上前一天还有一个和老师们线下见面的交流活动，也算是提前联络了一下关系。&lt;/p&gt;
&lt;p&gt;面试过程中主要介绍的是我在这个实验室里做过的项目。说实话，没什么需要特别解释的细节，因为即便我说得不清楚，老师们其实都知道这些事是我做的——从项目建立之初到现在的整个演进过程，大家都看在眼里，也就不会怀疑答不上来是我套用别人的内容什么的，更何况其实我这边回答的还是很流畅。基本上，我完成了绝大多数核心内容。&lt;/p&gt;
&lt;p&gt;最后老师们问了几个和领域相关的问题，比如对未来发展的看法、有什么计划之类的。因为没提前准备，我就干脆随便说了一下。我现在最看重的还是具身智能方向中，如何实现指令跟随级别的泛化能力。因为让一个机器人执行特定精细操作，其实从任务设计上并不是难事，关键是怎么让一个机器人，像 VLM 模型那样进行泛化，这是目前的关键问题。&lt;/p&gt;
&lt;p&gt;现在行业内的瓶颈还是数据为主的问题，要么靠人工构造数据，要么从 VLM 这样的模型里迁移泛化能力，这两条路线其实都在探索中。不过目前在底层建模这块，我们改用的是业内顶尖 VLM 模型的 Transformer 权重，虽然模型本身能力很强，但在我们的场景中并没有表现出很好的迁移能力，反而像是学了两个毫无 overlap 的技能：一个是语言，一个是动作。我其实认为这本质上是因为改变了模型本身的 Encoder，导致其实模型被迫去学习了新的分布，也就不具有之前的迁移能力了。假如说设计一种 Encoder frozen 的架构，或许就可以好很多。&lt;/p&gt;
&lt;h2&gt;琐事&lt;/h2&gt;
&lt;p&gt;夏令营结束后我就回学校处理了一些事务，包括继续推进之前 ECCV 报销的流程。只能说各种审批、盖章周期都很久，毕竟出去开会甚至已经是去年这个时候的事了，现在才刚刚走完流程。好在问题不大，不然这笔钱可能还得让之前课题组的老师来报销，那样就太麻烦了。虽然我现在和那边的老师关系还不错，但当初从那个课题组离开的时候，多少还是有点小小的不愉快。&lt;/p&gt;
&lt;p&gt;回到西安办完事后，我又参加了学校组织的一个线下专业实习动员会。之后就继续搞科研了。&lt;/p&gt;
&lt;p&gt;这周的工作主要围绕着 GenManip 框架的扩展。之前 GenManip 支持的是通用抓取，因此可以在一个几万个物体的大规模数据集上做 scaling 实验。虽然效果不错，但一些 concerns 依然是可能会带来比较多的不确定性。比如说目前的 grasp proposal 使用的是 AnyGrasp，从好的方面想，是提供了更 diversity 的 grasp pose，但是另一方面，更随机的内容必然也会更不好学，因此我们准备引入更多新的 feature，让它可以退回成其他那些做数据生成的系统，也就是手动的纯 step by step 的 design。&lt;/p&gt;
&lt;p&gt;在当前的框架中，扩展这些功能其实并不复杂，很多逻辑可以写成类似 script 或 config 的形式，简单设定后就能跑起来了。这部分就是我下周要完成的工作。&lt;/p&gt;
&lt;p&gt;说到底，这周其实一直都在奔波。因为上周末刚到上海，周中又回西安办事，然后又回到上海，光是路费就花了不少。接下来的日子会稍微平淡一点，毕竟课内任务已经都完成了，可以安心把精力投入到科研上。当然，学习还是得继续，这点没办法逃避。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-52-zh.webp"/><enclosure url="https://picr2.axi404.top/week-52-zh.webp"/></item><item><title>Cloudflare R2 图床搭建</title><link>https://axi404.github.io/blog/r2-image-host</link><guid isPermaLink="true">https://axi404.github.io/blog/r2-image-host</guid><description>使用 Cloudflare R2 搭建图床</description><pubDate>Sat, 05 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;之前刷别人的博客的时候，发现一些传闻，说 Github 建立的图床有可能被封掉，虽然说目前那个仓库使用 webp 压缩图片，基本上绝大多数的图片都很小，一共存储用了一年还不到 300 MB。即使是传说中，也是需要到 1GB 才有可能被封号，所以应该整体来看还好，但是毕竟我的 Github 账号活这么久也不容易，保险起见还是操作一下吧。&lt;/p&gt;
&lt;h2&gt;项目选择&lt;/h2&gt;
&lt;p&gt;本身我的需求其实比较低，所以直接随便找了一个项目。我在自己的仓库 fork 了 &lt;a href=&quot;https://github.com/Axi404/Picx4R2&quot;&gt;一份&lt;/a&gt;，并且改了一些 Bugs。&lt;/p&gt;
&lt;p&gt;本身我的需求大概是不需要 Hash 本来的图片名称，也不需要自动的 webp 压缩处理，事实上这一步骤我会在 &lt;a href=&quot;https://picx.xpoet.cn/&quot;&gt;picx&lt;/a&gt; 上面进行。因此我只需要存储就好了，这个项目刚好符合我的需求。&lt;/p&gt;
&lt;h2&gt;部署&lt;/h2&gt;
&lt;p&gt;按照项目的 README 进行了部署，首先还是要注册 cloudflare 的账号，并且 R2 需要绑一张信用卡，我的绑定一直说有问题，所以绑了 Paypal，问题也不大。&lt;/p&gt;
&lt;p&gt;Fork 项目之后直接部署就好，因为我的域名也是通过 Cloudflare 管理，所以这个项目干脆就不用 vercel 了。&lt;/p&gt;
&lt;p&gt;首先先在 Cloudflare 创建一个 R2 的存储桶以及一个 KV，都是直接创建好就行。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Ee1LBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/tlCLBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在这里 R2 可以自定义域名，假如说你已经管理了一些域名，你可以把它们或者它们的子域名分配给 R2：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/dFOMBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个域名是后续用于获得图片的那个图床的链接。&lt;/p&gt;
&lt;p&gt;之后通过 import Github 的仓库来创建一个 Page：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/nuvmBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/kV0NBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后输入编译的指令以及输出路径：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/4ABLBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;以及输入一些环境变量：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/UeHLBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这其中重要的是 &lt;code&gt;AUTH_TOKEN&lt;/code&gt;，类似于密码一类的东西，&lt;code&gt;COPY_URL&lt;/code&gt; 就是你的 R2 存储的链接。剩下两个的版本分别是 &lt;code&gt;NODE_VERSION&lt;/code&gt; 是 &lt;code&gt;20.11.1&lt;/code&gt; 以及 &lt;code&gt;NPM_VERSION&lt;/code&gt; 是 &lt;code&gt;10.2.4&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;之后正常部署，一段时间后可以看到成功。&lt;/p&gt;
&lt;p&gt;在设置中添加我们之前创建的 KV 以及 R2 存储：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/LN0LBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;并且重新部署：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/RphLBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后登录这个 Page 的网页即可，可以看到可以上传文件了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/ocylBVK.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;完结撒花~&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/r2-image-host-zh.webp"/><enclosure url="https://picr2.axi404.top/r2-image-host-zh.webp"/></item><item><title>Docker 设置 SSH</title><link>https://axi404.github.io/blog/docker-ssh</link><guid isPermaLink="true">https://axi404.github.io/blog/docker-ssh</guid><description>为 Docker 设置 SSH 连接</description><pubDate>Wed, 02 Jul 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;因为最近在实验室一直在配置新的 Docker，然后配置之后作为镜像传到阿里云来运行 DLC 实例。虽然只是一个小小的需求，但是还是记录一下。因为我之前的一些 Docker 都是没有配置过 SSH 的，所以每次上传都要再次配置一次 SSH，不然传到服务器并且开启 SSH 之后，就无法连接到服务器了。&lt;/p&gt;
&lt;h2&gt;安装并且配置 SSH&lt;/h2&gt;
&lt;p&gt;首先运行 Docker 容器，然后进入容器，安装 SSH 服务。&lt;/p&gt;
&lt;p&gt;先更新一下 apt 的源，然后安装 SSH 服务。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;apt-get update
apt-get install -y openssh-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后启动 SSH 服务。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo mkdir -p /run/sshd
sudo /usr/sbin/sshd
&lt;/code&gt;&lt;/pre&gt;</content:encoded><h:img src="https://picr2.axi404.top/docker-ssh-zh.webp"/><enclosure url="https://picr2.axi404.top/docker-ssh-zh.webp"/></item><item><title>网页浏览卡顿的解决方案</title><link>https://axi404.github.io/blog/browser-issues</link><guid isPermaLink="true">https://axi404.github.io/blog/browser-issues</guid><description>解决网页浏览卡顿的简单方法。</description><pubDate>Mon, 30 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;博客的起因是因为本网页在开发的过程中一系列的问题，渲染网页的时候，在加入了渐变的背景之后网页就会十分卡顿，因为里面有八个 &lt;code&gt;blur-2xl&lt;/code&gt;，所以产生了计算开销。不过实际上甚至手机都可以流畅浏览网页，所以一定是哪里的配置出了问题。&lt;/p&gt;
&lt;h2&gt;开启网页图形加速&lt;/h2&gt;
&lt;p&gt;在 Chrome 中，进入设置-系统，并且开启图形加速功能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.8ojyx0v4mu.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于 Edge 也是同理：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.3uv40wbn8u.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.ice6ivge9.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;即可。&lt;/p&gt;
&lt;p&gt;之前感觉电脑开启节能模式可能会有一些限制，但是开关测试之后感觉问题不大，本质上的限制还是因为这个图形加速导致的。完结~&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/browser-issues-zh.webp"/><enclosure url="https://picr2.axi404.top/browser-issues-zh.webp"/></item><item><title>周记 Week51</title><link>https://axi404.github.io/blog/week-51</link><guid isPermaLink="true">https://axi404.github.io/blog/week-51</guid><description>2025-06-16 ~ 2025-06-22.</description><pubDate>Sun, 29 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说，又是很久以后的一次周记，写一下这篇内容的时候，已经是下一周的周末了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;回想上一周，其实还算是意气风发。robotiq 的 gripper 的 URDF 总算配置好了，当时觉得一切终于告一段落了，虽然实际上，事情还是一件接着一件地来。&lt;/p&gt;
&lt;p&gt;首先还是关于添加机器人的问题。我们当前的代码使用 AnyGrasp 模型生成 Grasp proposal，这个过程虽是模型输出，存在一定不稳定性，但整体来看抓得还是比较准的。不过在实际替换成 Sapien 之后，我发现抓取落点在 XY 平面上总是有一点偏移。经过一些消融实验后发现，这个问题在 Isaac Sim 中是不存在的，因此推断问题出在 driver 的参数设置上。&lt;/p&gt;
&lt;p&gt;所谓 driver，就是 URDF 文件中定义的 Joint 在控制器中对应的 PD 控制参数，也就是 PID 中的 P 和 D，即 stiffness 和 damping。一般来说，stiffness 越大，回弹速度越快；damping 越大，阻尼越强，运动趋于缓和。根据经验，对于机器人关节，可以将 stiffness 设为 10000，damping 设为 100；而对于 gripper，stiffness 设为 100，若关闭速度过慢，则降低 damping，从 80 开始往下调。同时，Sapien 支持设置 force limit，因此可以配置出一个关闭速度不快但夹持力很大的 gripper。&lt;/p&gt;
&lt;p&gt;其他的依然是解决了一些 bugs，没什么好说的。&lt;/p&gt;
&lt;h2&gt;夏令营&lt;/h2&gt;
&lt;p&gt;另一件还算有趣的事，是去参加 AILAB 的夏令营。前期也做了不少准备工作，包括报名等等。这次行程直接坐了高铁，虽然家里能稍微补贴点，但总归花了不少钱。在上海期间的吃饭和打车花销也不低，再次感叹上海的生活成本，感觉比北京还贵不少。尤其打车，这个月花了快一千，来回机场就将近五百，真是有点肉疼。&lt;/p&gt;
&lt;p&gt;机试前一天，特地向好朋友请教了一些经验，得到了不少有价值的建议。尤其是他推荐我重新看一下&lt;a href=&quot;https://programmercarl.com/&quot;&gt;代码随想录&lt;/a&gt;，我才想起自己大一时其实还加入过他们的知识星球，看过一段时间八股文。当时和现在都对传统技术路线挺感兴趣的。&lt;/p&gt;
&lt;p&gt;考试的时候运气不错，机试题正好考了我复习过的 DP 和 DFS，虽然只遇到了一道原题，其余也能靠思想硬写出来。回头想想，其实自己一直对算法挺有热情的，只是一直没时间系统地刷题。这次考试题目本身不难，写起来也比较轻松，主要是借了 Python 的光，不太讲究卡常技巧，以及 &lt;code&gt;eval&lt;/code&gt; 这个函数也帮了大忙。另外还有一些附加的数学与机器学习题，难度不大，最终成功 AK。事后打听成绩，好像是 93 分，还不错。&lt;/p&gt;
&lt;p&gt;这周科研做得不算多，但也帮了两个朋友折腾了博客，分别是 &lt;a href=&quot;https://tianshijing.github.io/&quot;&gt;Juanxi Tian&lt;/a&gt; 和 &lt;a href=&quot;https://ruiwu.top/&quot;&gt;Rui Wu&lt;/a&gt;。此外还抽空处理了学校的一些流程，例如一年前 ECCV 报销的收尾和分散实习的申请。各种跑流程，真的挺累的。累累累，下周还要继续折腾。好在马上就要去上海了，一切也快告一段落了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-51-zh.webp"/><enclosure url="https://picr2.axi404.top/week-51-zh.webp"/></item><item><title>安装 Ubuntu 双系统</title><link>https://axi404.github.io/blog/install-ubuntu</link><guid isPermaLink="true">https://axi404.github.io/blog/install-ubuntu</guid><description>关于安装 Linux（Ubuntu）双系统，一些基础的教程。</description><pubDate>Fri, 06 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Aside } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本篇章最初为我作为西安交通大学 RoboMaster 笃行战队视觉组组长时写作的，之后历经了多次更新，现在重新整理并且发布。&lt;/p&gt;
&lt;h2&gt;前情提要&lt;/h2&gt;
&lt;p&gt;Ubuntu 20.04 安装是 Linux 系统安装的基础，本教程将详细讲解如何安装 Ubuntu 20.04 双系统。&lt;/p&gt;
&lt;p&gt;读者在完成任务的过程中需要注意以下的内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装系统是具有风险的选项，尽管约等于 0，读者或许可以选择备份一些自己的重要文件。&lt;/li&gt;
&lt;li&gt;在必要时候，读者可以在评论区进行提问，以获得来自笔者尽可能多的帮助。在提问之前，读者有必要了解 &lt;a href=&quot;https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md&quot;&gt;提问的智慧&lt;/a&gt;，但是我们并不对提问具有如此严苛的要求，但是我们希望你遵守以下内容：
&lt;ul&gt;
&lt;li&gt;检查相关问题是否已经在评论区被提出过。&lt;/li&gt;
&lt;li&gt;详细说明自己的环境配置，例如电脑型号，显卡，以及 Ubuntu 版本等。&lt;/li&gt;
&lt;li&gt;详细说明自己的问题，例如，我按照教程安装，但是出现了 xxx 错误，我尝试了 xxx，但是没有解决问题。&lt;/li&gt;
&lt;li&gt;在提问之前，尝试自己解决问题，但是不要花费过多的时间，因为你的时间同样宝贵，快速获得解决方案正是提问的价值所在。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;教程给出了 Ubuntu 20.04 的双系统的安装方法，但是并未给出虚拟机/WSL/服务器的安装方法，在这里简单列举一下对比：
&lt;ul&gt;
&lt;li&gt;虚拟机可以在 Windows 系统中运行，比较容易安装，但是具有较高的性能限制。&lt;/li&gt;
&lt;li&gt;WSL 是在 Windows 系统中最为优雅的方法之一。方法可以参考 &lt;a href=&quot;https://blog.csdn.net/weixin_45027467/article/details/106862520&quot;&gt;此篇博客&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;服务器具备一定的延迟，需要网络才可以使用，并且需要按量付费，但是配置十分地一键。同时，部分的如文件传输/GUI 对于原生的 Windows 需要额外下载软件，较为麻烦。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;读者在安装双系统的过程中可能遇到无法找到磁盘的问题，这可能与 BitLocker 有关。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;前置&lt;/h2&gt;
&lt;h3&gt;什么是Linux系统&lt;/h3&gt;
&lt;p&gt;在安装Ubuntu系统之前，我们必须了解什么是 Linux 系统，简单来说：&lt;/p&gt;
&lt;p&gt;Linux 系统是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统，其内核由林纳斯·本纳第克特·托瓦兹于 1991 年 10 月 5 日首次发布。它能运行主要的 Unix 工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计 思想，是一个性能稳定的多用户网络操作系统。Linux 有上百种不同的发行版，如基于社区开发的 debian、archlinux，和基于商业开发的 Red Hat Enterprise Linux、SUSE、Oracle Linux 等。&lt;/p&gt;
&lt;p&gt;而 Ubuntu Desktop 是由 Canonical 开发的 Linux 发行版（指由 Linux 内核开发的操作系统），由于其易用性，它是最受欢迎的发行版之一。它也是刚开始使用 Linux 的人的首选之一。&lt;/p&gt;
&lt;h3&gt;为什么选择Ubuntu系统&lt;/h3&gt;
&lt;p&gt;对于阅读这篇教程的人来说，主要是因为工控机中使用的是 Ubuntu 系统，这种系统对于各种内容的解释比闭源的 Windows 系统好得多。同时 Ubuntu 系统在日常的编程中因为一些指令的存在，对于配置依赖等也十分的便捷（相较于 Windows 简直便捷了无数倍），使用 Ubuntu/其他 Linux 系统是一名成熟的计算机领域人士的必备技能。&lt;/p&gt;
&lt;h3&gt;什么样的电脑可以装上Ubuntu系统&lt;/h3&gt;
&lt;p&gt;首先对于安装 Ubuntu 系统的电脑来说，根据本人个人经验，假如说是想配置一套可以长期使用的 Ubuntu 系统，电脑应当有一段完整的空白的空间位于某一分区的末尾，并且大小在 100GB 左右（假如说只是用于基础的编程，或许40GB左右就可以）。&lt;/p&gt;
&lt;h3&gt;选择双系统而非虚拟机&lt;/h3&gt;
&lt;p&gt;相较网上一部分的教学，关于虚拟机安装Ubuntu系统，我们更愿意选择使用双系统，虽然它需要重启才可以在不同的系统之间切换，但是这可以让 Ubuntu 系统发挥电脑的全部性能，同时直接使用那些 Ubuntu 的设计（Ubuntu 中的软件为 Ubuntu 而设计，而并非虚拟机，许多内容在虚拟机上进行配置需要额外的步骤，且网络的资料不多）。&lt;/p&gt;
&lt;p&gt;简单来说使用过虚拟机的读者应该有所了解，虚拟机会要求分配一定的资源给虚拟机，之后其和Windows系统会一同运行，这毫无疑问会消耗多余的性能。&lt;/p&gt;
&lt;h2&gt;启动盘&lt;/h2&gt;
&lt;p&gt;在经过了前面的介绍，我们已经知道了什么是 Ubuntu 系统，并且我们要具备怎样的条件才能搭建 Ubuntu 系统了，接下来就来到了制作启动盘的步骤。&lt;/p&gt;
&lt;h3&gt;什么是启动盘&lt;/h3&gt;
&lt;p&gt;启动盘不像其名字一样，像是每次启动这个系统的时候都需要使用启动盘才可以启动一样，要是更加形象的形容，更像是一个放置系统的安装包的地方。比方说你从 Windows 系统下载了 Ubuntu 系统的系统文件（一般为 &lt;code&gt;.iso&lt;/code&gt; 文件，也就是光盘映像文件），这个文件本身等于说是放在了 Windows 系统所属的存储空间中，你很难指望在一个系统里的文件能跳脱出这个系统而去在电脑中安装另一个系统，而假如说你将这个文件放到一个 U 盘中，虽然说 U 盘也可以在 Windows 系统中被识别，但是，值得注意的是，被识别。是的，U盘并不属于 Windows 系统，所以我们可以使用 U 盘进行安装，而这块 U 盘便被称为启动盘。&lt;/p&gt;
&lt;h3&gt;制作流程&lt;/h3&gt;
&lt;p&gt;现在网络上一般的教程都使用 Rufus 软件制作启动盘，但是这种方法无疑存在一种弊端，便是会「毁掉」你的 U 盘。&lt;/p&gt;
&lt;p&gt;在制作启动盘的过程中存在一步被称为「部署」的步骤，这一步会将 iso 文件放进你的U盘，但是使用 Rufus 之后，你的 U 盘将不能用于其他的用途。也就是说即使你有一个 1TB 大小的 U 盘，而 iso 文件一般只有不到 4GB，你的U盘也不能再装进去任何东西，否则就无法进行后续的系统安装了，而假如你想用这个U盘安装其他的系统，制作成其他系统的启动盘，你则需要格式化这个 U 盘。&lt;/p&gt;
&lt;p&gt;不过，技术总是在变革，在这里推荐软件 &lt;a href=&quot;https://www.ventoy.net/cn/index.html&quot;&gt;Ventoy&lt;/a&gt;，Ventoy 软件具有诸多的优势，此处让我们引用一段其官网的说明：「简单来说，Ventoy 是一个制作可启动U盘的开源工具。有了 Ventoy 你就无需反复地格式化U盘，你只需要把 ISO/WIM/IMG/VHD(x)/EFI 等类型的文件直接拷贝到U盘里面就可以启动了，无需其他操作。你可以一次性拷贝很多个不同类型的镜像文件，Ventoy 会在启动时显示一个菜单来供你进行选择」。&lt;/p&gt;
&lt;p&gt;让我用通俗的流程来讲解一下，以下讲解下载并安装 &lt;code&gt;Ubuntu20.04.5&lt;/code&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从清华源下载 &lt;a href=&quot;https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04.6/ubuntu-20.04.6-desktop-amd64.iso&quot;&gt;ubuntu-20.04.6-desktop-amd64.iso&lt;/a&gt;（点一下直接下载，或者不行的话可以手动前往 &lt;a href=&quot;https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04.6/&quot;&gt;清华源界面&lt;/a&gt; 选择 &lt;code&gt;ubuntu-20.04.6-desktop-amd64.iso&lt;/code&gt; 下载）。（Ubuntu 22.04 使用链接 &lt;a href=&quot;https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/22.04.4/&quot;&gt;22.04.4&lt;/a&gt;，选择 &lt;a href=&quot;https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/22.04.4/ubuntu-22.04.4-desktop-amd64.iso&quot;&gt;ubuntu-22.04.4-desktop-amd64.iso&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;下载 &lt;a href=&quot;https://mirrors.nju.edu.cn/github-release/ventoy/Ventoy/LatestRelease/&quot;&gt;Ventoy-LatestRelease&lt;/a&gt;（从 GitHub 下载需要一定网络条件，此处选择的链接源自南京大学镜像站），或者不行的话手动前往其 &lt;a href=&quot;https://www.ventoy.net/cn/index.html&quot;&gt;官网&lt;/a&gt; 挑选下载，如下载 &lt;a href=&quot;https://mirrors.nju.edu.cn/github-release/ventoy/Ventoy/LatestRelease/ventoy-1.0.99-windows.zip&quot;&gt;ventoy-1.0.99-windows.zip&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;解压下载的 Ventoy 压缩包，该压缩包开袋即食，进入解压后的文件夹，启动 &lt;code&gt;Ventoy2Disk.exe&lt;/code&gt;，应出现以下界面：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Ventoy2Disk%E5%90%AF%E5%8A%A8%E7%95%8C%E9%9D%A2.ic25bh550.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;在电脑上接入已经准备好的 U 盘（建议是空 U 盘，否则做好内容备份，安装 Ventoy 也会对 U 盘进行格式化，但之后可以依然作为存储介质），点击软件中「设备」右侧刷新图标，然后下拉设备菜单，选择你的 U 盘，之后点击安装。假如不出意外，可以看到你的U盘名字被改成了 Ventoy，之后把你事先下载好的 &lt;code&gt;ubuntu-20.04.5-desktop-amd64.iso&lt;/code&gt; 文件拷贝进 U 盘即可。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这里可以解释一下 Ventoy 的特性，这是一个可以制作多系统启动盘的软件，安装在你的 U 盘之后，一切被拷入的系统配置文件都会被自动配置，在重启并按照正常安装系统流程操作的过程中会出现一个菜单界面（下图），供你选择你想要安装的系统，而同时这个 U 盘也可以用于常规的拷贝文件与储存。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/Ventoy%E8%8F%9C%E5%8D%95%E7%95%8C%E9%9D%A2.45meg8u8y.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;到这里，你的启动盘已经制作完成了，貌似非常简单，接下来，将进入安装系统的步骤，请聚精会神，不要错过任何一个步骤。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;安装系统&lt;/h2&gt;
&lt;h3&gt;准备空间&lt;/h3&gt;
&lt;p&gt;首先，在安装系统之前，还记得我们之前说的吗？要保证你有空闲的 100GB 空间，当然，这前提是，你失去了这 100GB 空间之后，你的 Windows 系统的空间依然不会显得逼仄（尤其是 C 盘，Windows 系统的文件会不断变大，假如说不知道缩减方法，不建议盲目删除一些东西，而也因此需要为 C 盘留下一定的空间余量）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Windows+E&lt;/code&gt; 打开资源管理器，右键单击此电脑，选择管理（Windows11 需要先点击显示更多选项），进入计算机管理页面。点击存储--&gt;磁盘管理，会进入以下界面：&lt;/p&gt;
&lt;p&gt;在这里找到你之前觉得有空余空间的磁盘，单击右键选择压缩卷，在页面中「输入压缩空间量」中输入你需要压缩的空间大小，若按照之前的要求 100GB，你需要输入 102400，然后点击压缩，等待完成即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/%E8%BE%93%E5%85%A5%E5%8E%8B%E7%BC%A9%E9%87%8F.5xaknqwaic.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3&gt;关闭安全启动&lt;/h3&gt;
&lt;p&gt;不同的电脑的安全启动关闭方法各不相同，大多是在重启电脑之后狂按（没有夸张的成分，这是合理操作）&lt;code&gt;F2&lt;/code&gt; 或其他 F 区的键，进入系统的 BIOS，之后关闭安全选项中的 secure boot，不过不同的电脑型号的关闭方法各不相同，有的甚至有多余的操作，比如说需要关闭 VMD 等，请根据自己的电脑的品牌以及型号自行查找相关方法。&lt;/p&gt;
&lt;p&gt;检验是否成功的方法就是操作完之后装一下系统，如果能顺利进入安装界面，就没有问题。这一步一般不会对电脑有损伤，有报错不要慌张，重启电脑，平稳拔出，若电脑不能恢复，进入 BIOS 复原之前的修改，还不行则及时找计算机高手，切勿自己擅自操作。&lt;/p&gt;
&lt;p&gt;虽然如此说，这一步一般来说不会出现问题。&lt;/p&gt;
&lt;h3&gt;关闭 BitLocker&lt;/h3&gt;
&lt;p&gt;Windows 的 BitLocker 是一个加密系统，简单理解来说就是可以给你的磁盘加密，所以说假如说别人直接偷走了你的硬盘，不能直接插上你的硬盘来解锁其中的内容。对于安装过程，其实系统会认为你的硬盘发生了变动，从而激活 BitLocker，导致你的 Windows 不再进得去，因此导致的重装系统的悲剧屡见不鲜。尽管理论来说，只要你的密钥和你的 Windows 账号绑定，那么你就可以通过密钥对磁盘解锁，但是实际上很多人并未绑定，直接关闭 BitLocker 最为稳妥。&lt;/p&gt;
&lt;p&gt;对于加密的磁盘，你可以在计算机设备管理的磁盘管理中看到已加密的字样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/54f4abc44137913653dd664477a2f01e.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;Win + X&lt;/code&gt; 并且选择终端管理员，对于你被加密的磁盘，输入：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;manage-bde -off c: # c: 为你的磁盘的盘符，你需要将这个命令中的 c: 替换为你的磁盘的盘符。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后查看是否关闭成功即可，关闭需要等待一段时间，请耐心等待。&lt;/p&gt;
&lt;h3&gt;安装系统&lt;/h3&gt;
&lt;p&gt;插入制作好的启动盘，重启电脑，重启的过程中狂按 &lt;code&gt;F12&lt;/code&gt;（这一步为调用 one time boot，一些电脑没有此功能，可能需要在 BIOS 中的启动顺序中将你的 U 盘顺序调至最高），然后在出现的页面的左下角找到你的 U 盘（可以尝试辨别一下是哪个，或者先退出，然后去 Windows 里看看你的 U 盘的学名）即可进入上方给出过的 Ventoy 菜单界面，理论来说应该只有我们需要安装的 &lt;code&gt;Ubuntu20.04.5&lt;/code&gt;，按下 &lt;code&gt;Enter&lt;/code&gt;（回车）进行确认。&lt;/p&gt;
&lt;p&gt;值得一提的是在大多数的非图形化页面中，并不存在光标这一物体，需要通过键盘的上下左右键（不是 WASD）进行选择，用 Enter 键（回车键）进行确认。&lt;/p&gt;
&lt;p&gt;可能出现选项，选择第一项 Ubuntu 即可，之后会显示正在检测文件。&lt;/p&gt;
&lt;p&gt;等待一段时间，应该可以见到如下界面(暂时无需联网)：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;欢迎，在左侧栏选择 Chinese，点击继续。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.8ojmw093sl.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;键盘布局，在左侧栏选择 Chinese，右侧栏选择 Chinese，点击继续。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2krutkgetd.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;无线，在下方选择我现在不想链接 Wi-Fi 无线网络，点击继续。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.3d4qbay08x.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;更新和其他软件，选择最小安装，下方两个不勾选，点击继续。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.7w6rea3x6s.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;安装类型，其他选项，点击继续。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2obgrabh5o.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;之后不出意外会看到一个界面，大致为一个表格：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;| 设备           | 类型 | 挂载点 | 格式化? | 大小     | 已用 | 已装系统 |
| -------------- | ---- | ------ | ------- | -------- | ---- | -------- |
| /dev/nvme0n1p0 | ntfs |        |         | xxxx     | xxxx | xxxx     |
| /dev/nvme0n1p1 | ntfs |        |         | xxxx     | xxxx | xxxx     |
| /dev/nvme0n1p2 | ntfs |        |         | xxxx     | xxxx | xxxx     |
| 空闲           |      |        |         | 102400MB |      |          |&lt;/p&gt;
&lt;p&gt;选择这个我们之前建立出来的 102400MB 的空闲，表格的左下角应有一个加号和一个减号，点击加号，会出现窗口「创建分区」，选择逻辑分区，空间起始位置，用于 EFI，大小为 300MB。之后再创建两个分区，其中之一用于交换空间，其他不变，理论来说大小与你的内存大小一致，但是小一些设置个 2GB 也不是问题；其中之一用于 ext4 或者 btrfs，挂载点「/」，占用其他的空间。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.77dhu9hupm.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.969oklnrtc.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.1hs5ioomvr.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;以上创建的分区，均在下方选择下方的安装启动器的设备，选择新建的 EFI 对应的设备，并且请注意勾选格式化。之后继续。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.99taibiueg.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;配置完毕之后点击现在安装。&lt;/p&gt;
&lt;ol start=&quot;7&quot;&gt;
&lt;li&gt;您在什么地方，选择上海，点击继续。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.esg7su7wf.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;8&quot;&gt;
&lt;li&gt;您是谁？设置你的姓名、计算机名和用户名，值得一提的是密码可以设置的简短一些，因为在 Ubuntu 后续的操作中会使用 sudo 来获取 root 权限，这个过程需要输入密码，而 sudo 在你使用 Ubuntu 的时候经常出现。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.b8ua31c34.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;9&quot;&gt;
&lt;li&gt;重启你的电脑。会显示拔出你的 U 盘，拔出，然后按下 Enter 回车键。&lt;/li&gt;
&lt;li&gt;重启之后不会像之前一样直接进入 Windows 界面，而是进入 Grub 界面，也就是一个黑色的看上去很简陋的纯文字界面，在左上角会显示 Ubuntu 和 Windows 界面，通过上下键可以进行选择，按 Enter 回车键确定，此处选择 Ubuntu。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;更换软件源与语言&lt;/h3&gt;
&lt;h4&gt;软件源&lt;/h4&gt;
&lt;p&gt;Ubuntu 使用 apt 进行包管理。所谓包管理，读者可以简单理解为使用一种统一管理软件包的方式，统筹一切的依赖内容，并且构建依赖之间的依赖关系。当然，这种关系使得存在一个远程仓库，存有全部的 Ubuntu 的软件包，并且可以方便的下载安装。Ubuntu 的官方源是其中之一，而同时不少的开源组织对 Ubuntu 的软件源进行了克隆，以方便各个地区的使用者进行更加快捷的访问。&lt;/p&gt;
&lt;p&gt;在中国大陆，因为网络访问的问题，使用 Ubuntu 的官方源往往不太容易，因此更换软件源是一个必要的选择。&lt;/p&gt;
&lt;p&gt;点击左下角呼出应用列表，点击软件与更新：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.9gwidrfswi.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在「下载自」中选择中国的服务器，在这里使用中科大源，读者也可以使用其他源，然而需要注意的是，一些源在后续已经停止维护，使用这些源可能在安装软件包的时候引发问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4cktohgl31.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后点击关闭，在弹出的窗口中点击「重新载入」，并进行耐心等待，过程中可能要求输入密码，即前面设置的密码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.6f0mcjfg7h.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后使用 &lt;code&gt;ctrl + alt + t&lt;/code&gt; 打开终端，此时可以输入指令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以对全部的安装的软件包进行更新。&lt;/p&gt;
&lt;h4&gt;语言&lt;/h4&gt;
&lt;p&gt;尽管我们在安装的过程中要求读者安装中文，但是实际上，因为众所周知的语言支持问题，大多数的非英文的路径是不被推荐的，正如读者见到的系统中的若干默认文件夹，如 &lt;code&gt;下载&lt;/code&gt; 而非 &lt;code&gt;Downloads&lt;/code&gt;。安装中文只是因为中文输入法的安装在英文环境中较为困难，因此此时需要换回英文：&lt;/p&gt;
&lt;p&gt;点击左上角，选择设置，点击区域和语言：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.60u6lo7lpv.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.lvo38t2q2.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4917qrpbgy.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后重启，可以在终端中输入：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;reboot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再次进入系统的时候，可以发现弹窗，选择 &lt;code&gt;Update Names&lt;/code&gt;：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.54xp67zbfx.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;Ubuntu基本终端操作&lt;/h2&gt;
&lt;p&gt;简单介绍一些 Ubuntu 命令行操作，其他的内容可以由读者自行探索。&lt;/p&gt;
&lt;h3&gt;sudo&lt;/h3&gt;
&lt;p&gt;在一切的指令前面添加sudo意味着使其获得根权限，即最高权限。使用sudo之后需要输入密码，假如在同一终端内连续使用sudo只需要第一次输入密码。&lt;/p&gt;
&lt;h3&gt;ls&lt;/h3&gt;
&lt;p&gt;输入之后可以查看当前文件夹下的内容。&lt;/p&gt;
&lt;h3&gt;mv、cp与rm&lt;/h3&gt;
&lt;p&gt;即 move、copy 与 remove，移动、拷贝与删除。这三条指令分别用法为 &lt;code&gt;mv/cp &amp;#x3C;source_path&gt; &amp;#x3C;target_path&gt;&lt;/code&gt; 以及 &lt;code&gt;rm &amp;#x3C;target_path&gt;&lt;/code&gt; ，值得一提的是假如要修改一些根目录下的内容，这些语句需要加上sudo前缀。&lt;/p&gt;
&lt;h3&gt;mkdir、gedit、torch 与 vim&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;mkdir &amp;#x3C;folder_name&gt;&lt;/code&gt; 用于创建文件夹；&lt;code&gt;touch &amp;#x3C;file.filetype&gt;&lt;/code&gt; 用于创建文件，需要写上后缀；&lt;code&gt;gedit &amp;#x3C;filename&gt;&lt;/code&gt; 与 &lt;code&gt;vim &amp;#x3C;filename&gt;&lt;/code&gt; 是两种文件编辑器，其中 gedit 的界面更加适合新手使用，而 vim 则有一套自己的操作方法，需要系统的学习，建议学习之后再使用，别乱点。&lt;/p&gt;
&lt;h3&gt;chmod&lt;/h3&gt;
&lt;p&gt;chmod 一般用于修改文件的读写运行权限，其中读写与运行用二进制写出，并且分为三个用户组，大致如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4ub2cvfq0q.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用修改文件权限的执行，必须要在根权限下才可以执行，虽然说有如此多的内容，但是一般来说，使用 &lt;code&gt;chmod&lt;/code&gt; 只会使用 &lt;code&gt;chmod 777 文件名&lt;/code&gt;，也就是将该文件的权限完全开放，或者使用 &lt;code&gt;chmod +x 文件名&lt;/code&gt;，也就是将这个文件添加可执行权限。&lt;/p&gt;
&lt;h2&gt;额外&lt;/h2&gt;
&lt;p&gt;一些特殊的情况在这里说明，如奇怪的报错，这里给出一些可能的解决方案。&lt;/p&gt;
&lt;h3&gt;安装显卡驱动&lt;/h3&gt;
&lt;p&gt;在安装的过程中，可能会出现屏幕的黑屏/花屏等情况，此为正常现象，需要安装显卡驱动等。此 Bug 情况需要在两个步骤分别进行操作：&lt;/p&gt;
&lt;p&gt;在 Ventoy 选择 &lt;code&gt;Boot in normal mode&lt;/code&gt; 之后，选择 &lt;code&gt;Ubuntu (safe graphics)&lt;/code&gt; 而非 &lt;code&gt;Ubuntu&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;在安装完系统之后，在确认已经完成换源等操作之后，&lt;code&gt;Ctrl + Alt + t&lt;/code&gt; 进入终端，并执行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo ubuntu-drivers autoinstall
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完毕之后重启，&lt;code&gt;reboot&lt;/code&gt;。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/install-ubuntu-zh.webp"/><enclosure url="https://picr2.axi404.top/install-ubuntu-zh.webp"/></item><item><title>Ubuntu 22.04 安装搜狗输入法</title><link>https://axi404.github.io/blog/sogou-install</link><guid isPermaLink="true">https://axi404.github.io/blog/sogou-install</guid><description>在 Ubuntu 22.04 中安装搜狗输入法。</description><pubDate>Thu, 19 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;众所周知，在 Ubuntu 系统中，假如说在安装的时候选择了中文作为语言（一般来说我在写教程的时候会推荐这么做，之后再把中文换回英文，而把输入法留下来），那么你的电脑中会包含一个 Ubuntu 的默认的输入法，然而不说这个输入法不是很符合中国人的说话习惯，其也很难根据你的打字来学习你的打字习惯。一般来说唯一的解决方案就是使用搜狗输入法。&lt;/p&gt;
&lt;h2&gt;安装&lt;/h2&gt;
&lt;p&gt;具体的方法如下：&lt;/p&gt;
&lt;p&gt;前往 &lt;a href=&quot;https://shurufa.sogou.com/&quot;&gt;搜狗输入法的官网&lt;/a&gt; 并且下载 &lt;code&gt;Linux个人版&lt;/code&gt;，这时候就会开始下载搜狗输入法的 &lt;code&gt;.deb&lt;/code&gt; 包，并且进入搜狗输入法的教程界面。然而虽然说一般情况下这个教程是好用的，但是在 Ubuntu 22.04 的时候，或许需要额外进行一些操作，以下从头来讲。&lt;/p&gt;
&lt;p&gt;首先需要安装 fcitx：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt install fcitx
sudo cp /usr/share/applications/fcitx.desktop /etc/xgd/autostart
sudo apt remove --purge ibus
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后进入设置中的区域和语言（Region &amp;#x26; Language），选择 Manage Installed Languages，在 Keyboard input method system 中选择 &lt;code&gt;Fcitx 4&lt;/code&gt;。当然，假如说你本身没有配置过中文，需要先在 Install/Remove Languages 中选择简体中文并且点击 &lt;code&gt;Apply&lt;/code&gt;：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.ic2bux3q4.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.8hgf6x8yun.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;安装依赖&lt;/h2&gt;
&lt;p&gt;之后再安装一些依赖并且删除 ibus。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo dpkg -i sogoupinyin_4.0.1.2800_x86_64.deb
sudo apt install -f
sudo apt install libqt5qml5 libqt5quick5 libqt5quickwidgets5 qml-module-qtquick2 libgsettings-qt1
sudo apt remove --purge ibus
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后 &lt;code&gt;reboot&lt;/code&gt; 重启电脑，应该就会出现搜狗输入法了。假如没有的话，点击输入法，选择 &lt;code&gt;配置&lt;/code&gt; 或者 &lt;code&gt;Configure&lt;/code&gt;，添加点击加号并且搜索搜狗输入法（sogoupinyin）进行添加。保险起见，可以把别的输入法都按一遍减号来删除。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.64dspq5v6e.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时搜狗输入法就安装好了。其中主要的坑在于，安装依赖并且删除 ibus 这一步骤，在 &lt;a href=&quot;https://shurufa.sogou.com/linux/guide&quot;&gt;搜狗输入法自己的教程&lt;/a&gt; 中没有给出。&lt;/p&gt;
&lt;h2&gt;Chrome / Edge 的输入法无效问题&lt;/h2&gt;
&lt;p&gt;某个版本之后，在 Ubuntu 22.04 中安装最新版本的 Google Chrome 之后，会出现 Chrome 无法使用搜狗输入法的问题，现象为不显示输入的提示。在其他软件中均可以顺利使用搜狗拼音输入法，但是在 Chrome 中不行，按照网络搜索到的内容，在最新的 Edge 中也不行。&lt;/p&gt;
&lt;p&gt;这是因为 Chrome 在某一个版本之后，使用了 GTK4，而界面与输入法的关系则类似于客户端与服务器的关系，假如说输入法不适配客户端的请求，自然就没有反应了。而在某个版本之后，Edge 和 Chrome 均使用 Chrome 的内核。因此解决方案为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install ibus-gtk4 fcitx5-frontend-gtk4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后重启即可。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/sogou-install-zh.webp"/><enclosure url="https://picr2.axi404.top/sogou-install-zh.webp"/></item><item><title>ArchLinux 日志之 linux-firmware-nvidia 文件冲突</title><link>https://axi404.github.io/blog/archbug1</link><guid isPermaLink="true">https://axi404.github.io/blog/archbug1</guid><description>ArchLinux 滚包报错，linux-firmware-nvidia: 文件系统中已存在 /usr/lib/firmware/nvidia/*。</description><pubDate>Thu, 26 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这是我在一次 &lt;code&gt;yay -Syu&lt;/code&gt; 升级中遇到的棘手问题，涉及 &lt;code&gt;linux-firmware&lt;/code&gt; 拆包带来的文件冲突。最后成功解决，记录过程。&lt;/p&gt;
&lt;h2&gt;情况说明&lt;/h2&gt;
&lt;p&gt;我运行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yay -Syu
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;系统提示有大量核心组件更新，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;linux&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux-firmware&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nvidia&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;以及一些 AUR 包如 &lt;code&gt;google-chrome&lt;/code&gt;、&lt;code&gt;linuxqq&lt;/code&gt;、&lt;code&gt;rustdesk-bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一切正常，直到执行安装时出现错误：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;linux-firmware-nvidia: 文件系统中已存在 /usr/lib/firmware/nvidia/ad103
linux-firmware-nvidia: 文件系统中已存在 /usr/lib/firmware/nvidia/ad104
linux-firmware-nvidia: 文件系统中已存在 /usr/lib/firmware/nvidia/ad105
linux-firmware-nvidia: 文件系统中已存在 /usr/lib/firmware/nvidia/ad106
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新终止。尝试使用 &lt;code&gt;--overwrite&lt;/code&gt; 参数也失败：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -Syu --overwrite=&apos;/usr/lib/firmware/nvidia/*&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;依旧报错。&lt;/p&gt;
&lt;h2&gt;问题分析&lt;/h2&gt;
&lt;p&gt;用 pacman 查询文件归属：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pacman -Qo /usr/lib/firmware/nvidia/ad103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/lib/firmware/nvidia/ad103 属于软件包 linux-firmware
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;说明当前系统中的 NVIDIA 固件文件属于旧的 &lt;code&gt;linux-firmware&lt;/code&gt; 包，而现在新的包 &lt;code&gt;linux-firmware-nvidia&lt;/code&gt; 也试图写入这些路径。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这是 Arch Linux 在 2025 年的一次“固件包拆分”导致的文件所有权迁移问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;多次尝试失败&lt;/h2&gt;
&lt;p&gt;我尝试了各种常规方式，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加 &lt;code&gt;--overwrite&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;单独安装新包&lt;/li&gt;
&lt;li&gt;重跑 &lt;code&gt;yay -Syu&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;都以类似冲突报错结束。&lt;/p&gt;
&lt;h2&gt;解决方案&lt;/h2&gt;
&lt;p&gt;最终使用以下方案成功解决：&lt;/p&gt;
&lt;h3&gt;1. 卸载旧包（跳过依赖检查）&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -Rdd linux-firmware
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;2. 安装新包&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -S linux-firmware linux-firmware-nvidia
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;3. 再次系统更新&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yay -Syu
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;验证&lt;/h2&gt;
&lt;p&gt;确认迁移成功：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pacman -Qo /usr/lib/firmware/nvidia/ad103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出变为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/lib/firmware/nvidia/ad103 属于软件包 linux-firmware-nvidia
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后 &lt;code&gt;reboot&lt;/code&gt;，没有出现任何问题。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/archbug1-zh.webp"/><enclosure url="https://picr2.axi404.top/archbug1-zh.webp"/></item><item><title>周记 Week50</title><link>https://axi404.github.io/blog/week-50</link><guid isPermaLink="true">https://axi404.github.io/blog/week-50</guid><description>2025-06-09 ~ 2025-06-15.</description><pubDate>Sat, 21 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;总的来说这一周的周记又是严重迟到，虽然事情并不是很多，但是不知不觉间时间还是从指尖流逝了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这一周我基本上做的事情就是在上周的基础上进行开发。上回书说到我使用 Sapien 替代了 Isaac 作为仿真的内核，这个环节实际上就是发生上一周的周一深夜，在完成了之后，接下来的几天，我在 Pick and Place 的单一场景，以及 Objaverse 规模的 Scaling 上面进行了大量的测试，测试结果都表明 Sapien 表现出了相较于 Isaac 良好的物理表现。&lt;/p&gt;
&lt;p&gt;基本上我们可以这样分析，导致物理问题的原因其实有三种不同的可能，仿真器本身的问题、机器人设置的问题以及物体的问题，而实际上因为在抓取一些物体的时候，物理问题的出现概率远大于其他的物体，因此我严重怀疑是因为 Isaac 的 Convex Decomposition 导致的仿真不稳定，从而产生了 Collision 的穿模现象，产生的排斥力破坏了 Articulation Root 带来的约束。上周其实也提到过这个事情，并且提出过，实际上只需要将一个 Object USD 分为 Collision Mesh 以及 Visual Mesh，并且将 Collsion mesh 作为使用 VHACD 分解的 Mesh，对于每一个 part 只设置 collision，并且关闭 Visual；而对于 Visual Mesh 不设置 Collsion，但是开启 Visual，从而分别管理 Visual 以及 Collsion 的方案。这个方案其实不难实现，使用 &lt;a href=&quot;https://github.com/JYChen18/MeshProcess&quot;&gt;MeshProcess&lt;/a&gt; 对于 Objaverse 的原始 GLB 文件使用整合以及 COACD 分解得到 Collsion 之后，用 Isaac Sim 的 asset_converter 进行转换，之后用我自己写的脚本进行了整合，但是并没有给物理带来明显的好处。当然，事实上这套方案是有价值的，假如说你的 Robot 没有明显的物理问题，那么这套方案可以简化你的物理判断，并且保持视觉的丰富，后面会写个博客来开源（~~画饼+1~~）。&lt;/p&gt;
&lt;p&gt;原生方案的解决都宣布失败之后，我就正式开始投身于 Sapien 方案的支持，短短两天我将 Sapien 方案在 Objaverse level 的 grasp detection 上面进行了部署，进行了大量的测试，并且在其之后将 Sapien 的内容插入到了我全部的代码中，使得数据生成以及测试都支持了 Sapien 作为物理内核。由于一个在脑中设想了很久的设计，因此其实一共的代码并没有多少，也不需要对于原来的代码进行大量篇幅的修改，全自动注册物体到 Sapien，并且只需要将 &lt;code&gt;world.step()&lt;/code&gt; 改成我自己的函数，一共也没有几行的修改，就成功完全了一切我们需要的内容。说起来这个应该也可以写博客。测试后可以完全之前的一切需求，看上去蒸蒸日上。经过测试之后，性能也没有损耗，甚至还有提升。这是因为我们在 Scaling Object Level 的物体的时候，往往是会选择 preload 大量的物体，而在这个过程中，因为涉及添加物体并且 cook collider，而 cook 这一过程不但需要大量的时间，还可能导致程序直接报错退出，如今因为物理已经被 sapien 接管，而且 VHACD 分解都是已经事先分解好了，因此就没有了相似的烦恼。&lt;/p&gt;
&lt;p&gt;在完成了基础的部署之后，另一个问题就出现在了眼前，也就是引入 Robotiq hand。了解 Sapien 以及 ROS 的读者应该都知道，二者对于机器人的共同描述是使用 &lt;code&gt;.urdf&lt;/code&gt; 文件，而这一文件的问题就在于其并不支持一个 link 存在两个 parent joint，也就是不能实现 Robotiq hand 的闭环结构。这在仿真中可以说是致命伤，在寻找了大量的框架，比如说 RoboVerse 以及 RoboTwin 之后，发现他们并没有给出类似的解决方案，最后还是在 ManiSkill3 中找到了解决方案，也就是手动添加一个 Drive（可以理解为一个在两个点直接加上一个弹簧一样的东西，因此设置弹簧无限小且力无限大就等价于一个 Joint 了）。不过尽管 ManiSkill3 提供了 Robotiq hand 的资产，但是和我们的 Robotiq hand 的 Joint name 并不是对齐的，因为避免闭环需要缺失一个 Joint，而我们的 USD 文件和他们的文件缺失的不是同一个，因此最后我花费了大量的时间研究如何标定这样一个参数，这又值得一个博客了。&lt;/p&gt;
&lt;h2&gt;学习与生活&lt;/h2&gt;
&lt;p&gt;因为下周一又要考试，所以说还是稍微进行了一些复习的，简简单单看了一遍老师给的复习提纲，就期待考试顺利了。现在对于我来说，大概率考试是可以过就好，其他的事情其实并不是那么的核心，因此混一混就好，希望上九十。至于和乐小姐的生活，因为乐小姐医学生的身份，大量的需要疯狂背诵的考试也是少不了的，我不忍心打扰，也希望她别挂科，于是这周依然只是在一起吃吃饭，散散步，然后简单贴贴一下，朴实无华。&lt;/p&gt;
&lt;p&gt;总的来说，这一周还是取得了巨大的突破的，可以说我现在对于 Simulation 的理解已经相较于之前提升了很多，同时对于代码来说，项目也向前完成了一个重大的迭代，可以说进步斐然，并且有望一举完全解决物理问题。下一周将会是畅快的解决 Issues 以及向前开发，只能说之前讨厌解决 Issues，觉得没干正事，但是如今在做正事了，确是我完全没有了解的领域，只能反复踩坑，没有正反馈。且看我快刀斩乱麻~&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-50-zh.webp"/><enclosure url="https://picr2.axi404.top/week-50-zh.webp"/></item><item><title>周记 Week49</title><link>https://axi404.github.io/blog/week-49</link><guid isPermaLink="true">https://axi404.github.io/blog/week-49</guid><description>2025-06-02 ~ 2025-06-08.</description><pubDate>Wed, 11 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;现在是下一周的周二，所以说这一次周记还算比较及时，不过现在有的事情也不严格按照周来划分了，主要还是因为假如说有令人兴奋的进展，但是我又没有及时记录下来此刻的心情，那未免不是一件可惜的事情。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这一周在科研上并没有取得显著的进展，基本上仍在对已有框架进行修补和调整，顺带也尝试了一些新的方向。工作主要集中在物理仿真问题的处理上，而这些问题确实令人沮丧，甚至可以说已经困扰了我很久，严重阻碍了数据生成的进度。&lt;/p&gt;
&lt;p&gt;我目前所用的平台是 Isaac Sim。这个平台背后的物理引擎是 NVIDIA 自家开发的 PhysX，照理说在物理仿真层面应该是稳定可靠的。然而在实践中，问题却接连不断。当前的任务是构建一个典型的 pick-and-place 数据生成流程：使用 Franka 夹取一个物体，然后将其放置到另一个物体的上方。乍看之下是个很简单的流程，但随着我们使用的物体资产种类增多，问题也随之浮现。&lt;/p&gt;
&lt;p&gt;我们使用的资产主要来自 Objaverse，这是一个极为庞大的三维模型库，素材丰富得近乎“用之不竭”。但正因如此，资产质量参差不齐：有的模型表面极其不规则，有的面片数量极其庞大。这些特性对渲染或许是好事，但对物理仿真却是灾难性的。仿真本质上是解决冲突和碰撞的过程，面片过多或几何不规整会制造大量微小碰撞，使得物理引擎难以稳定地解算，从而引发穿模、爆力等现象。&lt;/p&gt;
&lt;p&gt;最初我怀疑是控制方式的问题。Isaac 默认使用的 Franka 资产看起来没有任何问题，我也基本穷举并测试了所有可调的 joint 参数。在其他仿真器中，比如 Mujoco 或 Sapien，控制往往是标准的力控方式，且相关配置中会明确指定每个关节的 damping 和 stiffness。然而 Isaac 中，大多数关节甚至没有这两个值，MaxForce 的设定也异常地大。即使我尝试手动修改并保存这些参数，在仿真中的表现似乎并无改观。&lt;/p&gt;
&lt;p&gt;从控制效果来看也能明显感受到差异：在 Mujoco 中，当机械臂触碰到一个刚体时，它会自然地停下并发生形变，反馈出合理的关节变化；而在 Isaac 中，机械臂反而倾向于尝试“穿过去”，仿佛牛顿第三定理不能重充分反馈给机械臂。这种异常表现尤其出现在夹爪抓取动作中，不仅会穿模，有时更严重的情况是 finger 和 hand 直接断开连接——这也促使我在 NVIDIA 的官方论坛发帖求助，至今仍在等待可能的反馈。&lt;/p&gt;
&lt;p&gt;另一个问题出在碰撞建模上。比如 Mujoco 默认会将每一个 mesh 简化为 convex hull，这种方式尽管失去了对复杂结构的精细描述，但能极大提升物理稳定性。Sapien 除了支持 convex hull，还支持导入 VHACD 分解的结果作为 convex hull collision mesh。VHACD 或者 CoACD 可以将一个非凸体分解为多个凸包，并且作为碰撞体，而原来的 mesh 则仅作为视觉 mesh，从而用凸包模拟了非凸包，做到了兼顾。Isaac 支持 convex decomposition，并且提供了大量参数用于调节，然而这和 convex hull 的 vhacd 并非一种概念。事实上 sapien 里面也支持 non convex 的模式，但是在作为 dynamic 物体的时候，穿模现象甚至比 Isaac 还要研究无数倍。对于 Isaac 这边，将 Collsion 以及 Articulation Controller 的可能性进行排列组合，经过大量实验后我可以明确地说，这方面确实难以达到稳定的状态。&lt;/p&gt;
&lt;p&gt;面对这些反复出现且短时间难以解决的问题，我开始认真考虑是否可以从根本上绕开它们。与其继续在 Isaac 上反复试错，不如直接将物理内核替换为别的仿真器，只保留渲染和结构逻辑。这一思路部分受到 RoboVerse 启发。RoboVerse 的设计就是将控制逻辑和渲染前端解耦，物理仿真可以灵活切换。我一开始尝试使用 Mujoco 作为替代内核，甚至尝试直接移植 RoboVerse 中的 Franka 控制实现。但最终还是失败了，主要是 RoboVerse 框架过于庞大，而我自己的系统也比较复杂，难以兼容。即便是裁剪后的移植，也在细节控制上存在问题，结果不稳定、调试困难。&lt;/p&gt;
&lt;p&gt;就在我差点放弃这个方案的时候，我尝试使用 Sapien 替代作为底层仿真器。效果令人惊喜：从构建到测试，一晚上基本就完成了核心流程的替换。后续我也大概会继续完善 Sapien 后端的替代接口，部署到我的整体框架，并且看看后续是不是确实没有问题。相关的实现也很简单，或许可以单独开源一波，感觉主要还是思路。&lt;/p&gt;
&lt;h2&gt;学习与生活&lt;/h2&gt;
&lt;p&gt;这一周学习和生活其实可以放在一起说，毕竟这一周没有考试，所以相对来说轻松了不少。但是尽管如此还是有非常多的琐事需要处理。&lt;/p&gt;
&lt;p&gt;首先就是各种的申请，包括说要去分散实习，以及之前去米兰参加 ECCV 的报销，同时还有各种的实验课以及实验报告需要提交，只说虽然一件件都是小事，但是连在一起就让人非常的应接不暇。&lt;/p&gt;
&lt;p&gt;之后就是和乐小姐一起的生活。感觉上一周可能也就是一起出去吃个饭，毕竟她那边还有考试，我这边也有很多事要做，然后就是又拿了一些大米给她买裙子，总的来说也是很开心的。&lt;/p&gt;
&lt;p&gt;不过可以再延展的说一下这周发生的事情，感觉这些比较的申请之类的都是比较顺利的在进行，也希望最后可以顺利结束吧，同时之前给我非常好的朋友内推了 ailab，最后也算是推进去了，感觉到时候在上海可以一起共事一段时间，狠狠合作。&lt;/p&gt;
&lt;p&gt;这一周大概就是这样，实际上有一些内容是下一周才发生的，但干脆就直接写进去了，而最近其实很有创作欲，因为又一次高考结束了，或许写一点新生入学如何开始内卷的教程会很有趣，而且也可以给大家一些帮助，让大家适应大学的节奏。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-49-zh.webp"/><enclosure url="https://picr2.axi404.top/week-49-zh.webp"/></item><item><title>为 Astro 博客添加 Markdown 脚注支持</title><link>https://axi404.github.io/blog/add-footnote</link><guid isPermaLink="true">https://axi404.github.io/blog/add-footnote</guid><description>详细介绍如何在 Astro 博客中配置和自定义 Markdown 脚注功能，让你的技术文档更加专业。</description><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在写技术博客或学术文章时，脚注是一个非常有用的功能。它可以让我们在不打断正文流畅性的前提下，提供额外的信息、引用来源或补充说明。因为最近打算写一些 Talk 类型的内容，所以需要引用一些 Paper，所以需要添加脚注支持。&lt;/p&gt;
&lt;p&gt;简单研究了一下如何操作，因此介绍下如何在 Astro 博客中添加 Markdown 脚注支持。&lt;/p&gt;
&lt;h2&gt;什么是 Markdown 脚注&lt;/h2&gt;
&lt;p&gt;Markdown 脚注使用 &lt;code&gt;[^标识符]&lt;/code&gt; 的语法来创建引用，然后在文档的任意位置（通常在末尾）定义脚注内容。例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;这是正文内容[^1]，这里还有一个脚注[^note]。

[^1]: 这是第一个脚注的内容。
[^note]: 这是一个有名称的脚注。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;安装必要的依赖&lt;/h2&gt;
&lt;p&gt;首先，我们需要安装 &lt;code&gt;remark-gfm&lt;/code&gt; 插件，它提供了 GitHub Flavored Markdown 的支持，包括脚注功能：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 使用 npm
npm install remark-gfm

# 使用 yarn
yarn add remark-gfm

# 使用 pnpm
pnpm add remark-gfm

# 使用 bun
bun add remark-gfm
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;配置 Astro&lt;/h2&gt;
&lt;p&gt;接下来，我们需要修改 &lt;code&gt;astro.config.mjs&lt;/code&gt; 文件来启用脚注支持。&lt;/p&gt;
&lt;h3&gt;基础配置&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;import { defineConfig } from &apos;astro/config&apos;
import remarkGfm from &apos;remark-gfm&apos;

export default defineConfig({
  markdown: {
    remarkPlugins: [remarkGfm],
  },
})
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;自定义脚注样式&lt;/h3&gt;
&lt;p&gt;为了让脚注更符合中文用户的习惯，我们可以自定义脚注的标签和返回链接：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;import { defineConfig } from &apos;astro/config&apos;
import remarkGfm from &apos;remark-gfm&apos;

export default defineConfig({
  markdown: {
    remarkPlugins: [remarkGfm],
    remarkRehype: {
      footnoteLabel: &apos;脚注&apos;,
      footnoteBackLabel: &apos;返回内容&apos;,
      footnoteBackContent: &apos;↑&apos;
    },
  },
})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置选项说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;footnoteLabel&lt;/code&gt;: 脚注区域的标题（对屏幕阅读器友好）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;footnoteBackLabel&lt;/code&gt;: 返回链接的无障碍标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;footnoteBackContent&lt;/code&gt;: 返回链接显示的符号&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;完整配置示例&lt;/h3&gt;
&lt;p&gt;如果你的博客已经有其他 remark 和 rehype 插件，完整的配置可能看起来像这样：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;import { rehypeHeadingIds } from &apos;@astrojs/markdown-remark&apos;
import { defineConfig } from &apos;astro/config&apos;
import rehypeKatex from &apos;rehype-katex&apos;
import remarkMath from &apos;remark-math&apos;
import remarkGfm from &apos;remark-gfm&apos;

export default defineConfig({
  markdown: {
    remarkPlugins: [remarkMath, remarkGfm],
    rehypePlugins: [
      [rehypeKatex, {}],
      rehypeHeadingIds,
      // 其他 rehype 插件...
    ],
    remarkRehype: {
      footnoteLabel: &apos;脚注&apos;,
      footnoteBackLabel: &apos;返回内容&apos;,
      footnoteBackContent: &apos;↑&apos;
    },
  },
})
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;脚注语法示例&lt;/h2&gt;
&lt;h3&gt;基本脚注&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;这是一个包含脚注的句子[^1]。

[^1]: 这是脚注的内容。
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;具名脚注&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;这里引用了一篇论文[^paper2024]。

[^paper2024]: 张三, 李四. &quot;关于某某技术的研究&quot;. 技术期刊, 2024.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;多行脚注&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;这是一个复杂的脚注引用[^complex]。

[^complex]: 这个脚注包含多行内容。
    
    它可以包含：
    - 列表项
    - 代码块
    
    ```javascript
    console.log(&quot;脚注中的代码&quot;);
    ```
    
    甚至多个段落。
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;内联脚注&lt;/h3&gt;
&lt;p&gt;GitHub Flavored Markdown 还支持内联脚注：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;这是一个内联脚注^[内联脚注的内容直接写在这里]。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;注意事项&lt;/h2&gt;
&lt;h3&gt;开发服务器重启&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：当你修改 &lt;code&gt;astro.config.mjs&lt;/code&gt; 文件后，需要重启开发服务器才能使配置生效。这是因为配置文件的更改涉及到 Astro 的构建流程，无法通过热重载来更新。&lt;/p&gt;
&lt;h3&gt;脚注标识符&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;脚注标识符可以是数字、字母或组合&lt;/li&gt;
&lt;li&gt;标识符区分大小写&lt;/li&gt;
&lt;li&gt;建议使用有意义的标识符，特别是在长文档中&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;无障碍性&lt;/h3&gt;
&lt;p&gt;脚注功能已经内置了无障碍性支持：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;aria-describedby&lt;/code&gt; 属性链接脚注引用和脚注内容&lt;/li&gt;
&lt;li&gt;提供了对屏幕阅读器友好的标签&lt;/li&gt;
&lt;li&gt;支持键盘导航&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;高级自定义&lt;/h2&gt;
&lt;h3&gt;自定义返回符号&lt;/h3&gt;
&lt;p&gt;你可以使用各种符号作为返回链接：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;footnoteBackContent: &apos;↑&apos;     // 简洁向上箭头
footnoteBackContent: &apos;⤴&apos;     // 弯曲箭头  
footnoteBackContent: &apos;🔝&apos;    // 表情符号
footnoteBackContent: &apos;返回&apos;   // 中文文字
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;实际应用示例&lt;/h2&gt;
&lt;p&gt;在技术博客中，脚注特别适用于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;引用来源&lt;/strong&gt;：学术论文、技术文档的引用[^academic]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;补充说明&lt;/strong&gt;：不影响主要内容流的额外信息[^additional]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;版本说明&lt;/strong&gt;：特定技术版本的注释[^version]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;相关链接&lt;/strong&gt;：相关但非必要的外部资源[^links]&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;通过添加脚注支持，我们的 Astro 博客现在具备了更专业的文档撰写能力。脚注不仅让内容更加丰富，还保持了正文的简洁性。记住在修改配置后重启开发服务器，就可以开始在你的博客文章中使用这个强大的功能了。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[^academic]: 例如引用技术规范、RFC 文档等权威来源。
[^additional]: 比如技术细节、历史背景、替代方案等。
[^version]: 如 &quot;此功能在 Node.js 16+ 中可用&quot; 之类的版本特定信息。
[^links]: 相关的 GitHub 仓库、官方文档等链接。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/add-footnote-zh.webp"/><enclosure url="https://picr2.axi404.top/add-footnote-zh.webp"/></item><item><title>周记 Week48</title><link>https://axi404.github.io/blog/week-48</link><guid isPermaLink="true">https://axi404.github.io/blog/week-48</guid><description>2025-05-26 ~ 2025-06-01.</description><pubDate>Mon, 02 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { WebVideo } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;这一周的周记倒是算是及时，并没有隔太久。首先还是无事一身轻，虽然说实际上不存在完整的没事情做的时候，但是之前的一些内容还是清理干净了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;首先还是先说一下科研的近况。这一周其实代码方面没有很多的更新，主要是因为 CVPR Poster 以及 Video 的 DDL 临近了，所以需要准备一下最终材料的事情。&lt;/p&gt;
&lt;p&gt;我的这篇工作 GenManip 一直都是属于一个 CVPR 的 Scope 早已经 Ready，但是因为一直向前开发，所以其实很难有时间回头去整理一些材料以及准备 release 相关的事情。&lt;/p&gt;
&lt;p&gt;因此这一周也是做了很多准备，做了大量的PPT动画，以及整理的很多的图和视频。这件事情做完之后，顿时感觉轻松了不少，下周应该会做点别的事情了。&lt;/p&gt;
&lt;p&gt;假如细看这一次视频，其实会发现不少的地方都有一些图表是动态的，这里其实也是一个细节。因为一直熟练使用前端的原因，所以我感觉一些炫酷的效果 PPT 很难做出来，但是前端就会很快，恰好又有 cursor 以及 Claude4，都是对于前端非常擅长的，因此最后是直接用前端写了网页并且做了录屏。这方面感觉也可以写些博客，关于 cursor 的常规使用以及用来科研作图的实战。&lt;/p&gt;
&lt;p&gt;除此之外，感觉认识的朋友们也要开始接触具身了，自己的一些理解和积累肯定不能落下，现在也是准备重新启动 paper reading 的 blog。只不过现在确实处于一个低谷期，感觉看任何的论文都没有兴趣，或者直白来说，就是感觉大家都在排列组合，虽然事实上这些组合之下的规律和 insight 是十分关键且重要的，但是还是让我打不起精神。&lt;/p&gt;
&lt;h2&gt;学习与生活&lt;/h2&gt;
&lt;p&gt;这周基本上一周的时间，除了在处理其他同学的反馈之外，就是考了一门考试，以及一直在做这个视频。所以说生活上虽然到了端午节，但是也并没有怎么过，也就是和乐小姐一起出去吃了饭。&lt;/p&gt;
&lt;p&gt;感觉除此之外，其实目前的实验课也比较有意思，使用小车做导航，虽然基本上代码都是老师给出来的，但是用 ROS 跑通，加上我这方面之前的积累，感觉也值得分享。&lt;/p&gt;
&lt;p&gt;同时我也把我的博客页面重构了一下，之前是 follow 的一个别人的主题，叫做 astro-pure，这个主题看上去不错，但是存在一个小问题，就是他的全部主题包都是写在 packages 目录下的，通过将 &lt;code&gt;packages.json&lt;/code&gt; 里面的包路径改为本地途径，确实是可以自由修改主题的组件，但是每次还都是需要重新安装一遍。不过 astro 其本身的结构就支持 components，所以把它都封到了 src 文件夹里面，就好了很多。&lt;/p&gt;
&lt;p&gt;同时也可以给自己做了一个 publication 界面以及一些别的效果，包括说鼠标的粒子点击效果，以及现在假如说你的浏览器没有聚焦在我的博客上，那么网页的标题也会改变。以及还做了一个 footnote。&lt;/p&gt;
&lt;p&gt;这一切基本上都是 Cursor 实现的，确实实力很不一般。&lt;/p&gt;
&lt;h2&gt;Todo&lt;/h2&gt;
&lt;p&gt;所以其实不能发现我最近有很多想做的事情，在这里总体梳理一下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;写一下具身相关的第一篇博客，应该是仿真器和数据主题的。这方面因为已经支持了 footnote，所以一些引用应该方便不少&lt;/li&gt;
&lt;li&gt;写一下我自己 cursor 的使用指南以及实战案例&lt;/li&gt;
&lt;li&gt;继续更新我的 paper list，支持更多的内容，及或许需要写一个脚本来批量创建 paper list 的模版，不然现在每次都要重新打字那个框架，让我感觉十分麻烦&lt;/li&gt;
&lt;li&gt;或者写一下 ROS 的博客&lt;/li&gt;
&lt;li&gt;高考临近了，那么新一年的学生又要来了，或许西安交大生存指南也可以重写一下&lt;/li&gt;
&lt;li&gt;Isaac 101 系列博客，关于引入机器人的方面可以写一写&lt;/li&gt;
&lt;li&gt;网站部署指南的 2.0 版本。第一个版本其实推荐使用 vercel，但是后面发现用户一旦上来之后边缘访问数量就炸了，上周直接吞了我 80 刀，访问了 30M 次，就是因为挂载了绿群的 ddl。目前来看其实 cloudflare CDN + Github Pages 已经很不错了&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;目前大概就是这些，可以说最近创作欲大发，希望可以做点有价值的东西&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-48-zh.webp"/><enclosure url="https://picr2.axi404.top/week-48-zh.webp"/></item><item><title>周记 Week47</title><link>https://axi404.github.io/blog/week-47</link><guid isPermaLink="true">https://axi404.github.io/blog/week-47</guid><description>2025-05-19 ~ 2025-05-25.</description><pubDate>Mon, 02 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一次的周记也是迟到了，就像是这一周也确实做了不少事情，但是也没有多少有意义的。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;要是给这周一个关键词的话，那毫无疑问就是考试了。所以甚至你可以看到我在第一项写的已经不是常规的科研，而是学习了。说是考了机器人学，不过感觉考得不太行，说起来就让我想到了之前学的计算机视觉考得很差，貌似在我所擅长的这些领域最后发挥的都不是很可以，可能这是我为什么喜欢科研的原因吧，成果论高下，而不是一些固定的考试。&lt;/p&gt;
&lt;p&gt;这一科考完之后就还有一科在下周，再之后就是很多周之后的一个在期末周考试的课程了，正好按照这段时间的间隙可以做一下本来应该收尾的事情。&lt;/p&gt;
&lt;p&gt;不过当然需要提起的，虽然考试结束了，但是仍然有很多的让人烦躁的实验课和报告，尽管我可以说是最擅长实验课的了，但是每次写报告还是让人感觉很麻烦，毕竟还要重复把自己可以拿很多时间做出来的事情，再用一个冗长的文字来去表述一遍。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;因为一直在忙着复习，所以科研上面也没有什么进展，硬要说的话可能是处理了很多同学提的问题，然后继续大量的生成数据。上次说的一些改版后产生的后续隐患也已经被我解决掉了。&lt;/p&gt;
&lt;p&gt;与此同时的话，因为 CVPR 也临近了，虽然说我不会亲自过去，但是一些相关的内容还是要提供的，包括说 Poster 以及视频，这可能是我这两周会主要做的事情，以及是将最后的整个工作发出来，再往后就可以直接躺平，做点自己想做的事情了，不然这个工作总是压在手里面比较难受。&lt;/p&gt;
&lt;p&gt;之前一个很大的节奏就是，一直有新的事情需要去开发以及解决，但是老的事情压在这里，同时负责老的事情的一些同学也已经去做新东西了，就导致我最后处于一个孤立无援的状态，而只靠我把全部内容处理完又不太可能，毕竟时间紧迫。&lt;/p&gt;
&lt;p&gt;虽然将这些收尾的事情解决掉并不会本质上减少我的工作量，但是我之前可能也说过，我存在一个主要的心理问题，当有很多事情没有做完的时候就会感觉生理性的不舒服，这确实会拖慢我的效率。&lt;/p&gt;
&lt;p&gt;但同时也是在筹划中的事情，就是可能打算写一些像是 Talk 之类的内容，以及继续更新 Isaac 相关教程。&lt;/p&gt;
&lt;p&gt;其中 Isaac 在最近的迭代过程中也发现很多问题，主要还是它的物理仿真真的是漏洞百出，导致会有不断的 issue 出现，而且并没有本质的解决方法，但是其他内容还是可以更新的，毕竟无论是背靠 OpenUSD 生态，还是本身 Nvidia 的技术力，这个平台必然是未来可期，只是基于 PhyX 进行的物理模拟实在是太烂了。之前看新闻说 Nvidia 推出的新物理仿真，Newton，感觉大有可为，而且感觉 Isaac Newton，都已经这样致敬了，不整合进去天理难容。&lt;/p&gt;
&lt;p&gt;这些内容迎刃而解之后，整个的迭代会加快无数倍，因为代码对我来说真的是很简单了。&lt;/p&gt;
&lt;p&gt;至于其他的 Talk，还是打算做得有深度一些，以及打磨的时间长一点，确保每一篇都是精品。这个想法其实是因为和一些同僚们聊天的时候，大家其实会觉得我的一些看法是有一些见地的，因此或许系统整理一下，并且写出来，一方面能对别人有帮助，一方面也可以归纳一下我发散的思考。&lt;/p&gt;
&lt;p&gt;目前可能第一篇是打算写一下当前的数据之路以及仿真，包括自己将来应该何去何从的一些想法，在具身这个领域数据确实很重要，也是绕不开的一环，我觉得要是从源头来去凝视这个领域，必须要从这里入手。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;既然上面已经讲到了这周的水深火热，因此生活部分自然就寥寥无几了。主要还是和小姐一起开心的生活吧，但是因为一直在复习，她那边也有考试，所以这周确实没有见多少次。&lt;/p&gt;
&lt;p&gt;每周都平淡如水，但其实我感觉一些日子已经快要过去了，马上就要进入新的阶段，希望不会太远。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-47-zh.webp"/><enclosure url="https://picr2.axi404.top/week-47-zh.webp"/></item><item><title>周记 Week46</title><link>https://axi404.github.io/blog/week-46</link><guid isPermaLink="true">https://axi404.github.io/blog/week-46</guid><description>2025-05-12 ~ 2025-05-18.</description><pubDate>Tue, 27 May 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以其实又是很久之后的一次周记了，实在是因为事情太多，我已经彻底忙得团团转，感觉完全没有休息下来的时间，但是这种事情肯定要尽量去避免。记下这篇周记的时候已经是两周后的星期二，这时候我在准备倒数第二门的考试，这门考试过后又是长久的没有考试的时间，就可以去做好很多事情，并且将之前各种欠下的烂帐收一下尾，但是这些烂帐的梳理还是等到下一周的周记，毕竟这一周其实是非常积极的主色调。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;还是因为 release 的相关需求，所以说又对整个项目的文件组织方式以及框架进行了大刀阔斧的修改，现在整个项目的文件组织成为了一种更合理的形式。&lt;/p&gt;
&lt;p&gt;具体说可以简单讲一下的是，之前项目中全部的资产以及各种内容都是被堆放在一个叫做 grasp_vla_assets 的文件夹里面的，虽然说我已经做了很好的隔离操作，使用相对路径进行各种文件管理，可以让代码在各种环境下轻松进行迁移，对于程序本身来说，其实不会引发任何错误，但是可读性非常差，所以进行了整理，现在看上去确实优雅了很多。同时另外一点是对于配置文件的结构调整，这部分因为还没有彻底公布，所以就暂且不详细说了，但大概可以提及的是这种新的结构可以使得后续的开发更加的方便，并且生成更加自定义的多样化数据。&lt;/p&gt;
&lt;p&gt;只能说得益于我之前开发的一些思想，也就是在这些思想确实奏效了，这两个看上去修改巨大的功能，其实并没有花费很长的时间。具体来说，其实主要就是分层的解耦思想。本身我的程序是 functional programming，而并非大多数人都喜闻乐见的 OOP，这是因为整个程序开发者导向的，而非用户导向。对于用户的使用来说，我们已经提供了 Config，可以用低侵入的方式来修改整个的框架，而包裹在 Config 之后的第一层就是将其解析为实际的格式，在经过串行的流式处理，最后变成可以执行的动作信息。上面说的包括进行配置文件的修改以及路径的修改，其实最后也就是将某一层的几个函数改掉就好了（虽然物体这方面要改的函数其实很多，毕竟要实现更丰富的功能），因为 FP 的思想只需要确保函数的输入输出和之前是对齐的就可以了，不需要进行其他的安全性检查，函数本身的可访问域是有限的。&lt;/p&gt;
&lt;p&gt;在进行这些修改之后，也是进行了大量的测试，并且让整体的程序保持和之前一致的性能，但同时自由度又大大提高了。&lt;/p&gt;
&lt;p&gt;但事实上在写的过程中也有其他同学给我的反馈很重要，在这里也简单分享一下，毕竟感觉纯是技术思想，而不是技术方案。就是我在 FP 的过程中，虽然看上去是这么美好，但是实际上函数的很多传参是会把一个巨大的 dict 进行传入和传出，这导致假如我的程序在函数里面进行了一些修改，我根本不知道会发生什么，而且毫无疑问是有问题的。最优雅的方式是在函数中创建临时变量，并且把真正的修改放到输出之后，用户在使用这个函数的时候，不用担心里面的东西会不会修改自己已有的一些内容。&lt;/p&gt;
&lt;p&gt;感觉我在程序方面的积累，其实已经很多了，最近也在和朋友们一起讨论一些思想的东西，不断进步不断提升自己吧，感觉具身智能有的时候发展的方向很奇怪，现在虽然其实已经卷到了白热化，但是感觉事实上还是有很多蓝海可以去做。只是现在很多人都 focus 在某个立刻产品化的巨大模型上面，所以看上去很像红海吧。&lt;/p&gt;
&lt;p&gt;以及可能我们实验室目前还是在一个早期的阶段，已经有了非常多中间结果，我们也在做那个最终的大的模型，不过现在还是进行时，可能等到做出来之后，我们才能做更加科研以及发掘本质的那些 study，这些感觉是目前学界做得不多的，而是只用模型去 scaling 并且展示性能，技术方案总是差不多。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;虽然说科研已经很累了，但是这周的学习也是热火朝天，虽然没有到考试周，但是考试都已经陆续开始了，也是有点难绷了。这周日考了移动计算，这是一个结合的计算机网络以及一些其他奇怪的东西的课程，不过感觉主要还是背诵，至于下周还要考机器人学，而在下周还有一门人工智能的平台什么的，也是很焦灼了。&lt;/p&gt;
&lt;p&gt;复习给我的压力，让我不能专注在科研上面，看着我的一些朋友们，不少已经是大四，现在可以随心所欲的科研，而且没有学习的琐事也可以做到 work life balance，我也只能是心生羡慕了。&lt;/p&gt;
&lt;p&gt;不过也可能是每个阶段都有每个阶段的烦恼吧，到了他们那个阶段又会为自己的去向烦恼，或者可能因为论文的发表之类的感到焦虑，不知道我在那个时候就会遇到什么新的问题，希望至少比现在好吧。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;说起来差点忘记的是，这周还正好过了我的生日，那天基本上也是一天在写程序中度过，给我发生日祝福的朋友寥寥无几，虽然貌似也是我也没有跟他们说我的生日的原因。&lt;/p&gt;
&lt;p&gt;好在其实还有乐小姐，过来陪我，然后让我度过了比较开心的一天，不过感觉随着成长了之后，对于生日也没有那么多的期待，仿佛就是生命中又一个平凡的一天，而自己在这时间中悄然又过了一岁。&lt;/p&gt;
&lt;p&gt;似乎小时候总是会期待生日到来，可能是因为有朋友的祝福，或者父母送来的礼物，可是那天心里有一个触发器，可以让绷紧的神经放松下来。不过也是今时不同往日，如今自己有了工资，平时就能给自己买起比较贵的东西，生日礼物的实际价值也就降低了，更多的是心灵上的抚慰，现实中貌似除了乐小姐，也没交到很多朋友，加上还有各种琐事和压力，那种喜悦也就自然而然在生活中淡化了。&lt;/p&gt;
&lt;p&gt;可能就像那句话说的一样，很多年后你才发现，那其实是你人生中最棒的一天，那样的一天永远不会再有了。此刻的我走在大学的校园里，我已经大三，新一年高考又要来了，不禁想到当初高三的我，万千感慨淤积在心中，终是巍然长叹。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-46-zh.webp"/><enclosure url="https://picr2.axi404.top/week-46-zh.webp"/></item><item><title>周记 Week45</title><link>https://axi404.github.io/blog/week-45</link><guid isPermaLink="true">https://axi404.github.io/blog/week-45</guid><description>2025-05-05 ~ 2025-05-11.</description><pubDate>Sun, 18 May 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;经典起手了，又是很久之后的一次周记，还是不断地拖延，还是非常多的事情，一眼望不到边。朋友们都去 NeurIPS 了投稿了，不过显然我上一个工作还在进一步的整理过程中，所以也没有什么显著的成果，倒是越发焦虑了起来。&lt;/p&gt;
&lt;h2&gt;科研与学习&lt;/h2&gt;
&lt;p&gt;这周整体来说还是在不断地处理各种的 Issue，五一的结果表明我参与的论文估计投稿 NeurIPS 2025 是没有希望了，所以就继续老老实实打磨沉淀。本身的框架和 mentor 又进行了不少的讨论，并且给出了很多的修改，大多数会在下一周进行尝试，并且看看能不能真的做出一些成果。因为又是隔了一周才开始做这些事情，所以说一些事情都不太记得清楚了。不过一个显然的事情是，我需要尽快的把之前的一些工作给整理好，不然真的太慢太慢了。&lt;/p&gt;
&lt;p&gt;不过话又说回来，其实现在其他的乱七八糟的事情也很多。我预期去的学校，也就是上海人工智能实验室的联培项目，发现多出了机试的环节，而且我也必须要参加，对于我这个并不是算法竞赛出身的纯科研选手，就需要再去复习一下算法。尽管我对于我的科研能力还是很自信的，但是相对来说我对于算法这方便确实了解就算不上多了。我其实一直都觉得算法这个领域是一个十分有趣的领域，但是确实一直没有精力去深度学习这些内容，因此多次的尝试都以失败告终，如今看来必须是需要有能力刷一些题并且写出来一些东西了，这个感觉就没什么别的办法了。&lt;/p&gt;
&lt;p&gt;已经太久没有集中注意力写一些东西了，感觉这几周的周记也是写的无聊，没有什么主线，也没有做什么出彩的东西，感觉下周也是这个状态，毕竟考试确实是越发临近了。不过感觉再下周还是要下定决心清理掉一些堆积已久的内容的，不然真的感觉要被这些东西给淹没了。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活方面，其实还是和乐小姐在一起，不过因为时间隔得太远，确实也没有记下来什么甜蜜的瞬间，只是依稀记得陪在她的身边的时候总是感觉很幸福。&lt;/p&gt;
&lt;p&gt;和我在学界的朋友们也聊的比较开心吧，不过貌似也没什么记下来的事情。我们聊的内容还是比较深入的，不过貌似还是不放在这里说比较好。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-45-zh.webp"/><enclosure url="https://picr2.axi404.top/week-45-zh.webp"/></item><item><title>周记 Week44</title><link>https://axi404.github.io/blog/week-44</link><guid isPermaLink="true">https://axi404.github.io/blog/week-44</guid><description>2025-04-28 ~ 2025-05-04.</description><pubDate>Tue, 13 May 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这又是一次隔了一周才记的周记，不过索性是没有什么可以记下来的东西，不知道是一种庆幸还是可惜。&lt;/p&gt;
&lt;p&gt;总的来说，这一周依然没有发生什么大事情，毕竟是假期，或者说是五一假期占据了这周的大部分时间，所以说实验室的一些同学们都没有什么回应，我也就自己按照自己的想法继续往后写代码了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;首先还是说一下科研吧，虽然我已经在尽力的去做 release 的工作了，确实还是有不少不完善的地方，不过感觉现在也没什么不能说的，毕竟论文都已经中稿了。&lt;/p&gt;
&lt;p&gt;整体来说，目前做的是一个仿真的框架，在 Isaac Sim 的基础上做的，CVPR 主要依靠这个框架做了一个 Bench，并且可能有一些其他的支持，然后后续会在这一次的基础上不断地去发布新的内容（超级超级多，都是写完了的不是画饼，大家可以期待）。实际上根据我目前的完成的内容来看，CVPR 的内容说是九牛一毛也不为过，基本上从目前的代码里面 checkout 之后删删减减就可以了，因为目前的一些任务考验的是模型的 reasoning 能力，所以说主要是可能支持一些 VLA 或者双系统，当然也对于 VLM 做了兼容。&lt;/p&gt;
&lt;p&gt;不过现在的问题其实在于尽管这么说，但是也是要支持一些 learning based 的模型，这些内容都是完备的，但是因为里面可能有一些环境配置及目录的设置问题，感觉 cleanup 目前 pending 给别的同学做，效率也不高。&lt;/p&gt;
&lt;p&gt;当然包括也会发现，目前基本上全部的仿真工作都会有同样的问题（这里的工作可能指的是在仿真器这个 level 上面搭建的内容，可能是一个 Bench，或者是一个 Scaling 数据的工具），就是一般来说文档不是很清晰。有的时候是会有文档的，也写得还好，比如说 Robocasa 或者 GRUtopia，但是大多数可能只有一个 README 或者几个 Markdown 的组合，这意味着要不然这个内容不是很丰富，要不然就是对用户很不友好，无论如何都是要做好修改的。mentor 确实对于这方面很严格，不过目前一遍遍的打磨之后还是很有成就感的，就是希望不要被人抢先摘了果实。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;接下来就是生活了，但其实说白了并没有什么生活。因为就像上面说的一样，尽管实验室里一些同学在放假，但是我这几天还是保持着每天都在写代码的强度，或者说除此之外，我貌似也没有什么其他事情可以做。&lt;/p&gt;
&lt;p&gt;可能主要的休闲还是和乐小姐一起去逛逛街，然后吃了饭，每一次和她在一起的时间总是能让我内心平静，感觉也是很不错的。&lt;/p&gt;
&lt;p&gt;我给自己买了几件衣服，虽然说是自己买的，最后还是让家里报销了。现在我已经有三件中性偏女一些的上衣了，不过看上去确实更加的版型好一些，只不过抬手可能会露腰，不过既然腰看上去也还行，所以无所谓吧，嗯。&lt;/p&gt;
&lt;p&gt;我和几个朋友的小群在流行 brainrot，或者国内接地气的翻译叫做 AI 山海经，就是用意大利语便写出来的怪物的名称，然后用图像生成来生成，之后配一段背景故事之类的，但是用这种无厘头显得比较有趣。网上有人用这种东西做了一些问答题的格式，类似于为这个角色和图片的对应关系，比较神秘。这些东西确实很魔性，这个问答题看上去也很有趣，所以我干脆直接就写了一个项目来做这个事情，大概就是一个网页，你可以进去答题，链接是&lt;a href=&quot;https://brainrot.axi404.top&quot;&gt;这里&lt;/a&gt;，大家有兴趣可以玩玩。&lt;/p&gt;
&lt;p&gt;除此之外，朋友们还在建立学术组织，我帮他们搭建了一个非常非常好玩的网页，但是毕竟还没有正式的官宣，大家可以期待一下。&lt;/p&gt;
&lt;p&gt;虽然说生活，但好像又说回了项目和一些代码的工作，不过这些内容都是有趣性质的，倒也无所谓。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;这也不能说是有趣的一周，不过假期的几天在适当的科研代码、有趣的项目代码和乐小姐的陪伴中度过，也算是一种很不错的调剂了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-44-zh.webp"/><enclosure url="https://picr2.axi404.top/week-44-zh.webp"/></item><item><title>周记 Week43</title><link>https://axi404.github.io/blog/week-43</link><guid isPermaLink="true">https://axi404.github.io/blog/week-43</guid><description>2025-04-21 ~ 2025-04-27.</description><pubDate>Wed, 30 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以又是很久之后的一次周记，但是这次并没有隔太久，只是因为懒。这周的周记也算是四月份的最后一篇周记了，主要的基调其实是复习。&lt;/p&gt;
&lt;h2&gt;科研与学习&lt;/h2&gt;
&lt;p&gt;这周可以说又一次回到了昼夜颠倒的模式，每天都是熬到五六点，然后简单睡个三四个小时之后继续。周三下午一个考试，周六上午一个考试，关键是我还差点记错时间，记成周六下午。当时假如不是为了睡觉，在睡觉之前为了定闹钟，看了一下考试的时间，直接就错过了。还记得很清楚当时看了一眼之后立刻就清醒了，毕竟我给自己定的复习计划，周六上午被安排的满满当当，结果一下子就要压缩到几个小时（当时大概是五点），不过还好最后也是赶上了，勉强复习完，题目也不是很难，可以说是有惊无险。&lt;/p&gt;
&lt;p&gt;因为复习的原因，所以这个星期的科研进度几乎为零，基本上都是在帮助其他同学提供支持，甚至说强化学习考试结束之后，尽管之后的考试在周六，但是我还是拿了两天时间去写程序，也算是解决了其他同学的不少 issue，希望可以对于整体的进度有一些帮助。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活上没什么特别的事情，毕竟一直在学习，也没有和乐小姐约会。本来是打算周六晚上考完可以稍微放松一下的，但实在太困了，回到寝室直接倒头就睡。现在回头看，这周虽然从时间安排上来说“非常紧凑”，但其实真正做成的事情不多。科研方面进度为零，生活也没有什么亮点，甚至连好好睡觉都做不到。好在考试算是结束了，周日报复性的写了很久的代码，把很多事情都解决掉了。&lt;/p&gt;
&lt;p&gt;所以说这大概就是四月份全部的事情了，五月份开始，还有四门考试，但是彼此之间多了一些时间间隔，也就可以把更多的时间投入到科研中。我也不奢求 work life balance 了，但是至少有一些时间去 look back，看那些自己读过的 paper，或者了解的技术，记下来~~水一些博客~~。感觉有的时候向前干得太快，是时候停下来，回头看一看，总结一下，再向前走了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-43-zh.webp"/><enclosure url="https://picr2.axi404.top/week-43-zh.webp"/></item><item><title>周记 Week42</title><link>https://axi404.github.io/blog/week-42</link><guid isPermaLink="true">https://axi404.github.io/blog/week-42</guid><description>2025-04-14 ~ 2025-04-20.</description><pubDate>Wed, 30 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以又是很久以后的一次周记了，这一周还是比较极限的，但是也没有什么好记录的东西。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;科研其实还是老样子，还是正常的推进整个的任务线，然后尝试去做一些东西，但是不得不说，确实之前论文的发表已经等了太久了，我现在也在陆续迭代一些材料，然后给到这边网页搭建以及代码的 refine。&lt;/p&gt;
&lt;p&gt;最近科研的进度只能说是马马虎虎，平时也有在读一些论文，但是确实时间已经被占满了，都是在帮同学做一些事情，导致留给自己的时间确实算不上多，但貌似也就只能这样了。其实有的时候还比较羡慕那些协作者，大家在论文投稿的期间把自己的本分工作做好，之后就可以再继续做自己想做的事情，但你确实在主力去做这个项目，你就必须要从头到尾一直负责，也是合理的吧。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;这周又陷入到了经典的焦虑中，因为要在兼顾科研，同时还需要去做课内的东西，尤其是考试已经迫在眉睫了，但是却没有空抽出时间来复习。&lt;/p&gt;
&lt;p&gt;所以只能说是经典的昼夜颠倒，然后去压榨自己的时间，来尝试把两边都做好，不过实际上只结束的时候复习还没有开始多少，可以说是把该看的都看了一些，但是对于考试那种背诵级别的要求，现在却完全没有开始。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;科研和学习已经让我的生活变得很紧张了，所以基本上没有做什么其他的事情，也就是和乐小姐出去逛街，然后一起吃饭。两个人在一起的时间还是很快乐的，尤其是乐小姐开心地来找我，然后扑进我的怀里，那种感觉真的很棒，能够让一整天的阴霾都烟消云散。&lt;/p&gt;
&lt;p&gt;我总是说马上就可以闲下来，闲下来之后就有很多东西可以去做，但是马上复马上，明日复明日。科研之路貌似就是如此，总有做不完的事情，总有需要去推进的进度，总有很多很多需要去学习的东西。当你做完了一件事情，马上就可以有下一件事情；当你投稿了一篇论文，马上又有下一个会议。我有的时候真的想慢下来，陪陪女朋友，靠在椅子上听一首歌，然后眯着眼睡上一个下午。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-42-zh.webp"/><enclosure url="https://picr2.axi404.top/week-42-zh.webp"/></item><item><title>RL 学习笔记（14）：基于人类反馈的强化学习 (RLHF)</title><link>https://axi404.github.io/blog/rl-note-14</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-14</guid><description>基于人类反馈的强化学习 (RLHF)</description><pubDate>Mon, 21 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;背景：大型语言模型 (LLM) 与对齐挑战&lt;/h2&gt;
&lt;h3&gt;大型语言模型概述&lt;/h3&gt;
&lt;p&gt;近年来，大型语言模型 (Large Language Models, LLM) 如 GPT 系列取得了显著进展。其发展依赖于 Transformer 架构、大规模预训练、指令微调以及与人类意图的对齐等关键技术。这些模型（如 ChatGPT, GPT-4）展示了强大的语言理解、生成和推理能力。&lt;/p&gt;
&lt;p&gt;LLM 的训练通常包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;预训练 (Pre-training)&lt;/strong&gt;：在海量文本数据上进行无监督学习，构建掌握语言知识的基础模型 (Base LLM)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对齐微调 (Alignment Fine-tuning)&lt;/strong&gt;：通过进一步训练使模型行为符合人类的指令和价值观。这通常包含：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;有监督微调 (SFT)&lt;/strong&gt;：学习遵循指令。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励建模 (RM)&lt;/strong&gt;：学习人类偏好。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习 (RL)&lt;/strong&gt;：根据学习到的偏好进一步优化模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;指令微调 (Instruction Fine-tuning / SFT)&lt;/h3&gt;
&lt;p&gt;SFT 使用高质量的“指令-回答”对数据微调预训练模型，使其能够理解并执行各种任务指令，学习期望的输出风格。这是对齐过程的重要一步，通常作为 RLHF 的起点。&lt;/p&gt;
&lt;h3&gt;人类对齐 (Human Alignment)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：确保 LLM 的行为符合人类的意图和价值观。这通常概括为 &lt;strong&gt;3H 原则&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;有用性 (Helpfulness)&lt;/strong&gt;：提供准确、相关、有创造性的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真实性 (Honesty)&lt;/strong&gt;：避免捏造信息或误导。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无害性 (Harmlessness)&lt;/strong&gt;：不生成冒犯性、歧视性或危险内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;挑战&lt;/strong&gt;：人类价值观复杂、主观且难以直接形式化为机器可优化的目标函数。&lt;/p&gt;
&lt;h2&gt;基于人类反馈的强化学习 (RLHF) 概述&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;RLHF (Reinforcement Learning from Human Feedback)&lt;/strong&gt; 是一种利用强化学习框架，并结合&lt;strong&gt;人类反馈&lt;/strong&gt;来解决 LLM 对齐挑战的关键技术。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：我们很难直接编写一个奖励函数来精确定义什么是“有用”、“真实”和“无害”。RLHF 的巧妙之处在于：
&lt;ol&gt;
&lt;li&gt;不要求人类直接打分，而是让人类对模型生成的多个输出进行&lt;strong&gt;比较和排序&lt;/strong&gt;（表达偏好）。&lt;/li&gt;
&lt;li&gt;训练一个&lt;strong&gt;奖励模型 (Reward Model, RM)&lt;/strong&gt; $r_\phi(x, y)$ 来学习和模拟人类的这种偏好模式。&lt;/li&gt;
&lt;li&gt;将学习到的 RM 作为奖励信号，使用&lt;strong&gt;强化学习&lt;/strong&gt;（通常是 PPO）来微调 LLM，使其生成的内更容易获得 RM 的高分，从而间接地符合人类偏好。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;发展与应用&lt;/strong&gt;：RLHF（尤其以 InstructGPT/ChatGPT 为代表）已成为训练高质量、对齐良好的对话式 AI 模型（如 ChatGPT、Claude 等）的标准流程之一。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;RLHF 实现流程：三阶段方法&lt;/h2&gt;
&lt;p&gt;RLHF 通常包含以下三个主要阶段：&lt;/p&gt;
&lt;h3&gt;监督微调 (Supervised Fine-tuning, SFT)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：为 LLM 提供一个良好的起点，使其初步具备遵循指令和生成符合格式要求回答的能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：使用高质量的“提示-回答”对数据对预训练 LLM 进行标准微调。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：SFT 模型 $\pi^{SFT}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;奖励模型训练 (Reward Model Training)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：训练一个模型 $r_\phi(x, y)$，能够根据人类偏好对（提示 $x$，回答 $y$）对进行打分。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据收集 (人类反馈)&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;选取一批提示 $x$。&lt;/li&gt;
&lt;li&gt;使用 SFT 模型（或后续 RL 模型）为每个 $x$ 生成多个不同回答 $y_1, y_2, \dots, y_k$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类标注者&lt;/strong&gt;对这些回答进行比较排序，最常用的是&lt;strong&gt;成对比较&lt;/strong&gt;：选出更好的回答 $y_w$ (winner) 和较差的回答 $y_l$ (loser)。&lt;/li&gt;
&lt;li&gt;收集大量偏好数据 $D = {(x, y_w, y_l)}$。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励模型 (RM)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;架构&lt;/strong&gt;：通常基于预训练 LLM，修改顶层为输出一个标量分数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：常用&lt;strong&gt;成对排序损失 (Pairwise Ranking Loss)&lt;/strong&gt;，鼓励 $r_\phi(x, y_w) &gt; r_\phi(x, y_l)$：
$$
loss(\phi) = - \mathbb{E}&lt;em&gt;{(x, y_w, y_l) \sim D} [\log(\sigma(r&lt;/em&gt;{\phi}(x, y_w) - r_{\phi}(x, y_l)))]
$$
其中 $\sigma$ 是 sigmoid 函数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练&lt;/strong&gt;：使用偏好数据集 $D$ 对 RM 参数 $\phi$ 进行有监督训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：训练好的奖励模型 $r_\phi$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;强化学习微调 (RL Fine-tuning with PPO)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：使用 RM 作为奖励信号，通过 RL 进一步优化 SFT 模型 $\pi^{SFT}$，得到最终的对齐模型 $\pi_\theta^{RL}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RL 设定&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;策略 (Policy)&lt;/strong&gt;：LLM $\pi_\theta(y|x)$，通常初始化自 $\pi^{SFT}$，参数为 $\theta$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作空间&lt;/strong&gt;：词汇表中的词元 (tokens)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励&lt;/strong&gt;：在生成完整回答 $y$ 后，由 RM 计算奖励 $r(x, y) = r_\phi(x, y)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;正则化&lt;/strong&gt;：为防止 RL 策略 $\pi_\theta$ 显著偏离初始 SFT 模型 $\pi^{SFT}$（可能导致模型能力下降或生成不连贯内容），通常在 PPO 目标中加入 &lt;strong&gt;KL 散度惩罚项&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PPO 优化目标&lt;/strong&gt;：
$$
\text{Objective}(\theta) = \hat{\mathbb{E}}&lt;em&gt;{x \sim D, y \sim \pi&lt;/em&gt;\theta(\cdot|x)} [r_\phi(x, y) - \beta KL(\pi_{\theta}(\cdot|x) || \pi^{SFT}(\cdot|x))]
$$
&lt;ul&gt;
&lt;li&gt;$\hat{\mathbb{E}}$ 表示基于策略 $\pi_\theta$ 采样得到的经验平均。&lt;/li&gt;
&lt;li&gt;$\beta$ 是 KL 惩罚系数。&lt;/li&gt;
&lt;li&gt;目标是最大化 RM 奖励，同时控制与 SFT 模型的偏离程度。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练流程&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;从提示数据集中采样提示 $x$。&lt;/li&gt;
&lt;li&gt;使用当前策略 $\pi_\theta$ 生成回答 $y$。&lt;/li&gt;
&lt;li&gt;计算 RM 奖励 $r_\phi(x, y)$。&lt;/li&gt;
&lt;li&gt;计算 KL 惩罚项（相对于参考策略 $\pi^{SFT}$）。&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;PPO 算法&lt;/strong&gt;（通常是 Actor-Critic 实现，需要一个价值网络 $V$）更新策略参数 $\theta$，以优化上述带 KL 惩罚的目标。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：最终的 RLHF 模型 $\pi_\theta^{RL}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;RLAIF：从 AI 反馈中强化学习&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;动机&lt;/strong&gt;：RLHF 依赖大量&lt;strong&gt;人类标注&lt;/strong&gt;，成本高昂且扩展性受限。&lt;strong&gt;RLAIF (Reinforcement Learning from AI Feedback)&lt;/strong&gt; 提出用&lt;strong&gt;强大的 AI 模型&lt;/strong&gt;代替人类进行偏好标注。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心流程&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;使用基础模型生成回答对 $(y_1, y_2)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI 标注器 (AI Labeler)&lt;/strong&gt;（一个强大的 LLM）被提示来比较 $(y_1, y_2)$ 并输出偏好（可能是硬标签或软标签/概率 $p_1$）。可以结合&lt;strong&gt;思维链 (CoT)&lt;/strong&gt; 提示来提高 AI 标注的质量。&lt;/li&gt;
&lt;li&gt;使用这些 &lt;strong&gt;AI 生成的偏好标签&lt;/strong&gt;训练奖励模型 (RM)。&lt;/li&gt;
&lt;li&gt;后续的 RL 微调阶段与 RLHF 完全相同，只是使用的 RM 是从 AI 反馈中学习得到的。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本质&lt;/strong&gt;：RLAIF 可以看作是将一个强大 AI 标注模型的偏好能力&lt;strong&gt;蒸馏&lt;/strong&gt;到一个更小的、可用于 RL 的奖励模型中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;讨论：RLHF vs SFT&lt;/h2&gt;
&lt;p&gt;RLHF 相对于单纯的 SFT 在对齐方面具有一些优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;反馈粒度&lt;/strong&gt;：RLHF 基于对整个生成结果的&lt;strong&gt;整体偏好&lt;/strong&gt;进行优化，更符合人类评估方式；而 SFT 基于每个词元的预测损失，粒度更细。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;探索与多样性&lt;/strong&gt;：RL 的探索机制和基于整体奖励的优化，使得 RLHF 更容易生成多样化的高质量回答，而 SFT 可能更容易过拟合训练数据的特定风格。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缓解“幻觉”&lt;/strong&gt;：通过精心设计 RM（例如，对不确定或错误的回答给予低分或负分，对“我不知道”的回答给予中性或正分），RLHF 可能比 SFT 更有效地抑制模型的凭空捏造（幻觉）行为。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RLHF (及 RLAIF) 是对齐大型语言模型与人类价值观和意图、提升模型（如对话助手）性能的关键技术范式。&lt;/li&gt;
&lt;li&gt;它巧妙地绕开了直接定义复杂奖励函数的难题，通过学习人类（或 AI）的&lt;strong&gt;偏好&lt;/strong&gt;来构建奖励模型 (RM)。&lt;/li&gt;
&lt;li&gt;结合 SFT（提供良好起点）、RM 训练（学习偏好）和 PPO（优化策略以最大化偏好得分并保持稳定性），形成了一套行之有效的 LLM 对齐流程。&lt;/li&gt;
&lt;li&gt;理解 RLHF 的原理对于开发和评估负责任、有用的 AI 系统至关重要。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-14-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-14-zh.webp"/></item><item><title>周记 Week41</title><link>https://axi404.github.io/blog/week-41</link><guid isPermaLink="true">https://axi404.github.io/blog/week-41</guid><description>2025-04-07 ~ 2025-04-13.</description><pubDate>Mon, 21 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以这又是在周末才来来得及写的上一次的周记了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这一周确实过的也十分无聊，可以说就是各种代码写来写去，然后做了不少的事情，但是你要细说，好像又说不出来什么。&lt;/p&gt;
&lt;p&gt;整体来说可以说 CVPR 的代码 release 已经拖了太久，这一周主要还是在收尾，以及其他同学提供一些支持，在我写这篇周记的时候已经是下一周的周末了，但是这件事情还没有做完。俗话说能力越大，责任越大，感觉确实向来如此。承接了更大的框架之后，就会有更多的需求以及更多的事情需要支持，然后进一步在这基础上多了很多其他的东西。&lt;/p&gt;
&lt;p&gt;当然，好处也是有的，目前来看还是大概率可以混上一篇 NeurIPS 的，大概率是共一末作，毕竟提供了很多支持，也就意味着在很多项目中给出了贡献，这感觉还是很不错的。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;当然另外一方面的话也就是也已经到了第八周了，一方面的话保研季要开始了，同时一些课也结课了，要开始准备期末考试了。只能说一眨眼时间过得还是很快的，之前三月份的时候感觉距离保研开始还有很久，但是现在马上就要到五月了，清北的夏令营也要开始了，最后就陆续的各种夏令营和一系列的盛况，想来还是很有意思的。只是说我应该只会去一个或者几个学校，还是有一点点遗憾，不能认识很多的朋友，当然我现在的朋友已经很多了，我也很知足。&lt;/p&gt;
&lt;p&gt;我计算了一下，这学期要考试的科目大概是五门，第十周有两门，第十二周有一门，后面几周又有两门。可以说考的也不是很多，主要还是第十周的考试来的比较密集，但是感觉挺过去会好很多。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活上倒是没有什么好说的，还是因为记录的时间太晚了，所以说一些生活中的细节都已经被忘记了。基本上还是在正常的生活，然后按照我自己的科研节奏来发展。&lt;/p&gt;
&lt;p&gt;短暂的周记吧，还是没有时间闲下来想想最近都发生了什么，忙碌的时光总是那么短暂，而持续那么漫长，这是一种不可抗力，推着我继续往前走，而很少回头。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-41-zh.webp"/><enclosure url="https://picr2.axi404.top/week-41-zh.webp"/></item><item><title>RL 学习笔记（13）：近端策略优化 (PPO)</title><link>https://axi404.github.io/blog/rl-note-13</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-13</guid><description>近端策略优化 (PPO)</description><pubDate>Sun, 20 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;从 TRPO 到 PPO：追求简单与稳定&lt;/h2&gt;
&lt;p&gt;在上一讲中，我们学习了&lt;strong&gt;置信域策略优化 (TRPO)&lt;/strong&gt;。TRPO 通过在每次更新时强制施加 &lt;strong&gt;KL 散度约束&lt;/strong&gt;来限制新旧策略之间的差异，从而保证了策略更新的稳定性，避免了普通策略梯度方法对步长敏感、容易崩溃的问题。&lt;/p&gt;
&lt;p&gt;然而，TRPO 的主要缺点是其&lt;strong&gt;实现复杂性&lt;/strong&gt;和&lt;strong&gt;计算开销&lt;/strong&gt;。它涉及到计算费雪信息矩阵 (FIM) 向量积、使用共轭梯度法 (CG) 求解近似的自然梯度方向、以及进行回溯线搜索等步骤，并且与某些网络结构（如参数共享、Dropout）的兼容性不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;近端策略优化 (Proximal Policy Optimization, PPO)&lt;/strong&gt; 的目标是：&lt;strong&gt;获得 TRPO 的数据效率和可靠性能，但使用更简单的方法（特别是仅依赖一阶优化）来实现，使其更容易实现、调试和扩展。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;PPO 核心思想：限制策略更新&lt;/h2&gt;
&lt;p&gt;PPO 的核心思想与 TRPO 类似：&lt;strong&gt;限制每次迭代中策略的更新幅度&lt;/strong&gt;，以避免过大的、可能导致性能崩溃的更新。但 PPO 不使用 TRPO 那样复杂的 KL 散度硬约束和二阶优化方法，而是通过&lt;strong&gt;修改目标函数&lt;/strong&gt;来间接达到限制更新幅度的目的。&lt;/p&gt;
&lt;p&gt;定义&lt;strong&gt;概率比率 (Probability Ratio)&lt;/strong&gt;：
$$
r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$
其中 $\pi_\theta$ 是当前正在优化的新策略，$\pi_{\theta_{old}}$ 是用于收集数据的旧策略。$r_t(\theta)$ 衡量了新策略选择动作 $a_t$ 的概率相对于旧策略的变化。&lt;/p&gt;
&lt;p&gt;PPO 主要有两种实现方式：KL 惩罚 (PPO-Penalty) 和目标函数裁剪 (PPO-Clip)。&lt;/p&gt;
&lt;h2&gt;KL 惩罚 (PPO-Penalty)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;思想&lt;/strong&gt;：将 TRPO 的 KL 硬约束改为目标函数中的&lt;strong&gt;软约束（惩罚项）&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标函数 $L^{KLPEN}$&lt;/strong&gt;：
$$
L^{KLPEN}(\theta) = \hat{\mathbb{E}}&lt;em&gt;t \left[ r_t(\theta) \hat{A}&lt;em&gt;t - \beta KL[\pi&lt;/em&gt;{\theta&lt;/em&gt;{old}}(\cdot|s_t), \pi_{\theta}(\cdot|s_t)] \right]
$$
其中 $\hat{A}_t$ 是优势估计，$\beta$ 是惩罚系数。优化这个目标函数会同时试图最大化预期优势，并最小化新旧策略的 KL 散度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自适应 KL 惩罚系数 $\beta$&lt;/strong&gt;：由于固定 $\beta$ 难以调整，实践中通常使用自适应机制：
&lt;ul&gt;
&lt;li&gt;计算每次更新后的实际平均 KL 散度 $d_{KL}$。&lt;/li&gt;
&lt;li&gt;如果 $d_{KL}$ 远大于目标值 $d_{targ}$，则增大 $\beta$（加强惩罚）。&lt;/li&gt;
&lt;li&gt;如果 $d_{KL}$ 远小于目标值 $d_{targ}$，则减小 $\beta$（减弱惩罚）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：在每次迭代中，收集数据、估计优势，然后使用当前的 $\beta$ 通过多轮 SGD (或 Adam) 优化 $L^{KLPEN}$ 来更新 $\theta$，最后根据实际 $d_{KL}$ 调整下一次迭代的 $\beta$。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;// PPO with Adaptive KL Penalty (概要)
算法参数: 初始 θ₀, 初始 β₀, 目标 KL d_targ, 优化 epochs K
循环 k = 0, 1, 2, ... :
  1. 使用 π_k = π(·|·; θ_k) 收集数据 D_k
  2. 计算优势估计 Â_t for t in D_k
  3. // 策略优化
     For epoch = 1 to K:
       For mini-batch in D_k:
         计算带惩罚损失 L^KLPEN (使用当前 β_k)
         通过梯度上升更新 θ (例如 Adam step)
     θ_{k+1} ← 更新后的 θ
  4. // 计算实际 KL
     d_KL ← average KL(π_k || π_{k+1}) over D_k
  5. // 调整 β
     根据 d_KL 和 d_targ 的比较调整 β_k 得到 β_{k+1}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：虽然比 TRPO 简单，但引入了新的超参数 $d_{targ}$ 和 $\beta$ 的调整机制，效果有时不如 PPO-Clip 稳定。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;目标函数裁剪 (PPO-Clip)&lt;/h2&gt;
&lt;p&gt;这是 PPO &lt;strong&gt;最常用、效果最好&lt;/strong&gt;的版本。它通过直接&lt;strong&gt;裁剪 (Clipping)&lt;/strong&gt; 替代目标函数来限制策略更新。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PPO-Clip 目标函数 $L^{CLIP}$&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t [ \min( r_t(\theta) \hat{A}_t, \quad \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t ) ]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\epsilon$ 是裁剪超参数（例如 0.1 或 0.2），它定义了概率比率 $r_t(\theta)$ 允许偏离 1 的范围 $[1-\epsilon, 1+\epsilon]$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{clip}(r_t, 1-\epsilon, 1+\epsilon)$ 将 $r_t$ 限制在该区间内。&lt;/li&gt;
&lt;li&gt;$\min(\dots, \dots)$ 操作是关键：取&lt;strong&gt;原始目标&lt;/strong&gt; $r_t \hat{A}_t$ 和&lt;strong&gt;裁剪后的目标&lt;/strong&gt; $\text{clip}(r_t) \hat{A}_t$ 中的&lt;strong&gt;较小者&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;裁剪机制的作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;当 $\hat{A}_t &gt; 0$&lt;/strong&gt;（该动作好于平均）：目标为 $\min(r_t \hat{A}_t, (1+\epsilon)\hat{A}_t)$。如果 $r_t &gt; 1+\epsilon$（策略更新过大，过度提高该动作概率），目标被限制为 $(1+\epsilon)\hat{A}_t$，阻止 $r_t$ 无限增大带来的目标提升。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;当 $\hat{A}_t &amp;#x3C; 0$&lt;/strong&gt;（该动作差于平均）：目标为 $\max(r_t \hat{A}_t, (1-\epsilon)\hat{A}_t)$ (因为 $\hat{A}_t&amp;#x3C;0$，$\min$ 相当于 $\max$)。如果 $r_t &amp;#x3C; 1-\epsilon$（策略更新过大，过度降低该动作概率），目标被限制为 $(1-\epsilon)\hat{A}_t$，阻止 $r_t$ 无限减小带来的目标提升（恶化）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果&lt;/strong&gt;：通过 $\min$ 和 $\text{clip}$ 操作，PPO-Clip 相当于设置了一个移动的、取决于优势符号的“软约束”，阻止 $r_t(\theta)$ 偏离 $[1-\epsilon, 1+\epsilon]$ 太远，从而保证了策略更新的稳定性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：与 PPO-Penalty 类似，主要区别在于策略优化阶段优化的目标函数是 $L^{CLIP}$。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// PPO with Clipped Objective (概要)
算法参数: 初始 θ₀, 裁剪参数 ε, 优化 epochs K
循环 k = 0, 1, 2, ... :
  1. 使用 π_k = π(·|·; θ_k) 收集数据 D_k
  2. 计算优势估计 Â_t for t in D_k
  3. // 策略优化
    For epoch = 1 to K:
      For mini-batch in D_k:
        计算概率比率 r_t(θ) = π_θ(a_t|s_t) / π_k(a_t|s_t)
        计算裁剪损失 L^CLIP (使用 min, clip, r_t, Â_t)
        通过梯度上升更新 θ (例如 Adam step, 最大化 L^CLIP 或最小化 -L^CLIP)
    θ_{k+1} ← 更新后的 θ
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PPO-Clip vs TRPO 对比&lt;/strong&gt;：PPO-Clip 通常能达到与 TRPO 相当的性能，但实现简单得多（仅需一阶优化），计算效率更高，且更容易与其他机制（如参数共享）结合。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PPO 在 Actor-Critic 框架下的实现&lt;/h2&gt;
&lt;p&gt;PPO 通常在 &lt;strong&gt;Actor-Critic&lt;/strong&gt; 框架下实现，并结合 &lt;strong&gt;Generalized Advantage Estimation (GAE)&lt;/strong&gt; 来获得稳定且低偏差的优势估计。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;架构&lt;/strong&gt;：Actor 网络 $\pi_\theta$ 和 Critic 网络 $V_w$（通常参数 $\theta$ 和 $w$ 有部分共享）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势估计 (GAE)&lt;/strong&gt;：计算 $\hat{A}&lt;em&gt;t^{GAE(\gamma, \lambda)} = \sum&lt;/em&gt;{l=0}^{T-t-1} (\gamma \lambda)^l \delta_{t+l}$，其中 $\delta_t = R_{t+1} + \gamma V_w(S_{t+1}) - V_w(S_t)$ 是 TD 误差，$\lambda \in [0, 1]$ 是 GAE 参数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;联合损失函数&lt;/strong&gt;：在优化阶段（多轮 epoch），通常最小化一个包含三项的联合损失函数：
$$
L(\theta, w) = \hat{\mathbb{E}}&lt;em&gt;t [ -L_t^{CLIP}(\theta) + c_1 L_t^{VF}(w) - c_2 S[\pi&lt;/em&gt;{\theta}] ]
$$
&lt;ul&gt;
&lt;li&gt;$L_t^{CLIP}(\theta)$：PPO 的裁剪策略损失（取负是为了最小化）。
&lt;ul&gt;
&lt;li&gt;$L_t^{VF}(w) = (V_w - V_t^{targ})^2$：价值函数损失（均方误差）。其中 $V_t^{targ}$ 通常是 $\hat{A}_t^{GAE} + V_w$ 或 n-步回报。&lt;/li&gt;
&lt;li&gt;$S[\pi_{\theta}]$：策略的负熵。最小化负熵（即最大化熵）可以鼓励探索。&lt;/li&gt;
&lt;li&gt;$c_1, c_2$ 是权重系数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练循环&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;收集数据&lt;/strong&gt;：N 个 Actor 并行跑 T 步，使用 $\pi_{\theta_{old}}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算目标&lt;/strong&gt;：计算优势 $\hat{A}&lt;em&gt;t^{GAE}$ 和价值目标 $V_t^{targ}$，使用 $V&lt;/em&gt;{w_{old}}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化网络&lt;/strong&gt;：重复 K 个 epoch，在收集到的 NT 步数据上，使用 mini-batch 和 Adam 等优化器，更新参数 $\theta$ 和 $w$ 以最小化联合损失 $L(\theta, w)$。&lt;/li&gt;
&lt;li&gt;更新 $\theta_{old} \leftarrow \theta, w_{old} \leftarrow w$。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt; 旨在简化 TRPO，通过修改目标函数（&lt;strong&gt;PPO-Clip&lt;/strong&gt; 使用裁剪，&lt;strong&gt;PPO-Penalty&lt;/strong&gt; 使用自适应 KL 惩罚）来限制策略更新幅度，实现稳定且高效的策略优化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PPO-Clip&lt;/strong&gt; 是最常用且推荐的版本，因其实现简单、性能稳健、超参数调整相对容易（主要是 $\epsilon$）。&lt;/li&gt;
&lt;li&gt;PPO 通常在 &lt;strong&gt;Actor-Critic&lt;/strong&gt; 框架下实现，结合 &lt;strong&gt;GAE&lt;/strong&gt; 进行优势估计，并优化一个包含策略损失、价值损失和熵奖励的&lt;strong&gt;联合目标函数&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;PPO 采用&lt;strong&gt;多轮 epoch 优化&lt;/strong&gt;，在同一批数据上进行多次梯度更新，提高了样本效率。&lt;/li&gt;
&lt;li&gt;PPO 已成为现代深度强化学习中&lt;strong&gt;最流行和最可靠&lt;/strong&gt;的算法之一，是许多复杂控制任务的强大基线。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-13-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-13-zh.webp"/></item><item><title>RL 学习笔记（12）：置信域策略优化</title><link>https://axi404.github.io/blog/rl-note-12</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-12</guid><description>置信域策略优化</description><pubDate>Sat, 19 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;策略梯度方法的挑战&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;策略梯度 (Policy Gradient, PG)&lt;/strong&gt; 方法直接优化参数化的策略 $\pi_\theta(a|s)$，通过梯度上升 $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$ 来最大化期望回报 $J(\theta)$。虽然 PG 方法有其优势（如处理连续动作空间），但它面临一个核心挑战：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对步长 $\alpha$ 的敏感性&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;步长过大&lt;/strong&gt;：一次更新可能导致策略性能急剧下降（“学崩了”），难以恢复。这是因为梯度只提供了局部改进方向，步子迈太大可能会远离最优点，甚至进入性能更差的区域。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;步长过小&lt;/strong&gt;：学习过程会非常缓慢。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理想情况下，我们希望每次策略更新都能保证性能&lt;strong&gt;单调不减 (Monotonic Improvement)&lt;/strong&gt;，或者至少是稳定、可靠的改进。标准策略梯度无法提供这样的保证。&lt;/p&gt;
&lt;h2&gt;置信域优化方法 (Trust Region Methods) - 概念&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;置信域方法&lt;/strong&gt;是数值优化中的一类技术，旨在提供比简单梯度上升/下降更稳健的更新步骤。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;在当前参数点 $\theta_{old}$ 附近定义一个&lt;strong&gt;置信域 (Trust Region)&lt;/strong&gt; $\mathcal{N}(\theta_{old})$。在这个区域内，我们相信一个&lt;strong&gt;简化的替代模型 (Surrogate Model)&lt;/strong&gt; $L(\theta | \theta_{old})$ 能够很好地近似原始目标函数 $J(\theta)$。&lt;/li&gt;
&lt;li&gt;我们&lt;strong&gt;不在整个参数空间&lt;/strong&gt;寻找 $J(\theta)$ 的最优点，而是在这个&lt;strong&gt;置信域内部&lt;/strong&gt;寻找使&lt;strong&gt;替代模型 $L$ 最优&lt;/strong&gt;的参数点 $\theta_{new}$。
$$
\theta_{new} \leftarrow \arg\max_{\theta \in \mathcal{N}(\theta_{old})} L(\theta | \theta_{old})
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;置信域的定义&lt;/strong&gt;：通常是一个以 $\theta_{old}$ 为中心的区域。对于策略优化，这个“区域”通常不是根据参数 $\theta$ 的欧氏距离来定义，而是根据&lt;strong&gt;策略本身的差异&lt;/strong&gt;来定义，例如使用&lt;strong&gt;平均 KL 散度 (Average Kullback–Leibler Divergence)&lt;/strong&gt;：
$$
\mathcal{N}(\theta_{old}) = {\theta | \bar{D}&lt;em&gt;{KL}(\pi&lt;/em&gt;{\theta_{old}} || \pi_{\theta}) \le \delta }
$$
其中 $\delta$ 控制了置信域的大小（即允许新旧策略有多大的差异）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;置信域策略优化 (TRPO) 算法&lt;/h2&gt;
&lt;p&gt;TRPO 将置信域思想应用于策略优化，旨在实现稳定且可靠的策略改进。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TRPO 优化目标&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TRPO 最大化一个&lt;strong&gt;替代目标函数 (Surrogate Objective)&lt;/strong&gt; $\mathcal{L}&lt;em&gt;{\theta&lt;/em&gt;{old}}(\theta)$，该函数衡量了新策略 $\pi_{\theta}$ 相对于旧策略 $\pi_{\theta_{old}}$ 的预期性能提升（通常用&lt;strong&gt;优势函数 $A_{\theta_{old}}$&lt;/strong&gt; 和&lt;strong&gt;重要性采样比率&lt;/strong&gt;表示）：
$$
\mathcal{L}&lt;em&gt;{\theta&lt;/em&gt;{old}}(\theta) = \mathbb{E}&lt;em&gt;{s \sim \rho&lt;/em&gt;{\theta_{old}}, a \sim \pi_{\theta_{old}}} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)} A_{\theta_{old}}(s, a) \right]
$$&lt;/li&gt;
&lt;li&gt;同时，施加一个&lt;strong&gt;置信域约束&lt;/strong&gt;，限制新旧策略之间的平均 KL 散度：
$$
\bar{D}&lt;em&gt;{KL}(\theta&lt;/em&gt;{old} || \theta) = \mathbb{E}&lt;em&gt;{s \sim \rho&lt;/em&gt;{\theta_{old}}} [D_{KL}(\pi_{\theta_{old}}(\cdot|s) || \pi_{\theta}(\cdot|s))] \le \delta
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;理论保证 (近似单调改进)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理论分析表明，新策略的真实性能 $J(\theta)$ 的提升量有一个下界，这个下界与替代目标 $\mathcal{L}$ 的提升量以及新旧策略之间的&lt;strong&gt;最大&lt;/strong&gt; KL 散度 $D_{KL}^{max}$ 相关。&lt;/li&gt;
&lt;li&gt;这启发我们：如果能优化替代目标 $\mathcal{L}$，同时严格控制策略变化（限制 KL 散度），就有望保证策略性能不下降。TRPO 使用&lt;strong&gt;平均&lt;/strong&gt; KL 散度作为约束，这是一个实践中更易于处理的近似，虽然失去了严格的理论保证，但效果通常很好。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TRPO 的实际实现：近似求解&lt;/strong&gt;
直接求解上述带 KL 约束的优化问题很困难。TRPO 采用以下步骤进行近似求解：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;采样与估计&lt;/strong&gt;：使用从当前策略 $\pi_{\theta_{old}}$ 采样得到的轨迹数据，计算：
&lt;ul&gt;
&lt;li&gt;优势函数估计 $\hat{A}&lt;em&gt;t \approx A&lt;/em&gt;{\theta_{old}}(s_t, a_t)$（例如，使用 GAE - Generalized Advantage Estimation）。&lt;/li&gt;
&lt;li&gt;替代目标 $\mathcal{L}&lt;em&gt;{\theta&lt;/em&gt;{old}}(\theta)$ 的样本估计。&lt;/li&gt;
&lt;li&gt;KL 散度约束 $\bar{D}&lt;em&gt;{KL}(\theta&lt;/em&gt;{old} || \theta)$ 的样本估计。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;泰勒展开近似&lt;/strong&gt;：在当前参数 $\theta_{old}$ 附近：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;线性化目标&lt;/strong&gt;：$\mathcal{L}&lt;em&gt;{\theta&lt;/em&gt;{old}}(\theta) \approx \mathcal{L}&lt;em&gt;{\theta&lt;/em&gt;{old}}(\theta_{old}) + g^T (\theta - \theta_{old})$，其中 $g = \nabla_\theta \mathcal{L}&lt;em&gt;{\theta&lt;/em&gt;{old}}(\theta)|&lt;em&gt;{\theta&lt;/em&gt;{old}}$ 是策略梯度估计。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;二次近似约束&lt;/strong&gt;：$\bar{D}&lt;em&gt;{KL}(\theta&lt;/em&gt;{old} || \theta) \approx \frac{1}{2} (\theta - \theta_{old})^T H (\theta - \theta_{old})$，其中 $H = \nabla_\theta^2 \bar{D}&lt;em&gt;{KL}(\theta&lt;/em&gt;{old} || \theta)|&lt;em&gt;{\theta&lt;/em&gt;{old}}$ 是平均 KL 散度关于 $\theta$ 的 Hessian 矩阵，也称为&lt;strong&gt;费雪信息矩阵 (Fisher Information Matrix, FIM)&lt;/strong&gt;，通常也需要通过样本来估计 $\hat{H}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;求解近似优化问题&lt;/strong&gt;：问题变为求解：
$$
\max_{\Delta \theta} \quad g^T \Delta \theta \quad \text{subject to} \quad \frac{1}{2} \Delta \theta^T \hat{H} \Delta \theta \le \delta
$$
其中 $\Delta \theta = \theta - \theta_{old}$。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更新方向 (自然梯度)&lt;/strong&gt;：该问题的解的方向是 $\Delta \theta \propto \hat{H}^{-1} g$，这被称为&lt;strong&gt;自然梯度 (Natural Gradient)&lt;/strong&gt; 方向。相比普通梯度 $g$，自然梯度考虑了参数空间对策略分布变化的曲率影响 (由 FIM $\hat{H}$ 描述)，通常是更有效的优化方向。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;共轭梯度法 (Conjugate Gradient, CG)&lt;/strong&gt;：TRPO 使用 CG 算法来高效地&lt;strong&gt;近似求解&lt;/strong&gt;线性方程组 $\hat{H} x = g$ 以获得更新方向 $x \approx \hat{H}^{-1} g$，&lt;strong&gt;避免了显式计算和存储 Hessian 逆 $\hat{H}^{-1}$&lt;/strong&gt;（这在大规模神经网络中是不可行的）。CG 只需要计算 Hessian 向量积 (Hessian-vector product, $\hat{H}v$) 的能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;步长计算 (线搜索)&lt;/strong&gt;：计算出方向 $x$ 后，需要确定步长大小。首先计算一个理论上的最大步长 $s = \sqrt{\frac{2\delta}{x^T \hat{H} x}}$，得到提议的参数更新 $\Delta \theta_{prop} = s \cdot x$。然后，使用&lt;strong&gt;回溯线搜索 (Backtracking Line Search)&lt;/strong&gt;，从 $\Delta \theta_{prop}$ 开始，不断缩小步长（例如 $\alpha^j \Delta \theta_{prop}, j=0,1,\dots$），直到找到第一个满足&lt;strong&gt;原始 KL 约束 (样本均值)&lt;/strong&gt; 且能&lt;strong&gt;提升原始替代目标 $\mathcal{L}$ (样本均值)&lt;/strong&gt; 的实际参数更新 $\Delta \theta$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TRPO 算法概要&lt;/strong&gt;：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;算法参数: 初始策略参数 θ₀, KL 散度限制 δ, 回溯系数 α ∈ (0, 1), 最大回溯步数 K
(通常还需要一个价值函数 V_ϕ 来估计优势)

循环 k = 0, 1, 2, ... :
  1. // 数据收集
     根据当前策略 π_k = π(·|·; θ_k) 收集一批轨迹数据 D_k
  2. // 优势估计
     对 D_k 中的每个时间步 t, 计算优势估计 Â_t (例如使用 GAE)
  3. // 计算策略梯度估计 g
     g ← average_{D_k, t} [∇_θ log π_θ(a_t|s_t)|_{θ_k} * Â_t]
  4. // 计算自然梯度方向 x (近似 H⁻¹g)
     使用共轭梯度法 (CG) 近似求解 Hx = g, 得到解 x  (H 是 FIM 的估计)
  5. // 计算提议步长 s 和参数更新 Δθ_prop
     s ← sqrt(2δ / (xᵀHx))
     Δθ_prop ← s * x
  6. // 回溯线搜索确定实际步长
     对于 j = 0, 1, ..., K:
       θ_new ← θ_k + α^j * Δθ_prop
       如果 KL(π_k || π_new) ≤ δ  并且  L_k(θ_new) ≥ L_k(θ_k) (=0): // 检查约束和目标改进
         θ_{k+1} ← θ_new
         跳出线搜索循环
     (如果线搜索失败，通常保持 θ_{k+1} = θ_k)
  7. // (可选但常用) 更新价值函数 V_ϕ (例如，通过回归拟合 MC 回报)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;讨论与总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TRPO vs 策略梯度&lt;/strong&gt;：TRPO 通过引入 KL 散度约束来限制策略更新的步长，避免了标准策略梯度对学习率 $\alpha$ 的极端敏感性，实现了更稳定、更可靠的策略改进。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TRPO 优势&lt;/strong&gt;：提供了（近似的）单调改进保证，使其在许多任务上表现稳健。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TRPO 不足&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;实现复杂&lt;/strong&gt;：涉及计算 FIM 向量积、共轭梯度求解、回溯线搜索等，实现和调试相对困难。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算开销&lt;/strong&gt;：相比一阶方法，计算量更大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;兼容性问题&lt;/strong&gt;：难以直接应用于某些网络架构（如包含 Dropout 或 Actor-Critic 间参数共享）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;后续发展：近端策略优化 (PPO)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PPO (Proximal Policy Optimization) 旨在&lt;strong&gt;简化 TRPO&lt;/strong&gt;，保留其稳定性和可靠性优势，但使用&lt;strong&gt;一阶优化&lt;/strong&gt;方法。&lt;/li&gt;
&lt;li&gt;PPO 通过&lt;strong&gt;修改替代目标函数&lt;/strong&gt;（例如，使用&lt;strong&gt;裁剪 (Clipping)&lt;/strong&gt; 来限制新旧策略概率比率）或将 &lt;strong&gt;KL 散度作为惩罚项&lt;/strong&gt;加入目标函数，来间接限制策略更新的幅度。&lt;/li&gt;
&lt;li&gt;PPO 实现更简单，效果通常与 TRPO 相当甚至更好，已成为当前策略优化算法中的主流选择之一。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：TRPO 是一种基于置信域方法的重要的策略优化算法，它通过 KL 散度约束保证了策略更新的稳定性。虽然实现复杂，但其核心思想（限制策略变化以保证改进）对后续算法（如 PPO）产生了深远影响。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-12-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-12-zh.webp"/></item><item><title>RL 学习笔记（11）：Actor-Critic 方法</title><link>https://axi404.github.io/blog/rl-note-11</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-11</guid><description>Actor-Critic 方法</description><pubDate>Fri, 18 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;我们已经学习了&lt;strong&gt;策略梯度 (Policy Gradient)&lt;/strong&gt; 方法（如 REINFORCE），它直接优化参数化的策略 $\pi_\theta(a|s)$。这类方法虽然强大，特别是能处理连续动作空间和学习随机策略，但其主要的缺点是基于蒙特卡洛回报 $G_t$ 的梯度估计通常具有很高的&lt;strong&gt;方差&lt;/strong&gt;，导致学习过程不稳定且效率低下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Actor-Critic (AC) 方法&lt;/strong&gt; 旨在解决这个问题。它引入了一个额外的学习组件——&lt;strong&gt;评论家 (Critic)&lt;/strong&gt;，来学习一个&lt;strong&gt;价值函数&lt;/strong&gt;，用于更有效地指导策略（&lt;strong&gt;演员 Actor&lt;/strong&gt;）的学习。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：AC 方法包含两个相互作用的组件：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;演员 (Actor)&lt;/strong&gt;：即策略 $\pi(a|s; \theta)$，负责根据当前状态 $s$ 选择动作 $a$。参数为 $\theta$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评论家 (Critic)&lt;/strong&gt;：即价值函数估计器（通常是 $V(s; w)$ 或 $Q(s, a; w)$），负责评估 Actor 在某个状态或采取某个动作的好坏。参数为 $w$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交互流程&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;Actor 根据状态 $s$ 选择动作 $a \sim \pi_\theta(\cdot|s)$。&lt;/li&gt;
&lt;li&gt;执行动作 $a$，环境给出奖励 $R_{t+1}$ 和下一状态 $s&apos;$。&lt;/li&gt;
&lt;li&gt;Critic 利用 $(s, a, R_{t+1}, s&apos;)$ 来评估刚刚发生的转移（例如，计算 TD 误差 $\delta$）。&lt;/li&gt;
&lt;li&gt;Critic 根据评估结果更新自身参数 $w$，力求更准确地估计价值。&lt;/li&gt;
&lt;li&gt;Actor 根据 Critic 的评估信号（例如，TD 误差 $\delta$）更新策略参数 $\theta$，鼓励那些带来更好结果（正 TD 误差）的动作，抑制导致较差结果（负 TD 误差）的动作。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过引入 Critic 进行&lt;strong&gt;自举 (Bootstrapping)&lt;/strong&gt;，AC 方法可以用（通常方差较低的）TD 误差来替代（高方差的）MC 回报 $G_t$ 来指导策略学习，从而实现更稳定、高效的在线学习。&lt;/p&gt;
&lt;h2&gt;Actor-Critic 基础&lt;/h2&gt;
&lt;p&gt;回顾策略梯度定理：$\nabla_\theta J(\theta) = \mathbb{E}&lt;em&gt;{\pi&lt;/em&gt;\theta} [ Q_{\pi_\theta}(S, A) \nabla_\theta \log \pi_\theta(A|S) ]$。
AC 方法的目标就是用 Critic 学习到的价值函数 $Q_w(S, A)$ 或相关量来近似理论上的 $Q_{\pi_\theta}(S, A)$。&lt;/p&gt;
&lt;h3&gt;基于 Q 值的 Actor-Critic (QAC)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Critic&lt;/strong&gt;：学习动作价值函数 $Q_w(s, a) \approx Q_{\pi_\theta}(s, a)$。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更新 (TD Learning, 类似 Sarsa)&lt;/strong&gt;：对于转移 $(s, a, r, s&apos;, a&apos;)$ （其中 $a&apos; \sim \pi_\theta(\cdot|s&apos;)$）：
&lt;ul&gt;
&lt;li&gt;TD 目标: $y = r + \gamma Q_w(s&apos;, a&apos;)$&lt;/li&gt;
&lt;li&gt;TD 误差: $\delta = y - Q_w(s, a)$&lt;/li&gt;
&lt;li&gt;更新 Critic 参数 $w$ (例如，对于线性 Critic $Q_w = \phi^T w$): $w \leftarrow w + \alpha_w \delta \phi(s, a)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor&lt;/strong&gt;：使用 Critic 估计的 $Q_w(s, a)$ 来更新策略。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更新&lt;/strong&gt;：$\theta \leftarrow \theta + \alpha_\theta Q_w(s, a) \nabla_\theta \log \pi_\theta(a|s)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;算法：Q Actor-Critic (QAC)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;算法参数: Actor 学习率 α_θ, Critic 学习率 α_w, 折扣因子 γ
初始化: 状态 s, 策略参数 θ, 价值参数 w

采样动作 a ~ π_θ(·|s)
循环 对每一步:
  // 与环境交互
  执行动作 a, 得到奖励 r 和下一状态 s&apos;
  // Actor 选择下一动作 (仅用于 Critic 更新)
  采样下一动作 a&apos; ~ π_θ(·|s&apos;)
  // Critic 更新
  TD_目标 ← r + γ * Q_w(s&apos;, a&apos;)
  TD_误差 δ ← TD_目标 - Q_w(s, a)
  w ← w + α_w * δ * ∇_w Q_w(s, a) // 梯度取决于 Q_w 的形式

  // Actor 更新
  ∇logπ ← ∇_θ log π_θ(a | s)
  θ ← θ + α_θ * Q_w(s, a) * ∇logπ // 注意：使用 Q_w 而非 δ

  // 更新状态和动作
  s ← s&apos;; a ← a&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;注意&lt;/strong&gt;：QAC 的 Actor 更新直接使用 $Q_w(s, a)$，虽然理论上可行，但 $Q_w$ 的绝对值可能很大且波动剧烈，导致 Actor 更新的方差仍然较大。引入基线是更常用的做法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;引入基线：优势函数 (Advantage Function)&lt;/h3&gt;
&lt;p&gt;为了降低 Actor 更新的方差，我们可以引入基线 $V_{\pi_\theta}(s)$，使用&lt;strong&gt;优势函数 (Advantage Function)&lt;/strong&gt; $A_{\pi_\theta}(s, a) = Q_{\pi_\theta}(s, a) - V_{\pi_\theta}(s)$ 来指导策略更新。策略梯度变为：
$$
\nabla_\theta J(\theta) = \mathbb{E}&lt;em&gt;{\pi&lt;/em&gt;\theta} [ A_{\pi_\theta}(S, A) \nabla_\theta \log \pi_\theta(A|S) ]
$$&lt;/p&gt;
&lt;h3&gt;基于 V 值的 Actor-Critic 与优势估计&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Critic&lt;/strong&gt;：学习状态价值函数 $V_w(s) \approx V_{\pi_\theta}(s)$。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更新 (TD(0))&lt;/strong&gt;：对于转移 $(s, a, r, s&apos;)$：
&lt;ul&gt;
&lt;li&gt;TD 目标: $y = r + \gamma V_w(s&apos;)$&lt;/li&gt;
&lt;li&gt;TD 误差: $\delta = y - V_w(s)$&lt;/li&gt;
&lt;li&gt;更新 Critic 参数 $w$: $w \leftarrow w + \alpha_w \delta \nabla_w V_w(s)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor&lt;/strong&gt;：需要优势 $A_{\pi_\theta}(s, a)$ 的估计。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;关键洞察&lt;/strong&gt;：TD 误差 $\delta_t = (R_{t+1} + \gamma V_w(S_{t+1})) - V_w(S_t)$ 正好是优势函数 $A_{\pi_\theta}(S_t, A_t) = Q_{\pi_\theta}(S_t, A_t) - V_{\pi_\theta}(S_t)$ 的一个（有偏但通常方差较低的）&lt;strong&gt;单步估计&lt;/strong&gt;！
&lt;ul&gt;
&lt;li&gt;因为 $Q_{\pi_\theta}(S_t, A_t) \approx \mathbb{E}[R_{t+1} + \gamma V_{\pi_\theta}(S_{t+1})]$&lt;/li&gt;
&lt;li&gt;所以 $A_{\pi_\theta}(S_t, A_t) \approx \mathbb{E}[R_{t+1} + \gamma V_{\pi_\theta}(S_{t+1})] - V_{\pi_\theta}(S_t) \approx \mathbb{E}[\delta_t]$ （如果 $V_w \approx V_{\pi_\theta}$）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor 更新&lt;/strong&gt;：使用 TD 误差 $\delta$ 作为优势估计：
$$
\theta \leftarrow \theta + \alpha_\theta \delta \nabla_\theta \log \pi_\theta(a|s)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种使用 V-Critic 并利用 TD 误差 $\delta$ 同时更新 Actor 和 Critic 的方法，就是&lt;strong&gt;优势 Actor-Critic (Advantage Actor-Critic, A2C)&lt;/strong&gt; 的基础。&lt;/p&gt;
&lt;h2&gt;优势 Actor-Critic (A2C)&lt;/h2&gt;
&lt;p&gt;A2C 是目前非常流行和有效的 Actor-Critic 框架。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;网络结构&lt;/strong&gt;：通常使用神经网络实现 Actor 和 Critic。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Actor 网络 $\pi(a|s; \theta)$&lt;/strong&gt;：输入状态 $s$，输出动作概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Critic 网络 $V(s; w)$&lt;/strong&gt;：输入状态 $s$，输出状态价值 $V$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;参数共享&lt;/strong&gt;：Actor 和 Critic 网络通常可以共享底层的特征提取层（例如处理图像输入的 CNN），只在最后分别接上策略头和价值头。这有助于提高学习效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A2C 更新规则 (单步)&lt;/strong&gt;：对于转移 $(s, a, r, s&apos;)$：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;计算 Critic 的 TD 误差 (优势估计)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;TD 目标: $y = r + \gamma V_w(s&apos;)$ （如果 $s&apos;$ 是终止状态，则 $y=r$）&lt;/li&gt;
&lt;li&gt;TD 误差: $\delta = y - V_w(s)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新 Critic 参数 $w$&lt;/strong&gt; (最小化 TD 误差)：
$$
w \leftarrow w + \alpha_w \delta \nabla_w V_w(s)
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新 Actor 参数 $\theta$&lt;/strong&gt; (使用 $\delta$ 作为优势信号)：
$$
\theta \leftarrow \theta + \alpha_\theta \delta \nabla_\theta \log \pi_\theta(a|s)
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法：优势 Actor-Critic (A2C - 单步)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;算法参数: Actor 学习率 α_θ, Critic 学习率 α_w, 折扣因子 γ
初始化: 策略网络参数 θ, 价值网络参数 w

循环 对每个 episode:
  初始化状态 s
  循环 对 episode 中的每一步:
    // Actor 选择动作
    采样动作 a ~ π_θ(·|s)
    // 与环境交互
    执行动作 a, 得到奖励 r 和下一状态 s&apos;
    // Critic 评估
    V_s ← V_w(s) // 当前状态价值
    如果 s&apos; 是终止状态:
      V_s_prime ← 0
    否则:
      V_s_prime ← V_w(s&apos;) // 下一状态价值
    // 计算 TD 目标和 TD 误差 (优势估计)
    TD_目标 y ← r + γ * V_s_prime
    TD_误差 δ ← TD_目标 - V_s
    // 更新 Critic 参数 w
    w ← w + α_w * δ * ∇_w V_w(s)
    // 更新 Actor 参数 θ
    ∇logπ ← ∇_θ log π_θ(a | s)
    θ ← θ + α_θ * δ * ∇logπ
    // 更新状态
    s ← s&apos;
    如果 s 是终止状态, 退出内层循环
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;扩展：n 步 A2C&lt;/h2&gt;
&lt;p&gt;类似于 n 步 TD 和 n 步 Sarsa，我们也可以在 A2C 中使用 &lt;strong&gt;n 步回报&lt;/strong&gt;来计算 TD 目标和 TD 误差，以在偏差和方差之间取得更好的平衡。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n 步 TD 目标 (用于 Critic)&lt;/strong&gt;：
$$
y_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V_w(S_{t+n})
$$
(如果 $t+k$ ($k&amp;#x3C;n$) 到达终止状态 $T$，则 $y_t^{(n)} = G_{t:T}$ )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n 步 TD 误差 (优势估计)&lt;/strong&gt;：
$$
\delta_t^{(n)} = y_t^{(n)} - V_w(S_t)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n 步 A2C 更新规则&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Critic 更新&lt;/strong&gt;：$w \leftarrow w + \alpha_w \delta_t^{(n)} \nabla_w V_w(S_t)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor 更新&lt;/strong&gt;：$\theta \leftarrow \theta + \alpha_\theta \delta_t^{(n)} \nabla_\theta \log \pi_\theta(A_t|S_t)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;注意&lt;/strong&gt;：这里的更新针对的是时间步 $t$ 的状态 $S_t$ 和动作 $A_t$，但更新计算需要等到时间 $t+n$ 之后才能完成（因为需要 $R_{t+n}$ 和 $S_{t+n}$）。实际实现通常涉及缓存数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;与 REINFORCE (带基线) 的关系&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 $n \to \infty$ 时，n 步 TD 目标 $y_t^{(n)}$ 趋向于蒙特卡洛回报 $G_t$。&lt;/li&gt;
&lt;li&gt;此时，n 步 A2C 的更新规则与带基线的 REINFORCE 非常相似。n 步 A2C 提供了一个从 TD 学习 (n=1) 到蒙特卡洛学习 ($n=\infty$) 的平滑过渡。&lt;/li&gt;
&lt;li&gt;实践中，使用一个有限的、适中的 $n$（例如 $n=5$ 或 $n=20$）通常能获得比 $n=1$（纯 TD 误差，偏差大）或 $n=\infty$（纯 MC 回报，方差大）更好的性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Actor-Critic (AC)&lt;/strong&gt; 方法通过结合策略学习（Actor）和价值学习（Critic）来克服纯策略梯度方法（如 REINFORCE）的高方差问题。&lt;/li&gt;
&lt;li&gt;Critic 学习价值函数 ($V_w$ 或 $Q_w$) 来评估状态或动作。&lt;/li&gt;
&lt;li&gt;Actor 根据 Critic 提供的反馈信号（通常是优势函数的估计）来更新策略 $\pi_\theta$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势 Actor-Critic (A2C)&lt;/strong&gt; 是标准范式，通常使用 V-Critic，并利用 &lt;strong&gt;TD 误差 $\delta_t$&lt;/strong&gt; 作为优势函数的（单步或 n 步）估计来同时更新 Actor 和 Critic。&lt;/li&gt;
&lt;li&gt;AC 方法（特别是 A2C）能够进行&lt;strong&gt;在线、单步更新&lt;/strong&gt;，通常比基于 MC 的 REINFORCE 更稳定、高效。&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;n 步回报&lt;/strong&gt;可以进一步调整偏差与方差的平衡。&lt;/li&gt;
&lt;li&gt;AC 框架是许多现代深度强化学习算法（如 A3C, DDPG, PPO 等）的基础。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-11-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-11-zh.webp"/></item><item><title>RL 学习笔记（10）：策略梯度方法</title><link>https://axi404.github.io/blog/rl-note-10</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-10</guid><description>策略梯度方法</description><pubDate>Thu, 17 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;之前我们主要学习了&lt;strong&gt;基于价值 (Value-Based)&lt;/strong&gt; 的强化学习方法（如 Q-learning, DQN）。这些方法的核心是学习价值函数（$V(s)$ 或 $Q(s, a)$），然后根据价值函数&lt;strong&gt;间接&lt;/strong&gt;地导出策略（例如，使用 $\epsilon$-greedy 选择 Q 值最高的动作）。&lt;/p&gt;
&lt;p&gt;然而，基于价值的方法在某些情况下存在局限性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;难以处理&lt;strong&gt;高维或连续动作空间&lt;/strong&gt;（Q-learning 需要对所有动作计算 $\max$）。&lt;/li&gt;
&lt;li&gt;无法自然地学习&lt;strong&gt;随机性策略&lt;/strong&gt;，而随机策略在某些场景（如部分可观测环境、需要探索或最优策略本身就是随机的）下是必要的或更优的。&lt;/li&gt;
&lt;li&gt;价值函数本身可能非常复杂，导致学习困难。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;基于策略 (Policy-Based)&lt;/strong&gt; 的方法提供了一种替代方案：&lt;strong&gt;直接参数化并优化策略本身&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：将策略表示为一个以参数 $\theta$ 控制的函数 $\pi_\theta(a|s) = \mathbb{P}[A=a | S=s, \theta]$。我们的目标是找到最优的参数 $\theta^&lt;em&gt;$，使得该策略 $\pi_{\theta^&lt;/em&gt;}$ 的性能（例如，累积回报）最大化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略函数近似&lt;/strong&gt;：为了使用梯度优化，策略函数 $\pi_\theta(a|s)$ 需要对参数 $\theta$ 可微。常用的有 Softmax 策略（离散动作）或高斯策略（连续动作）等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;基于策略方法的优劣势&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：收敛性较好（不易发散）；天然适用于连续动作空间；可以学习随机策略；策略本身可能更简单。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;劣势&lt;/strong&gt;：通常收敛到局部最优；策略评估（计算梯度）的样本效率可能较低，且方差较高。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;策略梯度定理 (Policy Gradient Theorem)&lt;/h2&gt;
&lt;p&gt;为了通过梯度上升法 $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$ 来优化策略参数 $\theta$，我们需要计算目标函数 $J(\theta)$ （衡量策略性能，如初始状态价值 $V^{\pi_\theta}(s_0)$ 或平均奖励）的梯度 $\nabla_\theta J(\theta)$，即&lt;strong&gt;策略梯度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;策略梯度定理&lt;/strong&gt;提供了一个计算策略梯度的基础表达式，它将目标函数的梯度与策略 $\pi_\theta$ 和&lt;strong&gt;动作价值函数 $q_{\pi_\theta}(s, a)$&lt;/strong&gt; 联系起来，并且（方便地）不依赖于状态分布 $d^{\pi_\theta}(s)$ 对参数 $\theta$ 的导数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：对于常用的性能目标 $J(\theta)$，策略梯度可以表示为：
$$
\nabla_{\theta} J(\theta) \propto \sum_{s \in \mathcal{S}} d^{\pi_{\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a|s, \theta)
$$
或者，更常用的&lt;strong&gt;期望形式&lt;/strong&gt;为：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}&lt;em&gt;{\pi&lt;/em&gt;{\theta}} [ q_{\pi_{\theta}}(S_t, A_t) \nabla_{\theta} \log \pi_{\theta}(A_t|S_t, \theta) ]
$$
其中，期望 $\mathbb{E}&lt;em&gt;{\pi&lt;/em&gt;{\theta}}[\cdot]$ 是在策略 $\pi_\theta$ 下对轨迹 $(S_t, A_t)$ 进行的。$\nabla_{\theta} \log \pi_{\theta}(A_t|S_t, \theta)$ 被称为&lt;strong&gt;得分函数 (score function)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推导概要 (利用对数导数技巧)&lt;/strong&gt;：
$\nabla_{\theta} J(\theta) \propto \sum_s d \sum_a q \nabla \pi = \sum_s d \sum_a \pi q \frac{\nabla \pi}{\pi} = \sum_s d \sum_a \pi q \nabla \log \pi = \mathbb{E}[q \nabla \log \pi]$&lt;/p&gt;
&lt;p&gt;这个定理告诉我们，策略梯度的方向是：增加那些具有较高动作价值 $q_{\pi_\theta}(S_t, A_t)$ 的动作 $A_t$ 的对数概率 $\log \pi_\theta(A_t|S_t, \theta)$。&lt;/p&gt;
&lt;h2&gt;估算策略梯度：REINFORCE 算法&lt;/h2&gt;
&lt;p&gt;策略梯度定理给出了梯度的期望形式，我们可以使用&lt;strong&gt;蒙特卡洛采样&lt;/strong&gt;来估计这个期望。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基本思想&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据当前策略 $\pi_\theta$ 运行一个完整的 episode，获得一条轨迹 $S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_T$。&lt;/li&gt;
&lt;li&gt;对于轨迹中的每一步 $(S_t, A_t)$：
&lt;ul&gt;
&lt;li&gt;使用从 $t$ 时刻开始的&lt;strong&gt;完整样本回报 $G_t$&lt;/strong&gt; 作为动作价值 $q_{\pi_\theta}(S_t, A_t)$ 的&lt;strong&gt;无偏估计&lt;/strong&gt;。
$$
G_t \doteq \sum_{k=t}^{T-1} \gamma^{k-t} R_{k+1}
$$&lt;/li&gt;
&lt;li&gt;计算得分函数 $\nabla_{\theta} \log \pi_{\theta}(A_t|S_t, \theta)$。&lt;/li&gt;
&lt;li&gt;构造策略梯度的&lt;strong&gt;单样本无偏估计&lt;/strong&gt;：$g(t) = G_t \nabla_{\theta} \log \pi_{\theta}(A_t|S_t, \theta)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用这个随机梯度来更新策略参数：
$$
\theta \leftarrow \theta + \alpha g(t)
$$
（通常在一个 episode 结束后，累积所有时间步的梯度进行一次更新，或者每一步都更新）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这就是 &lt;strong&gt;REINFORCE 算法&lt;/strong&gt; (也称 Monte Carlo Policy Gradient)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法：REINFORCE (基本形式)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;算法参数: 学习率 α &gt; 0
初始化: 策略参数 θ 任意

循环 对每个 episode:
  根据当前策略 π_θ 生成一个 episode: S₀, A₀, R₁, ..., S_{T-1}, A_{T-1}, R_T
  初始化 G = 0
  对于 t = T-1, T-2, ..., 0:
    // 计算从时间 t 开始的回报 G_t (这里使用迭代计算)
    G ← R_{t+1} + γ * G
    // 计算梯度项 g(t) = G_t * ∇logπ
    ∇logπ ← ∇_θ log π_θ(A_t | S_t, θ)
    // 更新策略参数 (可以在循环内每步更新，或累积梯度在 episode 结束后更新)
    θ ← θ + α * G * ∇logπ
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;REINFORCE 的主要问题&lt;/strong&gt;：使用 $G_t$ 作为 $q_{\pi_\theta}$ 的估计虽然无偏，但其&lt;strong&gt;方差非常高&lt;/strong&gt;，导致学习过程缓慢且不稳定。&lt;/p&gt;
&lt;h2&gt;降低方差：引入基线 (Baseline)&lt;/h2&gt;
&lt;p&gt;为了解决 REINFORCE 的高方差问题，可以在策略梯度估计中引入一个&lt;strong&gt;基线 (Baseline)&lt;/strong&gt; $b(s)$，它只依赖于状态 $s$，不依赖于动作 $a$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;策略梯度（带基线）&lt;/strong&gt;：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}&lt;em&gt;{\pi&lt;/em&gt;{\theta}} [ (q_{\pi_{\theta}}(S_t, A_t) - b(S_t)) \nabla_{\theta} \log \pi_{\theta}(A_t|S_t, \theta) ]
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为何可行&lt;/strong&gt;：减去基线 $b(S_t)$ &lt;strong&gt;不会改变梯度的期望值&lt;/strong&gt;（即梯度估计仍然无偏），因为 $\mathbb{E}&lt;em&gt;{A \sim \pi&lt;/em&gt;\theta(\cdot|S_t)} [ b(S_t) \nabla_{\theta} \log \pi_{\theta}(A|S_t, \theta) ] = 0$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基线的作用&lt;/strong&gt;：选择合适的基线 $b(s)$ 可以显著&lt;strong&gt;降低梯度估计的方差&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;常用基线：状态价值函数 $V_{\pi_\theta}(s)$&lt;/strong&gt;
一个非常自然且常用的选择是使用当前策略下的&lt;strong&gt;状态价值函数 $V_{\pi_\theta}(s)$&lt;/strong&gt; 作为基线。此时，$(q_{\pi_\theta}(S_t, A_t) - V_{\pi_\theta}(S_t))$ 正是&lt;strong&gt;优势函数 (Advantage Function)&lt;/strong&gt; $A_{\pi_\theta}(S_t, A_t)$。&lt;/p&gt;
&lt;p&gt;策略梯度变为：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}&lt;em&gt;{\pi&lt;/em&gt;{\theta}} [ A_{\pi_{\theta}}(S_t, A_t) \nabla_{\theta} \log \pi_{\theta}(A_t|S_t, \theta) ]
$$
这使得策略更新更直观：增加具有正优势（比平均好）的动作的概率，减少具有负优势（比平均差）的动作的概率。理论上，选择 $V_{\pi_\theta}(s)$ 作为基线可以（近似地）最小化梯度方差。&lt;/p&gt;
&lt;h2&gt;带基线的 REINFORCE 算法 (一种简单的 Actor-Critic)&lt;/h2&gt;
&lt;p&gt;为了在 REINFORCE 中使用 $V_{\pi_\theta}(S_t)$ 作为基线，我们需要一个对 $V_{\pi_\theta}(S_t)$ 的估计。这通常通过引入第二个函数近似器（神经网络）来实现，称为&lt;strong&gt;评论家 (Critic)&lt;/strong&gt;，用于学习状态价值函数 $\hat{v}(s; w) \approx V_{\pi_\theta}(s)$。原来的策略网络 $\pi(a|s; \theta)$ 则被称为&lt;strong&gt;演员 (Actor)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法思想&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Actor (策略网络 $\pi_\theta$)&lt;/strong&gt;：根据当前策略生成动作，并根据 Critic 提供的优势信号更新参数 $\theta$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Critic (价值网络 $\hat{v}_w$)&lt;/strong&gt;：评估 Actor 在某个状态下的表现（估计 $V_{\pi_\theta}(s)$），并为 Actor 提供基线。Critic 自身也需要学习和更新。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;训练过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于轨迹中的每一步 $(S_t, A_t)$：
&lt;ul&gt;
&lt;li&gt;计算蒙特卡洛回报 $G_t = \sum_{k=t}^{T-1} \gamma^{k-t} R_{k+1}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Critic 更新&lt;/strong&gt;：使用 $G_t$ 作为 $V_{\pi_\theta}(S_t)$ 的（有噪声的）目标。计算价值估计误差（或称为 TD 误差，尽管这里目标是 MC 回报） $\delta_t = G_t - \hat{v}(S_t; w)$。通过梯度下降最小化误差（例如均方误差 $(G_t - \hat{v}(S_t; w))^2$）：
$$
w \leftarrow w + \alpha_w \delta_t \nabla_w \hat{v}(S_t; w)
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor 更新&lt;/strong&gt;：使用 Critic 计算出的误差 $\delta_t$（它近似于优势 $A_{\pi_\theta}(S_t, A_t)$）来更新策略参数：
$$
\theta \leftarrow \theta + \alpha_\theta \delta_t \nabla_\theta \log \pi_\theta(A_t|S_t; \theta)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;算法：带基线的 REINFORCE&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;算法参数: 策略学习率 α_θ &gt; 0, 价值学习率 α_w &gt; 0
初始化: 策略网络参数 θ, 价值网络参数 w 任意

循环 对每个 episode:
  根据当前策略 π_θ 生成一个 episode: S₀, A₀, R₁, ..., S_{T-1}, A_{T-1}, R_T
  初始化 G = 0
  对于 t = T-1, T-2, ..., 0:
    // 计算从时间 t 开始的回报 G_t
    G ← R_{t+1} + γ * G
    // 计算优势函数 (TD 误差形式，使用 MC 回报 G 作为目标)
    δ ← G - v̂(S_t; w)  // v̂ 是价值网络的输出
    // 更新价值网络参数 w (Critic Update)
    w ← w + α_w * δ * ∇_w v̂(S_t; w)
    // 更新策略网络参数 θ (Actor Update)
    ∇logπ ← ∇_θ log π_θ(A_t | S_t; θ)
    θ ← θ + α_θ * δ * ∇logπ
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同时学习策略和状态价值函数。&lt;/li&gt;
&lt;li&gt;价值函数主要用作&lt;strong&gt;基线&lt;/strong&gt;来降低策略梯度的方差。&lt;/li&gt;
&lt;li&gt;这仍然是一种&lt;strong&gt;蒙特卡洛&lt;/strong&gt;方法，因为它依赖于完整的 episode 回报 $G_t$。因此，更新需要在 episode 结束后进行，并且可能学习较慢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;策略梯度方法直接优化参数化的策略 $\pi_\theta$，适用于各种（包括连续）动作空间和随机策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略梯度定理&lt;/strong&gt; $\nabla J = \mathbb{E}[q_\pi \nabla \log \pi]$ 是理论基础。&lt;/li&gt;
&lt;li&gt;直接使用蒙特卡洛估计策略梯度（如 &lt;strong&gt;REINFORCE&lt;/strong&gt; 算法）虽然无偏，但方差很高。&lt;/li&gt;
&lt;li&gt;引入&lt;strong&gt;基线&lt;/strong&gt;（特别是状态价值函数 $V_\pi(s)$，导致使用&lt;strong&gt;优势函数 $A_\pi$&lt;/strong&gt;）是降低方差的关键技术，且不引入偏差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;带基线的 REINFORCE&lt;/strong&gt; 算法通过学习一个价值函数 $\hat{v}(s; w)$ (Critic) 来提供基线，形成了简单的 &lt;strong&gt;Actor-Critic&lt;/strong&gt; 结构。&lt;/li&gt;
&lt;li&gt;虽然 REINFORCE with Baseline 降低了方差，但它仍然基于 MC 回报，效率可能不如使用 TD 误差的 Actor-Critic 方法（将在后续讨论）。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-10-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-10-zh.webp"/></item><item><title>RL 学习笔记（9）：集成规划与学习</title><link>https://axi404.github.io/blog/rl-note-9</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-9</guid><description>集成规划与学习</description><pubDate>Wed, 16 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;在之前的学习中，我们接触了两种主要的强化学习范式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;无模型学习 (Model-Free Learning)&lt;/strong&gt;：如 MC、TD（Sarsa, Q-learning）、n 步自举法等。这些方法直接从与环境交互产生的&lt;strong&gt;真实经验 (Real Experience)&lt;/strong&gt; 中学习价值函数或策略，不试图显式地构建环境模型。它们依赖于各种形式的&lt;strong&gt;回溯 (Backup)&lt;/strong&gt; 操作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于模型的强化学习 (Model-Based RL)&lt;/strong&gt;：通常包含两个阶段：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型学习 (Model Learning)&lt;/strong&gt;：从经验中学习环境模型 $\mathcal{M} \approx \langle \mathcal{P}, \mathcal{R} \rangle$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规划 (Planning)&lt;/strong&gt;：使用学习到的模型 $\mathcal{M}$ 来计算最优策略或价值函数（例如，通过 DP 方法如价值迭代）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本讲的目标是探讨如何&lt;strong&gt;集成 (Integrate)&lt;/strong&gt; 这两种方法，即在智能体与环境交互、进行无模型学习的同时，利用学习到的模型进行规划，以期&lt;strong&gt;提高数据利用效率&lt;/strong&gt;并&lt;strong&gt;加速学习过程&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型 (Model)&lt;/strong&gt;：是对环境动态（状态转移 $\mathcal{P}$）和奖励机制 ($\mathcal{R}$) 的表示。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规划 (Planning)&lt;/strong&gt;：利用模型生成&lt;strong&gt;模拟经验 (Simulated Experience)&lt;/strong&gt;，并使用这些模拟经验通过回溯等方式来改进价值函数或策略的过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;学习环境模型&lt;/h2&gt;
&lt;p&gt;为了进行规划，我们首先需要一个模型。如果环境模型未知，就需要从经验中学习。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型组成&lt;/strong&gt;：通常需要学习状态转移模型 $\mathcal{P}&lt;em&gt;\eta(s&apos;|s, a)$ 和奖励模型 $\mathcal{R}&lt;/em&gt;\eta(r|s, a)$（或期望奖励 $\mathcal{R}_\eta(s, a)$），其中 $\eta$ 是模型参数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型学习&lt;/strong&gt;：本质上是一个&lt;strong&gt;监督学习&lt;/strong&gt;问题。利用真实经验数据 $(S_t, A_t, R_{t+1}, S_{t+1})$，将 $(S_t, A_t)$ 作为输入，$(R_{t+1}, S_{t+1})$ 作为目标输出来训练模型参数 $\eta$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型类型&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;期望模型&lt;/strong&gt;：预测期望的下一状态和奖励。简单但丢失随机性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;样本/生成模型 (Sample/Generative Model)&lt;/strong&gt;：可以生成 $(R_{t+1}, S_{t+1})$ 的样本，反映随机性。这是基于采样的规划的基础。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分布模型&lt;/strong&gt;：显式表示完整的概率分布。信息最全但计算复杂。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查找表模型 (Lookup Table Model)&lt;/strong&gt;：适用于离散状态和动作空间。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;学习&lt;/strong&gt;：通过计数统计经验转移频率和平均奖励来估计 $\hat{\mathcal{P}}&lt;em&gt;{ss&apos;}^{a}$ 和 $\hat{\mathcal{R}}&lt;/em&gt;{s}^{a}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作为样本模型&lt;/strong&gt;：更简单的方式是存储所有观察到的转移元组 $(S_t, A_t, R_{t+1}, S_{t+1})$。当需要从 $(s, a)$ 进行模拟时，从所有以 $(s, a)$ 开始的存储元组中&lt;strong&gt;随机抽取&lt;/strong&gt;一个，并返回其 $(R_{t+1}, S_{t+1})$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;示例：查找表（样本）模型&lt;/strong&gt;
假设有经验：
Episode 1: A, R=0, B, R=0, (end)
Episode 2: B, R=1, (end)
Episode 3: B, R=1, (end)
Episode 4: B, R=0, (end)
... (假设还有多次 B, R=1, (end))&lt;/p&gt;
&lt;p&gt;一个简单的样本模型可能是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model(A, action1) -&gt; 返回 (R=0, S&apos;=B) （基于 Episode 1）&lt;/li&gt;
&lt;li&gt;Model(B, action1) -&gt; 以一定概率返回 (R=1, S&apos;=End)，以一定概率返回 (R=0, S&apos;=End) （根据所有从 B 开始的 episodes 的统计）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;基于模型的规划&lt;/h2&gt;
&lt;p&gt;有了模型（无论是精确的还是学习到的），就可以进行规划。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;规划目标&lt;/strong&gt;：利用模型 $\mathcal{M}_\eta$ 求解对应的（近似）MDP 的最优价值或策略。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基于采样的规划 (Sample-Based Planning)&lt;/strong&gt;：一种强大且通用的规划方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：不直接处理复杂的概率模型 $\mathcal{P}&lt;em&gt;\eta, \mathcal{R}&lt;/em&gt;\eta$，而是&lt;strong&gt;仅使用模型作为模拟器来生成样本经验&lt;/strong&gt; $(S, A, R&apos;, S&apos;)$。&lt;/li&gt;
&lt;li&gt;然后，将任意&lt;strong&gt;无模型 RL 算法&lt;/strong&gt;（如 MC, Sarsa, Q-learning）应用于这些&lt;strong&gt;模拟经验&lt;/strong&gt;，如同它们来自真实环境一样。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：概念简单，易于实现，可以直接复用成熟的无模型算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法示例：随机采样单步表格型 Q 规划 (Random-sample one-step tabular Q-planning)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;// 假设已有一个模型 Model(s, a) -&gt; (R&apos;, S&apos;) （可以是学习到的）
// 假设 Q(s, a) 已初始化

循环 无限次 (或进行一定数量的规划步骤):
  1. 随机选择一个状态 S (通常从之前访问过的状态中选取)
  2. 随机选择一个动作 A (通常是从在状态 S 下尝试过的动作中选取)
  3. (模拟) 查询模型: (R&apos;, S&apos;) ← Model(S, A)
  4. (更新 Q 值) 应用一步 Q-learning 更新:
     Q(S, A) ← Q(S, A) + α * [R&apos; + γ * max_{a&apos;} Q(S&apos;, a&apos;) - Q(S, A)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个过程可以看作是在智能体的“头脑中”利用模型进行排练和学习。&lt;/p&gt;
&lt;h2&gt;Dyna 架构：集成学习与规划&lt;/h2&gt;
&lt;p&gt;Dyna 架构是一种将&lt;strong&gt;直接从真实经验学习 (Direct RL)&lt;/strong&gt; 和 &lt;strong&gt;基于模型的规划 (Planning / Indirect RL)&lt;/strong&gt; 相结合的经典框架。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;智能体与&lt;strong&gt;真实环境&lt;/strong&gt;交互，产生&lt;strong&gt;真实经验&lt;/strong&gt; $(S, A, R&apos;, S&apos;)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;直接 RL&lt;/strong&gt;：使用真实经验&lt;strong&gt;直接更新&lt;/strong&gt;价值函数或策略（例如，用 Q-learning 更新 Q(S, A)）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型学习&lt;/strong&gt;：使用真实经验&lt;strong&gt;更新&lt;/strong&gt;环境模型 Model(S, A)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规划 (Planning)&lt;/strong&gt;：在与环境交互的间隙，执行若干步&lt;strong&gt;模拟&lt;/strong&gt;。从&lt;strong&gt;学习到的模型&lt;/strong&gt;中采样模拟经验 $(S_{plan}, A_{plan}, R&apos;&lt;em&gt;{plan}, S&apos;&lt;/em&gt;{plan})$，并使用这些模拟经验来&lt;strong&gt;更新&lt;/strong&gt;价值函数或策略（例如，使用 Q-planning 更新 $Q(S_{plan}, A_{plan})$）。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dyna-Q 算法&lt;/strong&gt;：Dyna 架构的一个具体实现，结合了 Q-learning 和 Q-planning。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;算法参数: 步长 α, 折扣因子 γ, 探索参数 ε, 规划步数 n
初始化:
  Q(s, a) 对所有 s ∈ S⁺, a ∈ A(s)
  Model(s, a) 对所有 s, a (初始化为空或默认)

循环 无限次 (对每个 episode):
  (a) S ← 当前状态 (从环境获取)
  循环 对 episode 中的每一步:
    (b) A ← 基于 Q(S, ·) 的 ε-greedy 策略选择动作
    (c) 执行动作 A, 观察真实奖励 R&apos; 和真实下一状态 S&apos;
    (d) // 直接 RL 更新
        Q(S, A) ← Q(S, A) + α * [R&apos; + γ * max_{a&apos;} Q(S&apos;, a&apos;) - Q(S, A)]
    (e) // 模型学习 (简单存储确定性模型)
        Model(S, A) ← (R&apos;, S&apos;)
    (f) // 规划步骤
        循环 重复 n 次:
          // 从过往经验中随机选取状态和动作
          S_plan ← 随机选择一个之前观察到的状态
          A_plan ← 随机选择一个之前在 S_plan 状态下采取过的动作
          // 查询模型获取模拟经验
          (R&apos;_plan, S&apos;_plan) ← Model(S_plan, A_plan)
          // Q-planning 更新
          Q(S_plan, A_plan) ← Q(S_plan, A_plan) + α * [R&apos;_plan + γ * max_{a&apos;} Q(S&apos;_plan, a&apos;) - Q(S_plan, A_plan)]
    (g) S ← S&apos;
    如果 S 是终止状态, 退出内层循环
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;参数 $n$ 控制了每次真实交互后执行的规划（模拟更新）次数。$n=0$ 时即为普通的 Q-learning。$n &gt; 0$ 时，模型被用来放大真实经验的效果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dyna-Q 效果&lt;/strong&gt;：规划步骤可以显著加速学习。例如，在迷宫问题中，当智能体偶然发现一条捷径（产生一个有价值的真实经验）后，规划步骤可以利用模型迅速将这个“好消息”通过模拟反向传播到相关的状态-动作对，比等待真实经验慢慢探索要快得多。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;处理模型不准确性：Dyna-Q+&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：Dyna 架构依赖于学习到的模型。如果模型不准确（例如，由于环境是随机的、模型学习不充分，或环境本身发生了变化），基于错误模型的规划可能会产生误导，甚至损害性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dyna-Q+ 思想&lt;/strong&gt;：在规划步骤中加入&lt;strong&gt;探索奖励 (exploration bonus)&lt;/strong&gt;，鼓励对那些&lt;strong&gt;模型可能不准确&lt;/strong&gt;（因为很久没有被真实经验验证过）的状态-动作对进行模拟探索。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机制&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;记录每个状态-动作对 $(s, a)$ 自上次在&lt;strong&gt;真实环境&lt;/strong&gt;中被访问以来经过的时间步数 $\tau(s, a)$。&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;规划步骤&lt;/strong&gt;中，当模拟 $(S_{plan}, A_{plan})$ 得到模拟奖励 $R&apos;&lt;em&gt;{plan}$ 时，额外增加一个 bonus：
$$
\text{用于规划更新的奖励} = R&apos;&lt;/em&gt;{plan} + \kappa \sqrt{\tau(S_{plan}, A_{plan})}
$$
其中 $\kappa$ 是一个小的正常数，控制探索奖励的大小。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果&lt;/strong&gt;：当环境变化导致模型失效时（例如，迷宫中出现新的通路），新通路对应的 $(s, a)$ 对的 $\tau$ 值会很大。Dyna-Q+ 的规划步骤会因为探索奖励而倾向于模拟这些“陈旧”的区域，从而更快地更新 Q 值以适应环境变化，发现新的最优策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;集成规划与学习的目标是结合无模型学习（从真实经验中学习）和基于模型的规划（利用模型进行模拟和推演）的优点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型学习&lt;/strong&gt;是基础，可以从经验中学习环境模型（如样本模型）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于采样的规划&lt;/strong&gt;是一种通用方法，将无模型 RL 算法应用于模型生成的模拟经验。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dyna 架构&lt;/strong&gt;提供了一个优雅的框架，通过交织直接 RL、模型学习和规划步骤来高效利用数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dyna-Q&lt;/strong&gt; 是具体的实现，展示了规划加速学习的效果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dyna-Q+&lt;/strong&gt; 通过引入探索奖励增强了 Dyna-Q 在变化环境或模型不准确情况下的鲁棒性。&lt;/li&gt;
&lt;li&gt;这种模型与学习的结合是提高强化学习样本效率和适应性的重要途径。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-9-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-9-zh.webp"/></item><item><title>RL 学习笔记（8）：n 步自举法</title><link>https://axi404.github.io/blog/rl-note-8</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-8</guid><description>n 步自举法</description><pubDate>Tue, 15 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;我们已经学习了两种主要的无模型强化学习方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;一步时序差分 (One-step TD)&lt;/strong&gt;：如 TD(0), Sarsa(0), Q-learning(0)。它们完全依赖&lt;strong&gt;自举 (Bootstrapping)&lt;/strong&gt;，使用下一步的奖励 $R_{t+1}$ 和下一状态的&lt;strong&gt;价值估计&lt;/strong&gt;（$V(S_{t+1})$ 或 $Q(S_{t+1}, \cdot)$）来更新当前状态（或状态-动作对）的价值。
&lt;ul&gt;
&lt;li&gt;优点：方差低，可以在线学习，每步更新。&lt;/li&gt;
&lt;li&gt;缺点：有偏（依赖于不准确的估计值）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;蒙特卡洛 (Monte Carlo, MC)&lt;/strong&gt;：完全不使用自举。更新依赖于从当前状态直到 episode 结束的&lt;strong&gt;完整实际回报 $G_t$&lt;/strong&gt;。
&lt;ul&gt;
&lt;li&gt;优点：无偏（基于真实回报）。&lt;/li&gt;
&lt;li&gt;缺点：方差高，必须等待 episode 结束才能更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;n 步自举法 (n-step Bootstrapping)&lt;/strong&gt; 提供了一种在这两种极端之间进行&lt;strong&gt;平滑过渡&lt;/strong&gt;的方法。其核心思想是：向前看 &lt;strong&gt;n 步&lt;/strong&gt; 的实际奖励，然后使用第 n 步之后的状态的价值估计来进行自举。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 $n=1$ 时，n 步法退化为一步 TD 方法。&lt;/li&gt;
&lt;li&gt;当 $n=\infty$ （或 $n$ 足够大以覆盖到 episode 结束）时，n 步法等价于 MC 方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;动机&lt;/strong&gt;：通过调整步数 $n$，我们可以在&lt;strong&gt;偏差和方差之间进行权衡&lt;/strong&gt;。通常，选择一个&lt;strong&gt;中间的 $n$ 值&lt;/strong&gt;可以结合两者的优点，获得比纯粹的一步 TD 或纯 MC 更好的学习性能。&lt;/p&gt;
&lt;h2&gt;n 步 TD 预测 (估计 $V_\pi$)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：在遵循策略 $\pi$ 的情况下，估计状态价值函数 $V_\pi$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n 步回报 (n-step Return) $G_{t:t+n}$&lt;/strong&gt;：这是 n 步 TD 的核心。它结合了未来 n 步的实际奖励和第 n 步之后的价值估计。
$$
G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n})
$$
其中 $V_{t+n-1}$ 表示在时间 $t+n$ 时可用的对 $V(S_{t+n})$ 的估计值（通常是 $t+n-1$ 时刻的估计）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;终止情况&lt;/strong&gt;：如果在 $n$ 步内 episode 终止于时间 $T$（即 $t+n \ge T$），则 n 步回报就是从 $t$ 开始的完整 MC 回报 $G_t$。
$$
G_{t:t+n} \doteq G_t \quad (\text{如果 } t+n \ge T)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n 步 TD 更新规则&lt;/strong&gt;：使用 n 步回报作为 TD 目标来更新状态 $S_t$ 的价值。
$$
V(S_t) \leftarrow V(S_t) + \alpha [G_{t:t+n} - V(S_t)]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;注意&lt;/strong&gt;：这个更新实际上发生在时间 $t+n$，因为只有到那时我们才能观测到 $R_{t+n}$ 和 $S_{t+n}$ 并计算出 $G_{t:t+n}$。这意味着 n 步法需要缓存过去 n 步的状态和奖励。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法：n 步 TD 预测&lt;/strong&gt;：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;算法参数: 步长 α ∈ (0, 1], 步数 n ≥ 1
初始化: V(s) 任意, 对所有 s ∈ S

循环 对每个 episode:
  初始化并存储状态序列 S = [S₀]
  初始化并存储奖励序列 R = [0] // R₀ 占位
  T ← ∞
  循环 t = 0, 1, 2, ... :
    如果 t &amp;#x3C; T:
      // 仍在 episode 内
      A ← 根据策略 π(·|Sₜ) 选择动作
      执行动作 A, 观察得到奖励 R_{t+1} 和下一状态 S_{t+1}
      将 R_{t+1} 存入 R 序列
      将 S_{t+1} 存入 S 序列
      如果 S_{t+1} 是终止状态:
        T ← t + 1
    // τ 是当前可以更新价值的时间步
    τ ← t - n + 1
    如果 τ ≥ 0:
      // 计算 n 步回报 G_{τ:τ+n}
      G ← 0
      对于 i = τ + 1 到 min(τ + n, T):
        G ← G + γ^(i - τ - 1) * Rᵢ
      如果 τ + n &amp;#x3C; T:
        G ← G + γ^n * V(S_{τ+n}) // 添加自举项

      // 更新状态 S_τ 的价值
      V(S_τ) ← V(S_τ) + α * [G - V(S_τ)]

    如果 τ = T - 1:
      终止内层循环 (episode 结束)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;误差减少性质&lt;/strong&gt;：理论上，n 步回报的期望值比一步 TD 目标更接近真实价值 $v_\pi(s)$，其偏差随 $\gamma^n$ 衰减。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;n 步同轨策略控制 (On-Policy Control)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：学习动作价值函数 $Q_\pi$，并同时改进策略 $\pi$（通常使用 $\epsilon$-greedy）。&lt;/p&gt;
&lt;h3&gt;n 步 Sarsa&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n 步 Sarsa 回报 $G_{t:t+n}$&lt;/strong&gt;：类似于 n 步 TD，但最后使用 Q 值进行自举。
$$
G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})
$$
如果 $t+n \ge T$，则 $G_{t:t+n} \doteq G_t$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n 步 Sarsa 更新规则&lt;/strong&gt;：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [G_{t:t+n} - Q(S_t, A_t)]
$$
更新发生在时间 $t+n$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法：n 步 Sarsa&lt;/strong&gt;：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;算法参数: 步长 α ∈ (0, 1], 步数 n ≥ 1, 探索参数 ε &gt; 0
初始化: Q(s, a) 任意, 对所有 s ∈ S⁺, a ∈ A(s), Q(终止状态, ·) = 0

循环 对每个 episode:
  初始化并存储状态序列 S = [S₀]
  初始化并存储奖励序列 R = [0]
  选择动作 A₀ ← 基于 Q 的 ε-greedy(S₀)
  初始化并存储动作序列 A = [A₀]
  T ← ∞
  循环 t = 0, 1, 2, ... :
    如果 t &amp;#x3C; T:
      执行动作 Aₜ, 观察奖励 R_{t+1} 和下一状态 S_{t+1}
      将 R_{t+1} 存入 R 序列
      将 S_{t+1} 存入 S 序列
      如果 S_{t+1} 是终止状态:
        T ← t + 1
      否则:
        选择动作 A_{t+1} ← 基于 Q 的 ε-greedy(S_{t+1})
        将 A_{t+1} 存入 A 序列
    // τ 是当前可以更新价值的时间步
    τ ← t - n + 1
    如果 τ ≥ 0:
      // 计算 n 步 Sarsa 回报 G_{τ:τ+n}
      G ← 0
      对于 i = τ + 1 到 min(τ + n, T):
        G ← G + γ^(i - τ - 1) * Rᵢ
      如果 τ + n &amp;#x3C; T:
        G ← G + γ^n * Q(S_{τ+n}, A_{τ+n}) // 添加自举项

      // 更新状态-动作对 (S_τ, A_τ) 的价值
      Q(S_τ, A_τ) ← Q(S_τ, A_τ) + α * [G - Q(S_τ, A_τ)]

    如果 τ = T - 1:
      终止内层循环 (episode 结束)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;n 步期望 Sarsa (n-step Expected Sarsa)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;思想&lt;/strong&gt;：为降低 n 步 Sarsa 回报的方差（因其依赖随机样本 $A_{t+n}$），在最后一步使用期望值代替采样值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n 步期望 Sarsa 回报 $G_{t:t+n}$&lt;/strong&gt;：
$$
G_{t:t+n} \doteq R_{t+1} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n \sum_a \pi(a|S_{t+n}) Q(S_{t+n}, a)
$$
（如果 $t+n \ge T$, 则 $G_{t:t+n} \doteq G_t$）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新规则&lt;/strong&gt;：与 n 步 Sarsa 相同，但使用期望 Sarsa 回报。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：通常方差更小，性能可能更稳定。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;n 步离轨策略学习 (Off-Policy Learning)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：使用行为策略 $\mu$ 生成的数据来学习目标策略 $\pi$ 的价值。&lt;/p&gt;
&lt;h3&gt;使用重要性采样 (Importance Sampling, IS)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心&lt;/strong&gt;：通过乘以重要性采样率 (IS Ratio) $\rho$ 来校正由行为策略 $\mu$ 产生的回报。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n 步 IS 比率&lt;/strong&gt;：
$$
\rho_{t:h} \doteq \prod_{k=t+1}^{\min(h, T-1)} \frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n 步离轨 TD 更新 (估计 $V_\pi$)&lt;/strong&gt;：
$$
V(S_t) \leftarrow V(S_t) + \alpha \rho_{t:t+n-1} [G_{t:t+n} - V(S_t)]
$$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;注意范围&lt;/strong&gt;：$\rho$ 只乘到 $t+n-1$，因为 $G_{t:t+n}$ 中的 $V(S_{t+n})$ 是对 $\pi$ 下价值的估计，不依赖于 $A_{t+n}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n 步离轨 Sarsa 更新 (估计 $Q_\pi$)&lt;/strong&gt;：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \rho_{t+1:t+n} [G_{t:t+n} - Q(S_t, A_t)]
$$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;注意范围&lt;/strong&gt;：$\rho$ 乘到 $t+n$，因为 $G_{t:t+n}$ 中的 $Q(S_{t+n}, A_{t+n})$ 依赖于实际动作 $A_{t+n}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：高方差（尤其是 $n$ 较大或 $\pi, \mu$ 差异大时），需要满足覆盖性假设。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;n 步树回溯算法 (n-step Tree Backup, TB)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;动机&lt;/strong&gt;：实现离轨学习，同时避免 IS 的高方差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：不使用 IS 比率，而是通过在每一步对目标策略 $\pi$ 下&lt;strong&gt;未被选择&lt;/strong&gt;的动作进行&lt;strong&gt;期望&lt;/strong&gt;回溯来构建目标。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n 步树回溯回报 $G_{t:t+n}^{(TB)}$ (用于 Q 值更新)&lt;/strong&gt;：可以通过递归或迭代方式计算。其思想是在每个时间步 $k$ ($t &amp;#x3C; k &amp;#x3C; t+n$)，回报由实际奖励 $R_{k+1}$ 加上一个对 $S_{k+1}$ 处价值的估计组成，这个估计是根据目标策略 $\pi$ 对所有可能动作 $a$ 的价值 $Q(S_{k+1}, a)$ （对于未采取的动作）或后续的树回溯回报（对于实际采取的动作 $A_{k+1}$）进行加权平均得到的。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;一步树回溯目标 ($n=1$)&lt;/strong&gt;：$G_{t:t+1}^{(TB)} = R_{t+1} + \gamma \sum_{a&apos;} \pi(a&apos;|S_{t+1}) Q(S_{t+1}, a&apos;)$，这与期望 Sarsa 目标形式相同（但 $\pi$ 是目标策略）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n 步树回溯更新规则&lt;/strong&gt;：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [G_{t:t+n}^{(TB)} - Q(S_t, A_t)]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：离轨学习，无 IS 方差问题，不依赖于知道行为策略 $\mu$ 的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;统一视角：n 步 Q($\sigma$)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;思想&lt;/strong&gt;：引入一个参数 $\sigma_t \in [0, 1]$ 来控制在每一步是进行&lt;strong&gt;采样&lt;/strong&gt; ($\sigma_t=1$) 还是&lt;strong&gt;期望&lt;/strong&gt; ($\sigma_t=0$) 回溯。&lt;/li&gt;
&lt;li&gt;提供了一个统一的框架，可以平滑地连接 n 步 Sarsa (若 $\sigma=1$ 且使用 IS 校正) 和 n 步树回溯 (若 $\sigma=0$)，以及它们之间的各种插值算法。&lt;/li&gt;
&lt;li&gt;该框架比较复杂，通常用于更深入的理论研究或特定应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结与讨论&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;n 步自举法是 TD 和 MC 方法的自然推广，通过步数 $n$ 在偏差和方差之间进行权衡。&lt;/li&gt;
&lt;li&gt;选择一个合适的中间值 $n$ 通常能获得比一步 TD 或 MC 更优的性能。&lt;/li&gt;
&lt;li&gt;n 步方法包括用于预测的 n 步 TD，用于同轨控制的 n 步 Sarsa（及其期望版本），以及用于离轨控制的基于 IS 的方法和树回溯算法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实践考量&lt;/strong&gt;：n 步法需要缓存最近 $n$ 步的数据，并且更新会延迟 $n$ 步。计算复杂度与 $n$ 相关（主要是回报计算）。&lt;/li&gt;
&lt;li&gt;这些思想是更高级方法（如资格迹 $TD(\lambda)$）的基础。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-8-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-8-zh.webp"/></item><item><title>RL 学习笔记（7）：Q 学习、DQN 及相关改进</title><link>https://axi404.github.io/blog/rl-note-7</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-7</guid><description>Q 学习</description><pubDate>Mon, 14 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;离轨策略学习与 Q 学习&lt;/h2&gt;
&lt;h3&gt;离轨策略学习 (Off-Policy Learning) 回顾&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;动机&lt;/strong&gt;：为何需要离轨学习？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：通常我们想学习和评估的是最优策略 $\pi_*$ 或某个特定目标策略 $\pi$ 的价值 ($v_\pi, q_\pi$)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据来源&lt;/strong&gt;：但我们实际拥有的经验数据 ${S_t, A_t, R_{t+1}, S_{t+1}, \dots}$ 往往是由另一个&lt;strong&gt;行为策略 $\mu$ (Behavior Policy)&lt;/strong&gt; 生成的，这个策略可能更具探索性或完全不同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重要性&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;允许从&lt;strong&gt;观察&lt;/strong&gt;中学习（例如，学习人类演示）。&lt;/li&gt;
&lt;li&gt;可以&lt;strong&gt;复用&lt;/strong&gt;历史数据（由旧策略生成）。&lt;/li&gt;
&lt;li&gt;核心优势：可以在遵循&lt;strong&gt;探索性&lt;/strong&gt;策略 $\mu$ 进行数据收集的同时，学习到&lt;strong&gt;最优&lt;/strong&gt;（通常是贪心）策略 $\pi_*$。&lt;/li&gt;
&lt;li&gt;甚至可以同时学习多个目标策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Q 学习 (Q-Learning)：离轨 TD 控制&lt;/h3&gt;
&lt;p&gt;Q 学习是强化学习中最著名和最广泛使用的算法之一，它是一种&lt;strong&gt;离轨的时序差分 (Off-Policy TD) 控制&lt;/strong&gt;算法，直接学习&lt;strong&gt;最优动作价值函数 $q_*(s, a)$&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：Q 学习的更新基于贝尔曼最优方程：
$$
q_&lt;em&gt;(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a&apos;} q_&lt;/em&gt;(S_{t+1}, a&apos;) | S_t=s, A_t=a]
$$
对于一个观测到的转移 $(S_t, A_t, R_{t+1}, S_{t+1})$，Q 学习使用 TD 方法来逼近这个目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q 学习 TD 目标 (Q-Learning TD Target)&lt;/strong&gt;：$R_{t+1} + \gamma \max_{a&apos;} Q(S_{t+1}, a&apos;)$。它使用了在下一状态 $S_{t+1}$ 所有可能动作中具有&lt;strong&gt;最大 Q 值估计&lt;/strong&gt;的那个动作来形成目标，这隐式地代表了最优策略在 $S_{t+1}$ 会采取的行动价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q 学习更新规则&lt;/strong&gt;：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ \underbrace{R_{t+1} + \gamma \max_{a&apos;} Q(S_{t+1}, a&apos;)}_{\text{Q 学习目标}} - Q(S_t, A_t) ]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;离轨 (Off-Policy) 特性&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;关键在于 Q 学习的&lt;strong&gt;更新目标不依赖于实际执行的行为策略 $\mu$ 在 $S_{t+1}$ 时选择了哪个动作 $A_{t+1}$&lt;/strong&gt;。它总是使用 $\max_{a&apos;} Q(S_{t+1}, a&apos;)$ 作为对未来最优回报的估计。&lt;/li&gt;
&lt;li&gt;这意味着 Q 学习可以使用由&lt;strong&gt;任何&lt;/strong&gt;能够保证持续探索所有状态-动作对的行为策略 $\mu$（例如 $\epsilon$-greedy）生成的数据，来学习最优动作价值函数 $q_*$。&lt;/li&gt;
&lt;li&gt;由于更新规则与行为策略 $\mu$ 无关，Q 学习&lt;strong&gt;不需要使用重要性采样 (Importance Sampling)&lt;/strong&gt; 来进行校正。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Q 学习控制算法 (表格形式)&lt;/strong&gt;：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;算法参数: 步长 α ∈ (0, 1], 探索参数 ε &gt; 0 (用于行为策略)
初始化:
  对于所有状态 s ∈ S⁺, 所有动作 a ∈ A(s):
    Q(s, a) ← 任意值 (例如 Q(s, a)=0)
  Q(终止状态, ·) ← 0

循环 对每个 episode:
  初始化 S (该 episode 的第一个状态)
  循环 对 episode 中的每一步:
    // (行为策略 μ) 根据当前 Q 值选择动作 A (例如使用 ε-greedy 策略)
    A ← ε-greedy(S, Q)
    执行动作 A, 观察得到奖励 R 和下一状态 S&apos;

    // Q 学习更新 (目标策略 π 是贪心策略)
    如果 S&apos; 是终止状态:
      Target ← R
    否则:
      Target ← R + γ * max_{a&apos;} Q(S&apos;, a&apos;)
    Q(S, A) ← Q(S, A) + α * [Target - Q(S, A)]

    S ← S&apos;
  直到 S 是终止状态
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;收敛性&lt;/strong&gt;：在保证所有状态-动作对被无限次访问（通过行为策略）且步长参数满足 Robbins-Monro 条件的情况下，Q 学习保证收敛到最优动作价值函数 $q_*$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sarsa vs. Q 学习 (悬崖行走 Cliff Walking 例子)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sarsa (同轨)：学习其探索性行为策略（如 $\epsilon$-greedy）的价值，倾向于选择更安全的路径以避免因探索而掉下悬崖，其学习过程中的平均回报可能更高。&lt;/li&gt;
&lt;li&gt;Q 学习 (离轨)：直接学习最优（贪心）策略的价值，该策略可能包含风险（如贴着悬崖走）。即使 Q 函数收敛到最优值，其实际行为（由 $\epsilon$-greedy 决定）仍可能导致偶尔失败。&lt;/li&gt;
&lt;li&gt;区别：Sarsa 学习“如何在探索时表现良好”，Q 学习学习“最优情况下该怎么做”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Q 学习相关问题与改进&lt;/h2&gt;
&lt;h3&gt;期望 Sarsa (Expected Sarsa)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;回顾&lt;/strong&gt;：Sarsa 更新目标 $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$ 使用了随机采样的下一个动作 $A_{t+1}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;期望 Sarsa 思想&lt;/strong&gt;：使用下一状态 $S_{t+1}$ 处动作价值的&lt;strong&gt;期望值&lt;/strong&gt;代替采样值，以减小方差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新规则&lt;/strong&gt;：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_{a&apos;} \pi(a&apos;|S_{t+1}) Q(S_{t+1}, a&apos;) - Q(S_t, A_t) \right]
$$
其中 $\pi$ 是当前遵循的行为策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与 Q 学习关系&lt;/strong&gt;：若 $\pi$ 是贪心策略，则 $\sum_{a&apos;} \pi(a&apos;|S_{t+1}) Q(S_{t+1}, a&apos;) = \max_{a&apos;} Q(S_{t+1}, a&apos;)$，期望 Sarsa 等价于 Q 学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;最大化偏差 (Maximization Bias)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：Q 学习（以及 Sarsa 在策略改进时）由于在更新目标中使用了 $\max$ 操作作用于带有噪声的 Q 值估计，会系统性地&lt;strong&gt;高估 (Overestimate)&lt;/strong&gt; 真实的动作价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;原因&lt;/strong&gt;：$\mathbb{E}[\max_a Q(s, a)] \ge \max_a \mathbb{E}[Q(s, a)] = \max_a q_*(s, a)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;危害&lt;/strong&gt;：非均匀的高估可能导致算法偏好实际上次优的动作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;双 Q 学习 (Double Q-Learning)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：解决最大化偏差问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：解耦目标动作的&lt;strong&gt;选择 (Selection)&lt;/strong&gt; 和&lt;strong&gt;评估 (Evaluation)&lt;/strong&gt;。维护两套独立的 Q 值估计 $Q_1$ 和 $Q_2$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新规则&lt;/strong&gt;：随机选择更新 $Q_1$ 或 $Q_2$。
&lt;ul&gt;
&lt;li&gt;更新 $Q_1$ 时：用 $Q_1$ 选择 $S_{t+1}$ 的最佳动作 $A^* = \arg\max_a Q_1(S_{t+1}, a)$，但用 $Q_2$ 评估该动作 $Q_2(S_{t+1}, A^*)$ 来计算 TD 目标。
$$
Q_1(S_t, A_t) \leftarrow Q_1(S_t, A_t) + \alpha [R_{t+1} + \gamma Q_2(S_{t+1}, \arg\max_a Q_1(S_{t+1}, a)) - Q_1(S_t, A_t)]
$$&lt;/li&gt;
&lt;li&gt;更新 $Q_2$ 时：对称操作。
$$
Q_2(S_t, A_t) \leftarrow Q_2(S_t, A_t) + \alpha [R_{t+1} + \gamma Q_1(S_{t+1}, \arg\max_a Q_2(S_{t+1}, a)) - Q_2(S_t, A_t)]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;行为策略&lt;/strong&gt;：通常基于 $Q_1 + Q_2$ 来选择动作 (例如 $\epsilon$-greedy)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果&lt;/strong&gt;：显著减少最大化偏差，得到更准确的价值估计。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;价值函数近似 (VFA) 与深度强化学习 (DRL) 简介&lt;/h2&gt;
&lt;h3&gt;价值函数近似 (VFA)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;动机&lt;/strong&gt;：当状态或动作空间巨大，无法使用表格存储所有 Q 值时（维度灾难）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：使用参数化的函数 $\hat{q}(s, a; \mathbf{w})$ 来近似 $q_*(s, a)$，其中 $\mathbf{w}$ 是需要学习的参数。通常需要将状态 $s$ （有时包括动作 $a$）表示为特征向量 $\mathbf{x}(s)$ 或 $\mathbf{x}(s, a)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习&lt;/strong&gt;：通过&lt;strong&gt;随机梯度下降 (SGD)&lt;/strong&gt; 调整参数 $\mathbf{w}$，以最小化近似值与 TD 目标之间的误差（例如均方误差）。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q 学习 VFA 更新&lt;/strong&gt;：
$$
\mathbf{w} \leftarrow \mathbf{w} + \alpha [ \underbrace{R_{t+1} + \gamma \max_{a&apos;} \hat{q}(S_{t+1}, a&apos;; \mathbf{w})}&lt;em&gt;{\text{TD 目标}} - \hat{q}(S_t, A_t; \mathbf{w}) ] \nabla&lt;/em&gt;\mathbf{w} \hat{q}(S_t, A_t; \mathbf{w})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;挑战：“死亡三角” (Deadly Triad)&lt;/strong&gt;：当&lt;strong&gt;函数近似&lt;/strong&gt;（尤其非线性）、&lt;strong&gt;自举&lt;/strong&gt;（TD 方法）和&lt;strong&gt;离轨学习&lt;/strong&gt;这三者结合时，训练过程可能不稳定甚至发散。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;深度强化学习 (DRL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心&lt;/strong&gt;：使用&lt;strong&gt;深度神经网络 (DNN)&lt;/strong&gt; 作为函数近似器 $\hat{q}(s, a; \mathbf{w})$。利用 DNN 强大的表示能力处理高维输入（如图像像素）和复杂价值函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引例：深度 Q 网络 (DQN)&lt;/strong&gt;：下一节将详细讨论 DQN 及其关键技术，这些技术正是为了克服“死亡三角”带来的挑战。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;深度 Q 网络 (Deep Q-Network, DQN) 及其改进&lt;/h2&gt;
&lt;p&gt;DQN 标志着深度学习与强化学习成功结合的里程碑，尤其在处理高维输入（如 Atari 游戏）方面取得了突破。&lt;/p&gt;
&lt;h3&gt;DQN 核心思想&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;使用深度神经网络（通常是 CNN + MLP）$Q(s, a; \mathbf{w})$ 来近似最优动作价值函数 $Q^*(s, a)$。&lt;/li&gt;
&lt;li&gt;输入为状态 $s$（如游戏画面帧），输出为每个离散动作 $a$ 的 Q 值。&lt;/li&gt;
&lt;li&gt;基于 Q 学习进行训练，但需要引入特殊技术来保证稳定性和效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;经验回放 (Experience Replay)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：直接使用按时间顺序的经验训练 DNN 会因数据相关性导致训练不稳定，且样本效率低。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：将经验转移 $(s_t, a_t, R_{t+1}, s_{t+1})$ 存储在&lt;strong&gt;回放缓冲区 (Replay Buffer)&lt;/strong&gt; $\mathcal{D}$ 中。训练时，从 $\mathcal{D}$ 中&lt;strong&gt;随机均匀采样小批量 (mini-batch)&lt;/strong&gt; 数据进行梯度更新。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：打破数据相关性，提高样本利用率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;目标网络 (Target Network)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：Q 学习的 TD 目标 $y_t = R_{t+1} + \gamma \max_{a&apos;} Q(S_{t+1}, a&apos;; \mathbf{w})$ 使用&lt;strong&gt;当前正在更新的网络 $\mathbf{w}$&lt;/strong&gt; 来计算目标值。这导致目标值随着 $\mathbf{w}$ 的微小变动而快速变化，容易造成训练不稳定（追逐移动的目标）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：引入一个独立的&lt;strong&gt;目标网络 $Q(s, a; \mathbf{w}^-)$&lt;/strong&gt;，其参数 $\mathbf{w}^-$ 更新得比在线网络 $\mathbf{w}$ 慢得多。TD 目标使用目标网络计算：
$$
y_t = R_{t+1} + \gamma \max_{a&apos;} Q(S_{t+1}, a&apos;; \mathbf{w}^-)
$$
在线网络 $\mathbf{w}$ 的更新目标是 $y_t$。目标网络参数 $\mathbf{w}^-$ 定期（硬更新）或缓慢（软更新）从 $\mathbf{w}$ 复制/跟踪而来。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：提供了更稳定的 TD 目标，显著提高训练稳定性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;双 DQN (Double DQN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：即使使用了目标网络，$\max_{a&apos;} Q(S_{t+1}, a&apos;; \mathbf{w}^-)$ 仍然存在&lt;strong&gt;最大化偏差&lt;/strong&gt;问题，因为它在（可能带噪声的）目标网络 Q 值上取最大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：结合目标网络和 Double Q-learning 的思想，解耦目标动作的&lt;strong&gt;选择&lt;/strong&gt;和&lt;strong&gt;评估&lt;/strong&gt;。
&lt;ul&gt;
&lt;li&gt;使用&lt;strong&gt;在线网络 $Q(\cdot; \mathbf{w})$&lt;/strong&gt; 选择下一状态 $S_{t+1}$ 的最佳动作：$A^* = \arg\max_{a&apos;} Q(S_{t+1}, a&apos;; \mathbf{w})$。&lt;/li&gt;
&lt;li&gt;使用&lt;strong&gt;目标网络 $Q(\cdot; \mathbf{w}^-)$&lt;/strong&gt; 评估这个选定动作 $A^*$ 的价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Double DQN TD 目标&lt;/strong&gt;：
$$
y_t^{\text{DoubleDQN}} = R_{t+1} + \gamma Q(S_{t+1}, \underbrace{\arg\max_{a&apos;} Q(S_{t+1}, a&apos;; \mathbf{w})}_{\text{用在线网络选择动作}}, \mathbf{w}^-)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：有效缓解最大化偏差，得到更准确的 Q 值估计。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;优先经验回放 (Prioritized Experience Replay)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：均匀采样经验可能效率不高，因为某些经验（通常是 TD 误差大的）包含更多学习信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：根据 TD 误差的绝对值赋予经验不同的&lt;strong&gt;优先级 $p_t$&lt;/strong&gt;。&lt;strong&gt;非均匀地采样&lt;/strong&gt;经验，优先级高的经验被选中的概率更大。同时，使用&lt;strong&gt;重要性采样 (IS) 权重 $\omega_t$&lt;/strong&gt; 来校正由此引入的偏差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：聚焦于“有价值”的经验，加速学习过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;对决 DQN (Dueling DQN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：标准 DQN 网络直接估计 Q 值，可能无法有效区分状态本身的价值和动作带来的相对优势。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：设计一种特殊的&lt;strong&gt;网络架构 (Dueling Network Architecture)&lt;/strong&gt;，将 Q 值分解为两部分：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;状态价值 $V(s)$&lt;/strong&gt;：由一个网络流输出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作优势 $A(s, a)$&lt;/strong&gt;：由另一个网络流输出。&lt;/li&gt;
&lt;li&gt;通过特定的&lt;strong&gt;聚合层&lt;/strong&gt;将 $V(s)$ 和 $A(s, a)$ 合并得到最终的 $Q(s, a)$，例如使用均值聚合：
$$
Q(s, a; \mathbf{w}) = V(s; \mathbf{w}^V) + \left( A(s, a; \mathbf{w}^A) - \frac{1}{|\mathcal{A}|} \sum_{a&apos;} A(s, a&apos;; \mathbf{w}^A) \right)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：能够更有效地学习状态价值，尤其在不同动作价值差异不大的情况下，通常能带来性能提升。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;DQN 算法总结&lt;/h3&gt;
&lt;p&gt;一个现代的 DQN 实现通常会结合上述多种技术：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;初始化:
  在线 Q 网络参数 w, 目标 Q 网络参数 w⁻ ← w
  回放缓冲区 D (容量 N)

循环 对每个 episode:
  初始化状态 S₁
  循环 对于 t = 1, T:
    选择动作 Aₜ ← 基于 Q(Sₜ, ·; w) 的 ε-greedy 策略
    执行动作 Aₜ, 观察奖励 R_{t+1}, 下一状态 S_{t+1}
    将转移 (Sₜ, Aₜ, R_{t+1}, S_{t+1}) 存入 D

    // 从 D 中采样 N 个转移 (Sⱼ, Aⱼ, R_{j+1}, S_{j+1}) (可能使用优先回放)
    // 计算采样权重 ωⱼ (如果使用优先回放，否则 ωⱼ=1)

    对于每个采样的转移 j = 1 to N:
      如果 S_{j+1} 是终止状态:
        Targetⱼ ← R_{j+1}
      否则:
        // 使用 Double DQN 计算目标值
        BestAction&apos; ← argmax_{a&apos;} Q(S_{j+1}, a&apos;; w)
        Targetⱼ ← R_{j+1} + γ * Q(S_{j+1}, BestAction&apos;; w⁻) // 使用目标网络评估

      // 计算损失，例如 Lⱼ = (Targetⱼ - Q(Sⱼ, Aⱼ; w))²
      // (如果使用优先回放，可能需要用 ωⱼ 加权损失或梯度)

    // 基于采样的 mini-batch 的总损失，执行一步梯度下降更新在线网络参数 w
    // (例如，Δw ∝ Σⱼ ωⱼ * (Targetⱼ - Q(Sⱼ, Aⱼ; w)) * ∇w Q(Sⱼ, Aⱼ; w))

    // 定期更新目标网络参数 w⁻
    // 例如，每 C 步: w⁻ ← w (硬更新)
    // 或每一步: w⁻ ← τw + (1-τ)w⁻ (软更新)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rainbow DQN&lt;/strong&gt;：是一个著名的例子，它成功地将上述多种改进（以及 Noisy Nets、Distributional RL 等）结合起来，显著提升了性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;结论&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Q 学习是强化学习中一种强大且广泛应用的&lt;strong&gt;离轨 TD 控制&lt;/strong&gt;算法，它直接学习最优动作价值函数。&lt;/li&gt;
&lt;li&gt;处理大规模问题时，需要结合&lt;strong&gt;价值函数近似 (VFA)&lt;/strong&gt;，特别是使用&lt;strong&gt;深度神经网络 (DNN)&lt;/strong&gt;，即&lt;strong&gt;深度 Q 网络 (DQN)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;为了使 DQN 训练稳定、高效，发展出了一系列关键技术：&lt;strong&gt;经验回放、目标网络、双 DQN、优先经验回放、对决网络架构&lt;/strong&gt;等。&lt;/li&gt;
&lt;li&gt;这些技术的组合（如 Rainbow DQN）极大地推动了 DRL 在复杂任务上的成功应用。理解这些基本组件及其原理对于深入学习和应用 DRL至关重要。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-7-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-7-zh.webp"/></item><item><title>RL 学习笔记（6）：时序差分学习</title><link>https://axi404.github.io/blog/rl-note-6</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-6</guid><description>时序差分学习</description><pubDate>Sun, 13 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;时序差分（TD）学习概述&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;动机：结合动态规划（DP）与蒙特卡洛（MC）的优点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DP&lt;/strong&gt;：利用&lt;strong&gt;自举 (Bootstrapping)&lt;/strong&gt;，即基于其他状态的&lt;strong&gt;估计值&lt;/strong&gt;来更新当前状态的估计值（例如，$V(S_t) \leftarrow \mathbb{E}&lt;em&gt;\pi[R&lt;/em&gt;{t+1} + \gamma V(S_{t+1})]$）。但它需要知道环境的&lt;strong&gt;完整模型&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MC&lt;/strong&gt;：&lt;strong&gt;无模型&lt;/strong&gt;，直接从&lt;strong&gt;完整经验片段&lt;/strong&gt;的最终&lt;strong&gt;实际回报 $G_t$&lt;/strong&gt; 中学习（例如，$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$）。它不使用自举，必须等待片段结束。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TD 学习&lt;/strong&gt;：旨在融合两者的长处：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;无模型 (Model-Free)&lt;/strong&gt;：像 MC 一样，直接从经验中学习，无需环境模型 $\mathcal{P}, \mathcal{R}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自举 (Bootstrapping)&lt;/strong&gt;：像 DP 一样，更新当前状态 $S_t$ 的价值时，会使用&lt;strong&gt;后续状态 $S_{t+1}$ 的当前价值估计 $V(S_{t+1})$&lt;/strong&gt;，而&lt;strong&gt;不需要等到片段结束&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：TD 学习在智能体与环境交互的每一步之后，利用观测到的&lt;strong&gt;即时奖励 $R_{t+1}$&lt;/strong&gt; 和&lt;strong&gt;下一状态 $S_{t+1}$ 的当前价值估计 $V(S_{t+1})$&lt;/strong&gt; 来构建一个&lt;strong&gt;目标值 (TD Target)&lt;/strong&gt;。然后，用这个目标值来更新当前状态 $S_t$ 的价值估计 $V(S_t)$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TD(0) 更新规则（最简单的 TD 算法）&lt;/strong&gt;：
$$
V(S_t) \leftarrow V(S_t) + \alpha \underbrace{[ \overbrace{R_{t+1} + \gamma V(S_{t+1})}^{\text{TD 目标 (TD Target)}} - V(S_t) ]}_{\text{TD 误差 (TD Error) } \delta_t}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TD 目标 (TD Target)&lt;/strong&gt;: $R_{t+1} + \gamma V(S_{t+1})$。这是对未来总回报 $G_t$ 的一个估计，它仅使用了一步的真实奖励 $R_{t+1}$ 和一步之后状态的价值估计 $V(S_{t+1})$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TD 误差 (TD Error) $\delta_t$&lt;/strong&gt;: $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$。它衡量了当前价值估计 $V(S_t)$ 与基于下一步信息的“更好”的估计（TD 目标）之间的差异，可以看作是对当前价值的一个调整信号。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;TD 学习的特点：自举与采样 (Bootstrapping and Sampling)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;采样 (Sampling)&lt;/strong&gt;：更新是基于智能体实际经历的转移 $(S_t, A_t, R_{t+1}, S_{t+1})$，而不是像 DP 那样考虑所有可能的后续状态和奖励。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自举 (Bootstrapping)&lt;/strong&gt;：更新的目标值中包含了&lt;strong&gt;当前的价值估计&lt;/strong&gt; $V(S_{t+1})$，而不是像 MC 那样依赖最终观测到的完整回报 $G_t$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;学习内容&lt;/strong&gt;：接下来我们将介绍 TD 如何用于策略评估（TD 预测）和策略改进（TD 控制）。&lt;/p&gt;
&lt;h2&gt;TD 预测 (TD Prediction)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：在给定策略 $\pi$ 的情况下，估计其状态价值函数 $v_\pi$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TD(0) 算法 (用于估计 $v_\pi$)&lt;/strong&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-plaintext&quot;&gt;输入: 需要评估的策略 π
算法参数: 步长 α ∈ (0, 1]
初始化:
  对于所有状态 s ∈ S⁺ (S⁺ 是包含终止状态的状态集):
    V(s) ← 任意值 (例如 V(s)=0)
  V(终止状态) ← 0

循环 对每个 episode:
  初始化 S (该 episode 的第一个状态)
  循环 对 episode 中的每一步:
    A ← 根据策略 π 在状态 S 选择的动作
    执行动作 A, 观察得到奖励 R 和下一状态 S&apos;
    // TD 更新
    V(S) ← V(S) + α * [R + γ * V(S&apos;) - V(S)]
    S ← S&apos; // 转移到下一状态
  直到 S 是终止状态
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;主要特点&lt;/strong&gt;：TD(0) 算法在&lt;strong&gt;每一步&lt;/strong&gt;交互后都进行价值更新，无需等待一个 episode 结束。这使得 TD 学习非常适合&lt;strong&gt;在线 (Online)&lt;/strong&gt; 学习场景。&lt;/p&gt;
&lt;h2&gt;TD 与 MC 的比较：性质与权衡&lt;/h2&gt;
&lt;p&gt;TD 和 MC 作为无模型学习方法，各有优劣：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更新时机与数据需求&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;TD 可&lt;strong&gt;在线学习&lt;/strong&gt;，每步更新；MC 需等待 episode 结束才能更新（&lt;strong&gt;离线更新&lt;/strong&gt;）。&lt;/li&gt;
&lt;li&gt;TD 能从&lt;strong&gt;不完整&lt;/strong&gt;的 episode 中学习；MC 必须基于&lt;strong&gt;完整&lt;/strong&gt; episode。&lt;/li&gt;
&lt;li&gt;TD 天然适用于&lt;strong&gt;连续性任务&lt;/strong&gt;（无终止状态）；MC 主要用于&lt;strong&gt;分幕式任务&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;偏差与方差 (Bias vs. Variance)&lt;/strong&gt;：这是两者最核心的区别之一。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MC 目标 ($G_t$)&lt;/strong&gt;：是 $v_\pi(S_t)$ 的&lt;strong&gt;无偏 (Unbiased)&lt;/strong&gt; 估计。因为 $G_t$ 是从 $t$ 时刻开始实际获得的总回报，其期望就是 $v_\pi(S_t)$。但 $G_t$ 的&lt;strong&gt;方差较高&lt;/strong&gt;，因为它受到从 $t$ 到片段结束所有随机因素（动作选择、状态转移、奖励）的影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TD(0) 目标 ($R_{t+1} + \gamma V(S_{t+1})$)&lt;/strong&gt;：是 $v_\pi(S_t)$ 的&lt;strong&gt;有偏 (Biased)&lt;/strong&gt; 估计。因为目标中使用的 $V(S_{t+1})$ 本身就是当前的估计值，可能并不准确。然而，这个目标的&lt;strong&gt;方差较低&lt;/strong&gt;，因为它只依赖于下一个时间步的随机性 ($R_{t+1}, S_{t+1}$)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;总结&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;MC：高方差，零偏差。对初始值不敏感。&lt;/li&gt;
&lt;li&gt;TD：低方差，有偏差。通常比 MC &lt;strong&gt;学习效率更高&lt;/strong&gt;（收敛更快），但对初始值比较敏感。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对马尔可夫性质的利用&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;TD 隐式地利用了马尔可夫性质，它假设状态 $S_{t+1}$ 的价值 $V(S_{t+1})$ 足够概括未来信息。在满足马尔可夫性的环境中，这种假设有助于提高效率。&lt;/li&gt;
&lt;li&gt;MC 不做此假设，它直接使用实际发生的完整回报序列。这使得 MC 在&lt;strong&gt;非马尔可夫环境&lt;/strong&gt;下可能比 TD(0) 更稳健。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;批量学习 (Batch Learning) 下的行为差异&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;当使用一个固定的、有限的经验数据集进行学习时：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;批量 MC&lt;/strong&gt; 收敛到的价值函数，能最小化训练数据中观测到的回报 $G_t$ 与估计值 $V(S_t)$ 之间的均方误差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;批量 TD(0)&lt;/strong&gt; 收敛到的价值函数，等于根据这批数据构建的&lt;strong&gt;最大似然马尔可夫模型&lt;/strong&gt;的真实价值函数（称为&lt;strong&gt;确定性等价估计 Certainty Equivalence Estimate&lt;/strong&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结论&lt;/strong&gt;：TD(0) 的解更符合 MDP 的结构假设（当前状态价值等于期望的下一步奖励加折扣后的下一状态价值），而 MC 的解直接反映样本平均回报。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;同轨时序差分控制 (On-Policy TD Control)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：使用 TD 方法学习&lt;strong&gt;最优策略 $\pi_*$&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基本思想&lt;/strong&gt;：与 MC 控制类似，在 GPI 框架中进行策略评估和改进。但使用 TD 预测来替代 MC 预测进行策略评估步骤，以利用 TD 的在线学习和高效率特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;需要动作价值 $Q(s, a)$&lt;/strong&gt;：同样，在无模型控制中，我们需要估计&lt;strong&gt;动作价值 $Q(s, a)$&lt;/strong&gt; 而非状态价值 $V(s)$，以便在不知道模型的情况下选择最优动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sarsa：同轨 TD 控制算法&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;名称来源&lt;/strong&gt;：算法的更新依赖于一个状态-动作-奖励-下一状态-下一动作的五元组 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：将 TD(0) 的更新思想直接应用于动作价值 $Q(s, a)$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;更新规则&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sarsa TD 目标&lt;/strong&gt;：$R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$。关键在于，这里使用的是智能体根据当前策略在 $S_{t+1}$ 状态下&lt;strong&gt;实际选择的下一个动作 $A_{t+1}$&lt;/strong&gt; 对应的 Q 值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sarsa TD 误差&lt;/strong&gt;：$\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q 值更新&lt;/strong&gt;：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法流程 (Sarsa)&lt;/strong&gt;：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&quot;language-plaintext&quot;&gt;算法参数: 步长 α ∈ (0, 1], 探索参数 ε &gt; 0 (用于 ε-greedy)
初始化:
  对于所有状态 s ∈ S⁺, 所有动作 a ∈ A(s):
    Q(s, a) ← 任意值 (例如 Q(s, a)=0)
  Q(终止状态, ·) ← 0

循环 对每个 episode:
  初始化 S (该 episode 的第一个状态)
  选择动作 A ← 根据 Q 从状态 S 推导出的策略 (例如 ε-greedy)

  循环 对 episode 中的每一步:
    执行动作 A, 观察得到奖励 R 和下一状态 S&apos;

    // 如果 S&apos; 不是终止状态，则选择下一个动作 A&apos;
    如果 S&apos; 是终止状态:
        A&apos; ← None // 或任意值，因为 Q(终止状态, ·)=0
    否则:
        选择动作 A&apos; ← 根据 Q 从状态 S&apos; 推导出的策略 (例如 ε-greedy)

    // Sarsa 更新
    如果 A&apos; 不是 None:
        Q(S, A) ← Q(S, A) + α * [R + γ * Q(S&apos;, A&apos;) - Q(S, A)]
    否则: // 处理到达终止状态的情况
        Q(S, A) ← Q(S, A) + α * [R + γ * 0 - Q(S, A)]

    S ← S&apos;
    A ← A&apos;

  直到 A 是 None (即 S 是终止状态)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;同轨 (On-Policy) 特性&lt;/strong&gt;：Sarsa 学习的是其&lt;strong&gt;当前正在执行&lt;/strong&gt;的策略（例如 $\epsilon$-greedy 策略）下的动作价值。因为更新目标 $Q(S&apos;, A&apos;)$ 使用的是下一步&lt;strong&gt;实际会遵循策略选择&lt;/strong&gt;的动作 $A&apos;$。它评估和改进的是同一个行为策略。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;例子：有风的网格世界 (Windy Gridworld)&lt;/strong&gt;：Sarsa 可以有效地学习在这个环境中如何抵抗风力到达目标。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sarsa 收敛性&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sarsa 算法能够收敛到最优动作价值函数 $q_*$，只要满足以下条件：
&lt;ol&gt;
&lt;li&gt;策略序列满足 &lt;strong&gt;GLIE (Greedy in the Limit with Infinite Exploration)&lt;/strong&gt;：所有状态-动作对被无限次探索；策略最终收敛到纯贪心策略（例如，$\epsilon$-greedy 中 $\epsilon \to 0$）。&lt;/li&gt;
&lt;li&gt;步长参数 $\alpha_t$ 满足 &lt;strong&gt;Robbins-Monro&lt;/strong&gt; 随机逼近条件：$\sum \alpha_t = \infty$ 且 $\sum \alpha_t^2 &amp;#x3C; \infty$。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;时序差分 (TD) 学习是强化学习中一种核心的无模型方法，巧妙地结合了 DP 的&lt;strong&gt;自举&lt;/strong&gt;和 MC 的&lt;strong&gt;采样&lt;/strong&gt;思想。&lt;/li&gt;
&lt;li&gt;TD 方法可以直接从与环境的交互经验中学习，无需模型知识。&lt;/li&gt;
&lt;li&gt;TD 可以在&lt;strong&gt;每一步&lt;/strong&gt;之后更新价值估计，支持在线学习，并能处理不完整的序列和连续性任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TD(0)&lt;/strong&gt; 是最基础的 TD 预测算法，用于评估给定策略的价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sarsa&lt;/strong&gt; 是最基础的&lt;strong&gt;同轨 (On-Policy)&lt;/strong&gt; TD 控制算法，它学习的是当前行为策略（如 $\epsilon$-greedy）下的动作价值，并通过 GPI 逐步改进策略。&lt;/li&gt;
&lt;li&gt;相比 MC，TD 通常具有&lt;strong&gt;更低的方差&lt;/strong&gt;和&lt;strong&gt;更高的学习效率&lt;/strong&gt;（尤其在马尔可夫环境中），但其估计是&lt;strong&gt;有偏&lt;/strong&gt;的。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-6-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-6-zh.webp"/></item><item><title>RL 学习笔记（5）：蒙特卡洛方法</title><link>https://axi404.github.io/blog/rl-note-5</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-5</guid><description>蒙特卡洛方法</description><pubDate>Sat, 12 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;蒙特卡洛 (Monte Carlo, MC) 方法&lt;/strong&gt; 是一大类依赖于&lt;strong&gt;重复随机抽样&lt;/strong&gt;来获取数值近似解的计算技术。在强化学习（RL）领域，蒙特卡洛方法特指一类&lt;strong&gt;无模型 (Model-Free)&lt;/strong&gt; 的学习算法，它们直接从与环境交互得到的&lt;strong&gt;完整经验片段 (Complete Episodes)&lt;/strong&gt; 中学习价值函数和策略，而&lt;strong&gt;不需要&lt;/strong&gt;关于环境动态（如状态转移概率 $\mathcal{P}$ 和奖励函数 $\mathcal{R}$）的先验知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：MC 方法利用&lt;strong&gt;大数定律&lt;/strong&gt;，通过计算许多样本回报的&lt;strong&gt;平均值 (Mean Sample Return)&lt;/strong&gt; 来估计期望回报，也就是状态或动作的&lt;strong&gt;价值 (Value)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基本要求&lt;/strong&gt;：MC 方法直接适用于&lt;strong&gt;分幕式任务 (Episodic Tasks)&lt;/strong&gt;，因为它们需要等待一个完整的片段结束后才能计算该片段中每个时间步的&lt;strong&gt;回报 (Return, $G_t$)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习内容&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;MC 预测 (Prediction)&lt;/strong&gt;：给定一个策略 $\pi$，估计其价值函数 $v_\pi$ 或 $q_\pi$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MC 控制 (Control)&lt;/strong&gt;：寻找最优策略 $\pi_*$。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;蒙特卡洛预测 (MC Prediction)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：给定策略 $\pi$，从遵循该策略生成的经验片段中估计其&lt;strong&gt;状态价值函数 $v_\pi(s)$&lt;/strong&gt; 或 &lt;strong&gt;动作价值函数 $q_\pi(s, a)$&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基本原理&lt;/strong&gt;：
价值函数定义为期望回报：$v_\pi(s) = \mathbb{E}&lt;em&gt;\pi[G_t | S_t=s]$ （状态价值）或 $q&lt;/em&gt;\pi(s, a) = \mathbb{E}_\pi[G_t | S_t=s, A_t=a]$ （动作价值）。
MC 方法通过收集大量样本回报并计算其平均值来近似这个期望。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 $v_\pi(s)$：收集策略 $\pi$ 下多个片段中，所有访问状态 $s$ 之后直到片段结束的回报 $G_t^{(i)}$，然后求平均。
$$
V(s) \approx \frac{\sum_{i} G_t^{(i)} \text{ where } S_t^{(i)} = s}{N(s)}
$$
其中 $N(s)$ 是状态 $s$ 被访问的总次数（根据首次/每次访问定义）。&lt;/li&gt;
&lt;li&gt;对于 $q_\pi(s, a)$：类似地，收集所有访问状态-动作对 $(s, a)$ 之后的回报 $G_t^{(j)}$，然后求平均。
$$
Q(s, a) \approx \frac{\sum_{j} G_t^{(j)} \text{ where } (S_t^{(j)}, A_t^{(j)}) = (s, a)}{N(s, a)}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;首次访问 (First-Visit) vs 每次访问 (Every-Visit)&lt;/strong&gt;：
在一个片段中，某个状态 $s$ （或状态-动作对 $(s,a)$）可能被访问多次。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;首次访问 MC&lt;/strong&gt;：对于每个片段，只使用该片段中状态 $s$ （或对 $(s,a)$）&lt;strong&gt;第一次&lt;/strong&gt;被访问时之后的回报 $G_t$ 来更新价值估计。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;每次访问 MC&lt;/strong&gt;：对于每个片段，使用该片段中状态 $s$ （或对 $(s,a)$）&lt;strong&gt;每一次&lt;/strong&gt;被访问时之后的回报 $G_t$ 来更新价值估计。
理论上，在无限数据下，两者都会收敛到真实的价值函数。首次访问 MC 在理论分析上更常用，且某些情况下方差可能更低。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;算法：首次访问 MC 预测 (用于估计 $v_\pi$)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;初始化:
  对于所有状态 s ∈ S:
    V(s) ← 任意值 (通常为 0)
    Returns(s) ← 空列表

循环 对每个 episode:
  生成一个 episode 遵循策略 π: S₀, A₀, R₁, S₁, A₁, R₂, ..., S_{T-1}, A_{T-1}, R_T
  G ← 0
  VisitedStates ← 空集合  // 记录本片段已访问的状态 (用于首次访问)

  循环 对于 t = T-1, T-2, ..., 0:
    G ← R_{t+1} + γ * G
    如果 S_t 不在 VisitedStates 中:
      将 G 添加到 Returns(S_t) 列表
      V(S_t) ← Average(Returns(S_t))  // 计算列表的平均值
      将 S_t 添加到 VisitedStates
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;(注意：上述伪代码使用了 Python 风格的 &lt;code&gt;defaultdict&lt;/code&gt; 和列表操作)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;增量式实现 (Incremental Implementation)&lt;/strong&gt;：
为了节省内存，避免存储所有回报，可以使用增量式更新（运行平均）：
对于状态 $s$ （或对 $(s,a)$）的第 $N$ 次（首次/每次）访问，得到回报 $G$：
$$
N \leftarrow N + 1
$$
$$
V(s) \leftarrow V(s) + \frac{1}{N} (G - V(s)) \quad \text{(或 } Q(s,a) \leftarrow Q(s,a) + \frac{1}{N(s,a)} (G - Q(s,a)) \text{)}
$$
或者使用&lt;strong&gt;常数步长 $\alpha \in (0, 1]$&lt;/strong&gt;，这有助于处理非平稳环境（奖励或动态随时间变化）或作为一种指数加权平均：
$$
V(s) \leftarrow V(s) + \alpha (G - V(s)) \quad \text{(或 } Q(s,a) \leftarrow Q(s,a) + \alpha (G - Q(s,a)) \text{)}
$$&lt;/p&gt;
&lt;h2&gt;蒙特卡洛控制 (MC Control)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：在不知道环境模型的情况下，找到最优策略 $\pi_*$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基于动作价值 (Action-Value $Q$)&lt;/strong&gt;：
在无模型情况下，仅仅知道状态价值 $V(s)$ 不足以改进策略，因为我们无法进行一步预测来比较不同动作的优劣（这需要模型 $\mathcal{P}$ 和 $\mathcal{R}$）。因此，MC 控制方法通常直接估计&lt;strong&gt;动作价值函数 $Q(s, a)$&lt;/strong&gt;，然后基于 $Q$ 值来改进策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;框架：广义策略迭代 (GPI)&lt;/strong&gt;：
MC 控制遵循 GPI 的模式：不断交替执行&lt;strong&gt;策略评估&lt;/strong&gt;和&lt;strong&gt;策略改进&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;评估 (E)&lt;/strong&gt;：使用 MC 预测方法（如首次访问 MC）估计当前策略 $\pi$ 的动作价值函数 $Q \approx q_\pi$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;改进 (I)&lt;/strong&gt;：基于当前的 $Q$ 值使策略 $\pi$ 变得更贪心。例如，对于每个状态 $s$，将策略更新为选择具有最高估计值的动作：$\pi(s) \leftarrow \arg\max_a Q(s, a)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心挑战：维持探索 (Maintaining Exploration)&lt;/strong&gt;：
如果策略在改进步骤中变得完全贪心（确定性策略），智能体可能会停止探索某些状态-动作对。如果未被探索的 $(s, a)$ 对恰好是最优策略的一部分，那么算法将永远无法发现真正的最优策略。MC 方法依赖于持续不断地访问所有需要评估其价值的状态-动作对。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案 (A)：试探性出发 (Exploring Starts, ES)&lt;/strong&gt; - 理论方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;假设&lt;/strong&gt;：每次开始一个新片段时，我们能够以非零概率随机选择&lt;strong&gt;任意一个&lt;/strong&gt;状态-动作对 $(S_0, A_0)$ 作为起点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算法：带 ES 的 MC 控制&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;初始化:
  对于所有状态 s ∈ S, 所有动作 a ∈ A(s):
    Q(s, a) ← 任意值 (通常为 0)
    Returns(s, a) ← 空列表
  对于所有状态 s ∈ S:
    π(s) ← 任意确定性动作从 A(s) 中选择

循环 无限次:
  选择起始状态 S₀ 和 起始动作 A₀，确保所有 (s, a) 都有非零概率被选为起点 (ES 假设)
  从 (S₀, A₀) 开始, 之后遵循策略 π 生成一个 episode: S₀, A₀, R₁, ..., S_{T-1}, A_{T-1}, R_T
  G ← 0
  VisitedPairs ← 空集合 // 记录首次访问的 (s, a) 对

  循环 对于 t = T-1, T-2, ..., 0:
    G ← R_{t+1} + γ * G
    令 Pair = (S_t, A_t)

    如果 Pair 不在 VisitedPairs 中:
      将 G 添加到 Returns(Pair) 列表
      Q(Pair) ← Average(Returns(Pair))  // (E) 策略评估

      // (I) 策略改进：更新状态 S_t 的策略
      π(S_t) ← argmax_{a&apos; ∈ A(S_t)} Q(S_t, a&apos;)

      将 Pair 添加到 VisitedPairs
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：在 ES 假设下，保证收敛到最优策略 $\pi_&lt;em&gt;$ 和最优动作价值 $q_&lt;/em&gt;$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：ES 假设在许多实际问题中不现实（例如，机器人不能随意瞬移到任意状态并执行任意动作）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;解决方案 (B)：维持策略本身的探索性&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：不依赖 ES，而是确保用于生成数据的策略本身始终保持一定的探索性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;同轨策略 (On-Policy) vs 离轨策略 (Off-Policy)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;同轨策略学习&lt;/strong&gt;：学习和改进的策略，与用于生成经验数据的策略是&lt;strong&gt;同一个&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;离轨策略学习&lt;/strong&gt;：学习和改进的策略（&lt;strong&gt;目标策略 $\pi$&lt;/strong&gt;），与用于生成经验数据的策略（&lt;strong&gt;行为策略 $b$&lt;/strong&gt;）&lt;strong&gt;不同&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$\epsilon$-软性策略 ($\epsilon$-Soft Policies)&lt;/strong&gt;：为保证持续探索，同轨方法通常采用 $\epsilon$-软性策略，即对于所有状态 $s$ 和动作 $a$，策略选择该动作的概率 $\pi(a|s) \ge \frac{\epsilon}{|\mathcal{A}(s)|} &gt; 0$。这意味着每个动作始终有至少 $\epsilon / |\mathcal{A}(s)|$ 的概率被选中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$\epsilon$-贪心策略 ($\epsilon$-Greedy Policies)&lt;/strong&gt;：是实现 $\epsilon$-软性的一种常用策略。
&lt;ul&gt;
&lt;li&gt;以 $1-\epsilon$ 的概率选择当前认为最优的动作（即 $\arg\max_a Q(s, a)$）。&lt;/li&gt;
&lt;li&gt;以 $\epsilon$ 的概率从所有 $|\mathcal{A}(s)|$ 个可用动作中（包括最优动作）均匀随机选择一个。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;算法：同轨首次访问 MC 控制 (使用 $\epsilon$-Greedy)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;初始化:
  对于所有状态 s ∈ S, 所有动作 a ∈ A(s):
    Q(s, a) ← 任意值 (通常为 0)
    Returns(s, a) ← 空列表
  初始化策略 π 为关于 Q 的 ε-greedy 策略

循环 无限次:
  (a) 使用当前策略 π 生成一个 episode: S₀, A₀, R₁, ..., S_{T-1}, A_{T-1}, R_T
  G ← 0
  VisitedPairs ← 空集合

  (b) 循环 对于 t = T-1, T-2, ..., 0:
    G ← R_{t+1} + γ * G
    令 Pair = (S_t, A_t)

    如果 Pair 不在 VisitedPairs 中:
      将 G 添加到 Returns(Pair) 列表
      Q(Pair) ← Average(Returns(Pair))  // (E) 策略评估

      // (c) 策略改进: 确保状态 S_t 的策略是关于更新后 Q 的 ε-greedy
      // (隐式或显式更新 π(·|S_t))
      令 A* = argmax_{a&apos; ∈ A(S_t)} Q(S_t, a&apos;)
      对于所有动作 a ∈ A(S_t):
        如果 a = A*:
          π(a|S_t) ← 1 - ε + ε / |A(S_t)|
        否则:
          π(a|S_t) ← ε / |A(S_t)|

      将 Pair 添加到 VisitedPairs
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;收敛性&lt;/strong&gt;：同轨 MC 控制（使用 $\epsilon$-Greedy）会收敛到最优的 $\epsilon$-贪心策略，而不是真正的最优策略 $\pi_*$（因为 $\epsilon &gt; 0$ 导致它永远在探索）。不过，这个策略通常也相当好。可以通过逐渐减小 $\epsilon$ 值（例如 $\epsilon_k = 1/k$）来使其在极限情况下趋近于最优策略 (GLIE - Greedy in the Limit with Infinite Exploration)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略改进保证&lt;/strong&gt;：可以证明，对于任意 $\epsilon$-贪心策略 $\pi$，基于其动作价值 $q_\pi$ 进行 $\epsilon$-贪心选择得到的新策略 $\pi&apos;$，仍然满足 $v_{\pi&apos;}(s) \ge v_\pi(s)$ 对所有状态 $s$ 成立。这保证了 GPI 过程的单调性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;离轨策略蒙特卡洛方法 (Off-Policy MC)&lt;/h2&gt;
&lt;p&gt;同轨策略 MC 为了保证探索，最终学习到的只是一个 $\epsilon$-软性策略。&lt;strong&gt;离轨策略学习&lt;/strong&gt; 的目标是：&lt;strong&gt;使用一个具有探索性的行为策略 $b$ (Behavior Policy) 来生成数据，但学习和评估的是一个不同的、通常是确定性的贪心策略——目标策略 $\pi$ (Target Policy)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以学习最优的确定性策略 $\pi_*$（目标策略），同时通过行为策略 $b$ 保证充分的探索。&lt;/li&gt;
&lt;li&gt;更灵活，允许从历史数据、人类演示或其他智能体的经验中学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;挑战与核心技术：重要性采样 (Importance Sampling, IS)&lt;/strong&gt;
由于数据来自 $b$ 而非 $\pi$，直接用 $b$ 产生的回报来评估 $\pi$ 是有偏的。我们需要一种方法来修正这种分布不匹配，这就是重要性采样的作用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;覆盖性假设 (Coverage Assumption)&lt;/strong&gt;：为了能够评估 $\pi$，行为策略 $b$ 必须覆盖 $\pi$ 可能采取的所有动作。即：如果 $\pi(a|s) &gt; 0$，那么必须有 $b(a|s) &gt; 0$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;重要性采样比率 (Importance Sampling Ratio)&lt;/strong&gt;：对于一个从时间 $t$ 开始到片段结束 (时间 $T-1$) 的轨迹片段 $S_t, A_t, S_{t+1}, A_{t+1}, \dots, S_{T-1}, A_{T-1}, S_T$，其在目标策略 $\pi$ 和行为策略 $b$ 下发生的相对概率由 IS 比率给出：
$$
\rho_{t:T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k) p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} b(A_k|S_k) p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}
$$
(环境动态 $p(S_{k+1}|S_k, A_k)$ 相同，被约掉了)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;离轨 MC 预测&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;要估计 $v_\pi(s)$（或 $q_\pi(s,a)$），我们可以使用行为策略 $b$ 生成的回报 $G_t$，并用 IS 比率 $\rho_{t:T-1}$ 对其加权：$G_t^{\pi/b} = \rho_{t:T-1} G_t$。&lt;/li&gt;
&lt;li&gt;理论上 $\mathbb{E}&lt;em&gt;b[G_t^{\pi/b} | S_t=s] = v&lt;/em&gt;\pi(s)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;普通重要性采样 (Ordinary IS, OIS)&lt;/strong&gt;：简单地对加权回报求平均 $V(s) = \frac{\sum \rho G}{\text{count}}$。这是无偏估计，但方差可能极大，尤其当 $\rho$ 值波动很大时。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加权重要性采样 (Weighted IS, WIS)&lt;/strong&gt;：$V(s) = \frac{\sum \rho G}{\sum \rho}$。这是一个有偏估计（偏差随样本增多趋于 0），但通常方差小得多，实践中更常用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;增量式 WIS 更新 (用于 Q 值)&lt;/strong&gt;：维护累积权重 $C(s, a)$ 和价值估计 $Q(s, a)$。当获得状态-动作对 $(s, a)$ 的第 $n$ 个回报 $G_n$ 和对应的权重 $W_n = \rho_{t:T(n)-1}$ 时：
$$
C_n \leftarrow C_{n-1} + W_n \quad (\text{初始 } C_0 = 0)
$$
$$
Q_n(s, a) \leftarrow Q_{n-1}(s, a) + \frac{W_n}{C_n} [G_n - Q_{n-1}(s, a)]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法：离轨 MC 预测 (WIS 估计 $q_\pi$)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;输入: 目标策略 π
初始化:
  对于所有状态 s ∈ S, 所有动作 a ∈ A(s):
    Q(s, a) ← 任意值 (通常为 0)
    C(s, a) ← 0  // 累积权重

循环 无限次 (对每个 episode):
  选择行为策略 b (确保覆盖 π, 例如关于当前 Q 的 ε-greedy 策略)
  使用 b 生成 episode: S₀, A₀, R₁, ..., S_{T-1}, A_{T-1}, R_T
  G ← 0
  W ← 1  // 重要性采样比率的累乘

  循环 对于 t = T-1, T-2, ..., 0:
    G ← R_{t+1} + γ * G
    令 Pair = (S_t, A_t)

    C(Pair) ← C(Pair) + W
    如果 C(Pair) = 0:
      跳转到循环的下一次迭代 (t-1) // 避免除零
    Q(Pair) ← Q(Pair) + (W / C(Pair)) * [G - Q(Pair)] // WIS 更新

    // 更新下次回溯的 IS 权重
    W ← W * (π(A_t | S_t) / b(A_t | S_t))

    如果 W = 0:
      终止内层循环 (对于 t 的循环) // 后续轨迹与 π 无关
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;离轨 MC 控制 (Off-Policy MC Control)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;结合离轨预测 (WIS) 和策略改进。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标策略 $\pi$&lt;/strong&gt;：通常是&lt;strong&gt;确定性贪心策略&lt;/strong&gt;，即 $\pi(s) = \arg\max_a Q(s, a)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;行为策略 $b$&lt;/strong&gt;：必须是探索性的，如 $\epsilon$-greedy 策略（基于当前 Q 值）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算法&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;初始化:
  对于所有状态 s ∈ S, 所有动作 a ∈ A(s):
    Q(s, a) ← 任意值 (通常为 0)
    C(s, a) ← 0
  目标策略 π(s) ← argmax_{a&apos; ∈ A(s)} Q(s, a&apos;)  // 初始贪心策略

循环 无限次 (对每个 episode):
  选择行为策略 b (例如关于当前 Q 的 ε-greedy 策略)
  使用 b 生成 episode: S₀, A₀, R₁, ..., S_{T-1}, A_{T-1}, R_T
  G ← 0
  W ← 1

  循环 对于 t = T-1, T-2, ..., 0:
    G ← R_{t+1} + γ * G
    令 Pair = (S_t, A_t)

    C(Pair) ← C(Pair) + W
    如果 C(Pair) = 0:
      跳转到循环的下一次迭代 (t-1)
    Q(Pair) ← Q(Pair) + (W / C(Pair)) * [G - Q(Pair)] // (E) Off-policy 评估

    // (I) 策略改进: 更新目标策略 π 使其对 S_t 贪心
    π(S_t) ← argmax_{a&apos; ∈ A(S_t)} Q(S_t, a&apos;)

    // 检查行为策略选择的动作是否是目标策略会选择的动作
    如果 A_t ≠ π(S_t):
      终止内层循环 (对于 t 的循环) // 因为 W 之后将变为 0

    // 更新 IS 权重 (假设 π 是确定性的, π(A_t|S_t) = 1)
    如果 b(A_t|S_t) = 0:
       W ← 0 // 覆盖性假设不满足 (理论上不应发生)
    否则:
       W ← W * (1 / b(A_t | S_t))

    如果 W = 0:
      终止内层循环 (对于 t 的循环) // 以防 b(A_t|S_t) = 0
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;关键点&lt;/strong&gt;：当目标策略 $\pi$ 是确定性贪心时，只要行为策略 $b$ 在某一步 $t$ 选择了一个非贪心动作 $A_t \neq \pi(S_t)$，那么 $\pi(A_t|S_t) = 0$，导致 IS 比率 $\rho_{t:T-1}$ 必然为 0。这意味着从该点往前的回报对于评估 $\pi$ 没有贡献，可以提前停止处理该 episode 的剩余部分。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结与讨论&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;蒙特卡洛 (MC) 方法&lt;/strong&gt; 是一种&lt;strong&gt;无模型&lt;/strong&gt;的 RL 方法，直接从&lt;strong&gt;完整经验片段&lt;/strong&gt;中学习价值函数和策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MC 预测&lt;/strong&gt; 使用平均样本回报来估计 $v_\pi$ 或 $q_\pi$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MC 控制&lt;/strong&gt; 在 GPI 框架下，通过估计 $q_\pi$ 并进行策略改进来寻找最优策略。&lt;strong&gt;动作价值 $Q$&lt;/strong&gt; 对于无模型控制至关重要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;探索&lt;/strong&gt; 是 MC 控制的核心挑战。可通过 &lt;strong&gt;ES（理论）&lt;/strong&gt;、&lt;strong&gt;同轨 $\epsilon$-软性策略（实用）&lt;/strong&gt; 或 &lt;strong&gt;离轨方法&lt;/strong&gt; 来解决。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;同轨 (On-Policy) MC&lt;/strong&gt; 学习最优的 $\epsilon$-软性策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;离轨 (Off-Policy) MC&lt;/strong&gt; 使用 &lt;strong&gt;重要性采样 (IS)&lt;/strong&gt;，允许从行为策略 $b$ 生成的数据中学习目标策略 $\pi$（通常是最优贪心策略）。&lt;strong&gt;加权重要性采样 (WIS)&lt;/strong&gt; 在实践中更常用以减小方差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MC 的优缺点&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;优点：无模型；概念简单；无偏估计（使用完整回报）。&lt;/li&gt;
&lt;li&gt;缺点：必须是分幕式任务；效率不高（需要等待片段结束）；方差可能较高（尤其是普通 IS）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;展望&lt;/strong&gt;：MC 方法的局限性（特别是高方差和对完整片段的依赖）引出了下一类重要的无模型学习方法——&lt;strong&gt;时序差分 (Temporal-Difference, TD) 学习&lt;/strong&gt;，它结合了 MC 和动态规划的思想。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-5-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-5-zh.webp"/></item><item><title>RL 学习笔记（4）：动态规划</title><link>https://axi404.github.io/blog/rl-note-4</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-4</guid><description>动态规划</description><pubDate>Fri, 11 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言：什么是动态规划及其在 RL 中的作用？&lt;/h2&gt;
&lt;p&gt;动态规划（Dynamic Programming, DP）并非单一算法，而是一系列用于解决特定类型问题的优化方法的总称，由 Richard Bellman 提出。在强化学习（RL）的背景下，DP 特指&lt;strong&gt;在已知环境模型（即马尔可夫决策过程 MDP）完全信息的情况下，计算最优策略 $\pi_*$ 的一组算法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：DP 充分利用了&lt;strong&gt;价值函数&lt;/strong&gt;的结构，特别是通过&lt;strong&gt;贝尔曼方程 (Bellman Equation)&lt;/strong&gt; 来组织计算，有效地寻找最优策略。DP 适用的问题通常具有&lt;strong&gt;最优子结构&lt;/strong&gt;和&lt;strong&gt;重叠子问题&lt;/strong&gt;的特性，而 MDP 正好满足这些条件（贝尔曼方程体现了递归分解，价值函数的计算会被重复利用）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DP 主要解决两类问题 (MDP 规划问题)&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;预测 (Prediction / Policy Evaluation)&lt;/strong&gt;：评估一个给定的策略 $\pi$ 有多好。即输入 MDP 模型和策略 $\pi$，输出该策略下的价值函数 $v_\pi$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;控制 (Control)&lt;/strong&gt;：寻找最优的行为方式。即输入 MDP 模型，输出最优价值函数 $v_&lt;em&gt;$ 和最优策略 $\pi_&lt;/em&gt;$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键前提&lt;/strong&gt;：DP 算法&lt;strong&gt;必须&lt;/strong&gt;知道环境的完整动态特性，即状态转移概率 $p(s&apos;,r|s,a)$ 和奖励函数 $\mathcal{R}_s^a$。这通常被称为“基于模型的规划”（Model-based Planning）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心工具：贝尔曼方程回顾&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DP 算法的基础是价值函数必须满足的贝尔曼方程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;贝尔曼期望方程 (Bellman Expectation Equation for $v_\pi$)&lt;/strong&gt;：描述了给定策略 $\pi$ 下状态价值 $v_\pi(s)$ 与其后继状态价值的关系。
$$
v_\pi(s) = \sum_a \pi(a|s) \sum_{s&apos;,r} p(s&apos;,r|s,a) [r + \gamma v_\pi(s&apos;)]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;贝尔曼最优方程 (Bellman Optimality Equation for $v_*$)&lt;/strong&gt;：描述了最优状态价值 $v_&lt;em&gt;(s)$ 与其最优后继状态价值的关系。
$$
v_&lt;/em&gt;(s) = \max_{a} \sum_{s&apos;,r} p(s&apos;,r|s,a) [r + \gamma v_&lt;em&gt;(s&apos;)]
$$
类似地，对于最优动作价值函数 $q_&lt;/em&gt;(s,a)$:
$$
q_&lt;em&gt;(s,a) = \sum_{s&apos;,r} p(s&apos;,r|s,a) [r + \gamma \max_{a&apos;} q_&lt;/em&gt;(s&apos;, a&apos;)]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;策略评估 (预测问题)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：计算给定策略 $\pi$ 的状态价值函数 $v_\pi$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法：迭代策略评估 (Iterative Policy Evaluation)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;该方法通过迭代式地应用贝尔曼期望方程来逼近 $v_\pi$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;初始化&lt;/strong&gt;：从一个任意的初始价值函数估计 $v_0$ 开始（通常全零）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代更新&lt;/strong&gt;：在第 $k+1$ 次迭代中，对&lt;strong&gt;所有&lt;/strong&gt;状态 $s \in \mathcal{S}$，使用第 $k$ 轮的价值 $v_k$ 来计算新的价值估计 $v_{k+1}(s)$：
$$
v_{k+1}(s) \leftarrow \sum_a \pi(a|s) \sum_{s&apos;,r} p(s&apos;,r|s,a) [r + \gamma v_k(s&apos;)]
$$
或使用模型定义的期望奖励 $\mathcal{R}&lt;em&gt;s^a = \sum&lt;/em&gt;{s&apos;,r} p(s&apos;,r|s,a) r$ 和状态转移概率 $\mathcal{P}&lt;em&gt;{ss&apos;}^a = \sum_r p(s&apos;,r|s,a)$：
$$
v&lt;/em&gt;{k+1}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi(a|s) \left( \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}_{ss&apos;}^a v_k(s&apos;) \right)
$$
这种对所有状态同时基于旧值进行更新的方式称为&lt;strong&gt;同步备份 (Synchronous Backup)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;收敛性&lt;/strong&gt;：由于贝尔曼期望算子是一个&lt;strong&gt;压缩映射 (Contraction Mapping)&lt;/strong&gt;（在折扣因子 $\gamma &amp;#x3C; 1$ 时），价值函数序列 $v_0, v_1, v_2, \dots$ 会保证收敛到真实的 $v_\pi$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法：迭代策略评估&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: MDP (S, A, P, R, γ), 策略 π, 阈值 θ &gt; 0
初始化 V(s) = 0 对于所有状态 s ∈ S

循环:
  Δ = 0  // 初始化本轮最大价值变化量
  对于 每个状态 s ∈ S:
    v = V(s)  // 存储旧价值
    // 应用贝尔曼期望备份更新 V(s)
    V(s) = Σ[a] π(a|s) * Σ[s&apos;,r] p(s&apos;,r|s,a) * (r + γ * V(s&apos;))
    Δ = max(Δ, |v - V(s)|) // 更新最大变化量
  如果 Δ &amp;#x3C; θ:
    终止循环 (收敛)

输出 V ≈ v_π
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;寻找最优策略 (控制问题)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：找到最优策略 $\pi_&lt;em&gt;$ 和相应的最优价值函数 $v_&lt;/em&gt;$ (或 $q_*$)。&lt;/p&gt;
&lt;p&gt;控制问题通常基于一个重要的思想：&lt;strong&gt;广义策略迭代 (Generalized Policy Iteration - GPI)&lt;/strong&gt;。GPI 指的是让&lt;strong&gt;策略评估&lt;/strong&gt;（Policy Evaluation）和&lt;strong&gt;策略改进&lt;/strong&gt;（Policy Improvement）两个过程相互作用、共同驱动策略和价值函数趋向最优的通用模式。几乎所有 RL 算法都可以看作是 GPI 的某种实现。&lt;/p&gt;
&lt;p&gt;DP 中实现 GPI 的两种主要算法是策略迭代和价值迭代。&lt;/p&gt;
&lt;h3&gt;策略迭代&lt;/h3&gt;
&lt;p&gt;策略迭代通过显式地交替进行完整的策略评估和策略改进步骤来寻找最优策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心流程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;初始化&lt;/strong&gt;：从一个任意（通常是随机的）策略 $\pi_0$ 和相应的（可能不准确的）价值函数 $v_0$ 开始。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重复&lt;/strong&gt;以下两个步骤直至策略稳定：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(E) 策略评估 (Policy Evaluation)&lt;/strong&gt;：使用当前策略 $\pi_k$，通过&lt;strong&gt;迭代策略评估&lt;/strong&gt;（见第 2 节）计算其精确的价值函数 $v_{\pi_k}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(I) 策略改进 (Policy Improvement)&lt;/strong&gt;：基于计算得到的 $v_{\pi_k}$，生成一个&lt;strong&gt;新的、改进的&lt;/strong&gt;策略 $\pi_{k+1}$。对每个状态 $s$，选择能够最大化基于 $v_{\pi_k}$ 的一步期望回报的动作（即对动作价值函数 $q_{\pi_k}(s,a)$ 进行贪心选择）：
$$
\pi_{k+1}(s) \leftarrow \arg\max_{a \in \mathcal{A}} q_{\pi_k}(s, a) = \arg\max_{a \in \mathcal{A}} \left{ \sum_{s&apos;,r} p(s&apos;,r|s,a) [r + \gamma v_{\pi_k}(s&apos;)] \right}
$$
即:
$$
\pi_{k+1}(s) \leftarrow \arg\max_{a \in \mathcal{A}} \left{ \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}&lt;em&gt;{ss&apos;}^a v&lt;/em&gt;{\pi_k}(s&apos;) \right}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;策略改进定理 (Policy Improvement Theorem)&lt;/strong&gt;：
该定理保证，通过对 $v_\pi$ 贪心选择动作得到的新策略 $\pi&apos;$，其价值函数 $v_{\pi&apos;}$ 对于所有状态 $s$ 都满足 $v_{\pi&apos;}(s) \ge v_\pi(s)$。如果 $v_{\pi&apos;} = v_\pi$，则 $v_\pi$ 必定等于 $v_*$，且 $\pi$ 是最优策略之一。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;收敛性&lt;/strong&gt;：由于状态和动作空间有限时，策略的数量也是有限的，且每次改进要么严格提升价值，要么保持不变（此时已达到最优），因此策略迭代保证在有限次迭代内收敛到最优策略 $\pi_&lt;em&gt;$ 和最优价值函数 $v_&lt;/em&gt;$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法：策略迭代&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. 初始化 V(s) ∈ R 和 π(s) ∈ A(s) 任意地对于所有 s ∈ S
2. 循环 (策略迭代主循环):
    // (E) 策略评估: 计算 v_π
    循环: // 策略评估的内层循环 (使用迭代策略评估)
        Δ = 0
        对于 每个状态 s:
            v = V(s)
            // 使用当前策略 π 的贝尔曼期望方程更新 V(s)
            V(s) = Σ[s&apos;,r] p(s&apos;,r|s, π(s)) * (r + γ * V(s&apos;))
            Δ = max(Δ, |v - V(s)|)
        如果 Δ &amp;#x3C; θ: 终止内层循环 (V ≈ v_π)

    // (I) 策略改进
    policy_stable = true  // 标记策略是否在本轮改进中发生变化
    对于 每个状态 s:
        old_action = π(s)  // 存储旧动作
        // 根据当前的 v_π 贪心选择动作，找到最大化 q_π(s, a) 的动作
        π(s) = argmax[a] Σ[s&apos;,r] p(s&apos;,r|s, a) * (r + γ * V(s&apos;))
        // 检查策略是否已经改变
        如果 old_action != π(s):
            policy_stable = false

    // 如果策略在所有状态上都没有改变，说明已达到最优策略
    如果 policy_stable:
        终止外层循环，返回 V ≈ v_* 和 π ≈ π_*
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;价值迭代&lt;/h3&gt;
&lt;p&gt;价值迭代是另一种寻找最优策略的 DP 算法。它不显式地进行完整的策略评估，而是将一步策略评估和策略改进结合，直接迭代贝尔曼最优方程来逼近最优价值函数 $v_*$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：策略迭代中的策略评估步骤可能需要很多次迭代才能收敛。价值迭代通过在每次迭代中直接应用贝尔曼&lt;strong&gt;最优&lt;/strong&gt;方程的备份操作，来更快速地逼近 $v_*$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;更新规则&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;初始化&lt;/strong&gt;：从一个任意的初始价值函数估计 $v_0$ 开始（通常全零）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代更新&lt;/strong&gt;：在第 $k+1$ 次迭代中，对&lt;strong&gt;所有&lt;/strong&gt;状态 $s \in \mathcal{S}$，使用第 $k$ 轮的价值 $v_k$ 来计算新的价值估计 $v_{k+1}(s)$，直接结合了最大化操作（隐式的策略改进）：
$$
v_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \left{ \sum_{s&apos;,r} p(s&apos;,r|s,a) [r + \gamma v_k(s&apos;)] \right}
$$
或使用 $\mathcal{R}$ 和 $\mathcal{P}$：
$$
v_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \left{ \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}_{ss&apos;}^a v_k(s&apos;) \right}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;收敛性&lt;/strong&gt;：由于贝尔曼&lt;strong&gt;最优&lt;/strong&gt;算子也是一个&lt;strong&gt;压缩映射&lt;/strong&gt;，价值函数序列 $v_0, v_1, v_2, \dots$ 会保证收敛到最优价值函数 $v_*$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;与策略迭代的区别&lt;/strong&gt;：价值迭代的中间价值函数 $v_k$ 不一定对应任何一个固定策略的价值函数（除非到最后收敛）。它直接朝着 $v_*$ 逼近。策略迭代则是在完整的 $v_\pi$ 和 $\pi$ 之间切换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法：价值迭代&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: MDP (S, A, P, R, γ), 阈值 θ &gt; 0
初始化 V(s) = 0 对于所有状态 s ∈ S

循环:
  Δ = 0
  对于 每个状态 s ∈ S:
    v = V(s)
    // 应用贝尔曼最优备份更新 V(s)
    V(s) = max[a] Σ[s&apos;,r] p(s&apos;,r|s,a) * (r + γ * V(s&apos;))
    Δ = max(Δ, |v - V(s)|)
  如果 Δ &amp;#x3C; θ:
    终止循环 (V ≈ v_*)

// 从收敛的 V 中提取确定性最优策略 π_*
初始化 π(s) 任意地 // 或者直接在下一步中赋值
对于 每个状态 s ∈ S:
  // 根据最优价值函数 V 选择最优动作
  π(s) = argmax[a] Σ[s&apos;,r] p(s&apos;,r|s,a) * (r + γ * V(s&apos;))

输出 π
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;DP 算法总结与比较&lt;/h2&gt;
&lt;p&gt;| 解决的问题类型 | 基于的贝尔曼方程        | 主要 DP 算法        |
| :------------- | :---------------------- | :------------------ |
| 预测 (评估策略) | 贝尔曼期望方程          | 迭代策略评估        |
| 控制 (寻找最优) | 贝尔曼期望 + 贪心改进 | 策略迭代 (PI)       |
| 控制 (寻找最优) | 贝尔曼最优方程          | 价值迭代 (VI)       |&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有这些基本的 DP 算法都依赖于状态价值函数 $v(s)$（或动作价值函数 $q(s,a)$）的迭代更新。&lt;/li&gt;
&lt;li&gt;每次&lt;strong&gt;同步&lt;/strong&gt;迭代（遍历所有状态）的计算复杂度大致为 $O(|\mathcal{S}|^2 |\mathcal{A}|)$ （对于基于 $v$ 的更新）。如果状态转移是稀疏的，可能会更低。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;异步动态规划&lt;/h2&gt;
&lt;p&gt;上述的&lt;strong&gt;同步 DP&lt;/strong&gt; 算法在每次迭代中都需要对整个状态集进行一次完整的扫描和更新。当状态空间很大时，这会非常耗时。&lt;strong&gt;异步 DP&lt;/strong&gt; 放宽了这一要求，允许更灵活的更新方式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：不进行全局同步扫描，而是以任意顺序、选择性地备份状态的价值。更新一个状态的价值时，使用其他状态的&lt;strong&gt;最新&lt;/strong&gt;可用值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;减少计算量&lt;/strong&gt;：可以避免对价值已经收敛或变化不大的状态进行计算。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;聚焦计算&lt;/strong&gt;：可以优先更新那些与目标相关、或者贝尔曼误差较大的状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可能更快收敛&lt;/strong&gt;：有时通过优先更新关键状态，能够更快地传播价值信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;收敛性&lt;/strong&gt;：只要保证所有状态最终都会被&lt;strong&gt;持续地&lt;/strong&gt;（无限次地）选中进行更新，异步 DP 仍然能够收敛到正确的价值函数（$v_\pi$ 或 $v_*$）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;常见变种&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;原位 DP (In-place DP)&lt;/strong&gt;：只维护&lt;strong&gt;一份&lt;/strong&gt;价值函数数组 $V(s)$，更新时立即写入，后续状态的更新会直接使用这个新值。更新顺序变得重要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优先级扫描 (Prioritized Sweeping)&lt;/strong&gt;：维护一个优先级队列，根据状态的&lt;strong&gt;贝尔曼误差&lt;/strong&gt;（当前价值与备份后价值的差值）的大小来决定更新哪个状态。优先更新误差大的状态，并将更新的影响传播给其前驱状态（那些可能转移到当前状态的状态）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实时 DP (Real-time DP)&lt;/strong&gt;：只更新智能体在与环境（或模拟环境）交互过程中&lt;strong&gt;实际访问到&lt;/strong&gt;的状态 $S_t$。非常适用于状态空间巨大，但智能体实际能到达或关心的状态子集有限的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DP 的局限性与展望&lt;/h2&gt;
&lt;p&gt;尽管 DP 是理解 RL 价值函数和最优策略的基础，但它有两大主要局限性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;维度诅咒 (Curse of Dimensionality)&lt;/strong&gt;：DP 算法的计算和存储需求随着状态数量 $|\mathcal{S}|$（有时还有动作数量 $|\mathcal{A}|$）的增长而急剧增加（通常是多项式级别，如 $O(|\mathcal{S}|^2 |\mathcal{A}|)$）。对于状态空间非常庞大的现实问题（如围棋、机器人控制），DP 变得不可行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要完美的环境模型 (Requires a Perfect Model)&lt;/strong&gt;：DP 假设状态转移概率 $\mathcal{P}$ 和奖励函数 $\mathcal{R}$ 是完全已知的。然而，在许多实际应用中，我们无法事先获得精确的环境模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;展望：超越 DP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;正是由于 DP 的这些局限性，特别是对模型的需求和计算复杂性问题，促使了&lt;strong&gt;无模型 (Model-Free)&lt;/strong&gt; 强化学习方法的发展，例如蒙特卡洛（Monte Carlo）方法和时序差分（Temporal-Difference, TD）学习。此外，为了解决维度诅咒问题，&lt;strong&gt;函数逼近 (Function Approximation)&lt;/strong&gt;（如使用神经网络）被引入来近似价值函数或策略，而不是使用表格存储，这引出了深度强化学习（Deep Reinforcement Learning）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;近似动态规划 (Approximate Dynamic Programming)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;即使在有模型的情况下，如果状态空间过大，也可以使用函数逼近（例如 $\hat{v}(s, w)$ 或 $\hat{q}(s, a, w)$，其中 $w$ 是参数）来代替表格存储价值。DP 的备份操作可以用来生成训练样本：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择一个状态 $s$（或一批状态）。&lt;/li&gt;
&lt;li&gt;使用当前的近似价值 $\hat{v}(s&apos;, w_k)$ 和贝尔曼（最优或期望）备份计算一个目标价值 $\tilde{v}_k(s)$。&lt;/li&gt;
&lt;li&gt;将 $(s, \tilde{v}&lt;em&gt;k(s))$ 作为监督学习的样本，更新参数 $w$ 以最小化预测误差，得到 $w&lt;/em&gt;{k+1}$ 和新的近似函数 $\hat{v}(s, w_{k+1})$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种方法结合了 DP 的思想和函数逼近的能力，有时也被称为 &lt;strong&gt;拟合价值迭代 (Fitted Value Iteration)&lt;/strong&gt; 或相关方法。&lt;/p&gt;
&lt;h2&gt;小结 (Summary)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;动态规划提供了一套理论上保证找到最优策略（在已知模型下）的算法基础。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略评估&lt;/strong&gt;用于计算给定策略的价值（预测问题），基于贝尔曼期望方程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略迭代&lt;/strong&gt;和&lt;strong&gt;价值迭代&lt;/strong&gt;用于寻找最优策略（控制问题），分别基于贝尔曼期望方程+贪心改进 和 贝尔曼最优方程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;广义策略迭代 (GPI)&lt;/strong&gt; 是评估和改进相互作用以趋向最优的核心思想。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;异步 DP&lt;/strong&gt; 提高了 DP 在实践中的效率，尤其适用于大规模问题。&lt;/li&gt;
&lt;li&gt;DP 的主要缺点是&lt;strong&gt;需要完整模型&lt;/strong&gt;和&lt;strong&gt;对维度诅咒敏感&lt;/strong&gt;，这激发了现代强化学习中无模型方法和函数逼近技术的发展。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-4-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-4-zh.webp"/></item><item><title>RL 学习笔记（3）：马尔可夫决策过程</title><link>https://axi404.github.io/blog/rl-note-3</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-3</guid><description>马尔可夫决策过程</description><pubDate>Thu, 10 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;本章介绍马尔可夫决策过程 (Markov Decision Process, MDP)，它是强化学习中用于建模完全可观测环境下的序贯决策问题的标准框架。我们将从基础的马尔可夫性质开始，逐步构建马尔可夫过程 (MP)、马尔可夫奖励过程 (MRP)，最终引出 MDP 及其核心概念，如策略、价值函数和贝尔曼方程。&lt;/p&gt;
&lt;h2&gt;马尔可夫性质与过程 (Markov Property and Processes)&lt;/h2&gt;
&lt;h3&gt;马尔可夫性质 (Markov Property)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义:&lt;/strong&gt; 当前状态 $S_t$ 包含了预测未来所需的所有历史信息。即下一状态 $S_{t+1}$ 的概率分布仅依赖于当前状态 $S_t$。
$$\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1, \dots, S_t]$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;意义:&lt;/strong&gt; 当前状态是未来的“充分统计量”，历史信息可被丢弃。环境状态 $S^e$ 通常假定满足此性质。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;马尔可夫过程 (Markov Process, MP) / 马尔可夫链 (Markov Chain)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义:&lt;/strong&gt; 一个满足马尔可夫性质的随机状态序列，是描述无外部控制（动作）和奖励的系统动态的模型。由元组 $(\mathcal{S}, \mathcal{P})$ 定义：
&lt;ul&gt;
&lt;li&gt;$\mathcal{S}$: （有限）状态集合。&lt;/li&gt;
&lt;li&gt;$\mathcal{P}$: 状态转移概率矩阵，$\mathcal{P}&lt;em&gt;{ss&apos;} = \mathbb{P}[S&lt;/em&gt;{t+1}=s&apos; | S_t=s]$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;马尔可夫奖励过程 (Markov Reward Process, MRP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义:&lt;/strong&gt; 在 MP 基础上增加了奖励和折扣因子。由元组 $(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$ 定义：
&lt;ul&gt;
&lt;li&gt;$\mathcal{S}, \mathcal{P}$: 同 MP。&lt;/li&gt;
&lt;li&gt;$\mathcal{R}$: 奖励函数，$\mathcal{R}&lt;em&gt;s = \mathbb{E}[R&lt;/em&gt;{t+1} | S_t=s]$ (离开状态 $s$ 的期望立即奖励)。&lt;/li&gt;
&lt;li&gt;$\gamma$: 折扣因子 ($\gamma \in [0, 1]$)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回报 (Return) $G_t$:&lt;/strong&gt; 从 $t$ 时刻开始的折扣累积奖励。
$$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;状态价值函数 (State-Value Function) $v(s)$:&lt;/strong&gt; 在 MRP 中，从状态 $s$ 开始的期望回报。
$$v(s) \doteq \mathbb{E}[G_t | S_t = s]$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MRP 的贝尔曼方程 (Bellman Equation for MRP):&lt;/strong&gt; 描述了状态价值与其后继状态价值的关系。
$$v(s) = \mathcal{R}&lt;em&gt;s + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}_{ss&apos;} v(s&apos;)$$
矩阵形式: $v = \mathcal{R} + \gamma \mathcal{P} v$，可直接求解 $v = (I - \gamma \mathcal{P})^{-1} \mathcal{R}$ (对于小型 MRP)。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;马尔可夫决策过程 (Markov Decision Process, MDP)&lt;/h2&gt;
&lt;p&gt;MDP 在 MRP 基础上引入了动作和策略，用于形式化&lt;strong&gt;完全可观测&lt;/strong&gt;的 RL 问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;定义:&lt;/strong&gt; 由元组 $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$ 定义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{S}$: 有限状态集。&lt;/li&gt;
&lt;li&gt;$\mathcal{A}$: 有限动作集 (可能依赖于状态 $\mathcal{A}(s)$)。&lt;/li&gt;
&lt;li&gt;$\mathcal{P}$: 状态转移概率函数，$\mathcal{P}&lt;em&gt;{ss&apos;}^a = \mathbb{P}[S&lt;/em&gt;{t+1}=s&apos; | S_t=s, A_t=a]$。&lt;/li&gt;
&lt;li&gt;$\mathcal{R}$: 奖励函数，$\mathcal{R}&lt;em&gt;s^a = \mathbb{E}[R&lt;/em&gt;{t+1} | S_t=s, A_t=a]$。&lt;/li&gt;
&lt;li&gt;$\gamma$: 折扣因子。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心假设:&lt;/strong&gt; 环境完全可观测 ($O_t = S_t^e$) 且状态满足马尔可夫性质。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;与相关模型的关系:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MAB:&lt;/strong&gt; 可视为单状态 MDP。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;POMDP (部分可观测 MDP):&lt;/strong&gt; 当 $O_t \neq S_t^e$ 时使用。需要维护信念状态 (Belief State) 并在此空间上求解。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;策略与价值函数 (Policy and Value Functions in MDPs)&lt;/h2&gt;
&lt;p&gt;在 MDP 中，智能体的行为由策略决定，价值函数用于评估策略的好坏。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;策略 (Policy) $\pi$:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义:&lt;/strong&gt; 智能体在状态 $s$ 选择动作 $a$ 的规则，通常是概率分布 $\pi(a|s) = \mathbb{P}[A_t=a | S_t=s]$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特性:&lt;/strong&gt; 通常假定是稳态的 (stationary) 和马尔可夫的 (Markovian)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;固定策略下的 MDP:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给定策略 $\pi$，MDP 退化为一个 MRP $(\mathcal{S}, \mathcal{P}^\pi, \mathcal{R}^\pi, \gamma)$，其中：
&lt;ul&gt;
&lt;li&gt;$\mathcal{P}&lt;em&gt;{ss&apos;}^\pi = \sum_a \pi(a|s) \mathcal{P}&lt;/em&gt;{ss&apos;}^a$ (策略下的状态转移概率)&lt;/li&gt;
&lt;li&gt;$\mathcal{R}_s^\pi = \sum_a \pi(a|s) \mathcal{R}_s^a$ (策略下的期望奖励)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MDP 的价值函数 (依赖于策略 $\pi$):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;状态价值函数 $v_\pi(s)$:&lt;/strong&gt; 从状态 $s$ 开始，遵循策略 $\pi$ 的期望回报。
$$v_\pi(s) \doteq \mathbb{E}_\pi [G_t | S_t = s]$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作价值函数 $q_\pi(s, a)$:&lt;/strong&gt; 从状态 $s$ 开始，执行动作 $a$，然后遵循策略 $\pi$ 的期望回报。
$$q_\pi(s, a) \doteq \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;贝尔曼期望方程 (Bellman Expectation Equation):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;描述了&lt;strong&gt;给定策略 $\pi$&lt;/strong&gt; 下 $v_\pi$ 和 $q_\pi$ 满足的一致性条件 (线性方程)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$v_\pi(s)$ 的方程:&lt;/strong&gt;
$$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) q_\pi(s, a) = \sum_{a \in \mathcal{A}} \pi(a|s) \left( \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}&lt;em&gt;{ss&apos;}^a v&lt;/em&gt;\pi(s&apos;) \right)$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$q_\pi(s, a)$ 的方程:&lt;/strong&gt;
$$q_\pi(s, a) = \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}&lt;em&gt;{ss&apos;}^a v&lt;/em&gt;\pi(s&apos;) = \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}&lt;em&gt;{ss&apos;}^a \sum&lt;/em&gt;{a&apos; \in \mathcal{A}} \pi(a&apos;|s&apos;) q_\pi(s&apos;, a&apos;)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;最优性 (Optimality in MDPs)&lt;/h2&gt;
&lt;p&gt;RL 的目标是找到使期望回报最大化的最优策略。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;最优价值函数:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;最优状态价值函数 $v_*(s)$:&lt;/strong&gt; 所有策略中可能达到的最大期望回报。
$$v_*(s) = \max_\pi v_\pi(s)$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最优动作价值函数 $q_*(s, a)$:&lt;/strong&gt; 执行动作 $a$ 后遵循最优策略能达到的最大期望回报。
$$q_*(s, a) = \max_\pi q_\pi(s, a)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;最优策略 $\pi_*$:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义:&lt;/strong&gt; 能够达到最优价值函数的策略，即 $v_{\pi_&lt;em&gt;}(s) = v_&lt;/em&gt;(s)$ 对所有 $s$ 成立。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存在性:&lt;/strong&gt; 至少存在一个最优策略，且总能找到确定性的最优策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;从 $q_&lt;em&gt;$ 导出 $\pi_&lt;/em&gt;$:&lt;/strong&gt; 通过贪心选择：
$$\pi_&lt;em&gt;(a|s) = 1 \iff a = \arg\max_{a&apos; \in \mathcal{A}} q_&lt;/em&gt;(s, a&apos;)$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;从 $v_&lt;em&gt;$ 导出 $\pi_&lt;/em&gt;$ (需要模型):&lt;/strong&gt; 通过一步前看来选择：
$$\pi_&lt;em&gt;(a|s) = 1 \iff a = \arg\max_{a&apos; \in \mathcal{A}} \left( \mathcal{R}&lt;em&gt;s^{a&apos;} + \gamma \sum&lt;/em&gt;{s&apos;} \mathcal{P}&lt;em&gt;{ss&apos;}^{a&apos;} v&lt;/em&gt;&lt;/em&gt;(s&apos;) \right)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;贝尔曼最优方程 (Bellman Optimality Equation):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;描述了&lt;strong&gt;最优价值函数 $v_&lt;em&gt;$ 和 $q_&lt;/em&gt;$ 必须满足的一致性条件&lt;/strong&gt; (非线性方程)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$v_*(s)$ 的方程:&lt;/strong&gt;
$$v_&lt;em&gt;(s) = \max_{a \in \mathcal{A}} q_&lt;/em&gt;(s, a) = \max_{a \in \mathcal{A}} \left{ \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}&lt;em&gt;{ss&apos;}^a v&lt;/em&gt;*(s&apos;) \right}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$q_*(s, a)$ 的方程:&lt;/strong&gt;
$$q_&lt;em&gt;(s, a) = \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}&lt;em&gt;{ss&apos;}^a v&lt;/em&gt;&lt;/em&gt;(s&apos;) = \mathcal{R}&lt;em&gt;s^a + \gamma \sum&lt;/em&gt;{s&apos; \in \mathcal{S}} \mathcal{P}&lt;em&gt;{ss&apos;}^a \max&lt;/em&gt;{a&apos; \in \mathcal{A}} q_*(s&apos;, a&apos;)$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特性:&lt;/strong&gt; 由于 $\max$ 算子，方程是非线性的，通常无法直接求解。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;求解方法:&lt;/strong&gt; 需使用迭代算法，如价值迭代、策略迭代 (动态规划，需要模型) 或 Q学习、Sarsa (强化学习，无需模型)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-3-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-3-zh.webp"/></item><item><title>RL 学习笔记（2）：赌博机问题</title><link>https://axi404.github.io/blog/rl-note-2</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-2</guid><description>赌博机问题</description><pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;本文主要介绍强化学习 (RL) 中的一个基础问题——赌博机问题 (Bandit Problems)，特别是 K 臂赌博机 (K-Armed Bandit, MAB) 问题。MAB 简化了 RL 问题，去除了“状态”的概念，专注于在单步决策中平衡探索 (Exploration) 与利用 (Exploitation) 的核心挑战。由于其无状态特性，本章状态价值 $V$ 以及动作价值 $Q$ 均不依赖于状态 $s$。&lt;/p&gt;
&lt;h2&gt;K 臂赌博机 (K-Armed Bandit, MAB) 问题&lt;/h2&gt;
&lt;h3&gt;问题定义&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;场景:&lt;/strong&gt; 存在 $K$ 个选项（“臂”或动作），选择每个臂 $a$ 会根据未知的概率分布 $P_a$ 产生一个奖励。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真实价值:&lt;/strong&gt; 每个臂 $a$ 的真实期望奖励为 $q_*(a) \doteq \mathbb{E}[R_t | A_t = a]$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;过程:&lt;/strong&gt; 在 $T$ 个时间步内，智能体在每一步 $t$ 选择一个臂 $A_t$，获得奖励 $R_{t+1}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标:&lt;/strong&gt; 最大化 $T$ 步内的累积总奖励 $\sum_{t=1}^T R_t$，等价于尽快找到并持续选择具有最高真实价值 $q_*(a)$ 的最优臂。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心挑战:&lt;/strong&gt; 智能体不知道 $q_*(a)$，必须通过尝试来估计。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特性:&lt;/strong&gt; 无状态，当前选择不影响未来的奖励分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;遗憾 (Regret)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义:&lt;/strong&gt; 衡量算法性能与理想最优策略差距的指标。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最优价值:&lt;/strong&gt; $v_* = \max_{a \in \mathcal{A}} q_*(a)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作间隙:&lt;/strong&gt; $\Delta_a = v_* - q_*(a) \ge 0$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;总遗憾 (Total/Cumulative Regret):&lt;/strong&gt; $T$ 步内的总期望机会损失。
$$L_T = \mathbb{E}[\sum_{t=1}^T (v_* - q_*(A_t))] = \sum_{a \in \mathcal{A}} \mathbb{E}[N_T(a)] \Delta_a$$
其中 $N_T(a)$ 是动作 $a$ 在 $T$ 步内被选择的总次数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标:&lt;/strong&gt; 设计算法使总遗憾 $L_T$ 随 $T$ 亚线性增长（如 $O(\log T)$），而非线性增长 $O(T)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;动作价值估计方法 (Action-Value Methods)&lt;/h2&gt;
&lt;p&gt;估计 $q_*(a)$ 是解决 MAB 的基础。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;采样平均法 (Sample-Average Method):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;思想:&lt;/strong&gt; 使用动作 $a$ 历史上获得的所有奖励的平均值作为其价值估计 $Q_t(a)$。
$$Q_t(a) \doteq \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbf{1}(A_i=a)}{\sum_{i=1}^{t-1} \mathbf{1}(A_i=a)}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;收敛性:&lt;/strong&gt; 根据大数定律，若每个动作被无限次选择， $Q_t(a)$ 会收敛到 $q_*(a)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;增量式实现 (Incremental Implementation):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的:&lt;/strong&gt; 高效计算，无需存储所有历史奖励。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新规则:&lt;/strong&gt; 对于动作 $a$ 的第 $n$ 次选择获得的奖励 $R_n$：
$$Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;通用形式:&lt;/strong&gt; &lt;code&gt;NewEstimate &amp;#x3C;- OldEstimate + StepSize * [Target - OldEstimate]&lt;/code&gt;，其中 &lt;code&gt;StepSize&lt;/code&gt; $\alpha_n = 1/n$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;处理非平稳问题 (Non-stationary Problems):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题:&lt;/strong&gt; 当 $q_*(a)$ 随时间变化时，采样平均法（步长 $1/n$）因给予所有历史奖励同等权重而效果不佳。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案:&lt;/strong&gt; 使用&lt;strong&gt;常数步长 (Constant Step-Size)&lt;/strong&gt; $\alpha \in (0, 1]$：
$$Q_{n+1} = Q_n + \alpha [R_{n+1} - Q_n]$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果:&lt;/strong&gt; 实现指数近因加权平均，更重视近期奖励，能追踪变化的目标，但不会完全收敛。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;步长参数选择 (Step-Size Parameter):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;收敛条件 (平稳问题，随机逼近理论):&lt;/strong&gt;
$$\sum_{n=1}^\infty \alpha_n(a) = \infty \quad \text{和} \quad \sum_{n=1}^\infty \alpha_n^2(a) &amp;#x3C; \infty$$&lt;/li&gt;
&lt;li&gt;$\alpha_n = 1/n$ 满足条件，常数 $\alpha$ 不满足第二个条件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;探索与利用策略&lt;/h2&gt;
&lt;p&gt;平衡尝试未知选项（探索）和选择当前最优选项（利用）是 MAB 的核心。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;$\epsilon$-贪心策略 ($\epsilon$-Greedy):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机制:&lt;/strong&gt; 以 $1-\epsilon$ 概率选择当前估计值 $Q_t(a)$ 最高的动作（利用），以 $\epsilon$ 概率从所有动作中随机均匀选择一个（探索）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优缺点:&lt;/strong&gt; 简单，保证持续探索；但探索是随机的，可能导致长期性能损失（线性遗憾）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;乐观初始值 (Optimistic Initial Values):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机制:&lt;/strong&gt; 将所有 $Q_1(a)$ 初始化为一个很高的值，然后始终采取纯贪心策略（选择 $A_t = \arg\max_a Q_t(a)$），使用采样平均法（步长 $1/n$）更新。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果:&lt;/strong&gt; 高初始值鼓励智能体尝试所有动作至少一次，实现早期系统性探索。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优缺点:&lt;/strong&gt; 实现简单；但对初始值敏感，随机环境下可能过早锁定次优动作。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;置信度上界 (Upper Confidence Bound, UCB):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;思想:&lt;/strong&gt; “在不确定性面前保持乐观”。选择潜力大的动作，潜力=高估计值 + 高不确定性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选择规则:&lt;/strong&gt;
$$A_t \doteq \arg\max_{a \in \mathcal{A}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]$$
其中 $Q_t(a)$ 是利用项， $c \sqrt{\frac{\ln t}{N_t(a)}}$ 是探索项（$N_t(a)$ 为动作 $a$ 被选次数，$t$ 为总步数，$c$ 为控制探索的参数）。若 $N_t(a)=0$，则该项为无穷大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优缺点:&lt;/strong&gt; 基于 Hoeffding 不等式，有较好的理论遗憾界 ($O(\log T)$) 和实践性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算法流程(UCB1)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;初始化 Q，并且将全部臂都拉取一次，获得更新&lt;/li&gt;
&lt;li&gt;每步 $t$，根据当前估计值 $Q_t(a)$ 和探索项 $c \sqrt{\frac{\ln t}{N_t(a)}}$ 选择动作 $A_t$。&lt;/li&gt;
&lt;li&gt;观察奖励 $R_{t+1}$，更新 $Q_t(a)$ 和 $N_t(a)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;汤普森采样 (Thompson Sampling / Posterior Sampling):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;思想:&lt;/strong&gt; 贝叶斯方法。维护每个动作价值 $q_&lt;em&gt;(a)$ 的后验概率分布 $P(q_&lt;/em&gt;(a) | \text{History})$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法流程:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每步 $t$，为每个动作 $a$ 从其后验分布中采样一个价值 $\tilde{q}_a$。&lt;/li&gt;
&lt;li&gt;选择采样值最大的动作 $A_t = \arg\max_a \tilde{q}_a$。&lt;/li&gt;
&lt;li&gt;观察奖励 $R_{t+1}$。&lt;/li&gt;
&lt;li&gt;使用贝叶斯更新规则更新动作 $A_t$ 的后验分布。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;探索机制:&lt;/strong&gt; 后验分布越宽（不确定性高），越有可能采样到高值而被选中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯更新与共轭先验:&lt;/strong&gt; (以伯努利赌博机为例)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;奖励为 0/1。似然为伯努利分布。&lt;/li&gt;
&lt;li&gt;使用 Beta 分布 $Beta(\alpha, \beta)$ 作为共轭先验。&lt;/li&gt;
&lt;li&gt;更新规则：观察到 1 次成功 (奖励=1) 则 $\alpha \leftarrow \alpha+1$，观察到 1 次失败 (奖励=0) 则 $\beta \leftarrow \beta+1$。后验仍为 Beta 分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;优缺点:&lt;/strong&gt; 实现简单（尤其使用共轭先验时），经验性能通常非常好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;与 Greedy 对比 (伯努利场景):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;| 步骤         | BernGreedy (贪心)                                        | BernThompson (汤普森采样)                                   |
| :----------- | :------------------------------------------------------- | :---------------------------------------------------------- |
| &lt;strong&gt;值计算/采样&lt;/strong&gt; | 计算期望值 $\theta_k = \alpha_k / (\alpha_k + \beta_k)$ | 从 $Beta(\alpha_k, \beta_k)$ 分布中采样 $\theta_k$          |
| &lt;strong&gt;动作选择&lt;/strong&gt; | 选择使 $\theta_k$ 最大的臂 $k$                            | 选择使采样值 $\theta_k$ 最大的臂 $k$                         |
| &lt;strong&gt;参数更新&lt;/strong&gt; | 根据奖励 $r_t$ 更新 $(\alpha_{x_t}, \beta_{x_t})$        | 根据奖励 $r_t$ 更新 $(\alpha_{x_t}, \beta_{x_t})$         |
&lt;em&gt;(循环和应用/观察步骤相同)&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;梯度赌博机算法 (Gradient Bandit Algorithms)&lt;/h2&gt;
&lt;p&gt;这类算法不估计动作价值，而是直接学习动作的&lt;strong&gt;偏好 (Preference)&lt;/strong&gt; $H_t(a)$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;动作选择 (Softmax Policy):&lt;/strong&gt;
$$\pi_t(a) = P(A_t=a) = \frac{e^{H_t(a)}}{\sum_{b=1}^K e^{H_t(b)}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;学习规则 (Stochastic Gradient Ascent):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更新选中动作 $A_t$ 的偏好：
$$H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t) (1 - \pi_t(A_t))$$&lt;/li&gt;
&lt;li&gt;更新未选中动作 $a \neq A_t$ 的偏好：
$$H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t) \pi_t(a)$$&lt;/li&gt;
&lt;li&gt;$\alpha$ 是学习率，$\bar{R}_t$ 是奖励基线（如历史平均奖励），用于减小方差。 $(R_t - \bar{R}_t)$ 衡量当前奖励的好坏。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优缺点:&lt;/strong&gt; 可处理非平稳环境；对学习率和基线敏感。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-2-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-2-zh.webp"/></item><item><title>RL 学习笔记（1）：强化学习简介</title><link>https://axi404.github.io/blog/rl-note-1</link><guid isPermaLink="true">https://axi404.github.io/blog/rl-note-1</guid><description>强化学习简介</description><pubDate>Tue, 08 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;RL Note&apos; categories={[
{
title: &apos;Basic Concepts&apos;,
items: [
{
title: &apos;强化学习简介&apos;,
href: &apos;/blog/rl-note-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;赌博机问题&apos;,
href: &apos;/blog/rl-note-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;马尔可夫决策过程&apos;,
href: &apos;/blog/rl-note-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;动态规划&apos;,
href: &apos;/blog/rl-note-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;蒙特卡洛方法&apos;,
href: &apos;/blog/rl-note-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;时序差分学习&apos;,
href: &apos;/blog/rl-note-6&apos;,
order: &apos;6&apos;
}]
},
{
title: &apos;RL Methods&apos;,
items: [
{
title: &apos;Q 学习、DQN 及相关改进&apos;,
href: &apos;/blog/rl-note-7&apos;,
order: &apos;1&apos;
},
{
title: &apos;n 步自举法&apos;,
href: &apos;/blog/rl-note-8&apos;,
order: &apos;2&apos;
},
{
title: &apos;集成规划与学习&apos;,
href: &apos;/blog/rl-note-9&apos;,
order: &apos;3&apos;
}]
},
{
title: &apos;Policy-Based Methods&apos;,
items: [
{
title: &apos;策略梯度方法&apos;,
href: &apos;/blog/rl-note-10&apos;,
order: &apos;1&apos;
},
{
title: &apos;Actor-Critic 方法&apos;,
href: &apos;/blog/rl-note-11&apos;,
order: &apos;2&apos;
},
{
title: &apos;置信域策略优化&apos;,
href: &apos;/blog/rl-note-12&apos;,
order: &apos;3&apos;
},
{
title: &apos;近端策略优化 (PPO)&apos;,
href: &apos;/blog/rl-note-13&apos;,
order: &apos;4&apos;
}]
},
{
title: &apos;Miscs&apos;,
items: [
{
title: &apos;基于人类反馈的强化学习 (RLHF)&apos;,
href: &apos;/blog/rl-note-14&apos;,
order: &apos;1&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客基于西安交通大学强化学习课程 PPT 改编，历经 Gemini 以及本人总结以及整理形成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;什么是强化学习？&lt;/h2&gt;
&lt;h3&gt;学习与机器学习&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;学习 (Learning)&lt;/strong&gt;：根据 Mitchell (1997) 的定义，若一个计算机程序在某类任务 $T$ 上的性能 $P$ 能通过经验 $E$ 改善，则称该程序从经验 $E$ 中学习了关于任务 $T$ 和性能 $P$ 的知识。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机器学习 (Machine Learning)&lt;/strong&gt;：设计能从经验（数据）中学习的算法，目标是持续改善系统在特定任务上的性能。其关键要素包括经验（数据）、学习算法，以及支撑现代人工智能发展的算力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机器学习三大范式&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;监督学习 (Supervised Learning)&lt;/strong&gt;：从有标签数据中学习输入到输出的映射。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无监督学习 (Unsupervised Learning)&lt;/strong&gt;：从无标签数据中发现隐藏结构或模式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习 (Reinforcement Learning)&lt;/strong&gt;：智能体通过与环境交互，依据获得的奖励或惩罚来学习最优决策，以最大化累积奖励。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;强化学习的核心思想与目标&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;强化学习旨在构建&lt;strong&gt;智能体 (Agent)&lt;/strong&gt;，使其能在&lt;strong&gt;动态、开放的环境&lt;/strong&gt;中通过&lt;strong&gt;学习&lt;/strong&gt;完成任务或实现目标。这区别于传统方法在静态环境中执行预设程序。&lt;/li&gt;
&lt;li&gt;核心问题：如何让智能体通过在动态环境中的&lt;strong&gt;试错 (Trial-and-Error)&lt;/strong&gt;，自主提升达成目标的能力？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励假设 (Reward Hypothesis)&lt;/strong&gt;：强化学习的一个基本假设是，所有形式的目标和意图都可以形式化为最大化期望累积标量奖励信号的过程。智能行为被认为是最大化奖励过程的自然涌现。
&lt;blockquote&gt;
&lt;p&gt;&quot;That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).&quot; - Rich Sutton, David Silver, et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;强化学习基本框架：智能体与环境交互&lt;/h2&gt;
&lt;p&gt;强化学习可以被视为智能体从与环境交互的&lt;strong&gt;经历 (Experience)&lt;/strong&gt; 中学习的计算理论。&lt;/p&gt;
&lt;h3&gt;交互过程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;智能体 (Agent)&lt;/strong&gt;：学习者和决策制定者。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;环境 (Environment)&lt;/strong&gt;：智能体外部的一切，是交互的对象。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交互循环&lt;/strong&gt;：在离散的时间步 $t = 0, 1, 2, \dots$：
&lt;ol&gt;
&lt;li&gt;智能体接收环境的&lt;strong&gt;状态 (State)&lt;/strong&gt; $S_t$ （或&lt;strong&gt;观测 Observation&lt;/strong&gt; $O_t$）。&lt;/li&gt;
&lt;li&gt;智能体基于状态 $S_t$ 选择并执行一个&lt;strong&gt;动作 (Action)&lt;/strong&gt; $A_t$。&lt;/li&gt;
&lt;li&gt;环境根据 $S_t$ 和 $A_t$ 转换到新的状态 $S_{t+1}$。&lt;/li&gt;
&lt;li&gt;环境向智能体反馈一个&lt;strong&gt;奖励 (Reward)&lt;/strong&gt; $R_{t+1}$。&lt;/li&gt;
&lt;li&gt;智能体接收新的状态 $S_{t+1}$ (或 $O_{t+1}$)，循环继续。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;序贯决策 (Sequential Decision Making)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RL 解决的是智能体需要连续做出一系列决策的问题，每个决策后接收新信息，直至任务结束。例如，游戏中每一帧都需要决定移动方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;经历 (Experience) 与 轨迹 (Trajectory)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;一次完整的交互过程（从开始到结束）称为一个&lt;strong&gt;回合 (Episode)&lt;/strong&gt; 或 &lt;strong&gt;轨迹 (Trajectory)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;交互产生序列数据：$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots$。RL 算法利用这些序列进行学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;关联任务 vs 非关联任务&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;非关联任务 (Non-associative)&lt;/strong&gt;：无需区分状态，目标是找到全局最优的单个动作或追踪变化环境中的最优动作。典型例子是&lt;strong&gt;k臂赌博机 (k-armed Bandit)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关联任务 (Associative)&lt;/strong&gt;：动作选择需与当前&lt;strong&gt;状态 (State)&lt;/strong&gt; 或&lt;strong&gt;情境 (Context)&lt;/strong&gt; 关联。智能体需学习从状态到最优动作的映射。&lt;strong&gt;上下文赌博机 (Contextual Bandit)&lt;/strong&gt; 是简单例子，而完整的 RL 问题（如导航）是典型的关联任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;强化学习问题的关键要素&lt;/h2&gt;
&lt;h3&gt;历史 (History) 与 状态 (State)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;历史 (History)&lt;/strong&gt; $H_t$：到时间 $t$ 为止的所有观测、动作、奖励序列：$H_t = O_1, R_1, A_1, \dots, A_{t-1}, O_t, R_t$。它包含了智能体交互的全部原始信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;状态 (State)&lt;/strong&gt; $S_t$：是对历史的&lt;strong&gt;总结&lt;/strong&gt;，包含决定未来所需的所有相关信息。状态是历史的函数：$S_t = f(H_t)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;环境状态 (Environment State)&lt;/strong&gt; $S_t^e$：环境内部决定下一状态/奖励的完整信息，不一定对智能体可见。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;智能体状态 (Agent State)&lt;/strong&gt; $S_t^a$：智能体内部用于决策的表示，是智能体可利用的信息，也是历史的函数 $S_t^a = f(H_t)$。RL 算法基于 $S_t^a$ 学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;环境的可观测性 (Observability)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;完全可观测环境 (Fully Observable)&lt;/strong&gt;：智能体观测等于环境真实状态 ($O_t = S_t^e$)。当前观测包含所有决策所需历史信息。这类问题通常用 &lt;strong&gt;马尔可夫决策过程 (Markov Decision Process, MDP)&lt;/strong&gt; 建模。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;马尔可夫性质 (Markov Property)&lt;/strong&gt;：未来只依赖于当前状态，与历史路径无关。即 $P[S_{t+1} | S_t] = P[S_{t+1} | S_1, \dots, S_t]$。环境状态 $S_t^e$ 通常满足此性质。在完全可观测时，智能体状态 $S_t^a (=S_t^e)$ 也满足。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;部分可观测环境 (Partially Observable)&lt;/strong&gt;：智能体观测仅提供部分环境信息 ($O_t \neq S_t^e$)。当前观测不足以确定状态，历史信息变得重要。这类问题用 &lt;strong&gt;部分可观测马尔可夫决策过程 (POMDP)&lt;/strong&gt; 建模。
&lt;ul&gt;
&lt;li&gt;处理方法：智能体需构建状态估计 $S_t^a$。常用方法包括使用历史 $H_t$、循环神经网络 (RNN) 或维护&lt;strong&gt;信念状态 (Belief State)&lt;/strong&gt; $b(s^e) = P(S_t^e = s^e | H_t)$（即关于真实状态的概率分布）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;奖励信号 (Reward Signal) $R_t$&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;奖励 $R_t$ 是一个&lt;strong&gt;标量&lt;/strong&gt;反馈信号，表明智能体在 $t$ 时刻动作后的即时“好坏”程度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：定义 RL 的目标。智能体旨在最大化累积奖励。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励设计 (Reward Shaping)&lt;/strong&gt; 非常关键，直接引导学习方向。例如：
&lt;ul&gt;
&lt;li&gt;游戏胜利：高正奖励。&lt;/li&gt;
&lt;li&gt;失败/碰撞：高负奖励。&lt;/li&gt;
&lt;li&gt;触发特定事件（如吃金币）：小正奖励。&lt;/li&gt;
&lt;li&gt;无事发生：零奖励或小的负奖励（鼓励效率）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;回报 (Return) 与 折扣 (Discounting)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;回报 (Return)&lt;/strong&gt; $G_t$：从时间步 $t$ 开始的未来累积奖励。它衡量了当前状态或状态-动作对在长期来看的价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;任务类型与回报定义&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;分幕式任务 (Episodic Tasks)&lt;/strong&gt;：有明确终止状态 $T$。
$$G_{t} \doteq R_{t+1} + R_{t+2} + \dots + R_{T} = \sum_{k=t+1}^{T} R_k$$
（$\doteq$ 表示定义）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续式任务 (Continuing Tasks)&lt;/strong&gt;：无终点，无限进行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;折扣回报 (Discounted Return)&lt;/strong&gt; $G_t$ (或 $U_t$): 为了处理无限和以及权衡近期与远期奖励，引入&lt;strong&gt;折扣因子 (Discount Factor)&lt;/strong&gt; $\gamma \in [0, 1]$。
$$G_{t} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
此定义统一适用于分幕式（令 $R_k=0$ for $k&gt;T$）和持续式任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么需要折扣 ($\gamma$)？&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;数学便利&lt;/strong&gt;：确保回报有界，利于算法收敛。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型不确定性&lt;/strong&gt;：远期奖励预测难度大，折扣降低其影响。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;偏好即时奖励&lt;/strong&gt;：符合直觉，现在的奖励通常更有价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模拟终止概率&lt;/strong&gt;：可视为每步有 $1-\gamma$ 概率终止。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;折扣因子的影响&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;$\gamma \approx 0$：智能体“近视”，更关注短期奖励。&lt;/li&gt;
&lt;li&gt;$\gamma \approx 1$：智能体“远视”，更关注长期奖励。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回报的递归关系&lt;/strong&gt;：折扣回报满足重要的递归性质：
$$G_t = R_{t+1} + \gamma G_{t+1}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回报的随机性&lt;/strong&gt;：由于未来的动作和状态可能随机，未来的奖励也是随机的。因此，在时间 $t$，$G_t$ 是一个&lt;strong&gt;随机变量&lt;/strong&gt;。算法通常使用实际观测到的回报值（一个回合结束后计算出的具体数值，有时记作 $g_t$ 或 $u_t$）来估计其期望。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;环境模型 (Environment Model)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;模型描述环境行为，预测环境对动作的响应。包含：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;状态转移概率 (State Transition Probability)&lt;/strong&gt; $p(s&apos;|s, a)$：在状态 $s$ 执行动作 $a$ 后，转移到状态 $s&apos;$ 的概率。
$$p(s&apos;|s, a) = \mathbb{P}(S_{t+1}=s&apos; | S_t=s, A_t=a)$$
环境可能是&lt;strong&gt;随机的 (Stochastic)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励函数 (Reward Function)&lt;/strong&gt; $r(s, a)$ 或 $\mathcal{R}&lt;em&gt;{s}^{a}$：在状态 $s$ 执行动作 $a$ 后，期望获得的&lt;strong&gt;立即&lt;/strong&gt;奖励。
$$\mathcal{R}&lt;/em&gt;{s}^{a} = \mathbb{E}[R_{t+1} | S_t=s, A_t=a]$$
(注意：有时奖励函数定义为 $p(r|s, a)$ 或 $p(s&apos;, r|s, a)$，包含奖励的分布)。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;引出两种主要方法类型：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基于模型 (Model-Based)&lt;/strong&gt;：尝试学习或利用环境模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无模型 (Model-Free)&lt;/strong&gt;：不依赖显式模型，直接从经验学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;随机性来源 (Randomness)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RL 中存在多个随机源：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;动作随机性 (Stochastic Action)&lt;/strong&gt;：智能体的策略 $\pi(a|s)$ 可能输出概率分布，$A_t \sim \pi(\cdot | S_t)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;状态转移随机性 (Stochastic State Transition)&lt;/strong&gt;：环境本身可能随机，$S_{t+1} \sim p(\cdot | S_t, A_t)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励随机性&lt;/strong&gt;：奖励 $R_{t+1}$ 也可能依赖于 $(S_t, A_t, S_{t+1})$ 并且是随机的。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;强化学习智能体的核心组成&lt;/h2&gt;
&lt;p&gt;一个典型的 RL 智能体通常包含以下一个或多个组件：&lt;/p&gt;
&lt;h3&gt;策略 (Policy) $\pi$&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;策略是智能体的“大脑”或行为方式，定义了智能体在给定状态下如何选择动作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;确定性策略 (Deterministic Policy)&lt;/strong&gt;：$a = \pi(s)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;随机性策略 (Stochastic Policy)&lt;/strong&gt;：$\pi(a|s) = P(A_t = a | S_t = s)$，输出动作的概率分布。随机策略在探索和处理不确定性时有优势。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：找到最优策略 $\pi^&lt;em&gt;$，使得期望累积回报最大化：
$$\pi^&lt;/em&gt; = \arg\max_{\pi} \mathbb{E}_{\pi, \text{Env}} [G_t]$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;价值函数 (Value Function) $V^{\pi}, Q^{\pi}$&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;价值函数用于评估状态或状态-动作对的“好坏”程度（长期价值），即遵循特定策略 $\pi$ 能获得的期望回报。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;状态价值函数 (State-Value Function)&lt;/strong&gt; $V^{\pi}(s)$：从状态 $s$ 开始，遵循策略 $\pi$ 的期望折扣回报。
$$V^{\pi}(s) = \mathbb{E}_{\pi} [G_t | S_t = s]$$
衡量处于状态 $s$ 的好坏。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作价值函数 (Action-Value Function)&lt;/strong&gt; $Q^{\pi}(s, a)$ (Q-function)：在状态 $s$ 采取动作 $a$ 后，继续遵循策略 $\pi$ 的期望折扣回报。
$$Q^{\pi}(s, a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a]$$
衡量在状态 $s$ 执行动作 $a$ 的好坏。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关系&lt;/strong&gt;：$V^{\pi}(s)$ 是 $Q^{\pi}(s,a)$ 在策略 $\pi$ 下的期望值：
&lt;ul&gt;
&lt;li&gt;离散动作: $V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^{\pi}(s, a)$&lt;/li&gt;
&lt;li&gt;连续动作: $V^{\pi}(s) = \int_{a \in \mathcal{A}} \pi(a|s) Q^{\pi}(s, a) da$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;价值函数帮助评估和改进策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;模型 (Model)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;模型是对环境动态的模拟。如果智能体拥有或学习了模型，就可以进行&lt;strong&gt;规划 (Planning)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;强化学习的关键挑战与权衡&lt;/h2&gt;
&lt;h3&gt;探索 (Exploration) vs 利用 (Exploitation)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RL 的核心困境：是利用当前已知最好的选择（&lt;strong&gt;利用&lt;/strong&gt;），还是尝试未知的选择以期发现更好的策略（&lt;strong&gt;探索&lt;/strong&gt;）？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;利用&lt;/strong&gt;：选择当前估计价值最高的动作，最大化短期收益。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;探索&lt;/strong&gt;：尝试非最优或未充分尝试的动作，收集信息，可能发现长期更优的策略，但可能牺牲短期收益。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;平衡&lt;/strong&gt;是关键。过度利用可能陷入局部最优，过度探索则效率低下。常用策略有 $\epsilon$-greedy、UCB、乐观初始值等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;预测 (Prediction) vs 控制 (Control)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RL 的两大任务类型：&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测 (Prediction) / 策略评估 (Policy Evaluation)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：给定策略 $\pi$，评估其价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：计算 $V^{\pi}(s)$ 或 $Q^{\pi}(s, a)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;控制 (Control) / 策略改进 (Policy Improvement)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：找到最优策略 $\pi^*$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：最大化长期回报，找到最优价值函数 $V^&lt;em&gt;(s), Q^&lt;/em&gt;(s, a)$ 和最优策略 $\pi^*$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关系&lt;/strong&gt;：控制问题通常通过迭代地进行预测和改进来解决，这个过程称为 &lt;strong&gt;广义策略迭代 (Generalized Policy Iteration, GPI)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;学习 (Learning) vs 规划 (Planning)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;学习 (Learning)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;环境模型未知。&lt;/li&gt;
&lt;li&gt;通过与&lt;strong&gt;真实环境&lt;/strong&gt;交互（试错）获得经验。&lt;/li&gt;
&lt;li&gt;直接从经验改进策略或价值函数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规划 (Planning)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;环境模型已知或已习得。&lt;/li&gt;
&lt;li&gt;利用&lt;strong&gt;模型&lt;/strong&gt;进行模拟计算（“思考”、“推演”），产生&lt;strong&gt;模拟经验&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;基于模拟经验改进策略或价值函数，无需与真实环境交互。&lt;/li&gt;
&lt;li&gt;例子：动态规划 (DP)、蒙特卡洛树搜索 (MCTS)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结合&lt;/strong&gt;：许多方法（如 Dyna-Q）结合两者，通过真实交互学习模型，再利用模型进行规划加速学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;强化学习方法分类&lt;/h2&gt;
&lt;p&gt;RL 算法可从不同维度分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基于价值 (Value-Based)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;学习价值函数 $V(s)$ 或 $Q(s,a)$。&lt;/li&gt;
&lt;li&gt;策略隐式（如对Q值贪心）。&lt;/li&gt;
&lt;li&gt;代表：Q-Learning, Sarsa, DQN。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于策略 (Policy-Based)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;直接学习策略 $\pi(a|s)$。&lt;/li&gt;
&lt;li&gt;可处理连续动作空间，能学随机策略。&lt;/li&gt;
&lt;li&gt;代表：REINFORCE, Policy Gradients。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;演员-评论家 (Actor-Critic)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;结合前两者。&lt;strong&gt;演员 (Actor)&lt;/strong&gt; 学策略，&lt;strong&gt;评论家 (Critic)&lt;/strong&gt; 学价值函数指导演员。&lt;/li&gt;
&lt;li&gt;代表：A2C, A3C, DDPG, SAC。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无模型 (Model-Free)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;不学习环境模型，直接从经验学习。&lt;/li&gt;
&lt;li&gt;样本效率通常较低。&lt;/li&gt;
&lt;li&gt;包括上述大部分方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于模型 (Model-Based)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;学习环境模型，并利用模型进行规划或生成模拟数据。&lt;/li&gt;
&lt;li&gt;样本效率通常较高，但受模型精度限制。&lt;/li&gt;
&lt;li&gt;代表：Dyna-Q, MCTS 相关方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;深度强化学习 (Deep Reinforcement Learning, DRL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;将&lt;strong&gt;深度学习 (Deep Learning)&lt;/strong&gt; 与 RL 结合。&lt;/li&gt;
&lt;li&gt;使用&lt;strong&gt;深度神经网络 (DNNs)&lt;/strong&gt; 作为&lt;strong&gt;函数逼近器&lt;/strong&gt;来表示策略、价值函数或模型，尤其适用于高维状态/动作空间。
&lt;ul&gt;
&lt;li&gt;策略网络 $\pi(a|s; \theta)$&lt;/li&gt;
&lt;li&gt;价值网络 $V(s; \theta)$ 或 $Q(s, a; \theta)$&lt;/li&gt;
&lt;li&gt;模型网络 (状态转移，奖励)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;学习过程即调整网络参数 $\theta$ 以最大化回报。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;符号约定&lt;/h2&gt;
&lt;p&gt;本笔记将遵循以下主要符号：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{S}$: 状态空间 (State Space)&lt;/li&gt;
&lt;li&gt;$\mathcal{A}$: 动作空间 (Action Space)&lt;/li&gt;
&lt;li&gt;$S_t, s$: 当前状态 (State at time t, a specific state) - 大写为随机变量，小写为具体值&lt;/li&gt;
&lt;li&gt;$A_t, a$: 当前动作 (Action at time t, a specific action) - 大写为随机变量，小写为具体值&lt;/li&gt;
&lt;li&gt;$R_{t}, r$: 奖励 (Reward at time t, a specific reward value) - 大写为随机变量，小写为具体值&lt;/li&gt;
&lt;li&gt;$\pi$: 策略 (Policy)&lt;/li&gt;
&lt;li&gt;$V^{\pi}(s)$: 策略 $\pi$ 下的状态价值函数 (State-value function under policy $\pi$)&lt;/li&gt;
&lt;li&gt;$Q^{\pi}(s, a)$: 策略 $\pi$ 下的动作价值函数 (Action-value function under policy $\pi$)&lt;/li&gt;
&lt;li&gt;$G_t$: 回报 (Return, cumulative future reward from t) - 随机变量&lt;/li&gt;
&lt;li&gt;$\gamma$: 折扣因子 (Discount factor)&lt;/li&gt;
&lt;li&gt;$p(s&apos;|s, a)$: 状态转移概率 (State transition probability)&lt;/li&gt;
&lt;li&gt;$\mathbb{E}[\cdot]$: 期望 (Expectation)&lt;/li&gt;
&lt;li&gt;$H_t$: 历史 (History)&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/rl-note-1-zh.webp"/><enclosure url="https://picr2.axi404.top/rl-note-1-zh.webp"/></item><item><title>周记 Week39</title><link>https://axi404.github.io/blog/week-39</link><guid isPermaLink="true">https://axi404.github.io/blog/week-39</guid><description>2025-03-24 ~ 2025-03-30.</description><pubDate>Mon, 07 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说就是很久没有记周记了，这已经是下下周的时候了，都是感觉还是有必要把这周的事情补一下，最大的区别肯定是已经自由了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;还是科研的近况，目前也是把 CVPR 的 camera ready 结束了，我以及 mentor 两个人一起把论文的文字以及一些图的内容修改了一下，然后我把表格填了一波，也算是阶段性完结吧。&lt;/p&gt;
&lt;p&gt;剩下的事情，大概就是可能两三周之后会把整体的 project page release 出来，不过不知道我之前没有说过，其实这篇论文只是我目前做的事情的冰山一角，至少从时间的开销上是这样的，所以那些真正耗费时间的事情还没有结束，一些代码的开发，对于更多人的支持，还是要继续的。&lt;/p&gt;
&lt;p&gt;因为将来几乎肯定是要留在这边读博士的，所以现在也接管了非常多的事情，类似于是一个根结点的存在，和若干同学进行对接，包括将来合作的机会应该就很多了。只能说这边确实有很大的平台让我发展，但事实上更让我感触的是，在之前论文投稿以及目前的这些时间里面，都是很多很厉害的研究员以及同学一直在帮助我。对于他们来说，我的论文挂一个很靠后的作者，这其实并没有什么价值，但是他们还是会熬夜帮我。以及之前线下的时候，大家也经常一起线下吃饭，这种扁平化的氛围，我真的非常喜欢。&lt;/p&gt;
&lt;p&gt;只能说一切都在稳步发展吧，我也正在尝试做一些新的思考，但是在此之前，我应该想把目前内容做得更好，毕竟这是一个可以长远发展的事情。&lt;/p&gt;
&lt;h2&gt;社交&lt;/h2&gt;
&lt;p&gt;这一次可能新加一个板块吧，也就是社交这一部分，确实现在认识了很多很有意思的人，或者说发生了很多因为和人交往所以才出现的事情，感觉还是很值得记录的。&lt;/p&gt;
&lt;p&gt;一方面还认识了很好的朋友吧。我在现实中的经常沟通的同学，只有年级第一的亚冬同学，倒也不是因为什么成绩或者什么歧视，只是确实偶然在若干的互动中熟悉了彼此，但是毕竟我在线下的活动也不是很多，主要的绝大多数的时间都是和乐小姐在一起，所以所能认识的就只有这一位了。剩下的就是在网上的朋友了。&lt;/p&gt;
&lt;p&gt;只能说我无时无刻不在赞叹互联网这种伟大的产物，可以让同一时间不同地点的人联系在一起，让大家彼此交流，彼此沟通。在绿群以及其他的各种群聊里面，我还是认识了很多朋友的，非常非常的多，大家每一个人的友情我也很珍惜，假如之后和大家有一些有趣的互动，应该会记录在这里。&lt;/p&gt;
&lt;p&gt;这周或者最近发生的主要的事情，还是被 tianxing 邀请共建了 Lumina 具身社区，感觉大家都是很厉害的人，我只是希望大家可以正常的开心相处，然后假如可以的话，有一些愉快的合作和科研，这几乎已经是我想象中最美好的画面了，希望可以实现。同时是某种所谓的开源精神的驱动吧，我在里面可能也主要想做一些分享类型的东西，我们应该会有某些教程或者博客的板块，我很想写一些东西，来帮助后面的人。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;这个确实也是我的问题，因为时间过得太久，所以说我也没有什么印象了，但是应该没有发生什么大事情。和乐小姐去吃了铁板烧，两个人吃了四百块钱，但是只能说并不是特别好吃。看起来菜单是十分高级的，都是什么牛排和鹅肝，以及还有黑虎虾之类的，但是实际上并不是特别好吃。还记得小时候吃铁板烧，家里人一起去吃，店的名字应该是叫坪亭，两百多块钱的自助，味道也比这家店好非常多，只是不知道这个店还开没开着，或许改天带乐小姐去旅游的时候可以吃一下。&lt;/p&gt;
&lt;p&gt;这一周可以说是久违的自由，因为没有一个很可怕的 ddl 存在，虽然说之前其实也不会有很大的压力，就像之前记载的一样，中稿之后的几个星期都在做其他事情，后面才开始忙活 camera ready，但是我的心理问题确实会让我感觉一直很难受，目前可以自由的写程序，尽管需要做的事情还有很多，但是也是很可喜可贺了。&lt;/p&gt;
&lt;p&gt;大概这周就发生了这些事情吧，毕竟记忆也不是很清楚了，下周的我会立刻开始写，应该就会好很多。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-39-zh.webp"/><enclosure url="https://picr2.axi404.top/week-39-zh.webp"/></item><item><title>周记 Week40</title><link>https://axi404.github.io/blog/week-40</link><guid isPermaLink="true">https://axi404.github.io/blog/week-40</guid><description>2025-03-31 ~ 2025-04-06.</description><pubDate>Mon, 07 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;事实上不难发现每一周都是平淡无奇的，或许这才是常态。总的来说之前对我来说确实有点 burn out 了，虽然这周也一直都在写程序，但是整体处于一种比较无精打采的状态。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;既然论文已经结束了，下一篇也迟迟没有开始，现在做的事情自然是不断地打磨。还是经典的都会写几个小时的内容，然后不断将功能继续完善。&lt;/p&gt;
&lt;p&gt;一个比较有趣的事情是，可以这样形容我做的事情，我为其他很多人提供了支持，所以在此之前我的代码仓库有很多人在用，但是大家主要都是从使用者的身份来去操作的，不过目前一切看上去都要进入正轨了。引入了一些企业的经验，在实验室里面的代码仓库中使用了 Gitflow 的规范以及 code review，并且为接下来的彻底的多人合作建立一个稳定的基础。关于这些内容的介绍，我其实放在了一个单独的文章，也就是对于 Gitflow 的讲解，感兴趣大家可以去看。&lt;/p&gt;
&lt;p&gt;以及我从目前开发的经验来说，是的，我将我目前做的事情几乎可以定义为开发，毕竟大多数事情都都是写程序，或者我其实感觉目前大多数的科研都是在写程序，真正思维火花的碰撞，可能往往只是几天或者那么几个时刻。所以从我的经验来说，我真心不是很喜欢 OOP，也就是面向对象编程，对于我来说，functional programming 更加合理，尤其是在 Python 这种脚本语言中。一个对象太过笨重，而且由对象管理的大量变量生命周期又难以判断。对于大量数据的维护，Python 可以使用 dict，而功能的实现则可以拆分出来作为函数，其他人来说更加简单易懂，我也按照这种思路制作了其他人如何和我协作的框架。&lt;/p&gt;
&lt;h2&gt;社交&lt;/h2&gt;
&lt;p&gt;这周可以说还是有很多社交活动的，首先是在星期一的时候在我的好朋友 SOTA 的组织下给了一个关于双系统 paper reading 的 talk，毕竟我的组现在就是类似的方向，所以我也读过不少的论文，以及有一些自己的感想，所以给出了一些看法。&lt;/p&gt;
&lt;p&gt;SOTA 是 25 fall 里面我见过最厉害的人之一了，之前也和我聊过很多，包括着科研以及其他的事情，我至少可以单方面认为他是我非常非常好的朋友（我的理解应该算是，他找也倾诉过一些事情，大概算得上好朋友吧），之前他一直在组织一个留学交流群，之后转成了科研群，然后最近在做 seminar，于是我也成功加盟。&lt;/p&gt;
&lt;p&gt;一方面也是我到处混脸熟，和各路大佬都比较熟悉，虽然我本身没有几斤几两，但是大家有什么事情还是乐意叫上我的；另一方面在一众全力在 ai 发展的同学中，我之前折腾过各种系统，以及一些博客的部署方案，包括说前端的一些东西，虽然在科班眼里完全是小儿科，但是在 ai 的大家这里，还算是比较罕见，因此大家有什么搭建网站的需求，我也可以立刻响应。搭建网站的事情基本上是 SOTA 说完，当天就建立起来了，这方面，因为是静态的，我一直有一套十分成熟的流程，改天可以写一个博客来分享，毕竟我的生态位大概已经成熟了，分享出来也不会有别人来卷我。&lt;/p&gt;
&lt;p&gt;组织叫做 SOTA-Seminar，也是很不错的。目前已经有了三场 talk，作为某种程度的加盟，以及更重要的，SOTA 的好朋友，还是希望他可以做出影响力的。他的交流中，我认为他是那种可以改变世界的人，对于一些事情有清晰的认知，能力也很强，各方面都很全面，虽然好像遭遇了一些不顺利，但是相信他的水平一定没问题。&lt;/p&gt;
&lt;p&gt;另外就是 Lumina 也在筹备过程中，我几乎是手搓了网页，现在看上去各种样式都十分优雅，几乎可以说是那种请人来做的效果了，全部都修好了之后应该会上线，说不定也可以开源网站的代码，反正目前做的我很喜欢。只能说从一开始那个折腾博客的少年到现在，在实践中成长确实一直以来都是我的信条，而从现在看来也确实让我学到了很多东西，也收获了很多除了知识之外的东西，比如说友谊。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;这周虽然有清明节假期，但是实际上大多数时间也都在学习，中间应该是和乐小姐出去了三次，大概就是去逛街，然后一起吃饭。现在基本上西安周边的一些地方都已经逛得差不多了，我们两个人确实不太喜欢踏出舒适区，基本上吃的东西都是之前的那几样。当然另一方面也是因为血的教训，想尝试一些新的好吃的，结果就吃到了非常垃圾的食物。&lt;/p&gt;
&lt;p&gt;乐小姐最近在看柯南，还是十分有趣的，我其实也是一个老二次元了，但是毕竟现在太繁忙，已经有两个季度没有追番了，都只是在网上看一看解说视频，不过那些老牌的民工漫以及知名动漫，我大概都是有看过的，其自然就包括一千集的柯南。所以我们两个人就在一起看动画片，一边看一边吃饭，然后吃完饭去买点贴纸，给乐小姐的手帐做准备，假如想吃的话再买个冰激凌。乐小姐穿着我给她买的可爱裙子，我们两个人牵着手走向回寝室的路，然后贴贴一下，分别，再等着下一天的相遇，可以说也是很温馨的画面了。&lt;/p&gt;
&lt;p&gt;这周大概就是这样，时间慢慢地度过。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-40-zh.webp"/><enclosure url="https://picr2.axi404.top/week-40-zh.webp"/></item><item><title>GitFlow 讲解</title><link>https://axi404.github.io/blog/gitflow</link><guid isPermaLink="true">https://axi404.github.io/blog/gitflow</guid><description>关于 GitFlow 的讲解</description><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Steps } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在日常开发中，一个良好的分支管理策略对于团队协作至关重要。因为最近在科研中涉及了更大的多人协作项目，所以了解了这方面的内容。Gitflow 是一套非常经典的 Git 分支模型，它为不同阶段的开发活动提供了明确的流程划分，尤其适合版本发布频繁的项目。&lt;/p&gt;
&lt;p&gt;本文将通过一张图直观讲解 Gitflow 的流程与各个分支的职责。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.361qy5ucbi.webp&quot; alt=&quot;Gitflow 工作流示意图&quot;&gt;&lt;/p&gt;
&lt;h2&gt;核心分支介绍&lt;/h2&gt;
&lt;p&gt;Gitflow 定义了以下五类分支，每种分支都有特定的用途：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Main（主分支）&lt;/strong&gt;：&lt;br&gt;
用于存放每一个正式发布的版本。所有的发布版本都应该是从该分支打的 tag。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Develop（开发分支）&lt;/strong&gt;：&lt;br&gt;
所有新功能、改动的集成基础。它是 Feature 分支的汇聚点，同时也是 Release 分支的起点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature（功能分支）&lt;/strong&gt;：&lt;br&gt;
用于开发具体的功能，一般从 &lt;code&gt;develop&lt;/code&gt; 分支拉出，开发完成后合并回 &lt;code&gt;develop&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Release（预发布分支）&lt;/strong&gt;：&lt;br&gt;
当 &lt;code&gt;develop&lt;/code&gt; 上的功能准备就绪，即可从 &lt;code&gt;develop&lt;/code&gt; 拉出 &lt;code&gt;release&lt;/code&gt; 分支做最后测试和优化，最终合并到 &lt;code&gt;main&lt;/code&gt; 和 &lt;code&gt;develop&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hotfix（热修复分支）&lt;/strong&gt;：&lt;br&gt;
如果线上版本（即 &lt;code&gt;main&lt;/code&gt; 分支）发现严重 bug，立即从 &lt;code&gt;main&lt;/code&gt; 分出 &lt;code&gt;hotfix&lt;/code&gt; 分支进行修复，修复后合并回 &lt;code&gt;main&lt;/code&gt; 和 &lt;code&gt;develop&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Gitflow 工作流程详解&lt;/h2&gt;
&lt;p&gt;结合上面的图，我们可以梳理出 Gitflow 的使用流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;从 &lt;code&gt;develop&lt;/code&gt; 分出 &lt;code&gt;feature&lt;/code&gt; 分支开发功能&lt;/strong&gt;&lt;br&gt;
多个功能可以并行开发，开发完毕后合并回 &lt;code&gt;develop&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;当一阶段功能开发完毕，从 &lt;code&gt;develop&lt;/code&gt; 分出 &lt;code&gt;release&lt;/code&gt; 分支&lt;/strong&gt;&lt;br&gt;
此分支用于版本测试、Bug 修复、文档编写等准备工作，稳定后合并到 &lt;code&gt;main&lt;/code&gt; 和 &lt;code&gt;develop&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;版本正式发布，从 &lt;code&gt;main&lt;/code&gt; 打 tag 标记版本号&lt;/strong&gt;&lt;br&gt;
这是对外发布的稳定版本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;线上发现 Bug，基于 &lt;code&gt;main&lt;/code&gt; 创建 &lt;code&gt;hotfix&lt;/code&gt; 分支快速修复&lt;/strong&gt;&lt;br&gt;
修复完成后，合并回 &lt;code&gt;main&lt;/code&gt;（以发布修复版本）和 &lt;code&gt;develop&lt;/code&gt;（同步代码）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当然，事实上对于我来说，可能会一直使用 Develop 分支。祝你好运。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/gitflow-zh.webp"/><enclosure url="https://picr2.axi404.top/gitflow-zh.webp"/></item><item><title>Isaac Sim 一百讲（5）：Rigid and Collision</title><link>https://axi404.github.io/blog/isaac-5</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-5</guid><description>从零开始的 Isaac Sim 之路，第一季开始！</description><pubDate>Thu, 27 Mar 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Isaac 101&apos;,
items: [
{
title: &apos;安装&apos;,
href: &apos;/blog/isaac-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;万物皆 Prim&apos;,
href: &apos;/blog/isaac-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;USD&apos;,
href: &apos;/blog/isaac-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Transformation&apos;,
href: &apos;/blog/isaac-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Rigid and Collision&apos;,
href: &apos;/blog/isaac-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Collision 进阶&apos;,
href: &apos;/blog/isaac-6&apos;,
order: &apos;6&apos;
},
{
title: &apos;Camera&apos;,
href: &apos;/blog/isaac-7&apos;,
order: &apos;7&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在此之前，我们已经了解了了解了如何在场景中添加一个物体了，那么在正式认识机器人之前，我们来简单地认识一下 Isaac Sim 中的碰撞。碰撞在内的物理仿真是 Isaac Sim 中一个非常重要的组成部分，甚至说，没有这些物理，Simulator 就无从谈起。毕竟从一开始，仿真关注的就是物理仿真，在此之后才有的图像渲染。&lt;/p&gt;
&lt;p&gt;让我们先思考一下一个现实中的物体的物理是如何运作的，一般来说，物体需要遵守牛顿定律，也就是在受到力的情况下会运动，而同时，这个物体还需要可以和其他物体进行碰撞。在这其中，牛顿定律的部分在 Isaac Sim 中被描述为 RigidBodyAPI，通过对物体施加这个 API，物体就可以受到力的作用。而同时，碰撞这一部分则是 collision 处理的范围。&lt;/p&gt;
&lt;h2&gt;Rigid Prim&lt;/h2&gt;
&lt;p&gt;让我们先创建一个初始的程序。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False}) # we can also run as headless.

from omni.isaac.core import World
from omni.isaac.core.objects import VisualCuboid
from omni.isaac.core.utils.prims import get_prim_at_path

world = World()
world.scene.add_default_ground_plane()
cube1 = world.scene.add(
    VisualCuboid(
        prim_path=&quot;/World/cube1&quot;,
        name=&quot;cube1&quot;,
        position=np.array([0, 0, 1.0]),
        scale=np.array([0.5015, 0.5015, 0.5015]),
        color=np.array([0, 0, 1.0]),
    ))
cube1_prim = get_prim_at_path(&quot;/World/cube1&quot;)
for _ in range(10000):
    world.step()
simulation_app.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行它，你可以看到一个 Cube 出现在了你的 GUI 中，点击左侧栏的播放键，此时就开始运行了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2vewz0ps1w.webp&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;不过读者不难发现，这个 Cube 是悬浮在半空中的，不会受到力的作用，也不会有任何的反馈。在 GUI 中，&lt;code&gt;shift&lt;/code&gt; + 左键可以用来在场景里面施加力的，但是你会发现你现在无从操作，等物体被添加了 Rigid 属性之后，你就可以一窥这个操作的效果了。&lt;/p&gt;
&lt;p&gt;接下来，让我们来加入一些物理设置。我们可以通过 pxr 来进行一些 USD 的操作。包括之前，读者应该也都不难发现，若干的 USD 相关的操作，都是通过 pxr 调用的。&lt;code&gt;pxr&lt;/code&gt; 是 Pixar 出品的 USD C++ API 的 Python 绑定，也就可以用它来调用一些喜闻乐见的函数。&lt;/p&gt;
&lt;p&gt;修改代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;cube1_prim = get_prim_at_path(&quot;/World/cube1&quot;)
from pxr import UsdPhysics # [!code ++]
rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(cube1_prim) # [!code ++]
for _ in range(10000):
    world.step()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再次打开运行程序的时候，并点击播放键的时候，不难看到这样的效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PixPin_2025-03-29_04-31-31.5j4d9dv6hm.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到我们的 Prim 已经被添加了 Rigid 属性，并且可以受到力了，只是因为没有碰撞，所以直接穿墙而过。&lt;/p&gt;
&lt;p&gt;我们同样可以用 Rigid Body 的 API 来进行一些有趣的操纵，比如说：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pxr import UsdPhysics
rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(cube1_prim)
from pxr import PhysxSchema # [!code ++]
rigid_body_schema = PhysxSchema.PhysxRigidBodyAPI.Apply(cube1_prim) # [!code ++]
rigid_body_schema.CreateDisableGravityAttr().Set(True) # [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行程序，你会发现这个 Cube 不会受到重力了。但是因为这个物体还没有被施加一个碰撞箱，因此我们不能用 &lt;code&gt;shift&lt;/code&gt; + 左键来施加力，后续我们会展示在这个无重力情况下的有趣视觉效果。&lt;/p&gt;
&lt;p&gt;在这里我们需要额外注意的是，我们同样是在使用 &lt;code&gt;pxr&lt;/code&gt; 来操作这个物体，但是此时我们使用的是 &lt;code&gt;PhysxSchema&lt;/code&gt; 而不是 &lt;code&gt;UsdPhysics&lt;/code&gt;。简单来理解的话，PhysxSchema 是 Physx 的 C++ API 的 Python 绑定，而 UsdPhysics 是 USD 的 C++ API 的 Python 绑定。在 UsdPhysics 中，本质上是 USD 的协议规定了一系列的物理属性，但是这些物理属性还难以严谨地描述物理仿真，因此我们使用 PhysxSchema 作为一个 「DLC」 来补充物理仿真的各种设置。&lt;/p&gt;
&lt;p&gt;关于 PhysxSchema 的更多内容，你可以参考这个网页：&lt;/p&gt;
&lt;p&gt;具体来说，比如刚才展示的 &lt;code&gt;CreateDisableGravityAttr&lt;/code&gt;，你可以依次打开 &lt;code&gt;Classes-&gt;Class List-&gt;PhysxSchemaPhysxRigidBodyAPI&lt;/code&gt;，就可以找到函数 &lt;code&gt;CreateDisableGravityAttr&lt;/code&gt; 的说明。&lt;/p&gt;
&lt;h2&gt;Collision&lt;/h2&gt;
&lt;p&gt;既然我们已经获得了一个 Rigid Prim 了，那么接下来，让我们给这个物体添加一些碰撞箱吧。使用以下的代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;cube1_prim = get_prim_at_path(&quot;/World/cube1&quot;)
from pxr import UsdPhysics
rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(cube1_prim)
collision_api = UsdPhysics.CollisionAPI.Apply(cube1_prim) # [!code ++]
mesh_collision_api = UsdPhysics.MeshCollisionAPI.Apply(cube1_prim) # [!code ++]
mesh_collision_api.CreateApproximationAttr().Set(&quot;convexDecomposition&quot;) # [!code ++]
collision_api.GetCollisionEnabledAttr().Set(True) # [!code ++]
for _ in range(10000):
    world.step()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行之后不难看到这样的效果，这下不会穿模了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PixPin_2025-03-29_04-51-47.45hu5da919.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;你可以在 GUI 的右侧看到这样的 Collider 设置：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.esok4o93w.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;Collision 进阶&lt;/h2&gt;
&lt;p&gt;Cube 太简单了，还记得我们在&lt;a href=&quot;isaac-3&quot;&gt;第三讲&lt;/a&gt;管理的资产吗，我们写了一个 &lt;code&gt;add_usd_to_world&lt;/code&gt; 函数，我们在这里可以展示另一种将 usd 添加到场景中的方案，当然，它们是等价的。然后我们把上述的内容都重新执行一次。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False}) # we can also run as headless.

from omni.isaac.core import World
from omni.isaac.core.objects import VisualCuboid
from omni.isaac.core.utils.prims import get_prim_at_path
from omni.isaac.core.utils.prims import create_prim # [!code ++]
world = World()
world.scene.add_default_ground_plane()
cube1 = world.scene.add( # [!code --]
    VisualCuboid( # [!code --]
        prim_path=&quot;/World/cube1&quot;, # [!code --]
        name=&quot;cube1&quot;, # [!code --]
        position=np.array([0, 0, 1.0]), # [!code --]
        scale=np.array([0.5015, 0.5015, 0.5015]), # [!code --]
        color=np.array([0, 0, 1.0]), # [!code --]
    )) # [!code --]
prim = create_prim( # [!code ++]
        prim_path=&quot;/World/banana&quot;, # [!code ++]
        prim_type=&quot;Xform&quot;, # [!code ++]
        usd_path=&quot;categories/usds/banana/0ac2e7f320d44e84a2647cd81f250107.usd&quot;, # [!code ++]
        translation=np.array([0, 0, 0.3]), # [!code ++]
        scale=np.array([0.01, 0.01, 0.01]), # [!code ++]
    ) # [!code ++]
cube1_prim = get_prim_at_path(&quot;/World/cube1&quot;) # [!code --]
banana_prim = get_prim_at_path(&quot;/World/banana&quot;) # [!code ++]
from pxr import UsdPhysics
rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(cube1_prim) # [!code --]
rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(banana_prim) # [!code ++]
collision_api = UsdPhysics.CollisionAPI.Apply(cube1_prim) # [!code --]
collision_api = UsdPhysics.CollisionAPI.Apply(banana_prim) # [!code ++]
mesh_collision_api = UsdPhysics.MeshCollisionAPI.Apply(cube1_prim) # [!code --]
mesh_collision_api = UsdPhysics.MeshCollisionAPI.Apply(banana_prim) # [!code ++]
mesh_collision_api.CreateApproximationAttr().Set(&quot;convexDecomposition&quot;)
collision_api.GetCollisionEnabledAttr().Set(True)
for _ in range(10000):
    world.step()
simulation_app.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;需要注意的是，因为加载进来的 USD 并不是具有某个标准尺寸，所以说你可能要调整 scale 以及 translation 的参数，才能让这个 USD 可以用一个正常的尺寸进行展示。在后续的内容中，我们会展示如何来用一个统一的尺度管理这些 USD，但是至于现在嘛，先手动调整一下。&lt;/p&gt;
&lt;p&gt;然后，读者可以在 GUI 里面开启 Collider 的可视化。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2yyiwsc7tq.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.4cl20tpm3l.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;读者可以看到这个 Collider 的贴合效果并不是很好。而一般来说，我们确实会使用 &lt;code&gt;convexDecomposition&lt;/code&gt; 来作为这个 Collider 的近似。除此之外，你还可以使用 &lt;code&gt;none&lt;/code&gt; 或者 &lt;code&gt;convexHull&lt;/code&gt; 或者 &lt;code&gt;sdf&lt;/code&gt;。&lt;code&gt;sdf&lt;/code&gt; 更加精细，但是需要配置若干 GPU 的内容，我们在将来的间章中再介绍，而另外两个从名字来看就不难发现效果不好。&lt;/p&gt;
&lt;p&gt;我们添加一段代码，来用 physx 来设置更加合理的参数：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False}) # we can also run as headless.

from omni.isaac.core import World
from omni.isaac.core.objects import VisualCuboid
from omni.isaac.core.utils.prims import get_prim_at_path
from omni.isaac.core.utils.prims import create_prim
world = World()
world.scene.add_default_ground_plane()
prim = create_prim(
        prim_path=&quot;/World/banana&quot;,
        prim_type=&quot;Xform&quot;,
        usd_path=&quot;/home/gaoning/grasp_vla_assets/categories/usds/banana/0ac2e7f320d44e84a2647cd81f250107.usd&quot;,
        translation=np.array([0, 0, 0.3]),
        scale=np.array([0.01, 0.01, 0.01]),
    )
banana_prim = get_prim_at_path(&quot;/World/banana&quot;)
from pxr import UsdPhysics
rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(banana_prim)
collision_api = UsdPhysics.CollisionAPI.Apply(banana_prim)
mesh_collision_api = UsdPhysics.MeshCollisionAPI.Apply(banana_prim)
mesh_collision_api.CreateApproximationAttr().Set(&quot;convexDecomposition&quot;)
collision_api.GetCollisionEnabledAttr().Set(True)
from pxr import PhysxSchema # [!code ++]
collision_schema = PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(banana_prim) # [!code ++]
collision_schema.CreateHullVertexLimitAttr().Set(64) # [!code ++]
collision_schema.CreateMaxConvexHullsAttr().Set(64) # [!code ++]
collision_schema.CreateMinThicknessAttr().Set(0.001) # [!code ++]
collision_schema.CreateShrinkWrapAttr().Set(True) # [!code ++]
collision_schema.CreateErrorPercentageAttr().Set(0.1) # [!code ++]
for _ in range(10000):
    world.step()
simulation_app.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时再次运行程序，你会发现这个香蕉的碰撞箱更加合理了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.5j4d9fpyln.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这其中 &lt;code&gt;CreateHullVertexLimitAttr&lt;/code&gt; 以及 &lt;code&gt;CreateMaxConvexHullsAttr&lt;/code&gt; 都是在设置 Collider 的面数，此处的面数不需要太高。&lt;code&gt;CreateMinThicknessAttr&lt;/code&gt; 设置了 Collider 最薄的地方，不然假如说物体本身很细，就可能会穿模。&lt;code&gt;CreateShrinkWrapAttr&lt;/code&gt; 和 &lt;code&gt;CreateErrorPercentageAttr&lt;/code&gt; 是一切的关键，你可以发现这两个参数使得 Colldier 贴合了，第一个是开启贴合的优化，第二个则是指定错误率的上限。&lt;/p&gt;
&lt;p&gt;在这里给出一个更加完整的 Code，包括其他的 Collider 的设置：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def set_colliders(prim_path, collision_approximation=&quot;convexDecomposition&quot;):
    prim = get_prim_at_path(prim_path)
    collider = UsdPhysics.CollisionAPI.Apply(prim)
    mesh_collider = UsdPhysics.MeshCollisionAPI.Apply(prim)
    mesh_collider.CreateApproximationAttr().Set(collision_approximation)
    collider.GetCollisionEnabledAttr().Set(True)
    if collision_approximation == &quot;convexDecomposition&quot;:
        collision_api = PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(prim)
        collision_api.CreateHullVertexLimitAttr().Set(64)
        collision_api.CreateMaxConvexHullsAttr().Set(64)
        collision_api.CreateMinThicknessAttr().Set(0.001)
        collision_api.CreateShrinkWrapAttr().Set(True)
        collision_api.CreateErrorPercentageAttr().Set(0.1)
    elif collision_approximation == &quot;convexHull&quot;:
        collision_api = PhysxSchema.PhysxConvexHullCollisionAPI.Apply(prim)
        collision_api.CreateHullVertexLimitAttr().Set(64)
        collision_api.CreateMinThicknessAttr().Set(0.00001)
    elif collision_approximation == &quot;sdf&quot;:
        collision_api = PhysxSchema.PhysxSDFMeshCollisionAPI.Apply(prim)
        collision_api.CreateSdfResolutionAttr().Set(1024)
    return prim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在此处不展开介绍。&lt;/p&gt;
&lt;h2&gt;休息一下&lt;/h2&gt;
&lt;p&gt;好了，我们现在已经了解了 Collider 以及 Rigid 如何设置，之后的间章中，我们可能会进一步讨论这些问题，比如说应该如何规避穿模之类的情况。不过，最后，让我们再看一次，太空中飞行的香蕉。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/PixPin_2025-03-29_05-34-48.73u48x1srl.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;祝你开心~&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/isaac-5-zh.webp"/><enclosure url="https://picr2.axi404.top/isaac-5-zh.webp"/></item><item><title>周记 Week38</title><link>https://axi404.github.io/blog/week-38</link><guid isPermaLink="true">https://axi404.github.io/blog/week-38</guid><description>2025-03-17 ~ 2025-03-23.</description><pubDate>Wed, 26 Mar 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这应该算是一次比较及时的周记了，现在是星期一，距离 CVPR camera ready 还有一天多，我把倒数第二批实验挂了起来，之后还差一个，脚本也写好了，等结束了之后可以继续挂。我现在不太敢做太大的并行度设计，考虑到一些性能问题，服务器可能并不能支持这种对 CPU 调度较多的任务被大量进行。&lt;/p&gt;
&lt;p&gt;现在我出来找点吃的，顺便记录一下上周发生的事情。同时上周有朋友和我说，很喜欢看我的周记，认为这是一种生活的感觉，所以我打算多写一些，他说想看我和乐小姐的故事，这周是科研地狱，戏份恐怕不是特别多。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;首先还是先说科研，上个星期其实过得并不是很顺利，因为一些合作问题，合作者是一个比较新手的人，因此在时间上没有把控的很好，我一直在处理一些自己的框架上的事情，导致直到上周周中的时候，才发现这里的进度已经不太可以接受。&lt;/p&gt;
&lt;p&gt;按照我个人的看法和风格来说，留给那位同学的时间有将近两周，而这个任务基本上是我一天可以做好的，但是学习成本确实也是考量的一环，这倒是也怪不了他，毕竟对方在和我沟通的时候也很积极，我觉得他学到了足够多的东西之后也会是很好的合作者。&lt;/p&gt;
&lt;p&gt;但是先不考虑将来的事情，按照眼下来看，尽管队伍里有其他的比较厉害的同学，但是最快来做实现还是我比较合适，毕竟剩下大概四五天，包括踩坑以及其他的时间，说实话并不是很富裕。所以就像上面说的这样，又要开始漫长的熬夜了。&lt;/p&gt;
&lt;p&gt;其实是一种很神奇的感觉，你自己已经做了很多事情，也已经很辛苦了，但是那条线就在前面，你很清楚来的很长一段时间内，你会在全神贯注且疲劳的状态中度过，但是你并没有逃避的方法，而只能硬着头皮面对。这就是我目前的感受。&lt;/p&gt;
&lt;p&gt;因为代码是很久以前写的，那时候我对于整体的代码结构以及仿真和模型的运作逻辑还没有很好的认知（这种认知，并不是说我不知道如何跑仿真或者跑程序，是一种在实践中得出来的，比如说你会知道把仿真环境和模型格离开会好一些，这里面又有若干种通信方案，服务器的网络配置比较复杂，你可以把端口转发到本地，转发之后，你或许会发现一些信息在带宽有限的时候又过于占用信道容量，所以又要调整），而我要测试的一个方法，可以理解为是某种 prompt baseline，这里面涉及了若干的程序和服务的调用与通信，因此把这个老框架变得可以使用，也花了一些脑子。&lt;/p&gt;
&lt;p&gt;大概是星期四开始写，然后就可以在服务器跑，但是跑一次要十个小时，发现是通信问题之后又改了结构，跑一次只用两个小时，这时候是星期五。再然后改了更方便的脚本，去调用各种东西以及传输参数，把一切调试明白，然后支持更多模型，这时候是星期六，之后再缝缝补补就是星期日和当下了。&lt;/p&gt;
&lt;p&gt;再包括前面的半周在做的事情，其实也是一种重复性工作，这其实是很糟糕的，或者说至少我很不喜欢。Python 和 bash 我都很喜欢，这些本质上都是脚本语言，可以帮助我快速完成很多重复的工作，但是有的时候开发一个脚本太麻烦，麻烦到甚至可以做一篇单独的论文，而又有一个时间线在后面追着你，这时候就不得不花费一些时间，做那些重复的本来可以用脚本完成的工作，因为目前你的 scope 只是 10，尽管脚本可以轻松到 10，甚至到 10K。&lt;/p&gt;
&lt;p&gt;下周等论文的终稿交了之后，应该就可以放松一段时间了，看一看论文，学一些我想学的东西，把之前的代码再进一步的 refine，并且写一写文档。这是我第一次做这种大型框架的开发，设置可以说绝大多数事情是我一个人完成的，作为某种中枢，无论是设计的理念还是技术都是进步了很多的。不过尽管本身没什么问题，但是伴随着版本的更新，很多人有新的需求，我就需要奔波于满足他们的需求，这些需求更改后对于整体框架影响，比如说运行效率。不过好在目前我意识到了这些问题，下周的时候可以好好处理一下。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;朋友说喜欢看我写周记，感觉大概是希望我写一些生活相关的东西，但是不得不做这一周事情有什么好说的，毕竟一直在跑实验的焦虑中度过。&lt;/p&gt;
&lt;p&gt;因为一直在通宵，我明显会感觉到自己的效率不是很好，一直处于一种恍惚的状态，晚上才可以彻底清醒，然后去做一些事情。&lt;/p&gt;
&lt;p&gt;这一周感觉西安的天气变化得很快，基本上感觉开始的时候还在穿羽绒服和羽绒绒裤，这一周结束的时候就已经是短袖和正常的长裤了。在中午的时候阳光从窗子打进来，甚至可以从空气中出一股浓郁的春天的味道。当然，要是我没有花粉过敏就更好了，学校有一些石楠花，味道难闻是一方面，而且会散发出浓郁的花粉，导致我不得不带上口罩。不过带上口罩，我就会昏昏欲睡，或许是因为喘不上气的原因，貌似也没有什么解决的办法。&lt;/p&gt;
&lt;p&gt;这周其实和乐小姐在一起的时间不是很多，毕竟基本上都扑在电脑前面敲代码，貌似也没什么办法做别的事情，然后就又给乐小姐买了新的 lo 裙，感觉还是很好看的，尤其是她站在我的面前的时候。现在貌似真应了那一句调侃的话，「我除了钱什么都不能给你了」，虽然倒也不是如此富裕，但是假如没有时间陪你，我还是希望你开心。&lt;/p&gt;
&lt;p&gt;这段时间其实还是属于一种 burn out 的状态，似乎有在之前的周记或者什么里面说过，从大概是初中开始我就有了「心理疾病」，毕竟没有在体检中查出来什么东西，所以大概也不是什么身体问题。具体来说当我有很多事情的时候，我就会感觉很焦虑，这种焦虑会体现在某些生理反应上，有点像一些心悸的感觉，要是需要去描述的话，读者在初中或者高中考试的时候，等待发成绩的时候心悬起来的感觉很像，我现在时刻就处于这种状态中。&lt;/p&gt;
&lt;p&gt;具体来说，我很有可能会经常担心自己是不是有什么事情忘记了，所以我有一套 obsidian 组织起来的工作流，上面有我用起来很顺手的 to list，列下来就会感觉好了不少。不过最近又开始听音乐了，现在另一种缓解这种情况的方法，就是靠在椅子上带着耳机听平克弗洛伊德，那时候还是周中，屋子外面的空气还有一点冷，就像是透过窗子撒在墙上的阳光一样清冷，然后我望着天花板，什么也不做，什么也不说，什么也不想。&lt;/p&gt;
&lt;p&gt;总的来说，应该是比较平平无奇的一周，下一周或许会有趣一些吧，至少生活里面的盼头多了不少，可以在没有 ddl 的压力下生活很长时间，想来也是很不错的。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-38-zh.webp"/><enclosure url="https://picr2.axi404.top/week-38-zh.webp"/></item><item><title>周记 Week37</title><link>https://axi404.github.io/blog/week-37</link><guid isPermaLink="true">https://axi404.github.io/blog/week-37</guid><description>2025-03-10 ~ 2025-03-16.</description><pubDate>Wed, 19 Mar 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;总体上来说，这是一次比较及时的周记，这一周是 CVPR camera ready 的倒数第二周，所以说一些事情也开始排上日程了，与此同时项目的维护也需要进行，这些都是需要推进的内容。&lt;/p&gt;
&lt;p&gt;按照我的理解来说，在这一周结束之后，也可以说是一个带引号的解放了，毕竟不会存在一些含有非做不可属性的任务需要去做。尽管说，这并不意味着我会去减少我任务的时间，但是至少不会有一种紧迫感，长时间在这种赶进度的状态下生存还是很难受的。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;学校的校园生活开始步入正轨。从开始几周的过渡时间，正式到了常规的校园节奏，布置的作业需要陆续去完成，好在老师也算是善良，没有给出非常困难的作业需要去做。&lt;/p&gt;
&lt;p&gt;在科研方面只能说稳步推进，我在我自己的代码空间上面不断地更新，并且给其他人提供支持，然而这显然并非是最好的情况，多个人一起开发仓库才是最合理的形式。&lt;/p&gt;
&lt;p&gt;同时长时间在自己的代码基础上进行开发，会逐渐消磨掉我对于技术的敏感度，比如说对于 isaac sim 相关的一些使用，我会发现其他人会的一些技术我已经没用过了，虽然只是几行代码的事情。学习更多别人的内容才是进一步提升自己的最好方，这方面还需要把握一下这个平衡。&lt;/p&gt;
&lt;p&gt;就像开头说的一样，这一周及下周应该是十分忙碌的，但是结束之后就可以慢下来做一些其他事情了。&lt;/p&gt;
&lt;h2&gt;写作&lt;/h2&gt;
&lt;p&gt;书接上文，之前一直说了很多需要写的东西，但是确实一直没有时间去写，伴随着下周假如结束，我也会开始逐渐完成之前想做的事情。&lt;/p&gt;
&lt;p&gt;假如一切顺利的话，在下周过后，我就可以开始着手进行采访的准备，然后去进行科研方面的一些分享采访。同时当然，isaac sim 101 也是一个重头戏，我认为这是非常有价值的分享，也可以帮助到很多人。最后就是和 tianxing 组建了 vita- robotics 的社区，我自己的倾向还是去写一些 tutorials，因此这方面也会做一些准备。&lt;/p&gt;
&lt;p&gt;当然，另一方面有必要说的是，我现在确实感觉需要阅读更多的论文，这是十分重要的，甚至可以说是我目前比较缺失的部分，走一步看一步吧。&lt;/p&gt;
&lt;p&gt;总的来说，这周还是平平无奇的一周，正常地打工，然后正常地科研学习，好好生活。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-37-zh.webp"/><enclosure url="https://picr2.axi404.top/week-37-zh.webp"/></item><item><title>周记 Week36</title><link>https://axi404.github.io/blog/week-36</link><guid isPermaLink="true">https://axi404.github.io/blog/week-36</guid><description>2025-03-03 ~ 2025-03-09.</description><pubDate>Sun, 16 Mar 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说这又是很久之后的一次周记了，只能说，伴随着工作的不断稳定，这些内容越来越不知道应该如何说起。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;尽管 CVPR 中稿，但是显然这一切并不是一个终点，我的这个项目还要继续 refine 并且为其他同学提供支持，camera ready 的优先级在这漫长的时间面前也并不是很高，所以这一个星期主要都是在写代码中度过的。本身倒是也没什么可说的。&lt;/p&gt;
&lt;p&gt;要是非要说一些什么的话，组里面又来了新的同学，也是要帮助我做一些事情。不含傲慢地说，回过头看这些刚刚在这方面涉足的同学，不得不感慨，自己相较于当初在刚刚加入实验室时候的自己相比，已经是进步了很多的了。作为经典的老好人，我自然也是很乐意和大家去分享我目前掌握的知识。事实上，我并不认为自己是一个绝对的天才，或者可以改变这个领域，但是我觉得，除了改变这个领域之外，另一个方面，则是改变这个社区。社区和领域里面的人可能是十分类似的，但是实际上的定义在我看来其实并不完全相同。&lt;/p&gt;
&lt;p&gt;所谓的改变领域，实际上就是提出诸如 GPT2 一样跨时代的架构，或者像是在后续过程中推出的一个个重要的成果 LoRA, MoE, RLHF, etc. 推出这种架构需要恰到好处的天赋以及运气，然而能够赶上这种浪潮，对于大多数人来说其实只是侥幸。改变领域可以改变很多人的研究方向，在这个领域的这一批人中拥有巨大的影响力。另一方面，则是改变这个社区，这个想法听上去又有一些不太一样了。所谓社区，不只是这些课题组，或者是这些正在领域的最前方各领风骚的一批人，他们或许决定着这个领域的方向，但是我们说的是另外的人，那些大多数的同学。包括说建立绿群，或者说将来写作各种各样的教程，其实目的都是如此，一句有点给自己上价值的话来说，「安得广厦千万间，大庇天下寒士俱欢颜」。&lt;/p&gt;
&lt;p&gt;最近和 tianxing 一起打算制作 VITA 具身智能社区，大概也是相同的想法，我并不是很在意宣传或者 credits，而是说能否具有影响力，让我的教程可以帮到更多的人，这会让我开心。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;剩下的也无外乎就是生活了，事情很多，之前说好的写作清单里面的东西基本上一个也都还没有写，等一切安顿下来之后应该可以好一些吧。不过说来也是，每一次在周记里面说到这些东西的时候，都是等一等，或者过一段时间，也不知道如此忙碌的时候什么时候可以是一个尽头，CVPR 投稿？项目的不断交付日期？CVPR 截稿？后续的对接？一个接一个，也不知道如何才好。&lt;/p&gt;
&lt;p&gt;除此之外，因为和老师聊完之后，对于自己将来的去向已经有了分寸，可以说是意料之中，之后和家里人聊了一聊。私以为我现在自己的生活确实有一些太单调了，基本上就是每天都在高强度的写程序，除此之外什么事情都没有做。作为科研人，至少要坚持看论文，这是一个基础。作为一个人，至少要运动，来保证身体健康，以及健康的作息。作为一名社会上的人，要保持正常数量与别人的交流，以及要开开心心地和乐小姐一起生活。作为一个需要更进一步陶冶情操的人，或许可以看一些艺术相关或者人物传记的书籍，这其实也是我最近比较感兴趣的一点。&lt;/p&gt;
&lt;p&gt;不过显然对于目前的工作强度来说，做到这些事情还是有一些太勉强了。一方面是自己的任务确实太多，一方面其实是自己目前有一些同学可以协助我，但是我却很难及时 track 大家的情况，项目管理实在是一门学问。继续努力进步吧，相信一切在未来会有答案。&lt;/p&gt;
&lt;p&gt;以及，博客现在更新频率还是太少了，感觉是一个危险的信号，需要小心。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-36-zh.webp"/><enclosure url="https://picr2.axi404.top/week-36-zh.webp"/></item><item><title>周记 Week35</title><link>https://axi404.github.io/blog/week-35</link><guid isPermaLink="true">https://axi404.github.io/blog/week-35</guid><description>2025-02-24 ~ 2025-03-02.</description><pubDate>Sun, 02 Mar 2025 00:00:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;你以为这天是又一个很平常的日子，多年之后才发现这其实是你人生里最棒的一天，这样的一天永远不会再有了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不知道从什么地方说起，但是确实很令人感慨，进入 SHAILAB 仿佛是上周刚发生的事情，一转眼一切已经抵达下一站。在 SHAILAB 实习的辛苦换来了回报，CVPR 5333-&gt;5442，成功中稿。&lt;/p&gt;
&lt;h2&gt;福灵剂&lt;/h2&gt;
&lt;p&gt;这一切的发生甚至有一些像哈利·波特小说的福灵剂，如此幸运的一天或者两天，接连发生在了我的身上。首先是上个学期的六级考试，虽然我的科研已经很不错了，但是假如六级没有通过，将来在夏令营可能还是会吃很多亏，而因为一直忙于科研，上个学期是我第一次参加六级考试，匆忙裸考，考完之后感觉良好，但是没有想到确实顺利通过了。&lt;/p&gt;
&lt;p&gt;然后就是 CVPR 出结果，因为委员会出现了奇怪的问题，导致结果比预计的时间晚了十二个小时，我本来的逾期是会提前十二小时出。因此我一边在煎熬中写代码，继续推进科研的进度，一边等待着结果。后面实在是撑不住了，趴下睡了一小会儿，醒来之后打开网站，发现中稿了。&lt;/p&gt;
&lt;p&gt;实际上我可以说我一直是有一种信念感的，就像是我认为我的六级一定可以通过一样，我同样认为这篇论文一定可以被接收，一方面是一直以来的积累和努力，一方面也是对自己的自信。&lt;/p&gt;
&lt;p&gt;当然，同时在同一天，mentor（伦哥）也在开会中说到了将来的一些计划，我做的工作在将来还能派上用场，可以和很多人合作，可以为课题组带来价值，这实在是太美好了。&lt;/p&gt;
&lt;p&gt;在上海的第一次投稿画上一个不太完美的句号（不能拿到 oral 了），六级顺利通过给将来的夏令营铺路，将来的科研也可以和很多人合作，以及顺利地改完了一个很大的需求，修好了很多程序的不足，一切都是如此的美好。&lt;/p&gt;
&lt;h2&gt;写作计划&lt;/h2&gt;
&lt;p&gt;因为最近发生了很多事情，所以有了很多的想法。同时，按理来说，后续的科研工作也应该会不断地趋于稳定，所以说可以适当让自己全面发展一些了。这其中包括一些陶冶情操，以及更多地，比如说发展社区以及一些其他的事情。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;小说&lt;/strong&gt;：其实一直以来很想写小说，也算是业余爱好了，但是一直没有时间，大概之前是想要写一个赛博朋克风格的小说，而最近感觉管理局的设定也很不错，感觉可以把两个融合一下。不过可能我更多还是在享受写设定的过程，毕竟确实很有趣。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isaac 101&lt;/strong&gt;：Isaac 101 也是我一直打算写的教程。Isaac Sim 是非常强大的仿真平台，但是网上关于它的教程非常少，所以我想写一个教程，同时这也可能会带来一些社区的影响力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AIDIY&lt;/strong&gt;：AIDIY 是一个 AI 科研自学指南，当然，与其说是自学指南，不如说是单纯的指南。毕竟科研到成熟的时候必须参与课题组的科研，所以更多地还是在汇总资讯、给出常识知识以及心法等。同时这方面的内容还打算 Base 绿群做一个分享，我打算采访一些同学，来问一下他们对于科研的建议、看法等内容，我之前没有采访过别人，这或许会是一个有趣的体验。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CSBAOYAN&lt;/strong&gt;：CSBAOYAN 也就是绿群的主站，现在依然是一个半成品的状态，后续也会邀请一些同学来一起完善。同时保研季也要来了，作为群主我也要开始汇总这些夏令营以及预推免的信息，并且更新在 CS-BAOYAN-DDL 中。&lt;/li&gt;
&lt;/ul&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-35-zh.webp"/><enclosure url="https://picr2.axi404.top/week-35-zh.webp"/></item><item><title>周记 Week34</title><link>https://axi404.github.io/blog/week-34</link><guid isPermaLink="true">https://axi404.github.io/blog/week-34</guid><description>2025-02-17 ~ 2025-02-23.</description><pubDate>Mon, 24 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说，这就是很多天之后的一次周记了，之所以又晚了这么久，主要是有两个原因。&lt;/p&gt;
&lt;h2&gt;Burn out&lt;/h2&gt;
&lt;p&gt;其中之一自然就是这一周的主旋律，也就是 burn out，可以说，从去年八月份开始一直到现在，基本上我一直处于全年无休的状态，而且每天可能要工作八小时或者更多。老实说我其实是喜欢科研的，但是一直这样下去也不是一个办法，尽管心理和心情是愉悦的，但是身体上的疲劳确是不可避免的。肉眼可见我需要做的事情还有很多，也会有更长的一段时间依然在这种状态中，就让我实在难以提起兴趣记下一些东西，而是只是埋头在写代码。&lt;/p&gt;
&lt;p&gt;这一周我可以说花了一些时间来调整，整个人靠在床上，播放音乐，大脑放空，看着天花板，让思绪飘到不知道哪里去。这样做确实带来了一些帮助，现在我已经有效地从状态中恢复了过来，但是以此带来的一段缓冲期是不可避免的。尽管我已经可以很有活力地在工作中投入，时间我更愿意发呆，而不是写一些其他的内容。&lt;/p&gt;
&lt;p&gt;当然，另外一点也是最近的天气也不是很好，没有一个惬意的晚上，空气中的凉意不是很充沛，我可以在校园中漫步，然后记下我想的东西。不过好在现在是有时间了，可以大概说一说。&lt;/p&gt;
&lt;h2&gt;学习与生活&lt;/h2&gt;
&lt;p&gt;其实要是来总结的话，倒是也没有什么可说的，第一周的上课还算正常，平时就在课上继续稳步推进自己的进度，并且听一些课。课程的内容也不是很难，基本上听一耳朵就可以了解。&lt;/p&gt;
&lt;p&gt;同时也和自己认识的同学，现在其实主要来说就是年级第一了（张亚冬，AKA 亚冬）有了一些交流，可以说是在乐小姐不在的时候为数不多可以和我聊天的人了。&lt;/p&gt;
&lt;p&gt;当然也有和其他同学有短暂的交流，包括袁行健同学，和他们交流了一下，目前我对于科研的一些看法，以及应该如何去上手科研。&lt;/p&gt;
&lt;p&gt;一个比较遗憾的事情在于，我最近并没有时间做很多的论文阅读，之前计划好了，在假期阅读若干篇论文的事情也落空了。实际上现在的工作真的很多，我也不知道哪里能够获得这些时间，但是一个直观的感觉是，现在确实还有很多时间被花在了发呆上。我认为我的效率是非常高的，可能会比正常的同学要高出一倍，但是我可能一天只能有五六个小时可以 focus 在我需要做的事情上，这是一件很糟糕的事情。&lt;/p&gt;
&lt;p&gt;事实上我可以很快地进入心流状态，并且直到完成这件事情之前都可以保持专注，但是在完成之后，我就很难继续留在状态中，而是感到疲惫。这自然就导致我可以经常稳定的完成自己的工作，这时候我的注意力是集中的，但是在完成之后，就难以再去提起力气，阅读论文或者做其他事情了。&lt;/p&gt;
&lt;p&gt;这就是大概这一周的经过，正常的学习，正常的科研，以及进行了适当的缓冲，缓解了我的 burn out。状态还需要进一步的调整，这是毫无疑问的一件事情，所以还请看后续的进展。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-34-zh.webp"/><enclosure url="https://picr2.axi404.top/week-34-zh.webp"/></item><item><title>Isaac Sim 一百讲（a）：Docker 使用</title><link>https://axi404.github.io/blog/isaac-docker</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-docker</guid><description>从零开始的 Isaac Sim 之路，间章！</description><pubDate>Thu, 23 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;Docker 是一个很强大的工具，可以快速地搭建一个开发环境，并且可以很方便地进行迁移，然而我们本不需要 Docker 来使用 Isaac Sim。假如说读者只需要在自己的电脑上使用 Isaac Sim，那么完全不需要 Docker，自己的电脑只有一张显卡，正常运行也不会有任何的问题。&lt;/p&gt;
&lt;p&gt;然而假如说在服务器上使用的时候，尤其是指多卡的服务器上，只要进行了简单的尝试，读者就会很快发现问题所在：Isaac Sim 并不会识别 CUDA Visible Devices，而是会默认使用所有可见的 GPU。Isaac Sim 的显卡占用策略并不是合理的，对于一个需要 6GB 显存的应用，假如说有 8 张显卡，那么 Isaac Sim 会占用 8 张显卡上面各 5GB，这毫无疑问的巨大的浪费。&lt;/p&gt;
&lt;h2&gt;上手 Docker&lt;/h2&gt;
&lt;p&gt;解决方法也很简单，使用 Docker 即可。对于大多数读者来说，可能都是久仰 Docker 的大名，然而对于如何使用，以及如何最快速地上手依然心存疑虑，事实证明 Docker 并非十分复杂，在你的 Isaac Sim 之旅中，也不需要过多的 Docker 知识。&lt;/p&gt;
&lt;p&gt;首先，在这里指路 Isaac Sim 的文档：&lt;/p&gt;
&lt;p&gt;首先你需要通过正常的 APT 方法安装 Docker 以及 CUDA 的 Toolkit，前者让你可以使用 Docker，后者则可以让你使用 GPU。&lt;/p&gt;
&lt;p&gt;安装 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Docker installation using the convenience script
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Post-install steps for Docker
sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker

# Verify Docker
docker run hello-world
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这其中你需要注意的是，假如说你的设备上已经安装了 Docker，那么你唯一需要进行的操作就是让管理员给你添加到 Docker 用户组里面，即：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo usermod -aG docker $USER
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接下来安装 CUDA 的 Toolkit：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Configure the repository
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  &amp;#x26;&amp;#x26; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed &apos;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&apos; | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \
  &amp;#x26;&amp;#x26; \
    sudo apt-get update

# Install the NVIDIA Container Toolkit packages
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# Configure the container runtime
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verify NVIDIA Container Toolkit
docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然，事实上这些 Docker 的 run 的指令你都没必要跑。&lt;/p&gt;
&lt;h2&gt;使用 Isaac Sim Docker&lt;/h2&gt;
&lt;p&gt;首先你需要下载 Isaac Sim 的 Docker，这是一个很简单的操作：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull nvcr.io/nvidia/isaac-sim:4.1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是紧接着，在大多数服务器上，你会发现问题，即网络问题导致你无法进行访问。服务器的一些设置使得你添加了代理之后也难以获取这些 Docker，因此你只能手动将文件上传。&lt;/p&gt;
&lt;h3&gt;手动上传&lt;/h3&gt;
&lt;p&gt;首先，你需要下载 Isaac Sim 的 Docker 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull nvcr.io/nvidia/isaac-sim:4.1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这一步操作发生在你个人的电脑上，并且使用了代理。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;docker images&lt;/code&gt; 查看你本地的 Docker 镜像，然后使用 &lt;code&gt;docker save -o isaac-sim-4.1.0.tar &amp;#x3C;image_id&gt;&lt;/code&gt; 将镜像保存为 tar 文件。&lt;/p&gt;
&lt;p&gt;之后可以通过 sftp 或者 scp 将文件上传到服务器上。在服务器上，使用 &lt;code&gt;docker load -i isaac-sim-4.1.0.tar&lt;/code&gt; 将镜像加载到本地。&lt;/p&gt;
&lt;p&gt;然后你就可以使用：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --name isaac-sim --entrypoint bash -it --runtime=nvidia --gpus all -e &quot;ACCEPT_EULA=Y&quot; --rm --network=host \
    -e &quot;PRIVACY_CONSENT=Y&quot; \
    -v ~/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \
    -v ~/docker/isaac-sim/cache/ov:/root/.cache/ov:rw \
    -v ~/docker/isaac-sim/cache/pip:/root/.cache/pip:rw \
    -v ~/docker/isaac-sim/cache/glcache:/root/.cache/nvidia/GLCache:rw \
    -v ~/docker/isaac-sim/cache/computecache:/root/.nv/ComputeCache:rw \
    -v ~/docker/isaac-sim/logs:/root/.nvidia-omniverse/logs:rw \
    -v ~/docker/isaac-sim/data:/root/.local/share/ov/data:rw \
    -v ~/docker/isaac-sim/documents:/root/Documents:rw \
    nvcr.io/nvidia/isaac-sim:4.1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Docker 快速上手&lt;/h2&gt;
&lt;h3&gt;Docker 的基本使用&lt;/h3&gt;
&lt;p&gt;你可以看着上面的东西会想，wait，发生了什么，这些指令都是什么意思，这很正常，让我娓娓道来。&lt;/p&gt;
&lt;p&gt;本身，你可以这样理解，Docker 这个工具是让你运行一个封闭的小型的 Linux 系统，这个系统里面有你需要的所有东西，你只需要在上面运行你的程序即可。这种感觉实际上很像我们常用的 miniconda，而事实上我们也可以用类似的思路去使用一些 Docker 镜像。&lt;/p&gt;
&lt;p&gt;镜像和容器是 Docker 两个十分重要的内容，镜像是某种系统，而容器则是一个实例，使得这个系统正在运行。假如说你想要修改镜像，一个显然的操作是，运行一个实例，然后修改，然后保存为新的镜像。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;docker images&lt;/code&gt; 可以查看你本地的镜像，使用 &lt;code&gt;docker ps -a&lt;/code&gt; 可以查看你的容器。假如说你退出了容器，一种可能的结果是这个容器就会停止，但是你依然可以重新启动它。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;docker run&lt;/code&gt; 可以运行一个容器，在我们的例子中，这个容器来自本地，通过你之前的 load 操作进行了加载，可以在 &lt;code&gt;docker images&lt;/code&gt; 中看到，并且可以运行。&lt;/p&gt;
&lt;h3&gt;Docker 参数&lt;/h3&gt;
&lt;p&gt;理解了 Docker 的运行发生了什么，让我们来理解一下这些参数。我们可以交互式地运行一个容器，也可以直接让容器运行固定的指令，跑在后台，假如说你还是不是很熟悉 Docker 的运行方式，那么你或许可以尝试一下交互的运行，这样你可以直接把 Docker 当作某个服务器去使用，这种感觉一模一样，使用 &lt;code&gt;-it&lt;/code&gt; 即可。而相对应的，使用 &lt;code&gt;-d&lt;/code&gt; 则可以让容器在后台运行。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--rm&lt;/code&gt; 来自于另一个简单的需求，在我运行完了这个容器，容器结束了之后，我希望其可以直接销毁，这样我就不用每次都手动去删除容器了。不过在这里依然介绍一下手动销毁的方式，使用 &lt;code&gt;docker rm &amp;#x3C;container_name&gt;&lt;/code&gt; 即可。对于全部的容器，你需要先使得其停止，才能销毁，即 &lt;code&gt;docker stop &amp;#x3C;container_name&gt;&lt;/code&gt;，你可以在 &lt;code&gt;docker ps -a&lt;/code&gt; 中看到全部的容器，并且了解他们是否还在运行。&lt;/p&gt;
&lt;p&gt;另一个常见的需求是使用 &lt;code&gt;-v&lt;/code&gt; 挂载卷，即挂载一个本地的目录到容器中，这样你就可以在容器中修改文件，并且修改会自动同步到本地。这种操作同样可以反过来，我有若干个容器都在使用同一个卷，那么我就可以在本地修改，这个修改会被反馈到全部的容器中。前者可以用于生产数据，后者可以用于共享代码。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--entrypoint&lt;/code&gt; 则可以让你指定容器运行的入口点，即容器运行的第一个命令，Isaac Sim 官方的容器默认会执行 &lt;code&gt;./python.sh&lt;/code&gt;，直接启动一个 Isaac Sim 程序，而我们现在只是想进去逛逛，于是设置为 bash，顶替了默认的入口点，使得可以以正常的命令行方式进入。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-e&lt;/code&gt; 则可以让你指定环境变量，这里我们使用到了 &lt;code&gt;ACCEPT_EULA&lt;/code&gt; 和 &lt;code&gt;PRIVACY_CONSENT&lt;/code&gt;，这两个变量分别表示你已经阅读并同意了 NVIDIA 的 EULA 和 Privacy Policy。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--gpus&lt;/code&gt; 则可以让你指定容器运行的 GPU，这里我们使用到了全部的 GPU，即 &lt;code&gt;all&lt;/code&gt;，假如说你想要指定一张特定的卡，那么你可以使用 &lt;code&gt;--gpus &quot;device=0&quot;&lt;/code&gt; 来指定第 0 张卡。&lt;/p&gt;
&lt;h2&gt;编辑你的 Docker 镜像&lt;/h2&gt;
&lt;p&gt;于是我们也就不难实际上手了，运行之前提及的指令，不过你可以不需要挂那些文件，这并非你的需求，同时删除 &lt;code&gt;--rm&lt;/code&gt; 参数，这样你退出之后就可以保留容器，并且进行 &lt;code&gt;docker commit&lt;/code&gt; 操作了。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --name isaac-sim --entrypoint bash -it --runtime=nvidia --gpus 1 -e &quot;ACCEPT_EULA=Y&quot; --network=host -e &quot;PRIVACY_CONSENT=Y&quot; nvcr.io/nvidia/isaac-sim:4.1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同时你可以注意到，我们分配了一张卡，即 &lt;code&gt;--gpus 1&lt;/code&gt;，这里的 1 是数量，而非编号。&lt;/p&gt;
&lt;p&gt;运行完这个指令之后，你不难发现，你进入了 &lt;code&gt;/isaac-sim&lt;/code&gt; 目录，并且处于命令行界面。你可以使用 &lt;code&gt;ctrl+d&lt;/code&gt; 退出。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;docker commit &amp;#x3C;container_name&gt; &amp;#x3C;image_name&gt;&lt;/code&gt; 即可将容器保存为镜像。你可以通过之前提及的 &lt;code&gt;docker ps -a&lt;/code&gt; 来查看你的容器名称。&lt;/p&gt;
&lt;p&gt;在你进行了这一次保存之后，我们姑且认为这个镜像叫做 isaac，那么你可以直接运行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run -it --gpus 1 isaac
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时你又一次进入了命令行中。你可以尝试给这个环境安装一些库，比如：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./python.sh -m pip install open3d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后再次执行你的 commit 流程，你就可以得到一个包含 open3d 的镜像。&lt;/p&gt;
&lt;h2&gt;Isaac Sim Docker 的一种运行逻辑&lt;/h2&gt;
&lt;p&gt;你现在已经可以使用 Isaac Sim 了，执行一些程序也不是很难，事实上有一种文件拷贝的方式，你可以通过 &lt;code&gt;docker cp&lt;/code&gt; 将文件拷贝到容器中，然后运行。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker cp &amp;#x3C;file_path&gt; &amp;#x3C;container_name&gt;:/path/to/destination
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然而这很麻烦，你最后会维护一个巨大的容器，每一次修改文件，你都需要进入容器，修改并且 commit。&lt;/p&gt;
&lt;p&gt;事实上，你完全可以使用 Docker 的卷挂载，将你的项目挂载到容器中，这样你就可以在容器中修改文件，并且修改会自动同步到本地，这个感觉很像一个大号的 miniconda。&lt;/p&gt;
&lt;p&gt;我们假设这样一个场景，你有一个项目想要用来生成数据，因此你有了三个文件夹：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; 用来存放数据&lt;/li&gt;
&lt;li&gt;&lt;code&gt;code&lt;/code&gt; 用来存放代码&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scenes&lt;/code&gt; 用来存放场景&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你想要在容器中运行这个项目，那么你只需要：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run -it --gpus 1 --name isaac-1 --gpus 1\
        -v ~/docker/isaac-sim/data:/isaac-sim/data \
        -v ~/docker/isaac-sim/code:/isaac-sim/code \
        -v ~/docker/isaac-sim/scenes:/isaac-sim/scenes \
        isaac ./python.sh code/main.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后比如说你在 &lt;code&gt;code&lt;/code&gt; 中修改了代码，你不需要进入容器，你只需要在本地修改，然后容器就会自动同步，因为他们是同一个文件。&lt;/p&gt;
&lt;p&gt;然后再次执行上述的指令即可。&lt;/p&gt;
&lt;h2&gt;在 Docker 中安装 CUDA&lt;/h2&gt;
&lt;p&gt;尽管我们之前已经拥有了一个可以使用 &lt;code&gt;nvidia-smi&lt;/code&gt; 的 Docker 了，但是稍微懂行的读者是知道的，&lt;code&gt;nvidia-smi&lt;/code&gt; 只是 nvidia-driver 的一个工具，可以呼出界面只能说明你安装了 nvidia-driver，而使用 &lt;code&gt;nvcc -V&lt;/code&gt; 则可以查看 CUDA 的版本，此时会报错，因为 Docker 中没有 CUDA。&lt;/p&gt;
&lt;p&gt;是的，Isaac Sim 会使用 CUDA，但是这是在 Isaac 的 Conda 环境中维护的一个 CUDA，而并不是 Docker 的 CUDA，但是在你使用 Docker 安装诸如 cuRobo 等库的时候，会需要使用 CUDA，因此本篇略微介绍一下。&lt;/p&gt;
&lt;p&gt;依然是依照官网的方法，一个有趣的技巧在于使用 runfile 而非 deb 文件，因为 runfile 会自动帮你安装一些内容。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意在这里安装的是 11.8 版本，这是 Isaac 社区里面目前很常见的一个版本，比如说 cuRobo 的默认版本也是 11.8。在安装之前，你需要安装一些依赖：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;apt update
apt install sudo gcc g++ kmod
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后你就可以安装了：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo sh cuda_11.8.0_520.61.05_linux.run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在安装的过程中注意取消掉安装驱动，因为驱动已经安装过了。&lt;/p&gt;
&lt;p&gt;安装结束之后会出现一些提出，乍一看其实很像是报错，因为无序列表的语法很像是报错 track back 的时候经典的缩进，但是细看可以发现只是提示而已。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.6bh7d44rsh.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后需要添加一些配置：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export PATH=/usr/local/cuda-11.8/bin:$PATH # [!code ++]
export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH # [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/usr/local/cuda-11.8/lib64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo ldconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后你就可以在 &lt;code&gt;source ~/.bashrc&lt;/code&gt; 之后使用 &lt;code&gt;nvcc -V&lt;/code&gt; 查看 CUDA 的版本了，此时不会报错。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;对于在服务器上运行 Isaac Sim 的读者来说，使用 Docker 显然是一个必须掌握的技巧，本内容中我们介绍了如何使用 Docker 来运行 Isaac Sim，并且介绍了如何使用 Docker 来编辑你的镜像，以及如何使用 Docker 来挂载卷。通过我们讲解的运行逻辑，你可以把 docker 当作一个 miniconda 来使用，本身你执行的指令 &lt;code&gt;./python.sh code/main.py&lt;/code&gt;，只是在其前面加了一大堆的前缀而已，符合大多数读者常见的使用逻辑。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/isaac-docker-zh.webp"/><enclosure url="https://picr2.axi404.top/isaac-docker-zh.webp"/></item><item><title>周记 Week33</title><link>https://axi404.github.io/blog/week-33</link><guid isPermaLink="true">https://axi404.github.io/blog/week-33</guid><description>2025-02-10 ~ 2025-02-16.</description><pubDate>Thu, 20 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一周的周记可以说也是略微迟到，但是现在终于是抽出了一些时间，可以闲下来稍微写一些了。总体来说，这一周终于返校了，回到了熟悉又陌生的西安。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;乐小姐和我同一天回来，因为我到的时间稍微早一些，所以还做了一些规划。首先要在回来之后把乐小姐在我这里寄存的行李运回去，然后再去火车站接乐小姐，最后两个人一起吃了巴奴火锅，然后拥抱、告别，回到各自的宿舍。只能说相较于海底捞，确实感觉巴奴火锅可以吃的东西稍微多一些，但是确实也是很贵就是了。&lt;/p&gt;
&lt;p&gt;除此之外，这一周的生活倒也算是平稳度过，没有什么大起大落，正常的生活，正常的熬夜，然后在 VRChat 里面添加了一些新的服装，这些内容都在持续地迭代。不过貌似现在确实启动 Unity 的时间比 VRChat 要多很多很多，一方面确实现在不认识很多 VRChat 的朋友，一方面在 Unity 中已经可以获得相当的乐趣了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这周的科研可以说并不是很顺利，总体来说，大概是结果不能稳定复现导致的若干问题，但是貌似现在还是不能透露太多，虽然口口声声说会在这个月 release，但是暂时来看还是遥遥无期。不过这件事情确实让我很沮丧，好在快要找到一些苗头了，似乎是 baseline 本身容易崩掉导致的问题。唉，关于这个 Project 实在是有太多想说的，但是估计还是要等到 release 之后才能说，到时候给一个复盘帖子吧，感觉可以写几万字，无论是做的事情还是经历的事情，实在是太多了。&lt;/p&gt;
&lt;p&gt;总的来说，平平无奇的一周，但是每每想到下一周就要开始上课，就开始发自内心的抗拒，该死的上课，实在是不喜欢。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-33-zh.webp"/><enclosure url="https://picr2.axi404.top/week-33-zh.webp"/></item><item><title>清除 Chrome 缓存</title><link>https://axi404.github.io/blog/clean-cache</link><guid isPermaLink="true">https://axi404.github.io/blog/clean-cache</guid><description>清除 Chrome 缓存，刷新页面。</description><pubDate>Thu, 13 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;因为在写自己的 Blog 的时候，有的时候会使用自己的图床来加载图片，本身我的图床的搭建可以看 &lt;a href=&quot;/blog/imagebed&quot;&gt;之前的博客&lt;/a&gt;。其根本的逻辑就是每次上传图片之后，其实就是 push 到了 Github 的 Branch 里面，而使用 Vercel 托管之后，每次 push 都会触发 Vercel 的自动部署，所以每次上传图片之后，都会添加到我的 &lt;code&gt;pic.axi404.top&lt;/code&gt; 的图片列表中。不过毕竟部署是需要一些时间的，假如再次之前就已经以诸如 Markdown 的语法引用图片，那么就会导致图片无法显示，这很合理。但是问题出现在，即使图片已经部署到了图床中，但是页面就是不显示，这毫无疑问是因为 Chrome 的对这些图片进行了缓存。所以如何清除 Chrome 的缓存呢？&lt;/p&gt;
&lt;h2&gt;方法&lt;/h2&gt;
&lt;p&gt;本身不是很难，以 Chrome 为例，直接 F12 打开控制台，然后选择停用缓存即可，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.5q7jdxkwin.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在这样选择了之后可以刷新页面，这时候就没问题了。&lt;/p&gt;
&lt;p&gt;当然还有另一种方法，可以直接清除缓存：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.9rjisbt1ix.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;希望对你有帮助~&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/clean-cache-zh.webp"/><enclosure url="https://picr2.axi404.top/clean-cache-zh.webp"/></item><item><title>周记 Week32</title><link>https://axi404.github.io/blog/week-32</link><guid isPermaLink="true">https://axi404.github.io/blog/week-32</guid><description>2025-02-03 ~ 2025-02-09.</description><pubDate>Tue, 11 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说，又是很久以后的一次周记了。时间一直在流逝，不到一个月的寒假时间很快就到达了结尾，随着这周的结束，我也要回到学校了，不禁感到有些唏嘘。&lt;/p&gt;
&lt;p&gt;事实上，我会很明显的感觉到这种时间的偏移，这使得我和其他人之间的距离越来越大。其实细致地算起来，去年一九年的时间中，我和父母在一起的时间也就只有可能一个多月，以往都是整年的陪伴相比，实在是少之又少。而伴随着之后的成长，我逐渐的将那些我熟悉的人落在了身后。因为要去实习，暑假可能也很难回到家里，又或者是，将来又是长期的科研工作，很难回家看一看。大四的时候也可能要去上海实习，而乐小姐则一个人在西安。&lt;/p&gt;
&lt;p&gt;我发现时间总是这样无情，将每个人和我的联系切断。这一周本来说要和同学聚一聚，虽然大家只是好朋友，也称不上非常非常亲密的兄弟关系，但我的好朋友已经不多了，在这样的语境下，依然没有时间和大家聚在一起。&lt;/p&gt;
&lt;p&gt;这不禁让我忍不住去多想，在这条漫长的路的尽头，我是会在鲜花的簇拥中迎来终点，还是只剩下我一个人。&lt;/p&gt;
&lt;h2&gt;科研与学习&lt;/h2&gt;
&lt;p&gt;这一周可以说又是科研常态化的一周，该写程序就写程序，然后平时稍微休息一下。主要还是因为在春节期间，在这种别人都在放松的时候的努力就显得格外显眼，也是受到了 mentor 的肯定，尽管这本身并没有什么实际的效益，但是还是令我十分开心。&lt;/p&gt;
&lt;p&gt;至于我的科研工作可能也要马上迎来一个逗号，这一篇内容说不定快要结束了，之后，要不然在这基础上去发展更多的新内容，要不然也可以探索一些别的领域，还是很令人兴奋的。&lt;/p&gt;
&lt;p&gt;不过需要注意的一点是，我依然很明显的发现我无法平衡写程序以及读论文的时间，甚至说绝大多数的时间我都在研究程序或者说技术上实现的细节，而不是在进行一些宏观上的思路的搭建，这对于长期的科研来说或许是不利的，虽然在当今的环境下，工程能力是非常重要的一环，但是做出一些有趣的工作也是我的追求之一，自然是不能一味地浪费时间在重复的工作上。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;因为发了压岁钱，而且本来也颇有积蓄，不记得有没有在周记或者其他地方提及过，事实上我有一个在北京读书的胞弟，与我同岁。这次回家自然也是带他出去逛了逛，他将来估计是要走出国的路线，而且不是科研向，现在基本上准备的已经差不多，平时倒还有不少休息的时间。&lt;/p&gt;
&lt;p&gt;按照和他在一块儿，我的观察来看，估计这个假期他又通关了六七款游戏，而讲实话，对于现在的我来说，完全没有打开任何一款游戏的勇气，实在是令人感慨。顺带我也送他了一款动物井，算是非常不错的游戏了。以及我也送了他一个高达，大概是将近六百块钱，也不算很贵。&lt;/p&gt;
&lt;p&gt;希望我珍惜的每个人都可以开心。&lt;/p&gt;
&lt;p&gt;除了这些事情之外，似乎倒也没有什么很多值得记录的东西，我确实入坑了 VRChat，以及熟悉了整体的流程，将来应该会在专门的博客中继续更新，认为这个游戏还是很神奇的，虽然我从中找到的并非是游玩的乐趣，而是一种类似于变装以及编程的乐趣。VRChat 可以给自己的角色添加插件，以及进行高度的自定义，这基本上就是拿程序可以重写自己的角色，在这个角度上看，称其一句「元宇宙」倒也不算完全夸张。不过相应的，其中的学问也是比较多的，估计还要慢慢去理解，这里面一方面包括 Unity，同时还有一些 VRChat 特有的一些特性，慢慢学吧。&lt;/p&gt;
&lt;p&gt;等到下周或许会总结一下这个假期的假期计划，按照我现在粗略去看，基本上一件也没有完成，但偏偏假期过得很充实，只说我的人生往往都是事与愿违的，但是希望没有时间被荒废。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-32-zh.webp"/><enclosure url="https://picr2.axi404.top/week-32-zh.webp"/></item><item><title>Isaac Sim 一百讲（4）：Transformation</title><link>https://axi404.github.io/blog/isaac-4</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-4</guid><description>从零开始的 Isaac Sim 之路，第一季开始！</description><pubDate>Sun, 02 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Isaac 101&apos;,
items: [
{
title: &apos;安装&apos;,
href: &apos;/blog/isaac-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;万物皆 Prim&apos;,
href: &apos;/blog/isaac-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;USD&apos;,
href: &apos;/blog/isaac-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Transformation&apos;,
href: &apos;/blog/isaac-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Rigid and Collision&apos;,
href: &apos;/blog/isaac-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Collision 进阶&apos;,
href: &apos;/blog/isaac-6&apos;,
order: &apos;6&apos;
},
{
title: &apos;Camera&apos;,
href: &apos;/blog/isaac-7&apos;,
order: &apos;7&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在上一章中，我们学习了如何创建一个 USD 并且将这个 USD 放到一个 Stage 中，这些操作使得我们可以成功在一个场景中引入我们的资产，然而这只是开始，事实上我们还需要更多。这一章中，我们将会回顾那些基础的知识，带领对这个领域一无所知的读者了解三维刚体的空间变化的最基础表示，并且使用 Isaac Sim 提供的封装来操作物体进行移动。&lt;/p&gt;
&lt;h2&gt;三维刚体的空间表示&lt;/h2&gt;
&lt;p&gt;让我们想象一个在空间中的长方形（因为圆形和正方形在空间中的旋转可能有歧义性，还是长方形好一些），作为这个世界中的全知者，你知道这个空间中的一切信息，但是你想要使用最少的信息来将这个物体每一个时刻的全部物理信息（即不包括颜色等渲染用到的光照信息）告诉我，请问应该怎么做？&lt;/p&gt;
&lt;p&gt;让我们给出一个假设，也是一个经典的假设，即，这个物体是一个刚体。换句话来说，这个物体在空间中不会因为任何的力的作用而产生任何的形变，这使得我们可以简单地将这个物体拆分成两个部分，一个是物体的形状信息，一个是物体的位置信息。&lt;/p&gt;
&lt;p&gt;事实上，前者在 Isaac Sim 里面通常以 Prim 进行表示，并且以 Mesh 的方式进行储存，即一些通过一系列的点，以及一些多元组表示的面组成的信息。&lt;/p&gt;
&lt;p&gt;作为一个例子，在大多数的 Mesh 中，面均以三角形存储，如一个 Mesh 可以被表示为 &lt;code&gt;[(0, 0, 0), (1, 1, 0), (1, 1, 1)]&lt;/code&gt; 以及 &lt;code&gt;[(0, 1, 2)]&lt;/code&gt; 来表示，前者表示这个 Mesh 的三个顶点在空间中的坐标，而后者则表示仅存在一个面，这个面由序号 0, 1, 2 的三个顶点组成。&lt;/p&gt;
&lt;p&gt;而后者则可以通过两个变量来组成，即 position 和 orientation。&lt;/p&gt;
&lt;h3&gt;平移&lt;/h3&gt;
&lt;p&gt;让我们思考最为简单的情况，一个物体在一个标准的右手坐标系下平移。我们可以任选物体上的任意一个点，作为物体的原点，换句话说，我们认为这个物体上的点始终代表着某个坐标系的 $(0, 0, 0)$ 的位置。在长方形的例子中，我们不妨选择长方形的质心，同时任选两个方向作为坐标系的 X, Y 轴方向，以获得进一步的 XYZ 轴信息，在这里不妨让 X 轴与长方形的长边平行，而 Y 轴与宽边平行，从而不难求出 Z 应该与高边平行。我们称这个坐标系为物体坐标系。&lt;/p&gt;
&lt;p&gt;因为已知这个物体是刚体，所以说这个物体不会发生任何的形变，在物体平移的过程中，随着物体向右 1m，物体上的每一个点都会向右 1m，这其中当然也包括物体坐标系的原点。而最有趣的点在于，这一切反过来的时候，我们知道物体坐标系的原点向右了 1m，我们就知道了物体上的每一个点都向右了 1m，这件事情是等价的，而因为我们已经事先知道了在向右 1m 之前的物体的全部几何形状（即 Mesh 信息），我们也就知道了其初始的每一个点的信息，那么求出向右 1m 后的每一个点的信息自然也就不难了。&lt;/p&gt;
&lt;p&gt;因此我们发现，物体的平移，完全可以通过物体坐标系的原点的坐标平滑来表示，或者说，物体的平移与物体坐标系的平移包含相同的信息，从表达的含义上来说它们是等价的。&lt;/p&gt;
&lt;h3&gt;旋转&lt;/h3&gt;
&lt;p&gt;通过相似的思路，我们不难得到物体的旋转和物体坐标系的旋转的等价关系，比如说物体绕 Z 轴顺时针旋转了 45 度，其长边和短边也就旋转了 45 度，则 XY 轴旋转了 45 度，反过来亦然。这个显然的道理在这里不再赘述，假如有读者无法理解，可以留言，将来我会进行更多的阐释。&lt;/p&gt;
&lt;p&gt;事实上在旋转这一话题上，我们要讨论的是另一种内容，即旋转的表示方式。平移是很好理解的，其本身就是对于点的加减法，然而旋转更多地是一种数学上的表述，其存在直观的表示方式，以及数学的运算方式，而这二者并不是同一种表示。&lt;/p&gt;
&lt;p&gt;假设我们表述一种混乱的变换，某个物体的旋转是这样的过程：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in range(1000):
    axis = random.choice([&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;])
    angle = random.randint(0,359)
    print(f&quot;物体绕 {axis} 轴旋转了 {angle} 度&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这意味着一种最为简单粗暴的方式是，我们记录下来这一千次的轴以及度数，并且组成一个 List，即 &lt;code&gt;[{&quot;axis&quot;: str, &quot;angle&quot;: int/float}]&lt;/code&gt;，这是一种最为基础的表述，然而并不足够简洁。&lt;/p&gt;
&lt;p&gt;假如读者尚且对于线性代数的知识有所印象，在线性代数中讲解过旋转矩阵的概念，即某一点 $\begin{bmatrix}x \y\end{bmatrix}$，可以使用 $\begin{bmatrix}x \y\end{bmatrix}^\prime=\begin{bmatrix}\cos{\theta} &amp;#x26; -\sin{\theta}\\sin{\theta}&amp;#x26;\cos{\theta}\end{bmatrix}$ 来计算其绕原点逆时针旋转（在旋转中，除非特殊说明，否则我们认为逆时针为正方向，不在阐述）$\theta$度之后的坐标，而假如将其换到三维坐标系下，岂不就是绕着 Z 轴进行旋转？&lt;/p&gt;
&lt;p&gt;因此我们不难给出绕 Z 轴旋转的角度的旋转矩阵，即 $\mathbf{R}_z (\alpha) = \begin{bmatrix}\cos\alpha &amp;#x26; -\sin\alpha &amp;#x26; 0 \\sin\alpha &amp;#x26; \cos\alpha &amp;#x26; 0 \0 &amp;#x26; 0 &amp;#x26; 1\end{bmatrix}$，而这个角度我们通常可以记为翻滚角 roll。同理我们不难给出绕 XY 轴的旋转对应的旋转矩阵，即$\mathbf{R}_x (\gamma) = \begin{bmatrix}1 &amp;#x26; 0 &amp;#x26; 0 \0 &amp;#x26; \cos\gamma &amp;#x26; -\sin\gamma \0 &amp;#x26; \sin\gamma &amp;#x26; \cos\gamma\end{bmatrix}$ 以及 $\mathbf{R}_y (\beta) = \begin{bmatrix}\cos\beta &amp;#x26; 0 &amp;#x26; \sin\beta \0 &amp;#x26; 1 &amp;#x26; 0 \-\sin\beta &amp;#x26; 0 &amp;#x26; \cos\beta\end{bmatrix}$，这两个角度记为俯仰角 Pitch 以及偏航角 Yaw。&lt;/p&gt;
&lt;p&gt;因此我们不难得到，我们上述提到的一系列旋转，最终可以用一系列的$3\times 3$的矩阵的连续乘积，即最后得到一个$3\times 3$的矩阵表示，这个矩阵可以表示任意的旋转，称之为旋转矩阵 $R$。关于旋转矩阵的若干性质不过多介绍。&lt;/p&gt;
&lt;p&gt;然而旋转矩阵具有过于冗余的参数，实际上旋转只有三个自由度（绕 XYZ 轴旋转），而旋转矩阵使用了 9 个参数。其他的角度的表示还包括欧拉角（具有万向锁）、轴角（自由度冗余），以及最为合理且广泛应用的角度表示，四元数。无它，四元数支持更加合理的插值。关于这些旋转的表示在此不进行过度的介绍，或许将来关于计算机视觉或者机器人学的内容会专门开一个篇章进行详细的介绍。&lt;/p&gt;
&lt;p&gt;四元数，即 $\begin{bmatrix}w&amp;#x26;x&amp;#x26;y&amp;#x26;z\end{bmatrix}$，也可以表示为$\begin{bmatrix}x&amp;#x26;y&amp;#x26;z&amp;#x26;w\end{bmatrix}$，前者的表示被称为 scalar-first，而后者则是 scalar-last，这两种方式均可以，因为只是排列的顺序改变，在数学含义上均为 $\mathbf{q}=w+xi+yj+zk$，然而在编程中需要注意这二者的区别，毕竟其使用一个维度为 4 的 &lt;code&gt;np.darray&lt;/code&gt; 表示，假如传参错误，就很可能导致错误的结果。&lt;/p&gt;
&lt;h3&gt;简要介绍 scipy&lt;/h3&gt;
&lt;p&gt;在这里简要介绍一下 scipy，scipy 是 python 的一个科学计算库，我们在编程的过程中会使用其中的 &lt;code&gt;R&lt;/code&gt;，即 &lt;code&gt;from scipy.spatial.transform import Rotation as R&lt;/code&gt; 来进行角度的运算。其语法十分简单，一个是 &lt;code&gt;from_xxx&lt;/code&gt;，以及另一个是 &lt;code&gt;as_xxx&lt;/code&gt;，如我们想要把一个旋转矩阵变量 &lt;code&gt;matrix&lt;/code&gt; 转为四元数，我们可以直接使用 &lt;code&gt;R.from_matrix(matrix).as_quat()&lt;/code&gt; 来获得。&lt;/p&gt;
&lt;p&gt;类似的语法的一个示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
from scipy.spatial.transform import Rotation as R

quaternion = [0, 0, np.sin(np.pi/4), np.cos(np.pi/4)]
# 使用四元数创建一个 Rotation 对象
rotation = R.from_quat(quaternion)
# 获取欧拉角（以弧度为单位）
euler_angles = rotation.as_euler(&apos;xyz&apos;, degrees=False)
# 获取欧拉角（以度数为单位）
euler_angles_degrees = rotation.as_euler(&apos;xyz&apos;, degrees=True)
# 获取旋转矩阵
rotation_matrix = rotation.as_matrix()
# 获取旋转向量
rotation_vector = rotation.as_rotvec()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不难发现，其中 euler，即欧拉角，可以选择轴的顺序（XYZ 即输入的长度为 3 的 &lt;code&gt;np.darray&lt;/code&gt; 依次为 XYZ 轴旋转的角度，且按照 XYZ 的顺序进行旋转），而 degrees 则表示是否使用角度，其他的都不难看懂。&lt;/p&gt;
&lt;p&gt;需要十分注意的一点是，scipy 的四元数默认使用的是 scalar_last 的表示，而这与绝大多数的 robotics 相关的库使用的 scalar_first 相悖，一些版本的 scipy 传参支持 &lt;code&gt;scalar_first=True&lt;/code&gt; 的传参，然而为了兼容性，我们推荐使用 &lt;code&gt;[[1, 2, 3, 0]]&lt;/code&gt; 以及 &lt;code&gt;[[3,  0, 1, 2]]&lt;/code&gt; 的方式进行转换。这一示例将在后续给出。&lt;/p&gt;
&lt;h2&gt;World Frame 与 Local Frame&lt;/h2&gt;
&lt;p&gt;通过上述的内容，相信读者事实上已经具备了关于坐标系的概念，并且任何一个坐标系都可以通过关于另一个坐标系的平移和旋转来表示。事实上这种通过相对关系来进行的表示并不方便，我们不希望坐标系依赖于某个物体，而是存在一个绝对的基准，我们只维护不同坐标系相对于这个绝对的基准的变换关系，而当我们想要求任意两个坐标系之间的变换，我们则使用这个基准来进行中转，使用 $R_1^T\cdot R_2$ 来求出两个坐标系之间的变换。&lt;/p&gt;
&lt;p&gt;这种想法是合理的，因此我们给出定义，World Frame，译为世界坐标系，是某个绝对的静止的坐标系（当然，假如一切发生在一辆行驶的车上，车的地板的某一处也可以是世界坐标系的原点，毕竟静止是相对的）。在 Isaac Sim 中，我们认为 Root，即 &lt;code&gt;get_prim_at_path(&quot;/&quot;)&lt;/code&gt; 所获得的 Prim 表示的坐标系为世界坐标系。&lt;/p&gt;
&lt;p&gt;同时，我们也存在另一种需求，诸如我们想要将一个 Camera 和 Franka hand 进行固连，即让 Camera 随着 Franka hand 而运动，似乎我们只能定义一个 Camera 相对于 Franka hand 的变换，从而使得这个变换为固定值。在绝大多数的三维资产管理中，事实上我们均使用 Local Frame 下的变换来维护资产，即某个 node 相较于其 parent 的变换，因此在 Prim 的树中来看，我们只需要将 Camera 放置在 Franka hand 的 prim_path 下方，就可以确保 franka hand 在变换的过程中使得 Camera 一起变换。&lt;/p&gt;
&lt;p&gt;因此在计算 World Frame 的变换的过程中，事实上是通过链式进行传播，将若干 Local Frame 的变换组合在一起而得到的。&lt;/p&gt;
&lt;h2&gt;Isaac Sim 的 API&lt;/h2&gt;
&lt;p&gt;事实上，我们不难想起在第三讲中介绍的，Isaac Sim 关于 Prim 存在一个管理平移和旋转的 attribute，其中保存的正是 Prim 在 Local Frame 下的变换，然而我们在这里先不进行这一部分的过多展开，本篇内容在一开始已经加入了让没有接触的读者感到比较硬核的内容了，因此这一章中的编程部分会尽可能地简化，而不是继续复杂下去。&lt;/p&gt;
&lt;p&gt;毕竟事实上，假如说我们时时刻刻都是使用 Prim 的 attribute 进行操作，也就成为了彻底在使用 OpenUSD 进行编程了，Isaac Sim 在提供了仿真平台的同时，大概还是给出了一些比较不错的接口的，即之前事实上提到过的，Isaac Sim XFormPrim。在这里给出程序示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from omni.isaac.core.prims import XFormPrim
from scipy.spatial.transform import Rotation as R

object_xform = XFormPrim(&quot;/World/object&quot;)
position, orientation = object_xform.get_world_pose()
position[2] += 2.0
orientation_R = R.from_quat(orientation[[1, 2, 3, 0]]) * R.from_euler(&quot;xyz&quot;, np.array([90,0,0]), degrees=True)
orientation = orientation_R.as_quat()[[3, 0, 1, 2]]
object_xform.set_world_pose(position=position, orientation=orientation)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不难理解，我们可以通过 &lt;code&gt;get_world_pose()&lt;/code&gt; 来获得 &lt;code&gt;position&lt;/code&gt; 以及 &lt;code&gt;orientation&lt;/code&gt;，而同时可以通过 &lt;code&gt;set_world_pose()&lt;/code&gt; 来设置。值得一提的是，同样的 API 包括 &lt;code&gt;set/get_local_pose()&lt;/code&gt;，其用法相同，而 &lt;code&gt;set_local_pose()&lt;/code&gt; 的传参为 &lt;code&gt;translation&lt;/code&gt; 以及 &lt;code&gt;orientation&lt;/code&gt; 而非 &lt;code&gt;position&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;同时，需要注意的一点在于，事实上 set_world_pose 是通过改变 local frame 下的变换以实现的，毕竟如果改变 parent 的 local frame 的变换，会影响其他 prim，而如何只改变其 child，则需要一个个都施加相同的变换，不如直接一步修改 local frame。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;本讲内容主要普及介绍了三维刚体变换相关的内容，并且讲解了 Isaac Sim 中如何使用 XFormPrim 来进行变换。作为一个实践，读者可以使用第三讲中的素材尝试改变其世界系下的坐标，而进一步的拓展，读者可以思考，如何使用 OpenUSD 的 API 求出任意两个 Prim 之间的变换。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/isaac-4-zh.webp"/><enclosure url="https://picr2.axi404.top/isaac-4-zh.webp"/></item><item><title>周记 Week31</title><link>https://axi404.github.io/blog/week-31</link><guid isPermaLink="true">https://axi404.github.io/blog/week-31</guid><description>2025-01-27 ~ 2025-02-02.</description><pubDate>Sun, 02 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;首先，祝大家新年快乐！又是一年春节，遥想去年，还是有一些唏嘘。当时那个依然前途一片未知的我正忙于在偷得一日闲与专心科研之间摇摆，在电脑间度过了一个惬意的假期，然而这一次，我依然在电脑间，却已经任务在身了。&lt;/p&gt;
&lt;h2&gt;科研与学习&lt;/h2&gt;
&lt;p&gt;书接上文，CVPR 的 rebuttal 还在继续，导致这一周我几乎还是将全部的时间给了科研。但是在此基础之上，依然有更多的内容需要我去做。因为现在我的手头多出了大量的计算资源，自然是不允许这些资源空转的，正好后续的项目也需要一些支持，因此又开始准备其之后的内容。&lt;/p&gt;
&lt;p&gt;同时，数学建模还是要打完的，一方面是确实答应了乐小姐，一方面，这一次的题目还是大有可为。事实证明，根据我的感觉，C 题依然是角逐激烈的题目，而 D 题因为难度较高，应该比较好打。同时，自然，我们也给出了非常不错的呈现，希望可以有比较好的结果。不过值得一提的是，我发现一个难绷的现象依然存在，即，本次比赛我是和乐小姐以及我们年级的年级第一一起组队，乐小姐我直接分配了一些找资料的工作，而年级第一则负责和我一起攻克难关。不过到了最后，依然是我做完了接近 90% 的工作。虽然需要注明，年级第一的实力非常非常厉害，然而不知道是因为我的能力问题导致往往自己做起来了事情就难以协调别人的事情，还是说其他原因，感觉每一次和其他人的小组合作总是这样子的结局。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;生活方面，因为过年，所以还是有很多事情可以做。主要的事情是，因为发了压岁钱，所以打算入坑一下 VR。实际上一开始只是因为想要一些生产力的需求，比如说使用 Vision Pro 之类的，但是在忍痛消费降级购买了 Quest3 之后，发现了可以用 Quest3 来玩 VRChat。&lt;/p&gt;
&lt;p&gt;VRChat 可以使用 Unity 来构建自己的模型，还是十分有趣的。不过暂时还是只能先买 VR，具体彻底入门之后还需要动捕设备，在家里没有地方放置，需要等去了上海之后再说了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-31-zh.webp"/><enclosure url="https://picr2.axi404.top/week-31-zh.webp"/></item><item><title>周记 Week30</title><link>https://axi404.github.io/blog/week-30</link><guid isPermaLink="true">https://axi404.github.io/blog/week-30</guid><description>2025-01-20 ~ 2025-01-26.</description><pubDate>Thu, 30 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;要用什么去形容这刚刚经过的一周呢，可以说主旋律大概是熬夜和折磨。尽管按理来说，现在应该是皆大欢喜的时刻，已经回家，然后狠狠躺在床上睡大觉，起来之后百无聊赖地打打游戏，看看电影，或者去外面走走，总之是放松的。然而事实上正相反。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;一方面，CVPR 已经出了分数，可以说分数还算可以，5333，然而不得不说的是，这可以说是我史无前例地获得了四个审稿人的回复，实在是回复的压力很大，毕竟 CVPR 只限制一页的回复，而且要求了一堆的实验。我的论文的方向以及 scope 使得其确实有阐述 scaling 相关内容的必要，这部分在原文的时候效果不是特别好，而现在在进行优化之后，效果上来了，并有了很多质量很高的数据，所以说进行了相关的实验。&lt;/p&gt;
&lt;p&gt;而且事实上不只是 CVPR，我的这个项目组成了一个巨大的库，从而要为后续的项目提供一些支持，因此甚至在春节期间也在持续进行写代码，来争取为后续的内容赶一些进度。&lt;/p&gt;
&lt;h2&gt;课内&lt;/h2&gt;
&lt;p&gt;在科研之余，课内也久违地迎来了一些需要做的事情，也就是美赛。尽管相较于其他的比赛，美赛实际上被称为画图大赛，只需要追求呈现的效果，然而不得不说的是，因为和队友之间的配合的问题，事实上这个队伍里面我依然承担了接近 90% 的工作量。为了不影响科研的进展，甚至这大量的 90% 工作量都需要在凌晨开始进行。&lt;/p&gt;
&lt;p&gt;我记不清我是否有相关的表述了，但是反正，我实际上全神贯注做一件事情的话，效率是极高的，基本上几个小时可以完成正常同学一天的工作量，但是同样会很耗费我的精神，这种精神上的疲惫是很难受的，而我要保持这种状态，既科研，又课内，实在是很折磨。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;除此之外，就是快要到了春节了，于是和家里人要一起回姥姥家。天可怜见，即使是回到姥姥家那边，我依然要进行科研，甚至要进行一些实验，而别人可能还在沉浸在春节的氛围中。&lt;/p&gt;
&lt;p&gt;呜呼哀哉，不过还是开开心心地向前看吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-30-zh.webp"/><enclosure url="https://picr2.axi404.top/week-30-zh.webp"/></item><item><title>周记 Week29</title><link>https://axi404.github.io/blog/week-29</link><guid isPermaLink="true">https://axi404.github.io/blog/week-29</guid><description>2024-01-13 ~ 2025-01-19.</description><pubDate>Fri, 24 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说，这其实又是姗姗来迟的一周的周记，只能说这种忙碌实在是太常见了，所以说倒也没有什么意外。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;这一周总的来说也没有发生特别多的事，主要就是在科研之中度过。因为之前给的一些交付压力，所以说任务还是很重的，也需要抓紧去做完，自然又是没日没夜的干活，只能说痛并快乐着吧。对目前阶段的我来说，研究技术以及写程序还是能给我带来很大的成就感的，而且陷入其中之后的那种沉浸的心流体验，只能说还是很不错的。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;这一周算是考试结束之后的一周，乐小姐那边也没有很多的事情需要做，和她一起开开心心地吃了饭，然后收拾收拾东西，送她去车站。只能说人的心态还是会变化的，和乐小姐在一起的时候总是开心且忙碌的，然而刚一分别不到一个小时，就感到生命中仿佛缺失了什么，迫切地想要且期待再次相见。在这边收拾了一下行李，之前为了考试租房，乐小姐的不少行李都留在了这边，所以说需要收拾一下，然后打包暂存在我的宿舍。同时在乐小姐的许可下，用她的 Lo 裙（后来那一件被送给了我）拍了一些照片，感觉还是比较可爱的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/1.9rjhqwq0nd.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于据说 Lo 圈大家还是对于男性女装不是很通融，所以我穿过的衣服就自己留着了，就不再污染市场了。不过 Lo 圈有一套很神奇的市场，大家彼此高强度买卖裙子，甚至有点像是交换裙子，很有趣。&lt;/p&gt;
&lt;p&gt;值得一提的是，搬运乐小姐的行李的时候，来来回回搬了好多大包，巨重无比，最后胳膊酸痛了好几天。&lt;/p&gt;
&lt;h2&gt;回家&lt;/h2&gt;
&lt;p&gt;最后在这周六，也是成功回家，回到温暖的家。清算了一批债务，把之前去上海实习的时候欠下的钱还了，也算是了却了一桩心事。总的来说，这周还是在忙碌中度过的，尽管没有重点提及，但是不可忽略的是每天都会高强度的进行科研，也收获了许多。尤其是最近和我合作的同学貌似都是正经的准博士生，一方面因为自己受到重视而感到欣喜，而另一方面，也感到自己需要更加努力，才能不辜负大家的期待。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-29-zh.webp"/><enclosure url="https://picr2.axi404.top/week-29-zh.webp"/></item><item><title>Isaac Sim 一百讲（3）：USD</title><link>https://axi404.github.io/blog/isaac-3</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-3</guid><description>从零开始的 Isaac Sim 之路，第一季开始！</description><pubDate>Sat, 18 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Isaac 101&apos;,
items: [
{
title: &apos;安装&apos;,
href: &apos;/blog/isaac-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;万物皆 Prim&apos;,
href: &apos;/blog/isaac-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;USD&apos;,
href: &apos;/blog/isaac-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Transformation&apos;,
href: &apos;/blog/isaac-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Rigid and Collision&apos;,
href: &apos;/blog/isaac-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Collision 进阶&apos;,
href: &apos;/blog/isaac-6&apos;,
order: &apos;6&apos;
},
{
title: &apos;Camera&apos;,
href: &apos;/blog/isaac-7&apos;,
order: &apos;7&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在上一讲中，我们介绍了 Prim 的概念，Isaac Sim 中的一切资产被通过 Prim 以梳状的结构组织起来，并且通过 API 来完成其他属性的赋予。作为新手使用 Isaac Sim 的时候，读者可能常常在创建诸如 Franka 的时候陷入长时间的卡顿，这种由于请求 Isaac Sim 服务器中的资产而陷入的卡顿事实上是一种多余的开销，那么一种想法很自然会出现，如何通过一种更加本地化的方法来管理我的全部资产？USD 是这一切的答案。&lt;/p&gt;
&lt;h2&gt;USD&lt;/h2&gt;
&lt;p&gt;如何理解 USD，可以理解为这就是 Isaac Sim 文件的保存形式，USD 文件和诸如 &lt;code&gt;.obj&lt;/code&gt; 或者 &lt;code&gt;.glb&lt;/code&gt; 等内容相似，只不过其管理了更多的内容，包括光照、物理等属性，换句话说，事实上 USD 里保存的就是我们上文提到的 Prim 的结构以及全部的属性信息。&lt;/p&gt;
&lt;p&gt;当我们默认创建 Franka 的时候，事实上就是在向服务器请求一个 Franka 的模型文件，以 USD 的形式，并且添加到 stage 中。其中 USD 文件包含二进制以及文字形式的全部信息，USDC 则是二进制形式，USDA 为文本的形式，USDZ 则管理一个压缩包。值得一提的是，在 USD 的家族中，事实上只有 Blender 才可以创建 USDZ 文件，而 Isaac Sim 则只能读取，却只能保存为 USDA/C/_ 的格式，这意味着其中的贴图文件使用默认的绝对路径进行管理，在传输给其他人的时候，也就很容易贴图丢失，打开的内容只剩白模。&lt;/p&gt;
&lt;h2&gt;GLB To USD&lt;/h2&gt;
&lt;p&gt;在读者了解 USD 之前，可能更多听说过的还是以 GLB 为典型的模型资产，而这更主要的原因其实在于，以 Objaverse 为首的大规模模型库都是以 &lt;code&gt;.glb&lt;/code&gt; 的形式进行保存的。直接通过某种传输的方式，让读者下载本文中提及的模型文件，并且进行进一步的处理（如转为 USD 并添加到场景中）是一件不现实的事情。一方面我们在后续可能会用到很多的资产，而另一方面，我们也很难每次都单独提供这些文件，于是授人以鱼不如授人以渔，我们将首先详解如何获得这些数字资产，并且转换为 USD 文件。&lt;/p&gt;
&lt;h3&gt;Objaverse&lt;/h3&gt;
&lt;p&gt;Objaverse 是一个包含了 800K 模型资产的 dataset，而更进一步，其升级版本 Objaverse-XL 则包含了 10M 的模型资产，这些内容事实上包含大量的脏资产，如质量较低的扫描物体，或者不常出现在现实中的物体及虚拟物体。清洗的部分并非我们的讲解范围，而部分的其他工作可能公布一些 Objaverse 的 subset，这些 subset 的质量更高，并且可能被加上了更多的标注。&lt;/p&gt;
&lt;p&gt;首先可以安装 objaverse：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install objaverse
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;并且可以通过以下的代码获得一个模型的路径：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import objaverse
print(objaverse.__version__)
uids = objaverse.load_uids()
print(len(uids))
annotations = objaverse.load_annotations(uids)
random_object_uids = random.sample(uids, 10)
import multiprocessing
processes = multiprocessing.cpu_count()
objects = objaverse.load_objects(
    uids=random_object_uids,
    download_processes=processes
)
print(objects)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;你可以注意到 &lt;code&gt;objects&lt;/code&gt; 是一个字典，其中的键是 &lt;code&gt;uid&lt;/code&gt;，而值则是对应的模型的路径，你可以在这个路径下面获得这个模型对应的 &lt;code&gt;glb&lt;/code&gt; 文件，或者直接使用&lt;code&gt;shutil&lt;/code&gt;来拷贝：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import shutil
os.makedirs(&apos;models&apos;, exist_ok=True)
for uid, path in objects.items():
    os.makedirs(f&apos;models/{uid}&apos;, exist_ok=True)
    shutil.copy(path, f&apos;models/{uid}.glb&apos;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;转换脚本&lt;/h3&gt;
&lt;p&gt;接下来就是转换的脚本了，这个脚本本身是十分复杂的，但是好在 Isaac Sim 已经给出了实践。你需要安装程序版本的 Isaac Sim，&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd /home/$USER/.local/share/ov/pkg/isaac-sim-4.1.0/standalone_examples/api/omni.kit.asset_converter/
vim asset_usd_converter.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;你就可以看到这个程序的基本实现的版本了，直接指定目录之后执行，即可获得 USD 文件了。在这里同样给出一个我自己稍微改了一些内容的版本，具体就是多了一个传参而已：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import argparse
import asyncio
from isaacsim import SimulationApp
import os
from pathlib import Path
from tqdm import tqdm

async def convert(in_file, out_file, load_materials=False):
    import omni.kit.asset_converter

    def progress_callback(progress, total_steps):
        pass

    converter_context = omni.kit.asset_converter.AssetConverterContext()
    converter_context.ignore_materials = not load_materials
    converter_context.use_meter_as_world_unit = True
    instance = omni.kit.asset_converter.get_instance()
    task = instance.create_converter_task(
        in_file, out_file, progress_callback, converter_context
    )
    success = True
    while True:
        success = await task.wait_until_finished()
        if not success:
            await asyncio.sleep(0.1)
        else:
            break
    return success


def asset_convert(args):
    supported_file_formats = [&quot;glb&quot;, &quot;obj&quot;, &quot;fbx&quot;]
    for folder in args.folders:
        local_asset_output = folder + f&quot;/../{args.dist_folder}&quot;
        result = omni.client.create_folder(f&quot;{local_asset_output}&quot;)
    for folder in args.folders:
        print(f&quot;\nConverting folder {folder}...&quot;)
        (result, models) = omni.client.list(folder)
        for i, entry in tqdm(enumerate(models)):
            if i &gt;= args.max_models:
                print(f&quot;max models ({args.max_models}) reached, exiting conversion&quot;)
                break
            model = str(entry.relative_path)
            model_name = os.path.splitext(model)[0]
            model_format = (os.path.splitext(model)[1])[1:]
            if model_format in supported_file_formats:
                input_model_path = folder + &quot;/&quot; + model
                converted_model_path = folder + f&quot;/../{args.dist_folder}/&quot; + model_name + &quot;.usd&quot;
                if not os.path.exists(converted_model_path):
                    status = asyncio.get_event_loop().run_until_complete(
                        convert(input_model_path, converted_model_path, True)
                    )
                    if not status:
                        print(f&quot;ERROR Status is {status}&quot;)
                    print(f&quot;---Added {converted_model_path}&quot;)


if __name__ == &quot;__main__&quot;:
    kit = SimulationApp()
    import omni
    from omni.isaac.core.utils.extensions import enable_extension
    enable_extension(&quot;omni.kit.asset_converter&quot;)
    parser = argparse.ArgumentParser(&quot;Convert GLB assets to USD&quot;)
    parser.add_argument(
        &quot;--folders&quot;,
        type=str,
        nargs=&quot;+&quot;,
        default=None,
        help=&quot;List of folders to convert (space seperated).&quot;,
    )
    parser.add_argument(
        &quot;--max-models&quot;,
        type=int,
        default=50,
        help=&quot;If specified, convert up to `max-models` per folder.&quot;,
    )
    parser.add_argument(
        &quot;--load-materials&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If specified, materials will be loaded from meshes&quot;,
    )
    parser.add_argument(
        &quot;--dist-folder&quot;,
        type=str,
        default=&quot;usd&quot;,
        help=&quot;If specified, converted assets will be placed in this folder.&quot;,
    )
    args, unknown_args = parser.parse_known_args()
    dist_folder = Path(args.dist_folder)
    dist_folder.mkdir(parents=True, exist_ok=True)
    if args.folders is not None:
        asset_convert(args)
    else:
        print(f&quot;No folders specified via --folders argument, exiting&quot;)
    kit.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在使用的时候可以执行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python mesh2usd.py --folders /mnt/data/assets/objaverse-glb --max-models 1000000 --load-materials --dist-folder objaverse-usd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里面，就可以创建一个 &lt;code&gt;/mnt/data/assets/objaverse-usd&lt;/code&gt; 的文件夹，里面包含所有的 USD 文件了。&lt;/p&gt;
&lt;h2&gt;添加 USD&lt;/h2&gt;
&lt;p&gt;在获得了 USD 之后，我们就可以通过某种方式将这些 USD 文件添加到 Isaac Sim 中，这样我们就不需要再使用一些 CUBE 来作为一个 Demo 的示例了，而是可以真正使用这些模型。&lt;/p&gt;
&lt;p&gt;在这里，读者需要回忆一下上一章的内容，我们现在已经有了一些 USD 了，我们想要把他们添加到场景里面，这里面自然也就包含了第一个关键要素，也就是，什么是场景。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在 Isaac Sim 中，每一次的程序运行会打开某种实例，从意义的角度来说，可以称之为 Scene，也就是一个场景，而从定义上来说，我们剋称之为一个 Stage。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因此，我们需要把物体注册为一个 prim 并且添加到 Stage 中，也就是以下的代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from omni.isaac.core.prims import XFormPrim
from omni.isaac.core.utils.stage import add_reference_to_stage

def add_usd_to_world(
    asset_path: str,
    prim_path: str,
    name: str,
    translation: Optional[Sequence[float]] = None,
    orientation: Optional[Sequence[float]] = None,
    scale: Optional[Sequence[float]] = None,
):
    reference = add_reference_to_stage(usd_path=asset_path, prim_path=prim_path)
    prim_path = str(reference.GetPrimPath())
    prim = XFormPrim(
        prim_path,
        name=name,
        translation=translation,
        orientation=orientation,
        scale=scale,
    )
    usd_prim = prim.prim
    if not usd_prim.IsValid():
        print(f&quot;Prim at path {prim_path} is not valid.&quot;)
        return prim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这里，我们首先通过 &lt;code&gt;add_reference_to_stage&lt;/code&gt; 来将 USD 添加到 Stage 中，这个函数是 Isaac Sim 的官方函数，直接 &lt;code&gt;import&lt;/code&gt; 就可以使用。通过输入 USD 的路径，以及指定添加到 Stage 中的 Prim Path，你就可以得到一个物体了。然而需要注意的是，虽然说现在这个物体已经被顺利地添加进来了，但是依然有很多不是很方便的地方，其中比较典型的就是我难以快捷地获得物体的 position 等信息。&lt;/p&gt;
&lt;p&gt;事实上，根据之前我们学到的内容，假如说这个物体包含了一个位置信息，那么这个位置信息一定是被保存为了这个 Prim 的一个 Attribute，那么我们就可以通过以下的方式来获得这个位置信息：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;prim.GetAttribute(&quot;xformOp:translate&quot;).Get()
prim.GetAttribute(&quot;xformOp:orient&quot;).Get()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是这种方法看上去就太繁琐了，本质是在直接和 OpenUSD 的 API 进行交互，而跳过了 Isaac Sim 这一中间层。Isaac Sim 事实上通过一系列的封装，让我们可以更加方便地获得这些信息，并且可以更加方便地进行修改，也就是使用 &lt;code&gt;XFormPrim&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;就像之前提及的一样，&lt;code&gt;XFormPrim&lt;/code&gt; 是 Isaac Sim 中对于物体类型的 Prim 的封装，你可以通过 &lt;code&gt;XFormPrim&lt;/code&gt; 来便捷地进行一些关于物体的操作，比如说平移旋转或者缩放。可以注意到上述的代码中，一共包括若干传参：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;prim_path: str,
name: str,
translation: Optional[Sequence[float]] = None,
orientation: Optional[Sequence[float]] = None,
scale: Optional[Sequence[float]] = None,
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;code&gt;prim_path&lt;/code&gt; 是这个物体在 Stage 中的路径，这个不难理解；&lt;code&gt;name&lt;/code&gt; 是这个物体在 Stage 中的名称，和 &lt;code&gt;prim_path.split(&quot;/&quot;)[-1]&lt;/code&gt; 保持一致即可；&lt;code&gt;translation&lt;/code&gt; 是这个物体在 World Frame 中的位置，&lt;code&gt;orientation&lt;/code&gt; 是这个物体在 World Frame 中的旋转，&lt;code&gt;scale&lt;/code&gt; 是这个物体相较于原始模型的大小的缩放关系。&lt;/p&gt;
&lt;p&gt;其中需要注意的内容是，众所周知四元数是一种表示旋转的方法，而且可以说是目前最常用的表示旋转的方法，Isaac Sim 中的全部旋转也都是用这种方式。四元数相较于欧拉角，可以避免万向锁的问题，并且可以更加方便地进行插值。但是事实上这种表示也分为两种储存的形式，也就是 &lt;code&gt;scalar_first&lt;/code&gt; 和 &lt;code&gt;vector_first&lt;/code&gt;，在 Isaac Sim 中，默认使用的是 &lt;code&gt;scalar_first&lt;/code&gt; 的形式，也就是 &lt;code&gt;[w, x, y, z]&lt;/code&gt; 的形式，而我们最常使用的旋转库，&lt;code&gt;scipy.spatial.transform.Rotation&lt;/code&gt; 默认使用的是 &lt;code&gt;vector_first&lt;/code&gt; 的形式，也就是 &lt;code&gt;[x, y, z, w]&lt;/code&gt; 的形式。&lt;/p&gt;
&lt;p&gt;关于 &lt;code&gt;XFormPrim&lt;/code&gt; 的更多内容，我们将在后续单独详细介绍。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;在这一讲中，我们介绍了 USD 的概念，并且介绍了如何通过 objaverse 来获得模型，并且转换为 USD 文件，最后介绍了如何将 USD 文件添加到 Isaac Sim 中，并且进行一些操作。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/isaac-3-zh.webp"/><enclosure url="https://picr2.axi404.top/isaac-3-zh.webp"/></item><item><title>Isaac Sim 一百讲（1）：安装</title><link>https://axi404.github.io/blog/isaac-1</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-1</guid><description>从零开始的 Isaac Sim 之路，第一季开始！</description><pubDate>Sat, 18 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Isaac 101&apos;,
items: [
{
title: &apos;安装&apos;,
href: &apos;/blog/isaac-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;万物皆 Prim&apos;,
href: &apos;/blog/isaac-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;USD&apos;,
href: &apos;/blog/isaac-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Transformation&apos;,
href: &apos;/blog/isaac-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Rigid and Collision&apos;,
href: &apos;/blog/isaac-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Collision 进阶&apos;,
href: &apos;/blog/isaac-6&apos;,
order: &apos;6&apos;
},
{
title: &apos;Camera&apos;,
href: &apos;/blog/isaac-7&apos;,
order: &apos;7&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;事实上在此之前已经写过一次相关的教程了，详情可以见 &lt;a href=&quot;isaac-sim-notes&quot;&gt;之前的教程&lt;/a&gt;，但是之前的教程实际上只是写了一下 Isaac Sim 正常的安装方法，并且对官方教程中的一个示例进行了讲解，但是事实上这个示例并不是足够有趣的，甚至和我们实际的实践也没有太大的关系。&lt;/p&gt;
&lt;p&gt;在经历了漫长的实践之后，我发现 Isaac Sim 实际的运行逻辑，以及如何去理解的逻辑，这种逻辑使得我们不应该轻易地直接尝试运行官方的教程，因为这并不能让我们清晰地知道这个框架的全貌。因此我决定开始一系列的新博客，来讲解 Isaac Sim 的 Python API 的使用，也大概是因为将来我的工作流程里面会大量地掺杂着 Isaac Sim，因此对于其他的新手来说，一个友好的教程可以提供更加丰富的信息，而且效率更高。&lt;/p&gt;
&lt;h2&gt;安装 Isaac Sim&lt;/h2&gt;
&lt;p&gt;在 &lt;a href=&quot;isaac-sim-notes&quot;&gt;之前的教程&lt;/a&gt; 中事实上已经提及过如何使用 omni launcher 进行 Isaac Sim 的安装，这是一种最为友好也是最容易获得的途径，通过这种方式，你可以用 &lt;code&gt;./python script.py&lt;/code&gt; 的方式运行你涉及 Isaac Sim 的程序，但是很显然这并不够优雅，尤其是对于使用 Conda 的人来说。Isaac Sim 的程序安装形式的脚本中有提供 setup conda 的脚本，然而我们仍然不打算采用。设想你在远程的服务器上安装 Isaac Sim，执行不显示 UI 界面的 headless 程序，并且服务器不支持 VNC 控制，显然直接使用 &lt;code&gt;pip install&lt;/code&gt; 是最为优雅的解法。&lt;/p&gt;
&lt;p&gt;注意，在安装之前确保你已经安装了 CUDA toolkit、CUDA 以及 CUDNN，相应的安装方法可以见我 &lt;a href=&quot;torch&quot;&gt;之前的博客&lt;/a&gt;，一般来说假如你使用的是实验室的主机，这些内容应当已经配置完毕。&lt;/p&gt;
&lt;p&gt;查阅 &lt;a href=&quot;https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/pip_installation.html&quot;&gt;Isaac Lab 的文档&lt;/a&gt; 以找到安装的指令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n isaac python=3.10
conda activate isaac
pip install isaacsim==4.1.0 isaacsim-extscache-physics==4.1.0 isaacsim-extscache-kit==4.1.0 isaacsim-extscache-kit-sdk==4.1.0 --extra-index-url https://pypi.nvidia.com
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;验证安装&lt;/h2&gt;
&lt;p&gt;可以写一个简单的验证程序来确认是否安装成功，在这里不过多地解释这个脚本的内容，我们将在后续循序渐进地了解 Isaac Sim 的全貌。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: True})
from omni.isaac.core import World
world = World()
world.step()
simulation_app.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假如可以成功执行，那么就没有问题，给自己鼓个掌吧，你掌握了 50% 的人都不了解的技巧，可以直接通过 Python 启动 Isaac Sim 而无需以来原来的软件。&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;写在最后，尽管我们可以通过 &lt;code&gt;pip install&lt;/code&gt; 的方式在正经的 conda 中建立 Isaac Sim 环境，但是笔者依然建议读者通过程序的方法将 Isaac Sim 安装在自己的主机（并非服务器）上。Isaac Sim 的程序通过 UI 界面启动，并且具备良好的交互功能，可以用来编辑场景，同时具备诸多方便的功能，因此假如说需要对自己创建的场景进行一些细微的调整，或者对于一些内容进行预览，依然建议通过程序模式进行启动。相应的应用场景我们也会在后续的教程中讲解，敬请期待。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/isaac-1-zh.webp"/><enclosure url="https://picr2.axi404.top/isaac-1-zh.webp"/></item><item><title>Isaac Sim 一百讲（2）：万物皆 Prim</title><link>https://axi404.github.io/blog/isaac-2</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-2</guid><description>从零开始的 Isaac Sim 之路，第一季开始！</description><pubDate>Sat, 18 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { LinkPreview, ManualTOC } from &apos;../../../components/advanced&apos;&lt;/p&gt;
&lt;p&gt;&amp;#x3C;ManualTOC title=&apos;&apos; categories={[
{
title: &apos;Isaac 101&apos;,
items: [
{
title: &apos;安装&apos;,
href: &apos;/blog/isaac-1&apos;,
order: &apos;1&apos;
},
{
title: &apos;万物皆 Prim&apos;,
href: &apos;/blog/isaac-2&apos;,
order: &apos;2&apos;
},
{
title: &apos;USD&apos;,
href: &apos;/blog/isaac-3&apos;,
order: &apos;3&apos;
},
{
title: &apos;Transformation&apos;,
href: &apos;/blog/isaac-4&apos;,
order: &apos;4&apos;
},
{
title: &apos;Rigid and Collision&apos;,
href: &apos;/blog/isaac-5&apos;,
order: &apos;5&apos;
},
{
title: &apos;Collision 进阶&apos;,
href: &apos;/blog/isaac-6&apos;,
order: &apos;6&apos;
},
{
title: &apos;Camera&apos;,
href: &apos;/blog/isaac-7&apos;,
order: &apos;7&apos;
}
]
}
]} /&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在上一讲中，我们介绍了 Isaac Sim 的安装，并且使用一个简单的程序验证了安装的正确性。读者大可兴奋地期待着本章内容讲解程序的编写，并且可以上手编写自己的程序，然而遗憾的是，这节内容将是概念相关的内容，尽管相对枯燥，但是理解这些概念，对于我们后续的实践是至关重要的。&lt;/p&gt;
&lt;h2&gt;万物皆 Prim&lt;/h2&gt;
&lt;p&gt;古希腊的一位哲学家兼数学家，毕达哥拉斯，曾经说过：「万物皆数」。这句话听上去十分带感，因此在这里对这句话进行一定的化用，在 Isaac Sim 中，万物皆 Prim。&lt;/p&gt;
&lt;p&gt;我们无法找到某个准确的翻译来翻译这一概念，不过在科研界，这种现象比比皆是，因此也就不过多纠结了。要是想要寻求一个恰当的类比，学习过 Java 的读者可能会想到 Java 中的 Object 类，在 Java 中一切的变量都是 Object 类的实例，这是一种很好的迁移，在 Isaac Sim 中，一切的物体都是 Prim 类的实例。&lt;/p&gt;
&lt;p&gt;Isaac Sim 的本质是对于物理世界的模拟，其中的关键在于模拟物理世界中的各种元素，例如物体、光照、纹理等，而在 OpenUSD 的管理范围下，这些东西都可以用 Prim 来替代，进一步在 Prim 上延伸出更多的种类，诸如 XFormPrim、RigidPrim 或者 Shader 等。Isaac Sim 搭建在 OpenUSD 的框架上，可以模拟 OpenUSD 中的物体的交互，使用 PhyX 进行模拟，并且使用管线对于视觉进行渲染等。&lt;/p&gt;
&lt;p&gt;对于每一个 Prim，其包括几个关键的属性，即 &lt;code&gt;prim_path&lt;/code&gt;, &lt;code&gt;API&lt;/code&gt; 以及 &lt;code&gt;Attributes&lt;/code&gt;。&lt;/p&gt;
&lt;h2&gt;Prim Path&lt;/h2&gt;
&lt;p&gt;在 Isaac Sim 中，每一次的程序运行会打开某种实例，从意义的角度来说，可以称之为 Scene，也就是一个场景，而从定义上来说，我们剋称之为一个 Stage。在一个 Stage 下面包括若干的 Prim，这些 Prim 以树状结构组织，每一个 Prim 作为一个结点，因此对于任何一个 Prim，可以使用其递归父结点的名称来表示其路径，例如 &lt;code&gt;/World/Table/LeftLeg&lt;/code&gt; 表示的是在 World 下面的一个 Table 下面的一个左腿，更进一步，在这个 LeftLeg 下面可能包括 Mesh 以及 Tex，前者表示左腿的形状信息，而后者表示左腿的纹理信息。&lt;/p&gt;
&lt;p&gt;在 Isaac Sim 中，所有的 Prim 的 &lt;code&gt;prim_path&lt;/code&gt; 都是唯一的，因此可以通过 &lt;code&gt;prim_path&lt;/code&gt; 来访问任何一个 Prim。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False}) # we can also run as headless.

from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid
from omni.isaac.core.utils.prims import get_prim_at_path

world = World()
world.scene.add_default_ground_plane()
cube1 = world.scene.add(
    DynamicCuboid(
        prim_path=&quot;/World/cube1&quot;,
        name=&quot;cube1&quot;,
        position=np.array([0, 0, 1.0]),
        scale=np.array([0.5015, 0.5015, 0.5015]),
        color=np.array([0, 0, 1.0]),
    ))

cube1_prim = get_prim_at_path(&quot;/World/cube1&quot;)
simulation_app.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这里面，我们创建了一个程序实例，即通过 &lt;code&gt;simulation_app = SimulationApp({&quot;headless&quot;: False})&lt;/code&gt; 创建了一个非无头模式的应用实例，然后通过 &lt;code&gt;world.scene.add_default_ground_plane()&lt;/code&gt; 创建了一个地面。&lt;code&gt;DynamicCuboid&lt;/code&gt; 是一个用于演示很方便的类，其可以创建一个立方体，这个立方体具有重力以及碰撞箱。&lt;/p&gt;
&lt;p&gt;读者可以使用代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;...
world.reset() # [!code ++]
for _ in range(10): # [!code ++]
    world.step(render=True) # [!code ++]
simulation_app.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;来查看这个立方体在重力作用下的运动，在这里不过多解释，后续会详细讲解，在这里可以理解为 step 是前向模拟一次。&lt;/p&gt;
&lt;p&gt;大多数的实际上在操作过程中我们遇到的实例，都是经过了封装的内容，例如大多数的资产都会通过 XFormPrim 进行封装，这些类都具有属性 &lt;code&gt;prim_path&lt;/code&gt;，也就可以直接获得他们的 Prim Path 了，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;...
print(cube1.prim_path) # [!code ++]
print(cube1_prim.GetPath()) # [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;API 与 Attributes&lt;/h2&gt;
&lt;p&gt;在了解了 Prim 的基本属性 Prim Path 之后，我们也有必要了解一下，是什么区分了不同的 Prim，让他们具有了不同的功能。&lt;/p&gt;
&lt;p&gt;在 Isaac Sim 中，API 是 Prim 的一个属性，一个 API 被添加之后，Prim 就可以被施加若干的属性，在这里以物理属性为例，以下代码可以查看一个 Prim 的 API 列表：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;...
print(cube1_prim.GetAppliedSchemas()) # [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以得到输出：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;[&apos;MaterialBindingAPI&apos;, &apos;PhysicsCollisionAPI&apos;, &apos;PhysicsMeshCollisionAPI&apos;, &apos;PhysxCollisionAPI&apos;, &apos;PhysicsMassAPI&apos;, &apos;PhysicsRigidBodyAPI&apos;, &apos;PhysxRigidBodyAPI&apos;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不难发现其包含的 Rigid 以及 Collision 相关的 API。&lt;/p&gt;
&lt;p&gt;每创建一个 API，其就会自动创建若干的属性，对于一些属性来说，其默认就会被创建，并且被设置为默认值，而其他的属性则需要手动创建，以应用这些变换。在后续的使用中，我们主要会对于 Physics 相关的一些 API 进行处理，这将在后续进行详细的展示。&lt;/p&gt;
&lt;p&gt;话题回到当下，于是也就不难通过函数来获得这些 API 下面的 Attribute 的内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;...
print(cube1_prim.GetAttribute(&quot;physxRigidBody:maxLinearVelocity&quot;).Get()) # [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;返回为 &lt;code&gt;inf&lt;/code&gt;，也就是对于最大速度没有限制。读者不难注意到 &lt;code&gt;physxRigidBody:maxLinearVelocity&lt;/code&gt; 是 &lt;code&gt;physxRigidBody&lt;/code&gt; 这个 API 下面的一个属性，但是这个名字究竟是如何获得的呢？在这里指路文档，其 &lt;a href=&quot;https://docs.omniverse.nvidia.com/kit/docs/omni_usd_schema_physics/106.1/class_physx_schema_physx_rigid_body_a_p_i.html#a809c9a1d401014d2a9d9ad734ebd9d1a&quot;&gt;某一处&lt;/a&gt; 包括这一说明。Isaac Sim 的文档中对于每一个 API 都有详细的介绍，而几乎没有任何的教程。当然，我们会使用的 Attribute 将在后续向大家详细的介绍，无需紧张。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;通过本章内容，读者不难对于 Isaac Sim 有一个宏观的理解：Isaac Sim 通过 OpenUSD 管理一系列的 Prim，使用 API 以及 Attributes 来管理这些 Prim 的属性，并且通过其自身的特性，对于这些内容之间的交互（如物理碰撞或者渲染）进行处理。我们还没有详细讲解 step 相关的内容，这便是 Isaac Sim 的仿真的运行，通过这些内容，我们可以在一个场景中，添加物体、光照、纹理，模拟重力以及碰撞，从而运行良好的仿真。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/isaac-2-zh.webp"/><enclosure url="https://picr2.axi404.top/isaac-2-zh.webp"/></item><item><title>周记 Week28</title><link>https://axi404.github.io/blog/week-28</link><guid isPermaLink="true">https://axi404.github.io/blog/week-28</guid><description>2024-01-06 ~ 2025-01-12.</description><pubDate>Tue, 14 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { CardList } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;p&gt;再次进行一次周记的记录，这一次倒也不能说隔了很久了，只能说在时间允许的范围之内给出了更新，但是，好吧，这一周确实没有什么可以说的。&lt;/p&gt;
&lt;h2&gt;考试&lt;/h2&gt;
&lt;p&gt;首先是考试周的结束，在这周的前期就完成了最后一门考试，也就是自然语言处理，考试内容应该不算很难，希望结果也不错。&lt;/p&gt;
&lt;p&gt;可以说在漫长的时间之后，这个学期的全部课内学习也就快要告一段落了，然而还有一个令人讨厌的小尾巴，包括了三个实验报告、一个综述（来自 DSP）以及一个大作业的汇报（来自 NLP）。虽说这些东西都是文书工作，只要花费一定的时间肯定可以完成，但是总归还是需要花费一定的时间的。同时人的潜意识里又总喜欢认为这种事情其实无足轻重，从而懒得去做，但是真轮到做起来又会耗费很多时间，至于不做，自然也会在心里放着一个负担，做其他事情都不踏实。但是至于现在还是先休息一会，等到 DDL 前再去考虑吧。&lt;/p&gt;
&lt;p&gt;以及除此之外，虽然说我的任务已经完成了，但是乐小姐距离考试结束还有很久的路要走，一直到这周结束才把全部考试考完，我这边一边要处理科研，也要帮助她那边的一些考试的准备，比如说整理他的往年选择题及简答题答案，这也是要消耗时间的。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;科研方面可以说还是显得比较古井无波，依然是之前需要去做的一些事情，写代码，以及配合其他同学的工作。一件比较难受的事情在于，不能向导师说自己因为要帮助女朋友复习考试而耽误了科研，毕竟这确实不是一个很正当的理由，不是很合适，因此也就只能默默地承受。&lt;/p&gt;
&lt;p&gt;总的来说这周是考试周的结尾，我也顺便定下来了自己的寒假计划，发在了交大门（西交的一个论坛），在这里也写一份：&lt;/p&gt;
&lt;p&gt;&amp;#x3C;CardList title=&apos;阅读一百篇论文并且更新在 Blog 中&apos; list={
[
{title: &apos;介绍：这属于是最简单的任务，按照目前的效率，除非非常数学或者需要非常详细观察实现细节的论文，基本上我的速度可以到半小时或者一小时一篇。更新到 Blog 是一种积累以及量化的体现。&apos;},
{title: &apos;进度：1/100&apos;, children: [{title: &apos;具身智能论文 Phase 2&apos;, link: &apos;paper-reading-eai2&apos;}]}
]
} collapse /&gt;
&amp;#x3C;CardList title=&apos;看完 Acwing 算法基础课与算法提高课&apos; list={
[
{title: &apos;介绍：鉴定为上次暑假就出现的古董计划，我虽然应该没有在保研的时候应付机试的需求，但是还是对于算法比较感兴趣。&apos;},
{title: &apos;进度：&apos;, children: [{title: &apos;Acwing 算法基础课：0/31&apos;}, {title: &apos;Acwing 算法提高课：0/52&apos;}]}
]
} collapse /&gt;
&amp;#x3C;CardList title=&apos;看完 Games101&apos; list={
[
{title: &apos;介绍：我们下学期有图形学的课程，但是因为貌似是闭卷考试所以我大概率不会选修，但是我依然是对于图形学很感兴趣的，算是一开始接触计算机时候的白月光了，有时间可以看看，但是 202 感觉实在没精力做了。&apos;},
{title: &apos;进度：6/22&apos;}
]
} collapse /&gt;
&amp;#x3C;CardList title=&apos;Release 当前的课题&apos; list={
[
{title: &apos;介绍：目前在 SHAILAB 进行的课题，本身已经投了 CVPR，但是依然在 refine 的过程中，应该会在一月中旬 release，这是一个 DDL&apos;},
{title: &apos;进度：0/1&apos;}
]
} collapse /&gt;
&amp;#x3C;CardList title=&apos;复现五个不同的 Manipulation 模型&apos; list={
[
{title: &apos;介绍：目前我在领域里面没有过多的接触不同的模型以及调参的经验，对于集群的使用也只是一知半解，但是手头有大量的 baseline 可以使用显然是在领域中长期科研的必修课&apos;},
{title: &apos;进度：0/5&apos;}
]
} collapse /&gt;
&amp;#x3C;CardList title=&apos;[Finished] 拍摄两张女装照&apos; list={
[
{title: &apos;介绍：好不容易放假了，在回家之前可以尝试一下！&apos;},
{title: &apos;进度：2/2&apos;, link: &apos;dress-collections&apos;}
]
} collapse /&gt;
&amp;#x3C;CardList title=&apos;[Finished] 使用 Astro 重新搭建我的 Blog&apos; list={
[
{title: &apos;介绍：之前的 Blog 使用别人的主题，自定义也不太方便，假如说使用 Astro，可玩性会高很多，并且可以加入不少自己喜欢的东西进去。属于是虽然有趣，但是也并非迫在眉睫的事情。&apos;},
{title: &apos;进度：1/1&apos;}
]
} collapse /&gt;&lt;/p&gt;
&lt;p&gt;之后也在这个 Blog 里面更新寒假计划进度吧，然后在寒假结束的总结里面再回顾一下。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-28-zh.webp"/><enclosure url="https://picr2.axi404.top/week-28-zh.webp"/></item><item><title>Isaac Sim 踩坑日记</title><link>https://axi404.github.io/blog/isaac-sim-notes</link><guid isPermaLink="true">https://axi404.github.io/blog/isaac-sim-notes</guid><description>关于 Isaac Sim，注定还有一段漫长的路。</description><pubDate>Mon, 19 Aug 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;因为科研的需要，所以说需要安装一下仿真的环境，领域里面最通用的环境就是 Isaac Sim 了，但是据说也比较复杂，老师推荐了另一个 simulator（Sapien），说是比较轻量级，但是为了以后和其他工作更好地对接，以及之后估计半年多一年还是远程，有必要成为模拟器大师，于是挑战一下自己。&lt;/p&gt;
&lt;p&gt;这篇日记依然和 Paper Reading 系列一样，应该是无限期更新的，包括说正常的安装以及操作的一些记录（对于一些涉密的内容，不会涉及），一些模块的学习，以及一些报错的整理。一方面是给自己作为一个笔记，一方面也是假如说有将来的同学进组，可以有一些更加明确的指引。毕竟本人是英文苦手，看英文的速度完全做不到「扫过」，所以还是有必要记录一下的。&lt;/p&gt;
&lt;p&gt;一些你有必要知道的网址：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Issac Sim 文档：&lt;a href=&quot;https://docs.omniverse.nvidia.com/py/isaacsim/index.html&quot;&gt;https://docs.omniverse.nvidia.com/py/isaacsim/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issac Sim 教程：&lt;a href=&quot;https://docs.omniverse.nvidia.com/isaacsim/latest/core_api_tutorials/index.html&quot;&gt;https://docs.omniverse.nvidia.com/isaacsim/latest/core_api_tutorials/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;mplib 文档：&lt;a href=&quot;https://motion-planning-lib.readthedocs.io/latest/index.html&quot;&gt;https://motion-planning-lib.readthedocs.io/latest/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Franka urdf：&lt;a href=&quot;https://github.com/haosulab/ManiSkill/tree/v0.5.3/mani_skill2/assets/descriptions&quot;&gt;https://github.com/haosulab/ManiSkill/tree/v0.5.3/mani_skill2/assets/descriptions&lt;/a&gt;，需要使用其中的 &lt;code&gt;panda_v2.urdf&lt;/code&gt; 并且下载 &lt;code&gt;franka_description/meshes&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;安装 Isaac Sim&lt;/h2&gt;
&lt;p&gt;首先先简单说一下什么是 Isaac Sim，这是一个在 Nvidia 的 omniverse 下的一个 App，可以完成各种的仿真，也支持 ROS 的接口（虽然我目前还不知道 Embodied 的这一套流程是否和 ROS 有接壤），所以说做机器人这方面，用这个的比较多。而且这个东西是可以生成 image（镜像）并且运行在服务器上的，所以说各种意义上的符合具身智能领域的各种需求。&lt;/p&gt;
&lt;p&gt;既然是 Nvidia 的产品，拥有一个 Nvidia 的账号也就是必须的事情了，一般来说还是推荐通过谷歌邮箱之类的 Mail 去注册，在这里不去赘述这个事情。&lt;/p&gt;
&lt;h3&gt;环境概述&lt;/h3&gt;
&lt;p&gt;按照常规的教程来说，反正首先概述一下环境。本人的环境如下，作为参考，当然，这套环境貌似在一些性能上不是很可以，不知道能否坚持到最后：&lt;/p&gt;
&lt;p&gt;以下是 CPU 以及系统信息：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;root:~$ linuxlogo -a
              .-. 
        .-&apos;``(|||) 
     ,`\ \    `-`.               88                         88 
    /   \ &apos;``-.   `              88                         88 
  .-.  ,       `___:    88   88  88,888,  88   88  ,88888, 88888  88   88 
 (:::) :        ___     88   88  88   88  88   88  88   88  88    88   88 
  `-`  `       ,   :    88   88  88   88  88   88  88   88  88    88   88 
    \   / ,..-`   ,     88   88  88   88  88   88  88   88  88    88   88 
     `./ /    .-.`      &apos;88888&apos;  &apos;88888&apos;  &apos;88888&apos;  88   88  &apos;8888 &apos;88888&apos; 
        `-..-(   ) 
              `-` 

Linux Version 5.15.0-117-generic, Compiled #127~20.04.1-Ubuntu
16 2.3GHz Intel i7 Processors, 128TB RAM, 73728 Bogomips Total
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于本人更换系统的意愿（见 &lt;a href=&quot;https://axi404.github.io/Blog/p/%E5%A5%87%E5%A5%87%E6%80%AA%E6%80%AA%E7%9A%84-bug-%E9%9B%86%E6%95%A3%E5%9C%B0/&quot;&gt;Strange Bugs&lt;/a&gt;，Ubuntu 20.04 日常使用已经很不方便），在安装 Isaac Sim 之后的内容均在 Ubuntu 22.04 上进行，如存在其他版本的信息，会专门注明补充。此系统的信息如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;root:~$ linuxlogo -a
              .-. 
        .-&apos;``(|||) 
     ,`\ \    `-`.               88                         88 
    /   \ &apos;``-.   `              88                         88 
  .-.  ,       `___:    88   88  88,888,  88   88  ,88888, 88888  88   88 
 (:::) :        ___     88   88  88   88  88   88  88   88  88    88   88 
  `-`  `       ,   :    88   88  88   88  88   88  88   88  88    88   88 
    \   / ,..-`   ,     88   88  88   88  88   88  88   88  88    88   88 
     `./ /    .-.`      &apos;88888&apos;  &apos;88888&apos;  &apos;88888&apos;  88   88  &apos;8888 &apos;88888&apos; 
        `-..-(   ) 
              `-` 

Linux Version 6.8.0-40-generic, Compiled #40~22.04.3-Ubuntu
16 4.6GHz Intel i7 Tigerlake Processors, 31.1GB RAM, 74k Bogomips
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下是显卡信息，因为是笔记本，我的显卡是 8GB 的 RTX 3070 Laptop：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;root:~$: nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Tue_Feb__7_19:32:13_PST_2023
Cuda compilation tools, release 12.1, V12.1.66
Build cuda_12.1.r12.1/compiler.32415258_0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我的电脑是 Dell 的 Alienware m15 R6。&lt;/p&gt;
&lt;h3&gt;下载 omniverse-launcher&lt;/h3&gt;
&lt;p&gt;就像是之前说到的一样，Isaac 的 omniverse 下的一个 App，所以说在安装 Isaac 之前要先安装 omniverse-launcher，也是比较简单的，在官网 &lt;a href=&quot;https://www.nvidia.com/en-us/omniverse/download/&quot;&gt;https://www.nvidia.com/en-us/omniverse/download/&lt;/a&gt; 进行安装就好。进入下载页面之前会要求输入一些个人信息，随意写一下就好，理论来说 nvidia 账号中已经包含了这些内容，所以会自动填写。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.6bh01xbi8d.webp&quot; alt=&quot;Download omniverse-launcher&quot;&gt;&lt;/p&gt;
&lt;p&gt;下载下来之后是一个 &lt;code&gt;.AppImage&lt;/code&gt; 的文件，按照我的惯例，就直接运行了：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd ~/Downloads
wget https://install.launcher.omniverse.nvidia.com/installers/omniverse-launcher-linux.AppImage
sudo chmod +x omniverse-launcher-linux.AppImage
./omniverse-launcher-linux.AppImage
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于 Ubuntu 22.04，可能会报错 &lt;code&gt;AppImages require FUSE to run.&lt;/code&gt;，按照提示信息，安装 &lt;code&gt;sudo apt install libfuse2&lt;/code&gt; 即可。&lt;/p&gt;
&lt;p&gt;运行之后产生登录页面，本质上还是 nvidia 账号，点击 &lt;code&gt;LOG IN&lt;/code&gt; 之后会跳转到网页，输入帐号密码登录即可。然后同意若干的协议，进入如下界面：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-1.3yedkpxp1r.webp&quot; alt=&quot;Path Selection&quot;&gt;&lt;/p&gt;
&lt;p&gt;这些路径按照默认配置即可。选择确认，进入主界面：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-2.b8tx71wjz.webp&quot; alt=&quot;Main page&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中比较重要的是 &lt;code&gt;Library&lt;/code&gt;/&lt;code&gt;Exchange&lt;/code&gt;/&lt;code&gt;Nucleus&lt;/code&gt;，第一个是已经安装的内容的管理，第二个是安装内容的途径，第三个是一种中央数据库和协作引擎。&lt;/p&gt;
&lt;h3&gt;安装并启动 Isaac Sim&lt;/h3&gt;
&lt;p&gt;进入 Exchange 进行安装，首先安装 cache，搜索之后下拉版本，选择 &lt;code&gt;2023.1.0&lt;/code&gt;，并点击 install 即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-3.5j44k6uwi9.webp&quot; alt=&quot;Install cache&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后点击 Nucleus，选择 Add local Nucleus Service：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-4.6pnfssjt3l.webp&quot; alt=&quot;Add local Nucleus Service&quot;&gt;&lt;/p&gt;
&lt;p&gt;会要求设置 path 以及 admin account，自行设置即可。&lt;/p&gt;
&lt;p&gt;最后在 Exchange 中安装 Isaac Sim，同样是搜索，版本选择 &lt;code&gt;2023.1.0-hotfix.1&lt;/code&gt;，点击 install。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本人目前选择安装 &lt;code&gt;4.1.0&lt;/code&gt; 版本，且之后内容均在此版本下进行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 Nucleus 下载完毕之后，可以找到两个本地的服务：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-5.1hs55sqt5e.webp&quot; alt=&quot;Nucleus&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中选择 Settings，可以在网页中看到如下内容：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;值得注意的是，在第二次或者以后启动的时候，可能会出现进入其 Settings 链接 &lt;code&gt;http://localhost:3080/&lt;/code&gt; 之后为一片白色的情况，而 Cache 没有正确启动，导致后续的程序无法运行，解决方法之一是，可以进入其子窗口 &lt;code&gt;http://localhost:3080/cache&lt;/code&gt;，再点击上方的 &lt;code&gt;Apps&lt;/code&gt;，之后 &lt;code&gt;Restart all&lt;/code&gt; 即可。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-6.8hgenp35zo.webp&quot; alt=&quot;Nucleus Settings&quot;&gt;&lt;/p&gt;
&lt;p&gt;假如出现问题，如显示 Stop 或者 Error，请检查之前说的版本问题。假如 cache 版本不对，重新卸载并且安装，然后点击 Launch 即可。&lt;/p&gt;
&lt;p&gt;选择文件夹图标的内容，可以在网页中看到如下的内容：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-7.3gobw4wbh1.webp&quot; alt=&quot;Nucleus content&quot;&gt;&lt;/p&gt;
&lt;p&gt;均确认无误之后，可以在 Library 中选择 Isaac Sim 并且点击 Launch。&lt;/p&gt;
&lt;h2&gt;Standalone Pick and Place 代码实现&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;此章节在 Ubuntu 22.04, CUDA 12.1, cudnn 9.3.0, Isaac Sim 4.1.0, cache 2023.1.0 下运行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下来就是写代码的环节了，一般来说这个代码有两种实现的方式，一种是在 Isaac Sim 里面添加一个 User Example，另一种是直接使用一个脚本，也就是 standalone script，这里面推荐使用脚本。因为 User Example 的方法必须要使用 GUI 才可以启动，还是不太方便，后续我们肯定是希望这个程序可以摆脱 GUI，当然，必要的时候也可以唤出。&lt;/p&gt;
&lt;p&gt;第一次需要找到你的 Isaac Sim 的环境在哪里，因为 Isaac Sim 使用了自己的 python 环境，因此需要找到他的解释器，假如你是默认安装的路径，那么应该可以看到路径:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo /home/`whoami`/.local/share/ov/pkg/isaac-sim-4.1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是假如不是，可以进入 Isaac Sim 软件，随便点击一个上方栏的 Isaac Examples，并且 Open Containing Folder 即可。&lt;/p&gt;
&lt;p&gt;例如 hello world 这个 example，这个文件夹应该在 &lt;code&gt;isaac-sim-4.1.0/exts/omni.isaac.examples/omni/isaac/examples/hello_world&lt;/code&gt; 中，以下全部的操作视作在 &lt;code&gt;isaac-sim-4.1.0&lt;/code&gt; 下进行。&lt;/p&gt;
&lt;h3&gt;创建项目&lt;/h3&gt;
&lt;p&gt;首先先创建我们接下来的程序的文件夹：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir -p Isaac_learning
touch Isaac_learning/demo.py
code .
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Hello World&lt;/h3&gt;
&lt;p&gt;打开这个新建的文件，在里面输入&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False})

from omni.isaac.core import World
world = World()

while simulation_app.is_running():
    world.step(render=True)

simulation_app.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这是一个最简单的程序，可以创建出来一个正常的模拟器的界面，可以进行运行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./python.sh Isaac_learning/demo.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接下来需要做的事情就是在里面添加东西了。&lt;/p&gt;
&lt;p&gt;在这里简单介绍一下 Isaac Sim 的物体的基本组织结构，基本上可以说，Isaac Sim 里面的物体都是由 Prim 组成的，也就是所谓的 XFormPrim，一般来说，存在一个 world，一个 world 里面会存在 scene，scene 里面的绝大多数内容都是 prim，可以理解为 isaac sim 里面的 object，同时支持嵌套。&lt;/p&gt;
&lt;h3&gt;添加物体&lt;/h3&gt;
&lt;p&gt;不过对于最基础的内容，我们存在一些 api 可以使用，更多的内容都可以在文档中查询，所以让我们简单修改代码，在里面加入一个地面和一个方块。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False}) # we can also run as headless.

from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid

world = World()
world.scene.add_default_ground_plane() # [!code ++]
cube1 =  world.scene.add( # [!code ++]
    DynamicCuboid( # [!code ++]
        prim_path=&quot;/World/cube1&quot;, # [!code ++]
        name=&quot;cube1&quot;, # [!code ++]
        position=np.array([0, 0, 1.0]), # [!code ++]
        scale=np.array([0.5015, 0.5015, 0.5015]), # [!code ++]
        color=np.array([0, 0, 1.0]), # [!code ++]
    )) # [!code ++]

while simulation_app.is_running():
    world.step(render=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行一下，不难发现里面多出来了一个方块和一个地板，也就是这两行的效果。一个物体有两个经常使用的属性，一个是 Rigid Body，也就是物体是否会受到力的影响，一个是 Colliders Preset，也就是物体是否会有碰撞。DynamicCuboid 默认具有这两个属性，所以你会看到它掉在地板上，而地板是不受到重力影响的，但是有碰撞，所以你不会看到地板和物体一起掉下去，而物体也不会穿过地板。&lt;/p&gt;
&lt;p&gt;同时可以注意到的是 prim_path 以及 name，第一个描述了 cube 的 prim 的嵌套关系，因为 world 也是一个 prim，而这个物体的名字则叫做 cube1。&lt;/p&gt;
&lt;h3&gt;添加机械臂&lt;/h3&gt;
&lt;p&gt;同样的方法，我们可以在里面加入一个 Franka，这也不难：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False}) # we can also run as headless.

from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid
from omni.isaac.franka import Franka # [!code ++]

world = World()
world.scene.add_default_ground_plane()
cube1 =  world.scene.add(
    DynamicCuboid(
        prim_path=&quot;/World/cube1&quot;,
        name=&quot;cube1&quot;,
        position=np.array([0, 0, 1.0]),
        scale=np.array([0.5015, 0.5015, 0.5015]),
        color=np.array([0, 0, 1.0]),
    ))
franka = world.scene.add(Franka(prim_path=&quot;/World/Franka&quot;, name=&quot;franka&quot;)) # [!code ++]

world.reset() # [!code ++]
franka.gripper.set_joint_positions(franka.gripper.joint_opened_positions) # [!code ++]

while simulation_app.is_running():
    world.step(render=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;world.reset()&lt;/code&gt; 应用了全部的变换，并且将全部的物体都重置到初始状态，这一步骤是很重要的初始化操作，需要记住。而 &lt;code&gt;world.scene.add(Franka)&lt;/code&gt; 则是将机械臂添加到场景中，&lt;code&gt;franka.gripper.set_joint_positions(franka.gripper.joint_opened_positions)&lt;/code&gt; 设置将机械臂的夹爪打开。&lt;/p&gt;
&lt;p&gt;这里面需要注意的是，假如说你只是想要看这个机械臂，你直接 Franka 就可以创建一个机械臂，但是假如说你想要 control，就需要使用 &lt;code&gt;world.scene.add(Franka)&lt;/code&gt; 这一步，不然会报错。&lt;/p&gt;
&lt;h3&gt;添加控制器&lt;/h3&gt;
&lt;p&gt;最后，让我们完成最后的一个环节，添加一个控制器。真实的控制器实现需要通过更加复杂的内容来进行，但是我们现在的目标并不复杂，只是希望给出一个简单的控制器，来完成一个垂直的 pick and place 的任务。Isaac Sim 提供了一个 PickPlaceController，可以完成这个任务。修改程序：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from isaacsim import SimulationApp
simulation_app = SimulationApp({&quot;headless&quot;: False}) # we can also run as headless.

from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid
from omni.isaac.franka import Franka
from omni.isaac.franka.controllers import PickPlaceController # [!code ++]
import numpy as np

world = World()
world.scene.add_default_ground_plane()

cube1 =  world.scene.add(
    DynamicCuboid(
        prim_path=&quot;/World/cube1&quot;,
        name=&quot;cube1&quot;,
        position=np.array([0, 0, 1.0]),
        scale=np.array([0.5015, 0.5015, 0.5015]),
        color=np.array([0, 0, 1.0]),
    ))

franka = world.scene.add(Franka(prim_path=&quot;/World/Franka&quot;, name=&quot;franka&quot;))

controller = PickPlaceController( # [!code ++]
            name=&quot;pick_place_controller&quot;, # [!code ++]
            gripper=franka.gripper, # [!code ++]
            robot_articulation=franka, # [!code ++]
        ) # [!code ++]

world.reset()

franka.gripper.set_joint_positions(franka.gripper.joint_opened_positions)

while simulation_app.is_running():
    position, orientation = cube1.get_world_pose() # [!code ++]
    goal_position = np.array([-0.3, -0.3, 0.02575]) # [!code ++]
    current_joint_positions = franka.get_joint_positions() # [!code ++]
    actions = controller.forward( # [!code ++]
        picking_position=cube_position, # [!code ++]
        placing_position=goal_position, # [!code ++]
        current_joint_positions=current_joint_positions, # [!code ++]
    ) # [!code ++]
    franka.apply_action(actions) # [!code ++]
    world.step(render=True)

simulation_app.close() # close Isaac Sim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行一下，可以看到机械臂已经成功将方块拿起，并且放置到了目标位置。&lt;/p&gt;
&lt;p&gt;不过先不管这些，让我们来分析一下这个程序。我们添加了一个 Controller，这个 Controller 需要一个 gripper，一个 robot_articulation，也就是其需要知道机械臂的夹爪是啥，以及机械臂本身的结构。&lt;/p&gt;
&lt;p&gt;之后我们在循环里面增加了不少的内容，首先我们获得了 cube 的 position 以及 orientation，get_world_pose 是大多数的被封装的 prim 都会具有的一个方法，返回其在 world frame 下面的位置以及姿态，其中姿态是一个 w 在前的四元数。然后我们设置了一个目标位置。机械臂的当前角度可以通过 &lt;code&gt;get_joint_positions&lt;/code&gt; 获得，然后我们使用 controller 的 forward 方法，输入了 picking_position，placing_position，以及 current_joint_positions，这里返回的是一个 ArticulationAction，这是一个包括一个序列（比如说对于 Franka 来说是 7 DoF + 2 个夹爪的关节）的 position 信息，以及一个等长的速度的序列。之后我们使用 franka.apply_action 方法，将 actions 应用到机械臂上。&lt;/p&gt;
&lt;p&gt;world.step 是整个程序的灵魂，它完成了整个物理引擎的更新，以及渲染的更新，所以假如说你想要在程序中看到机械臂的运动，就需要使用这个方法。当然，有的时候，比如说你的程序本身是一个强化学习的程序，那么渲染视觉信息并非是必须的，所以你可以使用 &lt;code&gt;world.step(render=False)&lt;/code&gt; 来关闭渲染，可以极大地提高速度。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;这是一篇很久之前我刚开始接触 Isaac Sim 的时候写的笔记，现在来看，事实上很多内容不是必须的，其中 Pick and Place 的代码实现其实是我当时按照 &lt;a href=&quot;https://docs.omniverse.nvidia.com/isaacsim/latest/core_api_tutorials/index.html&quot;&gt;Isaac Sim 教程&lt;/a&gt; 的章节进行实践的时候的记录。&lt;/p&gt;
&lt;p&gt;不过有必要向读者指出的是，Isaac Sim 的教程也就仅仅到了这种深度，之后讲了多个机器人的操作，然后就什么都没有了，事实上这完全还没有到入门的门槛，在实际中这种级别的问题实在是像玩一样。不过假如说这个实现是一个好的介绍，你作为一个新手，不会如何安装 Isaac Sim，没有了解一些基础的功能，那么还是可以尝试一下，而假如你是一路顺着博客看完了也写完了，感觉怎么样？是不是有一些入门的成就感。不过假如说你想要进行更加深入的了解，还是需要去看官方的文档，以及或许我将来的 Blog，尽请期待吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Isaac-Sim-Notes-zh.webp"/><enclosure url="https://picr2.axi404.top/Isaac-Sim-Notes-zh.webp"/></item><item><title>周记 Week27</title><link>https://axi404.github.io/blog/week-27</link><guid isPermaLink="true">https://axi404.github.io/blog/week-27</guid><description>2024-12-30 ~ 2025-01-05.</description><pubDate>Mon, 13 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一周又是平平无奇的一周考试周。&lt;/p&gt;
&lt;h2&gt;考试&lt;/h2&gt;
&lt;p&gt;总的来说也没什么大事发生，因为有考试的原因，所以说科研的进度被 block 了很多，而且我这边不但要准备自己的考试，好好科研，还要照顾女朋友。只能说我们的考试压力还是不大的，但是乐小姐基本上两天一考试，而且都是背诵很多东西的考试。&lt;/p&gt;
&lt;p&gt;为了复习方便，我们甚至在外面租了房，所以就可以任意作息通宵复习，这很大程度上节约了时间，并且达到了一种三天时间可以当作四天用的效果（通过压缩睡眠时间）。&lt;/p&gt;
&lt;p&gt;我给自己列了 2024 年的年终总结，实际上我的科研起步就是在 24 年，如此回头望去，还是硕果累累的。但是依然不得不感慨，我的效率依然不够，虽然说相较于同龄人已经很厉害了，但是这是因为我单位时间内能做的事情很多，然而最后的结果完全没有我理论上的产出那么多，因为大量的时间内浪费掉了。&lt;/p&gt;
&lt;p&gt;DSP 考试匆匆结束，接下来又要紧锣密鼓地复习 NLP 的考试，然而科研的进度也慢了很多，所以说我还需要汇报自己的复习并不是很麻烦，然后在科研里面加大投入，其中被压榨的自然也就是我自己的休息时间了。&lt;/p&gt;
&lt;p&gt;可以说现在我基本上处于一个半 burn out 的状态了，每天都在赶 DDL，不过相信等到考试结束，我就可以好好休息一下了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-27-zh.webp"/><enclosure url="https://picr2.axi404.top/week-27-zh.webp"/></item><item><title>Docker 调用 Nvidia 报错</title><link>https://axi404.github.io/blog/docker-nvidia-error</link><guid isPermaLink="true">https://axi404.github.io/blog/docker-nvidia-error</guid><description>正常安装 Nvidia Docker 后运行时报错 Failed to initialize NVML: Unknown Error。</description><pubDate>Sun, 12 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;最近在配置 Docker 环境，在安装 Nvidia Docker 后，运行时报错 &lt;code&gt;Failed to initialize NVML: Unknown Error&lt;/code&gt;。&lt;/p&gt;
&lt;h2&gt;情况复现&lt;/h2&gt;
&lt;p&gt;这个情况并不是很容易复现，本身我是在实验室的服务器上运行的 Docker，从而安装 docker 版本的 isaac sim，参照 Isaac Sim 的&lt;a href=&quot;https://docs.omniverse.nvidia.com/isaacsim/latest/installation/install_container.html&quot;&gt;官方文档&lt;/a&gt;，具体的 Docker 安装为：&lt;/p&gt;
&lt;h3&gt;安装 NVIDIA Driver&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt-get update
sudo apt install build-essential -y
wget https://us.download.nvidia.com/XFree86/Linux-x86_64/535.129.03/NVIDIA-Linux-x86_64-535.129.03.run
chmod +x NVIDIA-Linux-x86_64-535.129.03.run
sudo ./NVIDIA-Linux-x86_64-535.129.03.run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这一步我压根没操作，因为本身就已经配置好了。可以用 &lt;code&gt;nvidia-smi&lt;/code&gt; 查看是否安装成功，有输出就没问题。&lt;/p&gt;
&lt;h3&gt;安装 Docker&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Docker installation using the convenience script
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Post-install steps for Docker
sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker

# Verify Docker
docker run hello-world
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里面实际上直接 &lt;code&gt;sudo usermod -aG docker $USER&lt;/code&gt; 就好了，然后 &lt;code&gt;ctrl+d&lt;/code&gt; 退出，再 SSH 进来刷新一下就好了。&lt;/p&gt;
&lt;h3&gt;安装 Nvidia Container Toolkit&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Configure the repository
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  &amp;#x26;&amp;#x26; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed &apos;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&apos; | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \
  &amp;#x26;&amp;#x26; \
    sudo apt-get update

# Install the NVIDIA Container Toolkit packages
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# Configure the container runtime
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verify NVIDIA Container Toolkit
docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;安装 Isaac Sim&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull nvcr.io/nvidia/isaac-sim:4.2.0
docker run --name isaac-sim --entrypoint bash -it --runtime=nvidia --gpus all -e &quot;ACCEPT_EULA=Y&quot; --rm --network=host \
    -e &quot;PRIVACY_CONSENT=Y&quot; \
    -v ~/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \
    -v ~/docker/isaac-sim/cache/ov:/root/.cache/ov:rw \
    -v ~/docker/isaac-sim/cache/pip:/root/.cache/pip:rw \
    -v ~/docker/isaac-sim/cache/glcache:/root/.cache/nvidia/GLCache:rw \
    -v ~/docker/isaac-sim/cache/computecache:/root/.nv/ComputeCache:rw \
    -v ~/docker/isaac-sim/logs:/root/.nvidia-omniverse/logs:rw \
    -v ~/docker/isaac-sim/data:/root/.local/share/ov/data:rw \
    -v ~/docker/isaac-sim/documents:/root/Documents:rw \
    nvcr.io/nvidia/isaac-sim:4.2.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时你会交互式地进入 Docker 中，然后运行 &lt;code&gt;nvidia-smi&lt;/code&gt; 查看是否成功。&lt;/p&gt;
&lt;p&gt;一般来说，就会正常输出，但是本人在某个服务器上就遇到了报错 &lt;code&gt;Failed to initialize NVML: Unknown Error&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;想要了解是否与本人遇到的情况相同，可以尝试使用：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm -it --device=/dev/nvidiactl --device=/dev/nvidia0 --gpus all nvcr.io/nvidia/isaac-sim:4.2.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后尝试一下输出，会发现输出一张卡。&lt;/p&gt;
&lt;p&gt;然而这并非通用的方法，因为你把 nvidia0 换成 nvidia1 或者其他的，就找不到卡了。&lt;/p&gt;
&lt;h2&gt;解决方法&lt;/h2&gt;
&lt;p&gt;解决方法也很简单，直接修改 Docker 的一个配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo vim /etc/nvidia-container-runtime/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;...
no-cgroups = true # [!code --]
no-cgroups = false # [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后重启 docker 服务：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时再运行 &lt;code&gt;nvidia-smi&lt;/code&gt;，就会发现可以正常输出了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/docker-nvidia-error-zh.webp"/><enclosure url="https://picr2.axi404.top/docker-nvidia-error-zh.webp"/></item><item><title>2024 年终总结</title><link>https://axi404.github.io/blog/2024</link><guid isPermaLink="true">https://axi404.github.io/blog/2024</guid><description>你是否在尺子上留下了刻度。2024，剧终。</description><pubDate>Sun, 05 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;关于过去一年经历的事情的回顾，我其实已经写过了很多，重复的叙事并没有什么意义，只是在制造重复的回忆，而没有带来任何的价值。假如说回忆录是帮助我回忆起来过去的故事的文章，读者们也可以从中了解我的故事，那么年终总结就是一个彻头彻尾写给当下的我自己的短文，让我告诉自己，自己这过去的一年内，收获了什么，失去了什么，改变了什么，以及，有没有在人生的尺上留下刻痕。&lt;/p&gt;
&lt;p&gt;要是用三个关键词来概括过去的一年，可以是，「找到方向」、「内耗」、「成长」。&lt;/p&gt;
&lt;h2&gt;找到方向&lt;/h2&gt;
&lt;p&gt;在 2024 年之前，我是一个普普通通成绩还算不错的学生，有着一点点的信息差的高瞻远瞩，于是进组参加了科研。从时间线来说，那时候就已经投稿了 CVPR，尽管在 2024 年年初迎来了失败的噩耗，但是走向科研这条路确实不是 2024 年开始的。但是所谓方向，大抵说的我究竟有没有意识到自己将来想要做什么。&lt;/p&gt;
&lt;p&gt;我一直以来是确定我要保研的。我很清楚自己不是一个应试型的选手，尽管相较于大多数的同学，我耗时几天的成果已经可以比他们学习一学期的成绩要好一些，但是我并无法从这些重复性的学习陈旧知识并且刷题的过程中获得反馈，因为高考的空前失利，我也十分惧怕当我诉诸于考研时，再一次的失误会将我推向怎样的深渊。事到如今，我依然认为对于大多数的同学来说，考研是一条最合理的路：留下了绚烂的大学生活，简单，一次定胜负。&lt;/p&gt;
&lt;p&gt;反观保研，这显然并不是一个很好的想法。选择了保研，意味着需要维护三个学年的成绩，每一门考试都必须仔细复习，并且保证足够高的成绩，而这只是冰山一角。同时，你还需要关注竞赛以及各种其他的德育分加分项，因为这些内容在保研的排名中占比高达 10%，这显然是不可忽略的。要不然你具有一些特长，加入了诸如 ACM 或者 RM/RC 这种社团，一次性凑够十分竞赛加分，要不然你就需要辗转于各种竞赛（e.g., CUPT 等的校内选拔）来一分一分凑够这些分数。当你做完了这些之后，你的结果只是抵消了一次考研的考试，你依然在西交，依然和那些考研的同学没有什么区别，度过了相对自由的大四之后前往创新港。那些自己在大学前三年熬过的夜，流过的泪，以及诸如此类付出的一切，只是换来了一个正常到近乎平庸的结果。要是你还想要更进一步，那么试试保研外校吧。想要通过强 com 的夏令营，你需要在关键的大三下期末考试附近，连续参加不同的学校组织的考试，同时你或许还需要提前在 LeetCode 上面刷一些题目，以确保自己可以通过机试，而你的对手可能之前参加过 XCPC。假如是弱 com，那么你想要直博吗，你是否已经决定了你后面半生的道路要踏上科研？或者你已经决定好了，读一个硕士，不过想要获得老师欣赏，也需要提前参加科研，并且有科研产出，这则是一条更加漫长的路。&lt;/p&gt;
&lt;p&gt;不过正如我之前非常喜欢的诗歌：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;也许多少年后在某个地方，&lt;/p&gt;
&lt;p&gt;我将轻声叹息把往事回顾，&lt;/p&gt;
&lt;p&gt;一片树林里分出两条路，&lt;/p&gt;
&lt;p&gt;而我选了人迹更少的一条，&lt;/p&gt;
&lt;p&gt;因此走出了这迥异的旅途。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我很早就在保研与考研的岔路口上选择了前者。只是我现在在这条路上，又一次选择了那条更加布满荆棘的路，我喜欢科研，我想要在这条路上走的更远。&lt;/p&gt;
&lt;p&gt;我在今年的三月份投稿了 ECCV，然后六月份收到了 443 的得分，之后进行了 rebuttal，并且在七月份获得了 554 的修改后得分，收获了第一篇中稿。可以说，这是一次很不错的尝试，我参加了科研，并且在一个名不见经传的小领域里里面做出了 SOTA 的成果，得到了审稿人的认可。一个人从零开始，自己学习基础知识，然后申请进组寻找 idea，进行书写代码以及实验，并且调试，最后绘图以及完成论文。这是一次十分不容易的体验，对于一个初学者来说，这并不是一件十分简单的事情，而且中间也出现了诸如 CVPR 的失利的挫折，这些都是一种磨砺。&lt;/p&gt;
&lt;p&gt;之后我申请了上海人工智能实验室的 OpenRobotLab，在暑假前往了上海进行了一段时间的线下实习，并且在之后大三上学期开始了线上实习。正如我之前说的，我是想要进行科研的，也就是我的目标从一开始可能就是进行直博，而我从大一开始，其实就已经向往前往上海人工智能实验室（其一是听闻其良好的氛围以及计算资源，其二则是喜欢上海这个城市，~~本人实在想要女装逛漫展~~），因此也可以说是在逐渐收敛到自己预期的那条路上。在 SHAILAB 这边，我进行了我的第一个课题，并且会在 2025 年年初彻底完结这个工作，在此之间，也是以 CVPR 作为 DDL 将这篇工作投了出去。&lt;/p&gt;
&lt;h2&gt;内耗&lt;/h2&gt;
&lt;p&gt;内耗可以说是我这一年的另一个很大的主题。内耗这个词现在有一点点像是什么国潮单品一样，可以说任何一个人，都可以以内耗作为任何失利的借口，但是还是有必要在这里定义一下对于我来说的内耗，即因为个人的生理或者心理问题，导致办事效率的低下。&lt;/p&gt;
&lt;p&gt;和绝大多数同学相比，我的办事效率可以说已经很高了。通过熟练使用 GPT 以及各种软件（e.g., LaTeX for 写论文或者 Python for 简单的数据分析），我可以在短时间内完成很多的事情。同时，我可以可控地进入专注的状态，从而长达数小时乃至十数小时专注在一件事情中，然而显然这还不够。事实上我的生活中依然存在大量的时间，因为类似于 ADHD 的原因，在发呆或者转而做其他事情中被消耗，其中更加不能容忍的，比如，我甚至可能花费了一些时间去看网络小说，然而我甚至不能从这些行为中获得快乐，这对于我来说甚至连情绪价值都没有产生，只是在浪费时间。&lt;/p&gt;
&lt;p&gt;在这一年中这种无意义的内耗占据了可以说是大多数的时间，除非存在一个确切的 DDL 进行催促，不然我也难以专心来做这件事情。这对于一名将会长期从事科研的科研工作者来说显然是不可接受的。&lt;/p&gt;
&lt;p&gt;除此之外，我在这一年中的身体状况并不算好，因为经常性的昼夜颠倒，导致在白天的精力并不算充沛。尽管在晚上没有别人打扰，可以更加专注地做我需要做的事情，这是一个很不错的体验，但是无论是科研还是其他的合作，这都是需要多人一起完成的事情，但是这种昼夜颠倒会导致我难以和合作者建立及时的沟通，从而进一步在多人的任务线上因为这种情况而被 Block 住。心理状态也是一个需要关注的话题，现在因为基本上已经获得了 SHAILAB 的 offer（当然，还是需要继续努力的，毕竟我的目标并非获得一个 offer，而是可以在 SHAILAB 中做出一定的贡献，并且在科研的领域中做出具有影响力的工作），基本上不会因为同龄人的成绩或者科研进展而感到焦虑，但是依然会因为过度的劳累，而产生心理上的疲劳。同时，今年我也少量出现了存在主义危机，即，我从小便畏惧于死亡的概念，并且对于自己为何而存在，死后会去哪里等问题感到恐惧，这些因素随着我的劳累而再次被唤起。&lt;/p&gt;
&lt;h2&gt;成长&lt;/h2&gt;
&lt;p&gt;尽管有所前进，有所桎梏，但是今年的主旋律依然是成长，相较于之前的成长。无论是前往了新的实验室，中稿了第一篇顶会，熟悉更大的课题组的工作节奏，还是从技术上接触了具身智能领域的若干平台，学会使用集群，以及其他若干的细枝末节，我相较于去年还是进步了许多。~~甚至说尝试了更多风格的女装，从中获得治愈，这何尝不算一种成长呢。~~&lt;/p&gt;
&lt;p&gt;同时，我和乐小姐两个人的二人世界也正在成长，相较于一年以前的青涩，我们现在变得更加熟悉彼此，也更加亲密无间。&lt;/p&gt;
&lt;p&gt;与此同时，当然，在科研之余，我在其他地方也获得了不少的成长。我在绿群中制作了 CS-BAOYAN-DDL，获得了大量的认可，并且在后续成为了绿群这一组织的群主。我在课余时间整理了不少的复习资料，这些内容都可能被将来的同学使用到，对他们起到帮助。我制作了西安交大生存指南，在新生入学的时候进行了宣传，这是一个很不错的制作，讲述了我在当时的视角下认为的新生以及老生的成长路线，这些内容还会继续在将来被拓展以及更新。我搭建了新的博客，你现在正在通过这一博客看着我的文章。&lt;/p&gt;
&lt;h2&gt;新年展望&lt;/h2&gt;
&lt;p&gt;既然是新年，虽然说这是我的第一次年终总结，但是明年，也就是 2025 年，显然我依然会书写一篇 2025 年的年终总结，所以不妨立下一些计划，并且看看那时候可以兑现多少。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;产出三篇以上的科研工作&lt;/li&gt;
&lt;li&gt;前往 SHAILAB 直博&lt;/li&gt;
&lt;li&gt;在 Github 累计 commit 三百天（主要指连续更新 Blog）&lt;/li&gt;
&lt;li&gt;写三百篇论文感想&lt;/li&gt;
&lt;li&gt;每天写代码六小时、读论文一小时、跑步一千米&lt;/li&gt;
&lt;li&gt;写万字的小说设定&lt;/li&gt;
&lt;li&gt;更新西安交大生存指南&lt;/li&gt;
&lt;li&gt;坚持写周记&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们明年见。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/2024-zh.webp"/><enclosure url="https://picr2.axi404.top/2024-zh.webp"/></item><item><title>大二回忆录</title><link>https://axi404.github.io/blog/uni-memoir2</link><guid isPermaLink="true">https://axi404.github.io/blog/uni-memoir2</guid><description>我的大二生活。</description><pubDate>Sun, 05 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;随着 CVPR 投稿落下帷幕，补充材料也已提交完毕，阶段性的科研旅程暂时划上了一个逗号。此刻，我终于抽出一点空闲，来回顾这段久违的大二生活。&lt;/p&gt;
&lt;h2&gt;前路茫茫&lt;/h2&gt;
&lt;p&gt;大一的节奏堪称繁忙，而在踏入大二后，我的个人能力已和入学之初不可同日而语。不仅掌握了多种编程语言，还亲历并推动了 RM 这样中等体量的工程项目，同时在机器学习与深度学习的知识体系上也构建起了初步的轮廓，读完了数本经典教材，也啃下了约五十篇相关论文。&lt;/p&gt;
&lt;p&gt;是 WJH 学长将我引入了绿群，从此我开启了一段新的“水群”旅程。但这一次，我不再是单纯的热闹参与者，而是一个开始明确人生方向的探索者。经过一段时间的论文阅读与与人交谈，我对自己未来的意图愈发清晰：我喜欢科研，并愿意以学术作为主线，将它作为通往未来的第一路径。简单来说，我的目标是在本科阶段完成研究成果，并以此作为进入清北华五级别 PhD 项目的通行证。&lt;/p&gt;
&lt;p&gt;在向几位学长简要请教后，我迅速做出了决定——选择加入周三平老师的课题组，作为自己科研生涯的起点。&lt;/p&gt;
&lt;p&gt;讲真，这个选择既是理性，也带着一丝赌性。当时的我其实曾对计算机图形学非常感兴趣，曾抽空看过 Games101，但现实不允许我将一切兴趣逐一追逐。我必须选择那个既有趣、又具发展潜力的方向。而在西交人工智能学院中，图形学研究资源有限，而我对计算机学院的了解也不够，因此选择了放弃。&lt;/p&gt;
&lt;p&gt;经过学长推荐，我挑选了人机所中几位有潜力的导师，并发去了邮件。周老师迅速回复了我，而另一位老师则音讯全无（后来有同学在班主任搭桥后加入该课题组，结果却是整天做标注数据的杂务，不禁让我暗自庆幸自己选对了方向）。&lt;/p&gt;
&lt;p&gt;其实那时的我仍然忐忑，不知自己是否有资格“正式科研”。所以等到奖学金排名公布、自信多了一分后，我才鼓起勇气发出邮件——那已是十月初了。&lt;/p&gt;
&lt;p&gt;周老师约我面谈，我们简短交流了兴趣方向，他告诉我，目前组里并未主攻我感兴趣的图像生成领域，但可以与两位大三的学长一起参与医学影像的半监督学习课题。虽不是我最感兴趣的领域，但我还是欣然接受了。老师随后发给我一篇论文，安排我与两位师兄组队阅读、讨论。&lt;/p&gt;
&lt;h2&gt;RM 组长&lt;/h2&gt;
&lt;p&gt;科研的线索暂时告一段落，让我们把镜头切回到 RM 这边——一条早已交织进我大学主线的路径。&lt;/p&gt;
&lt;p&gt;如同上一章所述，我在大一成功转正后，成为了视觉组组长，也承担了新一届的招新与培训工作。关于比赛本身的部分已另作详述，在这里，我更想讲讲与比赛无关的“人”的故事。&lt;/p&gt;
&lt;p&gt;为了提高培训效率，我不再采用以往那种“知识点填鸭+任务应试”的方式，而是用心准备了一整套连贯的视频课程，将基础知识与实际项目结合，用更系统的方式推进训练。&lt;/p&gt;
&lt;p&gt;与此同时，我一如既往地活跃在新生群里。在那段时间里，我加了不少新同学的好友，也和几位后来在他们年级中影响力不小的人物聊过天，借机宣传 RM，并招募他们加入视觉组。&lt;/p&gt;
&lt;p&gt;这一届的招新，我承认是“偏温和”的。考核压力不大，最终留下的人不少，甚至有部分后来缺乏持续任务导致热情下降，但这已是后话。&lt;/p&gt;
&lt;p&gt;其中印象最深的是两位成员：同级的 LXW 和下一届的学弟 QZZ。&lt;/p&gt;
&lt;p&gt;QZZ 是我迄今为止在西交遇到最具科研天赋的学弟。在他大一下时便加入课题组，参与多个项目，科研能力突飞猛进。只可惜他在“主动争取主导权”这一点上略显保守，导致至今仍未完全展开属于自己的主线工作。在西交这种节奏里，做出成果已属不易，想要“发表”更是难上加难，假如缺乏自我推动力，这样的才华可能真的会被埋没。&lt;/p&gt;
&lt;p&gt;至于 LXW，他是一位具备工程能力、热爱技术但缺乏学术野心的朋友。我推荐他来 RM 参加比赛，希望通过技术积累帮他获得一些加分。在我心中他一直是“可以信赖的朋友之一”，但人际交往上仍有些小摩擦：他偶尔会不打招呼地用我的电脑、吃我的零食，这些虽非大错，但却让我心里有些别扭。人与人之间总是这样，有些裂缝，虽不会断裂，但却始终存在。&lt;/p&gt;
&lt;p&gt;我在社团里一向不是那种“强势领导者”的形象，而更像一个“亲民”的协调者。这样的风格虽然有助于营造融洽氛围，但也使得一些同学在完成任务上的积极性不高。不过最终，视觉组的整体工作还是顺利推进了，大多数目标都达成了，尽管表现未及预期，但至少，我尽了全力。&lt;/p&gt;
&lt;h2&gt;科研之二&lt;/h2&gt;
&lt;p&gt;回到科研这条主线。大二这一年，除了课程之外，我的生活大致被两件事占据——RM 以及科研。社交圈开始有意识地收缩，大部分时间都只与乐小姐相处，其余交流多发生在绿群里，渐渐构建起一种“数字社交”的舒适区。&lt;/p&gt;
&lt;p&gt;周老师给我的第一篇论文是 MCF（Mutual Correction Framework for Semi-Supervised Medical Image Segmentation）。尽管此前我已有一些项目经验，但这却是我第一次正式参与科研工作。收到论文的第二天，我便下载数据集，开始复现实验流程，熟悉代码结构，也尝试推理其背后的思路。&lt;/p&gt;
&lt;p&gt;MCF 主要借由两个模型之间伪标签差异来实现半监督训练，而这套机制最棘手的问题，就是“差异性”的消失。两个模型在共同的损失函数约束下，很容易趋同，从而让整个半监督体系失效。&lt;/p&gt;
&lt;p&gt;我当时直觉地想到一个方法——引入 Mean Teacher 框架。设计上，我保留了原有的双模型结构，额外附加两个教师模型，仅用于生成伪标签，参与推理但不更新参数。这种方法倒也有一丝可解释性，两个 Mean Teacher 就像大厦的风阻尼器，让协同训练框架下的两个模型不会过快趋同。实验效果一试便成，指标超过当前 SOTA，但问题也显而易见：方法过于工程堆叠，缺乏创新性，逻辑简单，难以撑起一篇顶会论文的框架。&lt;/p&gt;
&lt;p&gt;真正的转机，出现在某次组会上。我突发奇想，思考能否不依赖模型结构、损失函数，而以某种“外部属性”来打破性能的不均衡？MCF 中存在一个设计：在训练时评估两个模型在有监督数据上的表现，并让性能较好的模型担任“教师”，提供伪标签。这看似合理，实则有潜藏风险：假如模型结构天然存在差异——如性能强弱模型混搭——便会造成某一模型始终无法有效利用无监督数据，甚至逐步沦为“哑巴学生”。&lt;/p&gt;
&lt;p&gt;那么，是否存在一种不依赖模型复杂度，却能动态调节性能的方法？答案是“训练轮次”。我设计了一种交替训练的机制，让两个模型轮流更新，各自拥有不同程度的“成熟度”，实现了教师之间的平衡切换。这一框架被我命名为 Progressive Mean Teacher。&lt;/p&gt;
&lt;p&gt;在构建好基础框架后，我进行了首次完整实验，在公开数据集上直接超过了当前最优结果两个点。&lt;/p&gt;
&lt;p&gt;这距离我正式开始科研不过两周多一点，而距离 CVPR 截稿还有四周。&lt;/p&gt;
&lt;p&gt;我总是喜欢做很大的梦。于是，找老师沟通，决定投稿。&lt;/p&gt;
&lt;p&gt;论文初稿由我撰写，结构参考 MCF。开篇便对其方法进行反驳，继而提出我方改进，并列举三项贡献点。这个写法虽然沿袭了 MCF 的结构，但逻辑过于“攻击性”，缺乏顶层设计，老师光速否决。&lt;/p&gt;
&lt;p&gt;在老师的指点下，我开始重构论文思路。弃用“驳斥式”的写法，改为“顺势铺陈”：先提出领域问题，介绍已有方法，然后自然引出我们的设计。老师强调的写作准则之一便是——不要暴露你“思考的过程”，而要让方法看起来就像是“理所当然地这么做了”。这是一种自上而下的表达方式，尽管“虚构”，但它让科研故事更具“可信感”。&lt;/p&gt;
&lt;p&gt;随后，我进行了密集的实验补充，跑遍了主流方法，重测、对齐 batch size 与划分方式，并且将 MCF 的一些 trick 适配到我们的方法中。最终投出 CVPR。&lt;/p&gt;
&lt;p&gt;结果是：311，落选。&lt;/p&gt;
&lt;p&gt;审稿意见大致一致：一是怀疑我方法与 MCF 差异不足，二是质疑结果可信度。我虽采用统一评估方法，但因为复现使用了 MCF 的 K 折划分，而此前的多数工作采用的是单一划分，这无意间造成了显著的性能差距——一些模型在单一划分中夸张地超越了原文，在 K 折中却一落千丈。这使得我论文中的“公平比较”反而成为了“做低性能”的疑点。&lt;/p&gt;
&lt;p&gt;除此之外，还有一些书写层面的问题，比如模型命名残留、符号混乱等，导致整个工作难以服人。&lt;/p&gt;
&lt;p&gt;这次失败让我重新审视整个设计。一方面我不能再用自己的测试数据，必须用论文中报告的原始结果来对比；另一方面，两大辅助模块（MCF 的 trick）也必须重新设计，避免与 MCF 看起来过于相似；此外，论文写作也要进一步精炼，适应 ECCV 的风格。&lt;/p&gt;
&lt;p&gt;我开始对方法进行小幅重构，将精力集中于 Progressive 模块本身。两大辅助 trick 被弱化、重新包装。整体故事线调整为“强调结构稳定性与伪标签信任机制之间的动态权衡”，将“复杂叠加”变为“结构简约”。&lt;/p&gt;
&lt;p&gt;论文不断迭代，图表也由我主导绘制，但由于审美感一言难尽，后期图示多由老师请来的师兄师姐协助制作。在反复润色后，我们按时投出了 ECCV。&lt;/p&gt;
&lt;p&gt;投稿结束，我进入了一个等待与补偿的时期。&lt;/p&gt;
&lt;p&gt;虽然那时我已经拥有了“科研经历”，但因暂无正式发表，想申请外校实习，仍然缺乏底气。我只能静静等待 ECCV 的审稿结果。而在等待中，我也开始推进下一个项目。相比之下，那篇工作乏善可陈，是将一个“合理但平庸”的技巧包装成完整方法，难以引起太大兴趣。我虽然继续执行着实验、修改着代码，但整体状态确实趋于疲软，只是咬牙坚持。&lt;/p&gt;
&lt;p&gt;这段时间，我把更多精力投入文献阅读。多模态、具身智能、强化学习、GS……我读得很杂，也试图寻找下一段真正能让我兴奋的研究路径。&lt;/p&gt;
&lt;p&gt;老师说：“就算这篇也投不出去，将来还能转投 PR，价值仍在。”听完这句话，我放下了些许焦虑，给自己设定了一个较为现实的目标——在大三开始前，完成一篇真正属于自己的论文。&lt;/p&gt;
&lt;p&gt;这也是我选择科研路的第一个承诺。&lt;/p&gt;
&lt;h2&gt;平淡生活&lt;/h2&gt;
&lt;p&gt;科研之余，大二的日子过得并不热烈，甚至可以说颇为“平静”。&lt;/p&gt;
&lt;p&gt;乐小姐在大一下成功完成了专业分流，在我的鼓励与她自身的努力下，进入了她心仪的口腔医学专业。她所在的雁塔校区离主校区很远，坐车得二十分钟起跳。但相较之下，那里的地理位置反倒优越许多：附近就是小寨商圈，吃喝玩乐一应俱全，还有一座我常念叨的麦当劳——这是兴庆校区所无法享受的“奢侈”。&lt;/p&gt;
&lt;p&gt;于是我们踏上了“同城异地”的日常。我有时打车去找她，有时她来找我。我们在热气腾腾的海底捞里畅快吃肉，在齐齐哈尔烤肉店边吃边聊，后来这家店倒闭了，成了我西安饮食回忆中最大的遗憾之一。&lt;/p&gt;
&lt;p&gt;那段时间的生活可以说是岁月静好。CVPR 的结果尚未公布，我仍自信满满，口袋里有余钱，恋爱也甜蜜，课程不难，科研刚刚起步，一切看上去都稳步向前。&lt;/p&gt;
&lt;p&gt;与此同时，另一件事情悄然展开。&lt;/p&gt;
&lt;p&gt;由于不断有新生来向我请教课程选择、专业方向、学习规划等问题，我萌生了编写“西安交大生存指南”的想法。我希望它不仅是一份解惑手册，也是一份“干货+私货”的经验总结。第一版很快成稿，内容包含我对科研、竞赛、自学路径的初步理解。当时的判断也许不够成熟，但不少建议至今看来仍具价值。&lt;/p&gt;
&lt;p&gt;我也试图写一份 AI 自学指南，只可惜到现在仍未完工。琐事太多，心力难得安宁。但我留了一个空白的前言，默默祈祷 ECCV 有好结果，那样我就能以更坦然的姿态写下它。&lt;/p&gt;
&lt;h2&gt;大二下生活&lt;/h2&gt;
&lt;p&gt;寒假像是一场突然被允许的长梦，我难得有机会什么都不干。大概也是最后一次，能够如此心安理得地把日子交给“休息”这两个字。如今想来，那种能够“空白度日”的时光，已成奢侈。&lt;/p&gt;
&lt;p&gt;返校之后，生活重新上了轨道。QZZ 在我的“怂恿”下也开始科研，LXW 则参与了部分代码开发，虽然他提交的那段程序在我看来结构极其混乱，堪称屎山续写，但至少，它确实运行了。&lt;/p&gt;
&lt;p&gt;我的节奏没太多变化：白天在社团地下室看论文、改代码、测试模型，偶尔补点技术细节。绿群那边则成立了 AI 学组，由 YXJ 担任领头人。这件事我一直觉得有些迷惑：整个年级百分之八十的资料几乎都是我写的，而 YXJ 几乎无贡献，不知他为何“自然地”成了主理人。不过也罢，形式这种东西，从不妨碍我继续做事。&lt;/p&gt;
&lt;p&gt;我不太在乎名义。倒也曾怀抱一丝希望，以为这些聚集起来的同龄人能共同碰撞出一些新的东西。事实却是，我们年级的响应寥寥，倒是下一届学弟学妹中，出现了不少值得期待的面孔。&lt;/p&gt;
&lt;p&gt;接下来的生活渐渐趋于平稳。课程不重，比赛内容已在 RM 回忆录中详细记录。进入期末前，我忽然又燃起了写作的冲动：开始构思“西安交大生存指南”，并着手搭建个人博客，打算将这几年踩过的坑与经验写下来，供后来者参考。&lt;/p&gt;
&lt;p&gt;就在这个过程中，ECCV 开分。&lt;/p&gt;
&lt;p&gt;443。这是我收到的分数。&lt;/p&gt;
&lt;p&gt;一时间，我有些发愣。原本只期盼一个稍正面的评分来抚慰自己，但万万没想到，是这种几乎一边倒的好评。&lt;/p&gt;
&lt;p&gt;一切情绪在一瞬间翻转。&lt;/p&gt;
&lt;p&gt;此前 CVPR 的落选还历历在目，审稿人的冷漠、方法的怀疑、细节的否定，仿佛才刚刚过去。而现在，三个审稿人全给出正面意见，问题也集中在“写作需清晰”“细节需补充”之类的善意建议。这是一次彻底意义上的翻盘。&lt;/p&gt;
&lt;p&gt;我立刻草拟了长长的 rebuttal，计划逐条回应所有问题，也试图再推一推三分审稿人的评分。第二天，和老师开会定调，目标清晰：稳住 4 分，争取拉 3 分上岸。&lt;/p&gt;
&lt;p&gt;老师还找来了两位高年级师兄师姐帮忙。他们图像表达极具专业性，而我当时做图水平可以说一塌糊涂。在他们的帮助下，一天时间内，原本混乱的结构图被重新设计，变得具备了“顶会论文”的范儿。剩下的则是我擅长的部分：润色文字、删减冗余、统一术语，确保表达精炼有力。&lt;/p&gt;
&lt;p&gt;一切完成后，我们提交了 rebuttal，刚好赶上了期末复习周的开始。&lt;/p&gt;
&lt;p&gt;说实话，这份高分带来的希望，几乎成了一种折磨。我原本对 ECCV 已经不抱幻想，而如今，又被扯回焦虑之中：我开始每天刷知乎，查分布、看贴子，计算有多少人比我高，又有多少人被“意外拒掉”。&lt;/p&gt;
&lt;p&gt;我设想了各种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果我的领域整体分数偏高呢？&lt;/li&gt;
&lt;li&gt;如果审稿人互看评语后产生动摇呢？&lt;/li&gt;
&lt;li&gt;如果 meta reviewer 单方面不喜欢我呢？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一切皆有可能。我甚至和朋友打赌：如果中稿，就拍二十张认真设计的女装照——不只是自嘲，更像是给这段压抑旅程的一种仪式。&lt;/p&gt;
&lt;p&gt;也是在这一段焦灼等待中，我完善了《西安交大生存指南》，将大一大二的经验、思考、判断一一写下。虽然那时我还未正式发表，但我仍觉得这些内容值得留下。就像我在前言中所写的那样：无论结局如何，它们都是真实踩过的坑，值得被分享。&lt;/p&gt;
&lt;h2&gt;轻舟已过万重山&lt;/h2&gt;
&lt;p&gt;我记得那天的每一个细节。&lt;/p&gt;
&lt;p&gt;ECCV 原定凌晨出结果，我打算熬夜守着。然而大约傍晚六七点，投稿群突然传来消息，说结果提前发布。我迅速登陆，颤抖着输入编号“8535”，却没有任何结果。&lt;/p&gt;
&lt;p&gt;我怔住了，心脏仿佛漏跳一拍。&lt;/p&gt;
&lt;p&gt;随后我发现自己输错了编号。改正后再次检索，页面上赫然出现一个数字：“1”。&lt;/p&gt;
&lt;p&gt;那一刻，我仿佛被抽空了全身的力气。石头从肩头落地，我缓缓瘫坐在椅子上，久久说不出话来。然后转头对旁边的 GYT 说：“我中稿了。”&lt;/p&gt;
&lt;p&gt;接着才是迟来的情绪涌现。&lt;/p&gt;
&lt;p&gt;我没有哭，只是哽咽了一下，情绪卡在嗓子眼。过去的一年里走得太苦、太密、太长，早已耗尽眼泪。我开始一一报喜，发说说，告诉老师，写博客前言，像是终于可以堂堂正正地讲述这段旅程。&lt;/p&gt;
&lt;p&gt;那句我写在 QQ 中的话我记得很清楚：“轻舟已过万重山。”&lt;/p&gt;
&lt;p&gt;这一次的成功虽不如我少年时梦想中的“早早中稿、提前出发”来得完美，但依然足够耀眼。原计划是大二投两篇论文，一篇在上，一篇在下，然后手握成果去寻找实习机会。但事实是，我付出了整整一年，只换来一篇。&lt;/p&gt;
&lt;p&gt;无论如何，这篇，却真的拿到了。&lt;/p&gt;
&lt;h2&gt;绿群往事&lt;/h2&gt;
&lt;p&gt;说起绿群，那是我大一下就已接触的平台。它既是一个保研交流群，也是一个充满秩序与混乱交错的数字社区。&lt;/p&gt;
&lt;p&gt;因为我早期投入 RM，也养成了通宵和“全部工作电脑化”的生活节奏，我极少在寝室出现，和舍友交集不多。反而在网上结识了许多朋友，大多数是比我大一两届的，也有同龄人。&lt;/p&gt;
&lt;p&gt;我常在群里“卖萌”——比如“喵”口癖和女装照，也常分享学习经验与科研思路，所以在绿群里还算“人尽皆知”。&lt;/p&gt;
&lt;p&gt;这一年，我在群里做了不少事。&lt;/p&gt;
&lt;p&gt;最初是在观察中产生的灵感。每年都有无数同学焦虑自己能否保研，时常会在群里贴出自己的情况，寻求他人建议。这种“保研定位”本质上意义不大，却能带来心理安慰。然而由于消息太多，很多人很难被看见。&lt;/p&gt;
&lt;p&gt;于是我做了一个表格，用腾讯文档搭建最早的“保研定位表”。每个人可以匿名填入自己的学校、专业、排名、竞赛与科研情况，然后让其他群友进行定位打分。推出之后火速传播，数千人使用，甚至被其他保研群模仿。&lt;/p&gt;
&lt;p&gt;但好景不长。中介发现这个表格开始爬数据并滥用，最后还举报了文件，导致我不得不关闭开放权限，转用 Google 表格。但 Google 表单又因权限限制，导致填写率大减，热度渐退。&lt;/p&gt;
&lt;p&gt;尽管如此，它仍然是我在绿群事务中第一次“破圈”的产出。&lt;/p&gt;
&lt;p&gt;后来我又做了另一个工具——夏令营收录站，灵感来自于科研圈的 ccfddl 网站。我一口气肝了一个晚上的代码，做出原型，然后结合 GitHub Actions 实现数据自动同步。任何人只需发一个 issue，夏令营信息就能自动更新。&lt;/p&gt;
&lt;p&gt;这个工具长时间为数千保研生提供信息支持，获得了上百个 star，也让我在组织内部成为了 admin 与 GitHub owner。那段时间，我发群公告的次数越来越多，也算是逐渐承担起一部分维护者的责任。&lt;/p&gt;
&lt;h2&gt;上海实习&lt;/h2&gt;
&lt;p&gt;暑假的前半段，我依旧陪 RM 队伍南下深圳，征战全国赛。我们再次打入十六强，成绩虽无惊喜，但也算无愧于心。一个月的时间一晃而过，像是大二这座山峰的缓坡，走到这里，真正值得铭记的事，才刚刚开始。&lt;/p&gt;
&lt;p&gt;ECCV 中稿之后，我决定尽早开启实习，在实践中继续积累经验。经过绿群前辈们的指点，以及自己的长期关注，我毫不犹豫地投向了早已心仪的目的地——上海人工智能实验室（Shanghai AI Lab）。&lt;/p&gt;
&lt;p&gt;从大一开始，我便听说过这个地方：顶尖的科研氛围、良好的待遇、丰富的资源，仿佛是本科阶段遥不可及的科研乐土。真正到了选择时，我把目标聚焦在其中几个具身智能与多模态方向的小组。多模态是我此前读得最多的方向，也很吸引我；不过相比之下，具身智能那种交叉性、系统性、实验性更强的研究氛围，反倒愈发吸引我。&lt;/p&gt;
&lt;p&gt;我联系了 OpenRobotLab，投出简历后，很快收到回复。面试安排得干脆利落，流程也不复杂：简单介绍自己过往项目，阐述科研理解，分享自己之前的论文。没有被卡公式，也没有刷代码——或许 CV 上那篇 ECCV 已为我提前减轻了很多负担。&lt;/p&gt;
&lt;p&gt;RM 比赛结束后，我立刻北上，租好房子，开始一段新的科研生活。&lt;/p&gt;
&lt;p&gt;初到上海，我没有不适。一个人换城市、租房、上班，对我来说早已不再陌生。实验室氛围如传言那般自由高效，我迅速投入工作，带着那台大学买的沉重外星人笔记本，每天背着上下班。电脑电池几乎报废，续航不足一小时，但我仍坚持完成了几项工作：复现了一篇 prompt-based 但无现成代码的论文，熟悉了 Isaac Sim 平台，并开始了第一个具身智能方向的实验构建。&lt;/p&gt;
&lt;p&gt;这段实习期间，我结识了许多有趣的人。伦哥是我最直接的 mentor，lab 里的豪哥、洋哥、haifeng、xinyi 等人也都对我极为照顾。更有趣的是，绿群里许多同龄人也正好在那儿线下实习，我们甚至组织了几次线下聚餐，那是我罕有的“从线上走进现实”的社交尝试。&lt;/p&gt;
&lt;p&gt;当然，生活并非只有科研。&lt;/p&gt;
&lt;p&gt;在上海，我最大的感受就是——饭太贵了。西安的食堂，哪怕校外馆子也算便宜；北京虽贵，但大多时候是家里人请客。而在这里，动辄三十起步的工作日午餐让我不得不调整花销习惯。但即便如此，我依旧心甘情愿留在公司加班，只为多跑一次实验，或多 debug 几个小时。&lt;/p&gt;
&lt;p&gt;实习很快结束，我与 mentor 沟通，决定远程继续项目。设置好远程工作站后，我拖着箱子踏上归程，坐上了返回西安的高铁。那天的落日格外温柔，像是为这段短暂却厚重的科研之旅盖上一张温软的毯子。&lt;/p&gt;
&lt;h2&gt;尾声&lt;/h2&gt;
&lt;p&gt;与喧嚣而五光十色的大一相比，大二显得沉静许多。&lt;/p&gt;
&lt;p&gt;没有那么多社团活动、社交活动、穿梭在各个兴趣圈的热闹。我的世界被压缩成两块：一是科研，一是 RM。再往外走，就是绿群这样广阔而遥远的虚拟社交。而在现实中，我几乎只与一个人密切往来——乐小姐，她是我这一年最稳定的锚点。&lt;/p&gt;
&lt;p&gt;如果说大一是“首次触碰自由”的踉跄起跑，那么大二便是“朝着目标前进”的稳定步伐。科研从尝试到成果，从困惑到方向；社团从参与到引领；自学从阅读到分享；社交从广撒网到精准维系。我变得更笃定，也更独立。&lt;/p&gt;
&lt;p&gt;当然，也不是没有遗憾。&lt;/p&gt;
&lt;p&gt;我曾期望大二能投出两篇论文，走在所有人前头，提前出发，申请外实，掌握主动。但事实上，我只是发表了一篇工作，另一篇也仍在修改中；我也没有以“科研少年天才”的身份横空出世，而是依旧在泥泞中摸索，在代码堆里精疲力尽，在深夜无数次推翻思路又重来。&lt;/p&gt;
&lt;p&gt;但那又如何呢？&lt;/p&gt;
&lt;p&gt;从时间轴来看，我已比许多同龄人走得更远。我不再焦虑别人的排名、课程成绩、实验记录，不再将内卷视作天命，而是将它当作自己定义的一场漫长竞赛。前方没有标准答案，但我已有了方向。&lt;/p&gt;
&lt;p&gt;如果要用一句话来概括我的大二，或许是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“我走得不快，但从未停下。”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这并不是一段“少年得志”的轰烈故事，而是一段实实在在的攀登，一次次从怀疑、失落、疲惫中爬起来的经历。它不是终点，也不是高潮，而是一座山——我已经翻过它。&lt;/p&gt;
&lt;p&gt;而眼前，还有更多山峦，等着我一一走过。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Uni-Memoir2-zh.webp"/><enclosure url="https://picr2.axi404.top/Uni-Memoir2-zh.webp"/></item><item><title>周记 Week26</title><link>https://axi404.github.io/blog/week-26</link><guid isPermaLink="true">https://axi404.github.io/blog/week-26</guid><description>2024-12-23 ~ 2024-12-29.</description><pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;早安，周记，我是你们的主持人阿汐。这周的通宵是满打满算整整七个，但是两个考试也来了，我看你们全要泡汤。&lt;/p&gt;
&lt;h2&gt;考试&lt;/h2&gt;
&lt;p&gt;这周简单来说考了两次试，一个是开卷考试，另外一个是选修课考试，所以说压力都不是很大。选修课考试虽然是闭卷，但是老师处了一份复习大纲，基本就是按照题目的形式出的，我也就考前用 GPT 处理了一下，全部的题就没有问题了，也没有遇到类似于幻觉的情况。开卷考试反而是复习两天，但是考场上翻书依然很急，最后把试卷该写的都写上去了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;剩下的时间就是在科研了，项目一直在推进，在这周的结尾，我也开始接触了训练模型，以前做的工作马上就要成为一个闭环的体系，发表之后也会在这里详细说一下。&lt;/p&gt;
&lt;p&gt;实际上这个星期也没什么可以说的事情，但是还是要在这里忏悔一下，说好的要做很多的进步，比如说我希望以天为尺度来让自己比昨天更好，至少看一篇论文，或者学习点新的编程知识，记录下来一些感想，但是一方面因为考试的事情，一方面在科研下面花了很多时间（这部分主要说的是代码的时间，毕竟现在有一个需要推进的课题，因此已经有很长时间没有去主动看论文了），但是反正因为这种原因，之前说好的事情都没有兑现。&lt;/p&gt;
&lt;p&gt;正好 2024 年也结束了，应该会考虑出一个年终总结，以周为单位没什么进展，但是这一年还是比以前强了很多，可以说有不少的高光时刻可以回放，等我处理完了考试就会开始写，穿插在科研之中，算是是一种调剂。暂时先这样吧，而且也确实因为科研有一些保密的属性，导致一直以来的周记都没什么可以分享的东西，这些事情都会在年终总结里面提到，并且争取为新的一年定下一些计划。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-26-zh.webp"/><enclosure url="https://picr2.axi404.top/week-26-zh.webp"/></item><item><title>周记 Week25</title><link>https://axi404.github.io/blog/week-25</link><guid isPermaLink="true">https://axi404.github.io/blog/week-25</guid><description>2024-12-16 ~ 2024-12-22.</description><pubDate>Tue, 31 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说，又是很久之后的一次周记，主要有几件事情可以记录，但是和之前相比还是大差不差。&lt;/p&gt;
&lt;h2&gt;考试&lt;/h2&gt;
&lt;p&gt;一方面是考试，计算机视觉的考试正式结束，这门考试我复习的应该算还行，考试里面的大多数问题我都能立刻写出来。不过不得不说的是，这里面有很多对于一些人来说喜闻乐见的背诵问题，答案事实上这些东西并不应该是人工智能这门学科考察的本质（比如说问 RCNN 与 Fast- RCNN 的区别）。但是话也说到这里，尽管我可以这么不负责的说，但是毕竟是本科的课程，还需要考虑大多数的同学，一些背诵和一些计算，自然是最为恰当的，而进一步的科研话题，就交给科研的人去做吧。&lt;/p&gt;
&lt;p&gt;下周的考试有两门，但是都不是很难。一个是选修课，基本上就是按照老师发的所谓的复习大纲复习一下就好，毕竟那个复习大纲已经长得像题目一样了，很难不做出相应的怀疑。直到这周末结束，我才把这个大纲转化为其题目对应的答案，这个过程也很简单，直接交给 GPT 输入一遍就结束。其实这也进一步引发了一些思考，伴随着模型的幻觉越来越少，我是真心感觉绝大多数的文科专业没有了存在的必要。一门考试则是一个开卷考试，感觉在考前看一遍就好了，类似于做一遍键值对的索引，之后在考试上翻书也会快一些。&lt;/p&gt;
&lt;p&gt;剩下的考试就要到下下周和下下下周，各有一个比较难的考试，但是也不会带来很多的困扰吧，希望这方面很顺利。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;同时在科研方面也取得了一些进展，大概就是代码写得很顺利，都在按照之前的规划完成，并且根据目前的展望来看，距离一切结束已经快了，到时候也可以先把论文发出去，后续再去把代码整理一下。&lt;/p&gt;
&lt;p&gt;可以说考试和科研就是现在的两个主基调，除此之外就是给乐小姐买了裙子，然后和她度过了开心的很长时间。至于其他的事情，走一步看一步吧。我还是非常喜欢科研的，所以迫切的希望占据我时间的考试这一项消失掉。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-25-zh.webp"/><enclosure url="https://picr2.axi404.top/week-25-zh.webp"/></item><item><title>Anygrasp 踩坑</title><link>https://axi404.github.io/blog/anygrasp</link><guid isPermaLink="true">https://axi404.github.io/blog/anygrasp</guid><description>在 Ubuntu 22.04 以及 CUDA 12.1 下安装 Anygrasp。</description><pubDate>Sat, 21 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Aside } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;p&gt;最近正在配置 AnyGrasp，在这里记录一下遇到的问题。我的环境为 Ubuntu 22.04, CUDA 12.1, cudnn 9.3.0。&lt;/p&gt;
&lt;h2&gt;基础配置&lt;/h2&gt;
&lt;p&gt;首先先给出 AnyGrasp 的 Github 仓库链接：&lt;a href=&quot;https://github.com/graspnet/anygrasp_sdk&quot;&gt;https://github.com/graspnet/anygrasp_sdk&lt;/a&gt;，其中的 &lt;code&gt;Installation&lt;/code&gt; 部分给出了简略的安装步骤，但是因为其依赖的 MinkowskiEngine 已经年久失修，所以需要一些额外的操作。&lt;/p&gt;
&lt;p&gt;先配置一个 conda 环境：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n anygrasp python=3.10
conda install openblas-devel -c anaconda
pip install torch &apos;numpy&amp;#x3C;1.23&apos; ninja
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;MinkowskiEngine&lt;/h2&gt;
&lt;p&gt;接下来可以开始配置第一步，也就是 MinkowskiEngine：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/NVIDIA/MinkowskiEngine.git
cd MinkowskiEngine
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据经验来说，需要配置以下的环境变量：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export CXX=c++
export CUDA_HOME=/usr/local/cuda-12.1
export MAX_JOBS=2
export SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中后两者，&lt;code&gt;MAX_JOBS&lt;/code&gt; 是 CUDA: Out of memory 的 Issue，&lt;code&gt;SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True&lt;/code&gt; 是 sklearn 过期的 Issue。假如说之后执行安装操作：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python setup.py install --blas_include_dirs=${CONDA_PREFIX}/include --blas=openblas 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先可能存在的报错，核心问题为 &lt;code&gt;error: namespace &quot;thrust&quot; has no member &quot;device&quot;&lt;/code&gt;，本质上还是年久失修，和 CUDA 12.X 不兼容了。&lt;/p&gt;
&lt;p&gt;根据仓库里的 &lt;a href=&quot;https://github.com/NVIDIA/MinkowskiEngine/issues/543&quot;&gt;Issue#543&lt;/a&gt; 可以找到对于我适用的方法，即在四个不同的文件中添加 &lt;code&gt;#include&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;#x3C;thrust/execution_policy.h&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;#x3C;thrust/unique.h&gt;
#include &amp;#x3C;thrust/remove.h&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;#x3C;thrust/execution_policy.h&gt;
#include &amp;#x3C;thrust/reduce.h&gt; 
#include &amp;#x3C;thrust/sort.h&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;#x3C;thrust/execution_policy.h&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后可能会有报错 &lt;code&gt;ModuleNotFoundError: No module named &apos;distutils.msvccompiler&apos;&lt;/code&gt;，那么执行 &lt;code&gt;pip install &quot;setuptools &amp;#x3C;65&quot;&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;之后再次安装，也会有报错：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;Traceback (most recent call last):
  File &quot;/home/gaoning/miniconda3/envs/anygrasp/lib/python3.10/site-packages/torch/utils/cpp_extension.py&quot;, line 2105, in _run_ninja_build
    subprocess.run(
  File &quot;/home/gaoning/miniconda3/envs/anygrasp/lib/python3.10/subprocess.py&quot;, line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command &apos;[&apos;ninja&apos;, &apos;-v&apos;, &apos;-j&apos;, &apos;2&apos;]&apos; returned non-zero exit status 1.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以编辑 &lt;code&gt;setup.py&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;setup(
    name=&quot;MinkowskiEngine&quot;,
    version=find_version(&quot;MinkowskiEngine&quot;, &quot;__init__.py&quot;),
    install_requires=[&quot;torch&quot;, &quot;numpy&quot;],
    packages=[&quot;MinkowskiEngine&quot;, &quot;MinkowskiEngine.utils&quot;, &quot;MinkowskiEngine.modules&quot;],
    package_dir={&quot;MinkowskiEngine&quot;: &quot;./MinkowskiEngine&quot;},
    ext_modules=ext_modules,
    include_dirs=[str(SRC_PATH), str(SRC_PATH / &quot;3rdparty&quot;), *include_dirs],
    cmdclass={&quot;build_ext&quot;: BuildExtension.with_options(use_ninja=False)},
    author=&quot;Christopher Choy&quot;,
    author_email=&quot;chrischoy@ai.stanford.edu&quot;,
    ...,
)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 &lt;code&gt;use_ninja&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt;，之后再次执行，就没问题了。&lt;/p&gt;
&lt;p&gt;Noting that，在 CUDA 12.4 安装的时候，出现了额外的报错，其内容为 &lt;code&gt;error: no instance of overloaded funcntion &quot;std::__shared_ptr&amp;#x3C;_Tp&gt;::_M_enable_shared_from this ...&quot;&lt;/code&gt;，一共会唤起若干的报错，但是本身都是围绕 __shared_ptr 以及 __to_address 的，具体为 overload。&lt;/p&gt;
&lt;p&gt;这个问题需要修改 &lt;code&gt;/usr/include/c++/12/bits/shared_ptr_base.h&lt;/code&gt;（我是这个，Issue 中有人说是别的，为 &lt;code&gt;ptr_traits.h&lt;/code&gt;），搜索并替换：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;auto __raw = __to_address(__r.get()); // [!code --]
auto __raw = std::__to_address(__r.get()); // [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后还可能出现另一个错，是 &lt;code&gt;ld: cannot find -lopenblas: No such file or directory; collect2: error: ld returned 1 exit status&lt;/code&gt;，这个则在安装了 &lt;code&gt;conda install openblas-devel -c anaconda&lt;/code&gt; 之后，需要进行一次 cp，对于我来说，这个指令是 &lt;code&gt;cp /ssd/gaoning/miniconda3/envs/anygrasp/lib/libopenblas.so* /ssd/gaoning/miniconda3/envs/anygrasp/lib/python3.10/site-packages/torch/lib/.&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;之后照常安装即可。&lt;/p&gt;
&lt;h2&gt;AnyGrasp&lt;/h2&gt;
&lt;p&gt;之后安装 &lt;code&gt;anygrasp_sdk&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/graspnet/anygrasp_sdk.git
cd anygrasp_sdk
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;PointNet2&lt;/h2&gt;
&lt;p&gt;之后安装 &lt;code&gt;pointnet2&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd pointnet2
python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;值得一提的是，在安装 pointnet2 的过程中依然可能出现 &lt;code&gt;Command &apos;[&apos;ninja&apos;, &apos;-v&apos;, &apos;-j&apos;, &apos;2&apos;]&apos;&lt;/code&gt; 的报错，解决方法同上，依然是修改 &lt;code&gt;setup.py&lt;/code&gt; 中的 &lt;code&gt;setup()&lt;/code&gt; 函数的传参。&lt;/p&gt;
&lt;p&gt;在这一过程中还可能出现一个比较罕见的问题：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;gcc: fatal error: cannot execute ‘cc1plus’: execvp: No such file or directory
compilation terminated.
error: command &apos;/usr/bin/gcc&apos; failed with exit code 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一般来说直接 &lt;code&gt;sudo apt install build-essential&lt;/code&gt; 就已经可以了，但是我的问题不止于此，因为系统里的 &lt;code&gt;gcc&lt;/code&gt; 和 &lt;code&gt;g++&lt;/code&gt; 都没问题。检查之后发现，这是因为 pointnet2 的编译过程中涉及了使用 &lt;code&gt;gcc&lt;/code&gt; 并且调用 &lt;code&gt;g++&lt;/code&gt; 的操作，而 &lt;code&gt;gcc&lt;/code&gt; 大概率调用同版本的 &lt;code&gt;g++&lt;/code&gt;，可能是因为 &lt;code&gt;gcc --version&lt;/code&gt; 和 &lt;code&gt;g++ --version&lt;/code&gt; 两个的版本不一样，所以就导致了这个问题。使用指定版本的 &lt;code&gt;sudo apt install&lt;/code&gt; 进行重新安装（版本在 &lt;code&gt;Ubuntu 22.04&lt;/code&gt; 可以是 12）：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install gcc-12 g++-12
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后正常安装即可。&lt;/p&gt;
&lt;h2&gt;License Checker&lt;/h2&gt;
&lt;p&gt;最后是使用 AnyGrasp 需要 Key，而这个 Key 需要生成，因此需要使用 &lt;code&gt;./license_checker -f&lt;/code&gt;，而因为 Ubuntu 22.04，这个也会报错，一个是缺少 &lt;code&gt;libcrypto.so.1.1&lt;/code&gt;，一个是 &lt;code&gt;sh: 1: ifconfig: not found&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# to solve libcrypto.so.1.1 issue
find / -name libcrypto.so.1.1
# for example found a libcrypto.so.1.1 from cuda
sudo ln -s /usr/local/cuda-12.1/nsight-systems-2023.1.2/host-linux-x64/libcrypto.so.1.1 /usr/lib/libcrypto.so.1.1
# to solve ifconfig issue
sudo apt install net-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同时运行 &lt;code&gt;./lincense_checker -f&lt;/code&gt; 之后输出的机器码，大概率最后以 &lt;code&gt;%&lt;/code&gt; 结尾，这个因为在输出的时候没有添加换行提示符（Python 的 &lt;code&gt;print()&lt;/code&gt; 自带换行符，而写这个程序的编程语言有可能不带），提交不包含 &lt;code&gt;%&lt;/code&gt; 的内容到申请表中即可，另，邮箱需要使用教育邮箱。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/anygrasp-zh.webp"/><enclosure url="https://picr2.axi404.top/anygrasp-zh.webp"/></item><item><title>奇奇怪怪的 Bug 集散地</title><link>https://axi404.github.io/blog/strange-bugs</link><guid isPermaLink="true">https://axi404.github.io/blog/strange-bugs</guid><description>平时遇到的奇怪代码问题，记录并整理。</description><pubDate>Fri, 20 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;平时遇到一些奇怪的代码问题，记录并整理，内容如下。&lt;/p&gt;
&lt;h2&gt;博客渲染超时&lt;/h2&gt;
&lt;p&gt;在 Hugo 中，如果博客文章较多，渲染时间会非常长，导致渲染超时。具体考量可能是因为担心无限递归之类的，hugo 使用了粗暴的解决方法，超时就中断并且报错。所以解决方法也很简单，修改 &lt;code&gt;config.toml&lt;/code&gt; 文件中的 &lt;code&gt;timeout&lt;/code&gt; 配置项，增加渲染超时时间，单位貌似是毫秒。之前一直没有看 Github 详细报错，之前又出现过 Github Actions 瘫痪，我还以为又出现了，re-run 之后也就好了，估计是因为当初体量卡在临界点上，现在彻底超时了，也就发现了这个问题。&lt;/p&gt;
&lt;h2&gt;GPT API 调用显示 Unknown scheme for proxy URL&lt;/h2&gt;
&lt;p&gt;在使用 GPT API 的时候，正常的发送 request，显示：&lt;/p&gt;
&lt;p&gt;但是此时我已经将全部的代理关闭了，更不要说后续要需要开启代理才可以连接 &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;，经过检查之后，大概是因为自己的网络环境太过于乱七八糟：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;env | grep -i proxy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以查看到究竟是哪个环境出现了问题，之后正常使用 bash 或者 python 程序都可以进行修改，本人是发现 &lt;code&gt;ALL_PROXY&lt;/code&gt; 出现问题：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;unset ALL_PROXY
unset all_proxy

env | grep -i proxy

export ALL_PROXY=&quot;http://127.0.0.1:7890&quot;
export all_proxy=&quot;http://127.0.0.1:7890&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
os.environ[&apos;ALL_PROXY&apos;] = &apos;http://127.0.0.1:7890&apos;
os.environ[&apos;all_proxy&apos;] = &apos;http://127.0.0.1:7890&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重点其实在于找到哪个有问题，并且进行覆盖，&lt;code&gt;unset&lt;/code&gt; 是严谨起见，其实无所谓。&lt;/p&gt;
&lt;h2&gt;EndeavorOS 安装导致的多系统不兼容问题&lt;/h2&gt;
&lt;p&gt;在此之后我有尝试过使用 EndeavorOS，出于想要使用 ArchLinux 的想法，当然，在这个过程中还是出现了一些问题。我的 Ubuntu 22.04 是在 EndeavorOS 之前安装的，里面包含我目前进行科研所需要使用的一切环境以及内容，而 Arch 只是作为自己的日常使用，我为此删除了之前安装的 Ubuntu 20.04，但是也因此导致了不少的问题。&lt;/p&gt;
&lt;p&gt;首先就是在安装了 EndeavorOS 之后，Grub 无法找到 Ubuntu 的引导，这自然是因为 EndeavorOS 的引导替换了我本来使用的 Ubuntu 引导，但是按理来说不会出现这个问题，因为不同的系统我都是分配了不同的 EFI 分区的，就算有的安装会刷这个分区，在我的电脑里面按理来说也不会出现问题才对。&lt;/p&gt;
&lt;p&gt;经过了检查之后发现是一个比较简单的问题，需要更新 GRUB 以检测所有操作系统：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -S os-prober
sudo vim /etc/default/grub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;并且修改其中的 &lt;code&gt;GRUB_ENABLE_OS_PROBER=true&lt;/code&gt;，并再次更新 &lt;code&gt;sudo grub-mkconfig -o /boot/grub/grub.cfg&lt;/code&gt;，就没问题了。&lt;/p&gt;
&lt;p&gt;另一个问题在于发现重启之后进入 Ubuntu 的时候总是会十分的缓慢，这个检查了一下之后发现是因为我之前把 Ubuntu 20.04 使用的 swap 给格式化成 EndeavorOS 使用的 swap 了，因此 UUID 变了，每次启动的时候会为了寻找 swap 而等好久，需要进行修改，在 Ubuntu 中进行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo blkid
sudo vim /etc/fstab
sudo update-initramfs -u -k all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 vim 的部分可以在其中找到自己的 swap 分区的 UUID 并且进行修改，而后使用 &lt;code&gt;update-initramfs&lt;/code&gt; 来更新全部的内核。&lt;/p&gt;
&lt;h2&gt;Windows 新电脑配置&lt;/h2&gt;
&lt;p&gt;最近换了新电脑，于是在新电脑里面配置了 Git 以及 Github，还是按照我自己一贯的方法，详情见 &lt;a href=&quot;https://survivexjtu.github.io/%E5%89%8D%E8%A8%80/%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97.html&quot;&gt;西安交大生存指南贡献指南&lt;/a&gt;，但是出现了一些 Bugs。&lt;/p&gt;
&lt;p&gt;首先第一件事情就是使用 Git 的时候，在配置了密钥之后，SSH 还是会卡死，这个问题是因为 Git 版本导致的。我之前使用的是 &lt;code&gt;2.45.2&lt;/code&gt; 版本，而现在已经变成了 &lt;code&gt;2.47.0&lt;/code&gt;，不知道为什么就出现了这个问题。版本在 Git 的官网找不到，但是可以在 Git for Windows 的 &lt;a href=&quot;https://github.com/git-for-windows/git/releases&quot;&gt;Github Releases 界面&lt;/a&gt; 找到。&lt;/p&gt;
&lt;p&gt;然后就是在使用浏览器的时候，起因是因为我在使用 ToDesk 的时候，不知道操作了什么，有的时候会让电脑的 Web 相关的界面变得模糊，有点像是重影，这一现象可以通过关闭浏览器的硬件加速（或者叫做图形化 xxx）解决，但是 Wallpaper Engine 同样使用 Web 框架，暂时没找到对应的选项，其视频加速选项貌似不是。暂时不清楚是 CPU 问题还是电脑或者系统问题，希望将来的更新可以解决。&lt;/p&gt;
&lt;h2&gt;SSH 登录实验室堡垒机报错&lt;/h2&gt;
&lt;p&gt;在上海那边的实验室，登录并且操作集群需要使用堡垒机，在开通账号之后使用 SSH 即可，但是却出现了奇怪的报错，具体内容为 &lt;code&gt;no matching host key type found. Their offer: ssh-rsa&lt;/code&gt;，一开始我还觉得是类似于服务器那边的一些配置我没有做好，但是详细了解之后发现，按理来说直接使用账号密码在内网中就可以登录，于是问了一下 IT，得到了解决方法，适用于同样报错内容的场景。在 &lt;code&gt;~/.ssh/config&lt;/code&gt; 中添加以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;HostKeyAlgorithms +ssh-rsa
PubkeyAcceptedKeyTypes +ssh-rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Pip/Conda 安装空间不够&lt;/h2&gt;
&lt;p&gt;这个事情严格来说不能说是一个 Bug，但是算是程序里面的杂事，而且找了半天问题，所以也记录一下。本身的症状很简单，就是在 pip install 的时候输出了 &lt;code&gt;no space left on device&lt;/code&gt;，问题已经写在脸上了，就是空间不够，问题是如何解决。&lt;/p&gt;
&lt;p&gt;我使用的是实验室的服务器，这个服务器是一堆人一起在使用。因为一些历史原因，我有 sudo 权限，所以 &lt;code&gt;cd home&lt;/code&gt; 然后 &lt;code&gt;du -sh */&lt;/code&gt; 了一下，扫了一圈大家的空间，确实有人一下子用了大几十个 GB 的空间。在这里有必要介绍一下服务器的使用礼仪，一般来说会有专门的数据盘，在这个里面被挂载在 &lt;code&gt;/ssd&lt;/code&gt; 下面，而按理来说 &lt;code&gt;/home&lt;/code&gt; 下面应该几乎没有东西才合理，不然容易出现各种的问题。&lt;/p&gt;
&lt;p&gt;既然如今已经出问题了，这些人一时半会联系不上，而且也不能指望他们。根据我们的数据盘叫做 ssd 来说，应该速度还可以，所以说干脆直接把环境全部迁移到 &lt;code&gt;/ssd&lt;/code&gt; 下面。理解一下，整体需要迁移的包括 conda 的安装路径（我使用的是 miniconda），以及需要修改一下 pip install 的 cache。&lt;/p&gt;
&lt;p&gt;于是复制 conda，这里面我之前安装在了 &lt;code&gt;~&lt;/code&gt; 下面，也就是先 &lt;code&gt;cp -r miniconda3 /ssd/gaoning&lt;/code&gt;，然后 &lt;code&gt;vim .bashrc&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;...
export PATH=&quot;/home/gaoning/miniconda3/bin:$PATH&quot; # [!code --]
export PATH=&quot;/ssd/gaoning/miniconda3/bin:$PATH&quot; # [!code ++]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后运行 &lt;code&gt;source ~/.bashrc&lt;/code&gt;。确认没问题之后就可以 &lt;code&gt;rm -rf /home/gaoning/miniconda3&lt;/code&gt; 了。然后就是 pip 的 cache，需要修改这个的默认目录。&lt;/p&gt;
&lt;p&gt;首先 &lt;code&gt;mkdir -p /ssd/gaoning/.pip_cache&lt;/code&gt;，然后使用 &lt;code&gt;vim ~/.pip/pip.conf&lt;/code&gt;，假如说之前没有这个文件（可能你之前没有操作过类似于设置 pip 的 index url 的操作），那就创建一个，然后写入：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;[global]
cache-dir = /ssd/gaoning/.pip_cache/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是按照这个操作之后，发现还是有问题，很诡异。然后简单查了一下，就发现问题了，因为在下载的时候其实会使用默认的 TMP 目录，于是需要 &lt;code&gt;mkdir -p /ssd/gaoning/tmp&lt;/code&gt;，之后 &lt;code&gt;vim ~/.bashrc&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export TMPDIR=/ssd/gaoning/tmp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后 &lt;code&gt;source ~/.bashrc&lt;/code&gt;，之后就没有问题了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Strange-Bugs-zh.webp"/><enclosure url="https://picr2.axi404.top/Strange-Bugs-zh.webp"/></item><item><title>Ceres 1.14 在 Ubuntu 22.04 的安装</title><link>https://axi404.github.io/blog/ceres-install</link><guid isPermaLink="true">https://axi404.github.io/blog/ceres-install</guid><description>在 Ubuntu 22.04 安装 Ceres 1.14，出现了一些之前在 Ubuntu 20.04 没有出现过的问题，提供了解决方案。</description><pubDate>Thu, 19 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;在 Ubuntu 22.04 安装 Ceres 1.14，出现了一些之前在 Ubuntu 20.04 没有出现过的问题，所以在这里记录一下，以及写一下解决的方法。&lt;/p&gt;
&lt;h2&gt;默认安装&lt;/h2&gt;
&lt;p&gt;首先先安装一下依赖：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install -y libgoogle-glog-dev libgflags-dev libatlas-base-dev libeigen3-dev libsuitesparse-dev libtbb-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后下载 Ceres 库：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget https://github.com/ceres-solver/ceres-solver/archive/refs/tags/1.14.0.zip
unzip 1.14.0.zip
cd ceres-solver-1.14.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;报错处理&lt;/h2&gt;
&lt;p&gt;如果直接进行编译会出现两个报错，一个来自于 &lt;code&gt;tbb_stddef.h&lt;/code&gt;，另一个则是 &lt;code&gt;gtest&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;前者进行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd /usr/include/tbb
sudo touch tbb_stddef.h
sudo gedit tbb_stddef.h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输入：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;/*
    Copyright (c) 2005-2020 Intel Corporation

    Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
*/

#ifndef __TBB_tbb_stddef_H
#define __TBB_tbb_stddef_H

// Marketing-driven product version
#define TBB_VERSION_MAJOR 2020
#define TBB_VERSION_MINOR 2

// Engineering-focused interface version
#define TBB_INTERFACE_VERSION 11102
#define TBB_INTERFACE_VERSION_MAJOR TBB_INTERFACE_VERSION/1000

// The oldest major interface version still supported
// To be used in SONAME, manifests, etc.
#define TBB_COMPATIBLE_INTERFACE_VERSION 2

#define __TBB_STRING_AUX(x) #x
#define __TBB_STRING(x) __TBB_STRING_AUX(x)

// We do not need defines below for resource processing on windows
#if !defined RC_INVOKED

// Define groups for Doxygen documentation
/**
 * @defgroup algorithms         Algorithms
 * @defgroup containers         Containers
 * @defgroup memory_allocation  Memory Allocation
 * @defgroup synchronization    Synchronization
 * @defgroup timing             Timing
 * @defgroup task_scheduling    Task Scheduling
 */

// Simple text that is displayed on the main page of Doxygen documentation.
/**
 * \mainpage Main Page
 *
 * Click the tabs above for information about the
 * - &amp;#x3C;a href=&quot;./modules.html&quot;&gt;Modules&amp;#x3C;/a&gt; (groups of functionality) implemented by the library
 * - &amp;#x3C;a href=&quot;./annotated.html&quot;&gt;Classes&amp;#x3C;/a&gt; provided by the library
 * - &amp;#x3C;a href=&quot;./files.html&quot;&gt;Files&amp;#x3C;/a&gt; constituting the library.
 * .
 * Please note that significant part of TBB functionality is implemented in the form of
 * template functions, descriptions of which are not accessible on the &amp;#x3C;a href=&quot;./annotated.html&quot;&gt;Classes&amp;#x3C;/a&gt;
 * tab. Use &amp;#x3C;a href=&quot;./modules.html&quot;&gt;Modules&amp;#x3C;/a&gt; or &amp;#x3C;a href=&quot;./namespacemembers.html&quot;&gt;Namespace/Namespace Members&amp;#x3C;/a&gt;
 * tabs to find them.
 *
 * Additional pieces of information can be found here
 * - \subpage concepts
 * .
 */

/** \page concepts TBB concepts

    A concept is a set of requirements to a type, which are necessary and sufficient
    for the type to model a particular behavior or a set of behaviors. Some concepts
    are specific to a particular algorithm (e.g. algorithm body), while other ones
    are common to several algorithms (e.g. range concept).

    All TBB algorithms make use of different classes implementing various concepts.
    Implementation classes are supplied by the user as type arguments of template
    parameters and/or as objects passed as function call arguments. The library
    provides predefined  implementations of some concepts (e.g. several kinds of
    \ref range_req &quot;ranges&quot;), while other ones must always be implemented by the user.

    TBB defines a set of minimal requirements each concept must conform to. Here is
    the list of different concepts hyperlinked to the corresponding requirements specifications:
    - \subpage range_req
    - \subpage parallel_do_body_req
    - \subpage parallel_for_body_req
    - \subpage parallel_reduce_body_req
    - \subpage parallel_scan_body_req
    - \subpage parallel_sort_iter_req
**/

// tbb_config.h should be included the first since it contains macro definitions used in other headers
#include &quot;tbb_config.h&quot;

#if _MSC_VER &gt;=1400
    #define __TBB_EXPORTED_FUNC   __cdecl
    #define __TBB_EXPORTED_METHOD __thiscall
#else
    #define __TBB_EXPORTED_FUNC
    #define __TBB_EXPORTED_METHOD
#endif

#if __INTEL_COMPILER || _MSC_VER
#define __TBB_NOINLINE(decl) __declspec(noinline) decl
#elif __GNUC__
#define __TBB_NOINLINE(decl) decl __attribute__ ((noinline))
#else
#define __TBB_NOINLINE(decl) decl
#endif

#if __TBB_NOEXCEPT_PRESENT
#define __TBB_NOEXCEPT(expression) noexcept(expression)
#else
#define __TBB_NOEXCEPT(expression)
#endif

#include &amp;#x3C;cstddef&gt;      /* Need size_t and ptrdiff_t */

#if _MSC_VER
    #define __TBB_tbb_windef_H
    #include &quot;internal/_tbb_windef.h&quot;
    #undef __TBB_tbb_windef_H
#endif
#if !defined(_MSC_VER) || _MSC_VER&gt;=1600
    #include &amp;#x3C;stdint.h&gt;
#endif

//! Type for an assertion handler
typedef void(*assertion_handler_type)( const char* filename, int line, const char* expression, const char * comment );

#if __TBBMALLOC_BUILD
namespace rml { namespace internal {
 #define __TBB_ASSERT_RELEASE(predicate,message) ((predicate)?((void)0) : rml::internal::assertion_failure(__FILE__,__LINE__,#predicate,message))
#else
namespace tbb {
 #define __TBB_ASSERT_RELEASE(predicate,message) ((predicate)?((void)0) : tbb::assertion_failure(__FILE__,__LINE__,#predicate,message))
#endif

    //! Set assertion handler and return previous value of it.
    assertion_handler_type __TBB_EXPORTED_FUNC set_assertion_handler( assertion_handler_type new_handler );

    //! Process an assertion failure.
    /** Normally called from __TBB_ASSERT macro.
        If assertion handler is null, print message for assertion failure and abort.
        Otherwise call the assertion handler. */
    void __TBB_EXPORTED_FUNC assertion_failure( const char* filename, int line, const char* expression, const char* comment );

#if __TBBMALLOC_BUILD
}}  // namespace rml::internal
#else
} // namespace tbb
#endif

#if TBB_USE_ASSERT

    //! Assert that predicate is true.
    /** If predicate is false, print assertion failure message.
        If the comment argument is not NULL, it is printed as part of the failure message.
        The comment argument has no other effect. */
    #define __TBB_ASSERT(predicate,message) __TBB_ASSERT_RELEASE(predicate,message)

    #define __TBB_ASSERT_EX __TBB_ASSERT

#else /* !TBB_USE_ASSERT */

    //! No-op version of __TBB_ASSERT.
    #define __TBB_ASSERT(predicate,comment) ((void)0)
    //! &quot;Extended&quot; version is useful to suppress warnings if a variable is only used with an assert
    #define __TBB_ASSERT_EX(predicate,comment) ((void)(1 &amp;#x26;&amp;#x26; (predicate)))

#endif /* !TBB_USE_ASSERT */

//! The namespace tbb contains all components of the library.
namespace tbb {

    namespace internal {
#if _MSC_VER &amp;#x26;&amp;#x26; _MSC_VER&amp;#x3C;1600
        typedef __int8 int8_t;
        typedef __int16 int16_t;
        typedef __int32 int32_t;
        typedef __int64 int64_t;
        typedef unsigned __int8 uint8_t;
        typedef unsigned __int16 uint16_t;
        typedef unsigned __int32 uint32_t;
        typedef unsigned __int64 uint64_t;
#else /* Posix */
        using ::int8_t;
        using ::int16_t;
        using ::int32_t;
        using ::int64_t;
        using ::uint8_t;
        using ::uint16_t;
        using ::uint32_t;
        using ::uint64_t;
#endif /* Posix */
    } // namespace internal

    using std::size_t;
    using std::ptrdiff_t;

//! The function returns the interface version of the TBB shared library being used.
/**
 * The version it returns is determined at runtime, not at compile/link time.
 * So it can be different than the value of TBB_INTERFACE_VERSION obtained at compile time.
 */
extern &quot;C&quot; int __TBB_EXPORTED_FUNC TBB_runtime_interface_version();

/**
 * @cond INTERNAL
 * @brief Identifiers declared inside namespace internal should never be used directly by client code.
 */
namespace internal {

//! Compile-time constant that is upper bound on cache line/sector size.
/** It should be used only in situations where having a compile-time upper
    bound is more useful than a run-time exact answer.
    @ingroup memory_allocation */
const size_t NFS_MaxLineSize = 128;

/** Label for data that may be accessed from different threads, and that may eventually become wrapped
    in a formal atomic type.

    Note that no problems have yet been observed relating to the definition currently being empty,
    even if at least &quot;volatile&quot; would seem to be in order to avoid data sometimes temporarily hiding
    in a register (although &quot;volatile&quot; as a &quot;poor man&apos;s atomic&quot; lacks several other features of a proper
    atomic, some of which are now provided instead through specialized functions).

    Note that usage is intentionally compatible with a definition as qualifier &quot;volatile&quot;,
    both as a way to have the compiler help enforce use of the label and to quickly rule out
    one potential issue.

    Note however that, with some architecture/compiler combinations, e.g. on IA-64 architecture, &quot;volatile&quot;
    also has non-portable memory semantics that are needlessly expensive for &quot;relaxed&quot; operations.

    Note that this must only be applied to data that will not change bit patterns when cast to/from
    an integral type of the same length; tbb::atomic must be used instead for, e.g., floating-point types.

    TODO: apply wherever relevant **/
#define __TBB_atomic // intentionally empty, see above

#if __TBB_OVERRIDE_PRESENT
#define __TBB_override override
#else
#define __TBB_override // formal comment only
#endif

#if __TBB_CPP17_FALLTHROUGH_PRESENT
#define __TBB_fallthrough [[fallthrough]]
#elif __TBB_FALLTHROUGH_PRESENT
#define __TBB_fallthrough __attribute__ ((fallthrough))
#else
#define __TBB_fallthrough
#endif

template&amp;#x3C;class T, size_t S, size_t R&gt;
struct padded_base : T {
    char pad[S - R];
};
template&amp;#x3C;class T, size_t S&gt; struct padded_base&amp;#x3C;T, S, 0&gt; : T {};

//! Pads type T to fill out to a multiple of cache line size.
template&amp;#x3C;class T, size_t S = NFS_MaxLineSize&gt;
struct padded : padded_base&amp;#x3C;T, S, sizeof(T) % S&gt; {};

//! Extended variant of the standard offsetof macro
/** The standard offsetof macro is not sufficient for TBB as it can be used for
    POD-types only. The constant 0x1000 (not NULL) is necessary to appease GCC. **/
#define __TBB_offsetof(class_name, member_name) \
    ((ptrdiff_t)&amp;#x26;(reinterpret_cast&amp;#x3C;class_name*&gt;(0x1000)-&gt;member_name) - 0x1000)

//! Returns address of the object containing a member with the given name and address
#define __TBB_get_object_ref(class_name, member_name, member_addr) \
    (*reinterpret_cast&amp;#x3C;class_name*&gt;((char*)member_addr - __TBB_offsetof(class_name, member_name)))

//! Throws std::runtime_error with what() returning error_code description prefixed with aux_info
void __TBB_EXPORTED_FUNC handle_perror( int error_code, const char* aux_info );

#if TBB_USE_EXCEPTIONS
    #define __TBB_TRY try
    #define __TBB_CATCH(e) catch(e)
    #define __TBB_THROW(e) throw e
    #define __TBB_RETHROW() throw
#else /* !TBB_USE_EXCEPTIONS */
    inline bool __TBB_false() { return false; }
    #define __TBB_TRY
    #define __TBB_CATCH(e) if ( tbb::internal::__TBB_false() )
    #define __TBB_THROW(e) tbb::internal::suppress_unused_warning(e)
    #define __TBB_RETHROW() ((void)0)
#endif /* !TBB_USE_EXCEPTIONS */

//! Report a runtime warning.
void __TBB_EXPORTED_FUNC runtime_warning( const char* format, ... );

#if TBB_USE_ASSERT
static void* const poisoned_ptr = reinterpret_cast&amp;#x3C;void*&gt;(-1);

//! Set p to invalid pointer value.
//  Also works for regular (non-__TBB_atomic) pointers.
template&amp;#x3C;typename T&gt;
inline void poison_pointer( T* __TBB_atomic &amp;#x26; p ) { p = reinterpret_cast&amp;#x3C;T*&gt;(poisoned_ptr); }

/** Expected to be used in assertions only, thus no empty form is defined. **/
template&amp;#x3C;typename T&gt;
inline bool is_poisoned( T* p ) { return p == reinterpret_cast&amp;#x3C;T*&gt;(poisoned_ptr); }
#else
template&amp;#x3C;typename T&gt;
inline void poison_pointer( T* __TBB_atomic &amp;#x26; ) {/*do nothing*/}
#endif /* !TBB_USE_ASSERT */

//! Cast between unrelated pointer types.
/** This method should be used sparingly as a last resort for dealing with
    situations that inherently break strict ISO C++ aliasing rules. */
// T is a pointer type because it will be explicitly provided by the programmer as a template argument;
// U is a referent type to enable the compiler to check that &quot;ptr&quot; is a pointer, deducing U in the process.
template&amp;#x3C;typename T, typename U&gt;
inline T punned_cast( U* ptr ) {
    uintptr_t x = reinterpret_cast&amp;#x3C;uintptr_t&gt;(ptr);
    return reinterpret_cast&amp;#x3C;T&gt;(x);
}

#if __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT

//! Base class for types that should not be assigned.
class no_assign {
public:
    void operator=( const no_assign&amp;#x26; ) = delete;
    no_assign( const no_assign&amp;#x26; ) = default;
    no_assign() = default;
};

//! Base class for types that should not be copied or assigned.
class no_copy: no_assign {
public:
    no_copy( const no_copy&amp;#x26; ) = delete;
    no_copy() = default;
};

#else /*__TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT*/

//! Base class for types that should not be assigned.
class no_assign {
    // Deny assignment
    void operator=( const no_assign&amp;#x26; );
public:
#if __GNUC__
    //! Explicitly define default construction, because otherwise gcc issues gratuitous warning.
    no_assign() {}
#endif /* __GNUC__ */
};

//! Base class for types that should not be copied or assigned.
class no_copy: no_assign {
    //! Deny copy construction
    no_copy( const no_copy&amp;#x26; );
public:
    //! Allow default construction
    no_copy() {}
};

#endif /*__TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT*/

#if TBB_DEPRECATED_MUTEX_COPYING
class mutex_copy_deprecated_and_disabled {};
#else
// By default various implementations of mutexes are not copy constructible
// and not copy assignable.
class mutex_copy_deprecated_and_disabled : no_copy {};
#endif

//! A function to check if passed in pointer is aligned on a specific border
template&amp;#x3C;typename T&gt;
inline bool is_aligned(T* pointer, uintptr_t alignment) {
    return 0==((uintptr_t)pointer &amp;#x26; (alignment-1));
}

//! A function to check if passed integer is a power of 2
template&amp;#x3C;typename integer_type&gt;
inline bool is_power_of_two(integer_type arg) {
    return arg &amp;#x26;&amp;#x26; (0 == (arg &amp;#x26; (arg - 1)));
}

//! A function to compute arg modulo divisor where divisor is a power of 2.
template&amp;#x3C;typename argument_integer_type, typename divisor_integer_type&gt;
inline argument_integer_type modulo_power_of_two(argument_integer_type arg, divisor_integer_type divisor) {
    __TBB_ASSERT( is_power_of_two(divisor), &quot;Divisor should be a power of two&quot; );
    return (arg &amp;#x26; (divisor - 1));
}


//! A function to determine if arg is a power of 2 at least as big as another power of 2.
// i.e. for strictly positive i and j, with j being a power of 2,
// determines whether i==j&amp;#x3C;&amp;#x3C;k for some nonnegative k (so i==j yields true).
template&amp;#x3C;typename argument_integer_type, typename power2_integer_type&gt;
inline bool is_power_of_two_at_least(argument_integer_type arg, power2_integer_type power2) {
    __TBB_ASSERT( is_power_of_two(power2), &quot;Divisor should be a power of two&quot; );
    return 0 == (arg &amp;#x26; (arg - power2));
}

//! Utility template function to prevent &quot;unused&quot; warnings by various compilers.
template&amp;#x3C;typename T1&gt; void suppress_unused_warning( const T1&amp;#x26; ) {}
template&amp;#x3C;typename T1, typename T2&gt; void suppress_unused_warning( const T1&amp;#x26;, const T2&amp;#x26; ) {}
template&amp;#x3C;typename T1, typename T2, typename T3&gt; void suppress_unused_warning( const T1&amp;#x26;, const T2&amp;#x26;, const T3&amp;#x26; ) {}

// Struct to be used as a version tag for inline functions.
/** Version tag can be necessary to prevent loader on Linux from using the wrong
    symbol in debug builds (when inline functions are compiled as out-of-line). **/
struct version_tag_v3 {};

typedef version_tag_v3 version_tag;

} // internal

//! Dummy type that distinguishes splitting constructor from copy constructor.
/**
 * See description of parallel_for and parallel_reduce for example usages.
 * @ingroup algorithms
 */
class split {
};

//! Type enables transmission of splitting proportion from partitioners to range objects
/**
 * In order to make use of such facility Range objects must implement
 * splitting constructor with this type passed and initialize static
 * constant boolean field &apos;is_splittable_in_proportion&apos; with the value
 * of &apos;true&apos;
 */
class proportional_split: internal::no_assign {
public:
    proportional_split(size_t _left = 1, size_t _right = 1) : my_left(_left), my_right(_right) { }

    size_t left() const { return my_left; }
    size_t right() const { return my_right; }

    // used when range does not support proportional split
    operator split() const { return split(); }

#if __TBB_ENABLE_RANGE_FEEDBACK
    void set_proportion(size_t _left, size_t _right) {
        my_left = _left;
        my_right = _right;
    }
#endif
private:
    size_t my_left, my_right;
};

} // tbb

// Following is a set of classes and functions typically used in compile-time &quot;metaprogramming&quot;.
// TODO: move all that to a separate header

#if __TBB_CPP11_SMART_POINTERS_PRESENT
#include &amp;#x3C;memory&gt; // for unique_ptr
#endif

#if __TBB_CPP11_RVALUE_REF_PRESENT || __TBB_CPP11_DECLTYPE_PRESENT || _LIBCPP_VERSION
#include &amp;#x3C;utility&gt; // for std::move, std::forward, std::declval
#endif

namespace tbb {
namespace internal {

#if __TBB_CPP11_SMART_POINTERS_PRESENT &amp;#x26;&amp;#x26; __TBB_CPP11_RVALUE_REF_PRESENT &amp;#x26;&amp;#x26; __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT
    template&amp;#x3C;typename T, typename... Args&gt;
    std::unique_ptr&amp;#x3C;T&gt; make_unique(Args&amp;#x26;&amp;#x26;... args) {
        return std::unique_ptr&amp;#x3C;T&gt;(new T(std::forward&amp;#x3C;Args&gt;(args)...));
    }
#endif

//! Class for determining type of std::allocator&amp;#x3C;T&gt;::value_type.
template&amp;#x3C;typename T&gt;
struct allocator_type {
    typedef T value_type;
};

#if _MSC_VER
//! Microsoft std::allocator has non-standard extension that strips const from a type.
template&amp;#x3C;typename T&gt;
struct allocator_type&amp;#x3C;const T&gt; {
    typedef T value_type;
};
#endif

// Ad-hoc implementation of true_type &amp;#x26; false_type
// Intended strictly for internal use! For public APIs (traits etc), use C++11 analogues.
template &amp;#x3C;bool v&gt;
struct bool_constant {
    static /*constexpr*/ const bool value = v;
};
typedef bool_constant&amp;#x3C;true&gt; true_type;
typedef bool_constant&amp;#x3C;false&gt; false_type;

//! A template to select either 32-bit or 64-bit constant as compile time, depending on machine word size.
template &amp;#x3C;unsigned u, unsigned long long ull &gt;
struct select_size_t_constant {
    //Explicit cast is needed to avoid compiler warnings about possible truncation.
    //The value of the right size,   which is selected by ?:, is anyway not truncated or promoted.
    static const size_t value = (size_t)((sizeof(size_t)==sizeof(u)) ? u : ull);
};

#if __TBB_CPP11_RVALUE_REF_PRESENT
using std::move;
using std::forward;
#elif defined(_LIBCPP_NAMESPACE)
// libc++ defines &quot;pre-C++11 move and forward&quot; similarly to ours; use it to avoid name conflicts in some cases.
using std::_LIBCPP_NAMESPACE::move;
using std::_LIBCPP_NAMESPACE::forward;
#else
// It is assumed that cv qualifiers, if any, are part of the deduced type.
template &amp;#x3C;typename T&gt;
T&amp;#x26; move( T&amp;#x26; x ) { return x; }
template &amp;#x3C;typename T&gt;
T&amp;#x26; forward( T&amp;#x26; x ) { return x; }
#endif /* __TBB_CPP11_RVALUE_REF_PRESENT */

// Helper macros to simplify writing templates working with both C++03 and C++11.
#if __TBB_CPP11_RVALUE_REF_PRESENT
#define  __TBB_FORWARDING_REF(A) A&amp;#x26;&amp;#x26;
#else
// It is assumed that cv qualifiers, if any, are part of a deduced type.
// Thus this macro should not be used in public interfaces.
#define  __TBB_FORWARDING_REF(A) A&amp;#x26;
#endif
#if __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT
#define __TBB_PARAMETER_PACK ...
#define __TBB_PACK_EXPANSION(A) A...
#else
#define __TBB_PARAMETER_PACK
#define __TBB_PACK_EXPANSION(A) A
#endif /* __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT */

#if __TBB_CPP11_DECLTYPE_PRESENT
#if __TBB_CPP11_DECLVAL_BROKEN
// Ad-hoc implementation of std::declval
template &amp;#x3C;class T&gt; __TBB_FORWARDING_REF(T) declval() /*noexcept*/;
#else
using std::declval;
#endif
#endif

template &amp;#x3C;bool condition&gt;
struct STATIC_ASSERTION_FAILED;

template &amp;#x3C;&gt;
struct STATIC_ASSERTION_FAILED&amp;#x3C;false&gt; { enum {value=1};};

template&amp;#x3C;&gt;
struct STATIC_ASSERTION_FAILED&amp;#x3C;true&gt;; //intentionally left undefined to cause compile time error

//! @endcond
}} // namespace tbb::internal

#if __TBB_STATIC_ASSERT_PRESENT
#define __TBB_STATIC_ASSERT(condition,msg) static_assert(condition,msg)
#else
//please note condition is intentionally inverted to get a bit more understandable error msg
#define __TBB_STATIC_ASSERT_IMPL1(condition,msg,line)       \
    enum {static_assert_on_line_##line = tbb::internal::STATIC_ASSERTION_FAILED&amp;#x3C;!(condition)&gt;::value}

#define __TBB_STATIC_ASSERT_IMPL(condition,msg,line) __TBB_STATIC_ASSERT_IMPL1(condition,msg,line)
//! Verify condition, at compile time
#define __TBB_STATIC_ASSERT(condition,msg) __TBB_STATIC_ASSERT_IMPL(condition,msg,__LINE__)
#endif

#endif /* RC_INVOKED */
#endif /* __TBB_tbb_stddef_H */
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;后者则需要在 &lt;code&gt;CMakeList.txt&lt;/code&gt; 中取消 &lt;code&gt;Test&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;# Enable the use of Eigen as a sparse linear algebra library for
# solving the nonlinear least squares problems.
option(EIGENSPARSE &quot;Enable Eigen as a sparse linear algebra library.&quot; ON)
option(EXPORT_BUILD_DIR
  &quot;Export build directory using CMake (enables external use without install).&quot; OFF)
option(BUILD_TESTING &quot;Enable tests&quot; ON)  // [!code --]
option(BUILD_TESTING &quot;Enable tests&quot; OFF)  // [!code ++]
option(BUILD_DOCUMENTATION &quot;Build User&apos;s Guide (html)&quot; OFF)
option(BUILD_EXAMPLES &quot;Build examples&quot; ON)
cmake_dependent_option(
  BUILD_BENCHMARKS &quot;Build Ceres benchmarking suite&quot; ON &quot;CXX11&quot; OFF)
option(BUILD_SHARED_LIBS &quot;Build Ceres as a shared library.&quot; OFF)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后进行正常的 CMake 编译安装即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir build
cd build
cmake ..
make -j8
sudo make install
&lt;/code&gt;&lt;/pre&gt;</content:encoded><h:img src="https://picr2.axi404.top/ceres-install-zh.webp"/><enclosure url="https://picr2.axi404.top/ceres-install-zh.webp"/></item><item><title>Ubuntu 22.04 三系统安装以及安装显卡驱动后无线网卡恢复</title><link>https://axi404.github.io/blog/ubuntu-install</link><guid isPermaLink="true">https://axi404.github.io/blog/ubuntu-install</guid><description>在已有 Ubuntu 20.04 以及 Windows 的时候安装 Ubuntu 22.04 双系统并且修复一些 Bugs。</description><pubDate>Thu, 19 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;因为 Ubuntu 20.04 的若干的内容已经不再支持，使用起来最新的一些软件基本上全是报错，比较经典的就是 &lt;code&gt;GLIBC 2.3.1&lt;/code&gt; 以及 &lt;code&gt;libssl.so.3&lt;/code&gt; 等内容，而前者的安装十分的麻烦，所以干脆直接安装三系统。&lt;/p&gt;
&lt;p&gt;三系统的安装不是很困难，将新创建的 EFI 分区作为引导器就好（理论来说，全部的系统都可以使用同一个 EFI 分区，但是我之前安装的时候，当时太过于稚嫩，胡乱操作出现过问题，现在不太敢尝试，所以没有踩过坑，在这里不作为介绍的方法），之后在系统的 GRUB 界面就可以看到三个系统了。&lt;/p&gt;
&lt;h2&gt;切换 Grub&lt;/h2&gt;
&lt;p&gt;一个常见的问题在于，如何切换 Grub。比如说我之前已经给我的 Ubuntu 20.04 的 Grub 安装了一个主题，而安装了新的系统之后，这个 Grub 会被新系统的 Grub 覆盖掉，那么应该如何处理呢。&lt;/p&gt;
&lt;p&gt;假如说按照上述的方法，那么你在进入系统的时候其实是可以看到自己的之前的系统的，进入之前的系统之后，可以运行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo update-grub
lsblk
# 输出中可以找到 MOUNTPOINTS 为 /boot/efi 的项，记住其 NAME
sudo grub-install /dev/nvme0n1p1 # 以 nvme0n1p1 为例
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后重启即可。&lt;/p&gt;
&lt;h2&gt;无线网卡恢复&lt;/h2&gt;
&lt;p&gt;Ubuntu 22.04 有一个比较经典的问题，就是安装显卡驱动之后，会导致无线网卡消失，按照正常的流程进行操作之后，运行 &lt;code&gt;sudo ubuntu-drivers autoinstall&lt;/code&gt; 并且重启，再次进入默认的系统之后，就会发现网卡消失了。&lt;/p&gt;
&lt;p&gt;再次重启，进入 GRUB 之后选择 &lt;code&gt;Advanced options for ubuntu&lt;/code&gt;，进去之后可以看到两个 Ubuntu 的版本以及对应的两个 recovery mode。两个版本里面比较新的一个是在安装显卡驱动之后新安装的版本，可以理解为显卡驱动对于较高版本的内核具有依赖，但是配套的无线没有一起安装，记下来两个版本的型号，然后选择较低版本的内核（不是 recovery mode）进入。&lt;/p&gt;
&lt;p&gt;进入这一内核之后，可以发现网卡是有的，但是使用 &lt;code&gt;nvidia-smi&lt;/code&gt;，并没有正常的那个输出界面，因为这个系统中内核不满足显卡驱动的依赖，那么把这个系统的版本提上去就好了。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;sudo dpkg --get-selection | grep linux&lt;/code&gt; 可以看到一些信息，其中一些项目包含版本号，有新版本的版本号，以及旧版本的，记下来这些旧版本的，并且使用 &lt;code&gt;sudo apt install&lt;/code&gt; 安装使用新版本号覆盖旧版本号的这些内容。本人安装内容如下，作为参考：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install linux-headers-6.8.0-40-generic linux-image-6.8.0-40-generic linux-modules-6.8.0-40-generic linux-modules-extra-6.8.0-40-generic
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再次重启，正常进入正常的系统，恢复。&lt;/p&gt;
&lt;p&gt;需要注意的是，越早设置这些内容，与本文档的对齐程度最高，本人的安装流程为，正常安装系统（将全部硬盘空间都挂在在 &lt;code&gt;/&lt;/code&gt; 下）并设置语言为中文，进入系统之后更换语言为英文（因为不然的话输入法的安装比较麻烦），重启，将文件夹变为英文名，再重启，连接网络，&lt;code&gt;sudo apt update&lt;/code&gt; 以及 &lt;code&gt;sudo apt upgrade&lt;/code&gt;，最后就开始安装显卡驱动 &lt;code&gt;sudo ubuntu-drivers autoinstall&lt;/code&gt; 并 &lt;code&gt;reboot&lt;/code&gt; 重启。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/ubuntu-install-zh.webp"/><enclosure url="https://picr2.axi404.top/ubuntu-install-zh.webp"/></item><item><title>周记 Week24</title><link>https://axi404.github.io/blog/week-24</link><guid isPermaLink="true">https://axi404.github.io/blog/week-24</guid><description>2024-12-09 ~ 2024-12-15.</description><pubDate>Mon, 16 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;可以说这一周是我度过的最折磨的一周了，往前追溯，可能要到九月份的时候，那时候需要三线程打工，这一周需要做的事情虽然基本上是单线程，但是可以说是有过之无不及。&lt;/p&gt;
&lt;h2&gt;考试&lt;/h2&gt;
&lt;p&gt;具体来说，这一周迎来了本学期最难的两个考试之一，也就是现代控制方法的考试，而我复习的时间只有三天。所以说周日简单的把全书看了一遍，然后周一就开始刷题，把全部题目全都看懂了，之后星期二开始整理知识点，写成 cheatsheet 的形式，这样一方面可以考前再看一遍，一方面也可以加深记忆。按照知识点背了五六遍，然后重新开始作业题，刷了两遍，就急匆匆去考试了。考试感觉下来一般，属于是差不多答出来了，但是能有多少分还是听天由命。&lt;/p&gt;
&lt;p&gt;之后星期三上午考完试，下午立刻转去完成一个实验报告，准确的说是 DSP 的实验结果报告。可以说这个是比较离谱的，具体来说，我们处理的是 EEG 信号的分类任务，可以理解为脑电识别，或者说时序信号处理。本身肯定是不难做的，毕竟之前其实处理过其他的东西，包括说从时域去分析音频信号，以及去提取频率特征。本来可以说 EEG 信号处理，按照我的理解来说，就是一个正常的在时域上进行处理，可以说看上去是人畜无害，而且更何况之前已经有了音频的经验。我本身是一个连接主义者，所以也除了老师要求的特征提取的方案之外，也尝试了直接降采样并且进行分类，结果是效果很差。可以说全部的方法在 EEG 信号上都是不奏效的，经过我的一些技巧之后，模型可以在训练的早期快速收敛，但是这是仅仅在训练集上的，而测试集的准确度依然离谱，只有 50% 左右。要注意的是，这是一个二分类的任务，这约等于随机猜测。后来我了解了这个领域，发现了一个很离谱的 setting（毕竟我也有复现很多的论文，这些论文在文章里面汇报的点数高达 99%，但是复现出来的结果不到 50%），那就是他们的训练的 train test split 并不是直接把数据集进行划分，而是把一条数据里面的一部分作为训练集，另一部分作为测试集。可以说是十分的离谱了，我们之前已经有 setting 去把同属于一个人的数据分到训练集和测试集来确保模型对于全部的被试都有一个记忆能力（毕竟据说目前这个领域的结果还是依赖于个人以及时效性），通过这种设置来降低模型泛化的难度。不过尽管如此，这个 setting 依然过于不合理了。按照人来划分 train 和 test 还可以说只是一个 split 的 policy，但是在一条数据里面划分，可以说是完全不合理的。一条数据的前后，他们只把前 3s 的数据作为训练，可以说这 3s 的数据包含的特征一定在之后出现过，那么在之后的分类任务中，可以说完全就是 train 以及 test 是同一个分布了，那还有什么过拟合的难度呢？放回到现实的应用里面，等于说这种脑电的「读心」操作，需要被试先说出来自己刚才的感受，才能判断接下来的感受，那有啥用呢？堪称弱智。这个数据集叫做 DEAP，感觉读者看后可以避雷，而这个数据集作为 EEG 领域最流行的数据集之一，居然还有大量的人去刷，而且乐此不疲，实在是难以让人接受。之后换了一个别的数据集，DEAP 是判断人的心情，而这个新的是通过刺激人的身体来判断被刺激的部位，合理了不少，也成功做出来了。第二天到了 DSP 的展示上面，直接进行了大量的批驳，对于整个的领域。并且还是惊讶地发现，大多数同学都没有去做这个，而是选择了 DSP 的另一个可以选择的课题，也就是说话人识别（以及部分甚至就是直接炒冷饭，选择了孤立字识别，可以理解为说一个数字，然后识别数字（0~9），是之前的任务，加上包装一个 UI，不过可以说这个东西我在 DSP 实验的第一节课，甚至就已经做完了包括 UI 的东西了，一个组的工作量居然如此，实在是令人摸不到头脑）。&lt;/p&gt;
&lt;p&gt;在做完了 DSP 之后，接下来紧接着就是 CVPR 的实验（这里的 CVPR 说的是我们的课程，而非那个知名的学术会议）以及现代控制方法的实验，其中 CVPR 可以说还算简单，做目标检测，跑一下模型，然后写一个报告，可以说是非常轻松，按照要求写了一个 HOG+SVM 的复现，以及用 YOLOv8 跑了一下数据，凑了一个看上去十分好看的实验报告。而现代控制方法的实验，可以说是非常离谱，因为老师要求我们用 MATLAB 去写一个控制器以及观测器，然后去控制一个二阶倒立摆，并且需要用 MATLAB 的 Simulink 去写。最后也是在同学的帮助下，借鉴了不少的代码，以及自己又稍微调优了一点点，做了一点点的差异性，然后提交上去了。可以说我对于 MatLab 还是不是很擅长的，至于用 Python 去写这个东西，更是我完全不想去做，所以还是算了吧，差不多得了。&lt;/p&gt;
&lt;p&gt;所以读者可以试想，星期三考完试，前几天也一直在通宵，之后星期三晚上通宵写 DSP，写完实验再自己写 PPT，然后星期四早上汇报 DSP，汇报之后简单睡一小会，就去通宵写 CVPR 的实验，写成实验报告第二天交，然后再写 MatLab 的现控实验，这时候已经星期六了。周六的下午又是六级考试，所以说又需要临阵磨枪刷一刷手感，刷了两套题，之后匆匆去考试。在这些事情的间隙中，我还需要兼顾上海那边的事情，甚至不能说是兼顾，可以说是我主要做的事情。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;上海那边一方面要改自己的代码，一方面还要和其他的同学来对接，可以说是工作量十分的大，而我这一周关键还生病了，长期的熬夜让我的身体状态十分差，已经是有点不行了，但是还是需要带病继续工作，但是毕竟是科研，我也乐在其中吧。&lt;/p&gt;
&lt;p&gt;这就是这周发生的事情，有点离谱，有点劳累，但是日子总还要继续。大量的实验基本上都已经处理掉了，接下来就是复习考试，以及期末考试了。与此同时推进上海这边的进度。希望人没事。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-24-zh.webp"/><enclosure url="https://picr2.axi404.top/week-24-zh.webp"/></item><item><title>周记 Week23</title><link>https://axi404.github.io/blog/week-23</link><guid isPermaLink="true">https://axi404.github.io/blog/week-23</guid><description>2024-12-02 ~ 2024-12-08.</description><pubDate>Sat, 14 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;又是很久之后的一次姗姗来迟的周记，只能说还是很劳累的一个星期，但是确实也没有发生什么其他的大事。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;一方面还是在一直修改程序，大概类似于一直在和实验室里的其他同学进行对接，因为我之前阐述了一些比较有价值的程序，而这些程序可以作为模块插入到其他内容里面，但是因为之前的代码写得比较随意，因此还是需要进行很多的整理和修改，并且保持之后的一个灵活性。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;其他的事情倒也没有什么了，这一周确确实实过得十分的无聊，基本上就是在写代码以及复习中度过。其实事实上，由于下一周的星期三就需要考试了，而且是十分困难的一门课程，现代控制工程。这时候我就又需要继续抱怨了，我并不是十分的喜欢认可人工智能专业当前的课程，说实话，这些课程除了给我增加压力以及缩减我参加科研的时间之外实在是没有其他的任何作用了。不过不得不说，这门课程老师讲的还是很不错的，可以说数学功底比较深厚，而且看上去这门课程也是很成体系，只是可惜在当下的知识背景下，实在是没有什么用武之地。&lt;/p&gt;
&lt;p&gt;这周可以说，又是患上了拖延症，或者说进行科研以及写代码可以让我获得更多的正反馈，所以说迟迟没有开始复习，实际上下周三的考试我直到这周的周日凌晨才开始复习，于是乎也就需要狠狠的压榨时间了，可以预见下一周也不会很轻松，希望生活可以善待我，我之后肯定好好努力。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-23-zh.webp"/><enclosure url="https://picr2.axi404.top/week-23-zh.webp"/></item><item><title>周记 Week22</title><link>https://axi404.github.io/blog/week-22</link><guid isPermaLink="true">https://axi404.github.io/blog/week-22</guid><description>2024-11-25 ~ 2024-12-01.</description><pubDate>Fri, 06 Dec 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;可以说这一周是一个全新的开始了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;之前的 CVPR 投稿的这段时间完全过去了，所以说可以花一些时间来沉淀。这周确实已经不存在一个很死的时间线来让我做一些事情，然而压力是存在的。因为在产出了论文之后，事实上领域内的竞争十分激烈，所以说要尽快把自己的代码放出去，至少要先把论文放出去，这使得我们需要先组织一下论文内容，对其一下论文里面之前画的一些饼，哪些是可以实现的，哪些其实实现起来还有一些问题。尽管我们已经把主体功能都完成了，但是周边当时承诺的一些小功能，在后续放出代码的时候也都需要具备。&lt;/p&gt;
&lt;p&gt;与此同时，两周之后还有考试，而这几周也有大量的实验需要提交。尽管我们在进行小组合作，但是实际下小组里面能做事情的人，一般来说只有我和我们的年级第一两个人，或者在没有他的组里面，我就需要一个人承担全部。&lt;/p&gt;
&lt;p&gt;其实在这一周开始的时候是有打算做一些沉淀的，类似于去准备一些将来的知识储备，以及准备之后的考试。但是事实上十分不幸的是，因为之前的几个星期，改论文导致的身体机能紊乱，尽管我试图在课题组里面保持正常的产能，但是在此之外，我可能就难以很好地进行其他内容了，基本上勉强完成课内的任务就已经是极限。&lt;/p&gt;
&lt;p&gt;而且可能现在因为压力比较大的原因，有的时候也会在心理上经常出现难以专注的情况，往往是打算播放一个视频作为背景音来工作，但是却专注看视频去了。又或者是想要克制自己，既然不看视频就不会有这种事情发生，但是却看着电脑屏幕，半天敲不下一行。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;抛开生活中这些不顺，和乐小姐两个人的二人世界倒还算幸福，在我比较有压力的时候，可以和她一起出去吃顿饭，然后再获得一个爱的抱抱，不免让我的心灵得到治愈。&lt;/p&gt;
&lt;p&gt;现在的科研其实对于我来说是无趣的，因为并不像是一些使用模型的工作，事实上需要维护的程序不算很多，我目前的工作可能实际各个方面都有一些涉及，超过大几千行的代码，现在要进行重头的整理，实在是压力山大，但是又只有我一个人来进行。&lt;/p&gt;
&lt;p&gt;记得以前听人说过，玩游戏最开心的时候就是开坑和完结的时候，这句话放在科研上也同样适用，尤其是现在我的时期，明明已经处在完结之后了，但是还有大量的收尾工作需要进行，这可能意味着很多的代码量，需要通宵，并且很难进行成果的量化。这样的重复工作实在是令我兴奋不起来，但是随着大家的更多的对于仿真平台的讨论，或许将来还是有希望将这些他们讨论的精髓融入到我的工作里面，在后面将这个工作变成一个我使用起来十分得心应手的工具，并且具备大量的功能。&lt;/p&gt;
&lt;p&gt;顺便一提的是，之前在十月份申请的奖学金也终于发下来了，确实是因为体育成绩不好，所以说一直不能拿到国家奖学金，但是学院提供的这个奖学金也是很不错的，也是一度让我的存款又一次变多了。事实上现在发现了一个奇怪的处境，这也是之前我多次预言，但是认识我的人都不太相信的一个情况，就是现在确实我已经失去了消费的欲望，没有什么想要买的东西了。&lt;/p&gt;
&lt;p&gt;之前可能需要一个好用的鼠标，或者说键盘显示器，以及耳机和方便我在课上使用的轻薄本电脑，这些都花了很多钱，也给了很多认识我的朋友一种我花钱大手大脚的感觉。虽然确实比较大手大脚，可能会买下来一些不是特别特别有用的东西，但是我买的大多数东西还是具备一定使用刚需的，而现在这种具备刚需的消费已经不存在了，所以现在的开销基本上也就只剩下每天的吃饭，再没有其他事情。&lt;/p&gt;
&lt;p&gt;总的来说，这个星期依然是正常的一个星期，平平无奇，伴随着身体有一些不适，以及依然满功率负荷的工作。这种处境或许我有必要改变，按照之前记录周记的想法来说，我一直打算的是可以通过周记发现自己相较于上一周具备哪些明显的提升，但是伴随着期末的临近以及各种事情的变多，每周好像都和上一周相同。不过目前的压力确实很大，其实有的时候也会在半夜因为太累而失声哭泣，我倒也不打算再逼自己一把，还是尽量要在假期的时候把状态调整回每天都可以进步的情况，在每天写代码的同时，也需要注意适当的跑步健身，论文阅读，以及学习其他的技术知识。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-22-zh.webp"/><enclosure url="https://picr2.axi404.top/week-22-zh.webp"/></item><item><title>GPT 转发站使用与收集</title><link>https://axi404.github.io/blog/gpt_collection</link><guid isPermaLink="true">https://axi404.github.io/blog/gpt_collection</guid><description>使用 NextChat 调用 GPT api key，并且无推广单纯记录一些 GPT 转发站。</description><pubDate>Sun, 24 Nov 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;写这篇内容没有别的原因，单纯就是因为，很多人问我关于 GPT 相关的内容，包括说各种如何使用，各种充值相关的，但是实际上虽然说在 OpenAI 的官方去使用确实一些功能强大不少，但是实际上，很多人还是不太会用，且充值也需要大费周章。而转发站没有门槛，可以使用国内的网络链接，所以说在这里记录一下。相同的内容我有放在我之前给 RoboMaster 社团写的内容里面，即 &lt;a href=&quot;https://xjtu-rmv.github.io/%E5%BF%AB%E9%80%9F%E6%9F%A5%E9%98%85/gpt.html&quot;&gt;[RMV001|使用 GPT 转发站]&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;为什么要使用 GPT 转发站&lt;/h2&gt;
&lt;p&gt;使用 GPT 转发站出于一个十分简单的初衷，即，GPT 本身需要具备代理才可以正常访问，这本身对于一些环境或者对于一些初学者来说就已经十分的不友好，而同时，假如说想要使用 GPT 的高级功能，比如说 GPT-4 或者 GPT-4o，更是需要充值成为 GPT 的会员，这意味着你需要 VISA 卡并且容忍高额的开销。&lt;/p&gt;
&lt;p&gt;然而一个事实是，一般来说你并没有机会用到这么多的 GPT，一个月 20 美元的消费完全是多余的，但是 GPT-4o 的使用需求又大概率始终存在，因此此时找到一个可以按量计费的方法并且价格便宜的途径就至关重要了，即使用 GPT 转发站订阅 GPT API。&lt;/p&gt;
&lt;h2&gt;如何使用 GPT 转发站&lt;/h2&gt;
&lt;p&gt;首先你需要注册一个 GPT 转发站的账号，一般来说，你只需要一个邮箱即可，然后你就可以在 GPT 转发站上进行充值并且创建一个令牌。这个令牌就是你用来访问 GPT API 的凭证，你可以在 GPT 转发站上看到你的令牌，并且可以在 GPT 转发站上看到你的使用情况。&lt;/p&gt;
&lt;p&gt;由于目前市面上大多数的转发站使用的都是 New API 这个开源平台进行创建的，因此长得都十分的相似，在这里随便举一个例子，注册账号之后进行充值：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.64dt4n029m.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后点击侧边栏的令牌，可以创建一个令牌，比如正常来说，选择永不过期以及无限额度即可，然后保证自己的令牌不要泄露：&lt;/p&gt;
&lt;p&gt;复制你的令牌，并浏览网站找到你的转发站的接口地址，此时你就已经可以使用 GPT API 了。&lt;/p&gt;
&lt;h2&gt;NextChat&lt;/h2&gt;
&lt;p&gt;NextChat 是一个开源项目，可以直接在 NextChat 中使用 GPT API，并且 NextChat 提供了非常友好的界面，使得你可以方便的使用设置 Prompt，修改聊天内容并且支持历史消息（这些功能，当然，你使用 Python 进行 request 同样可以实现，但是过于过于过于复杂）。&lt;/p&gt;
&lt;p&gt;前往&lt;a href=&quot;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases&quot;&gt;其 Github 的 Release&lt;/a&gt; 中找到 &lt;code&gt;-setup.exe&lt;/code&gt; 结尾的 Windows 安装包，值得一提的是，这一软件同样可以在 Ubuntu 使用，使用 &lt;code&gt;.deb&lt;/code&gt; 的安装包即可，然后正常进行安装。&lt;/p&gt;
&lt;p&gt;安装之后，可以在设置界面，设置接口地址以及令牌即可：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.60u76xvf8t.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后就可以正常使用了，尝试说一句话试试。&lt;/p&gt;
&lt;h2&gt;GPT 转发站收集&lt;/h2&gt;
&lt;p&gt;这些 GPT 转发站都是我随手收集来的，假如有站主或者其他同学有其他的转发站想要推荐或者推广，也可以在下面留言，我会进行更新，本身不支持充值，按照价格以及站名字典序排序。本人只是进行信息收集，大家在充值的过程中需要小心转发站跑路的可能性，本人已经进行了警告，不负任何责任。以下格式为，[网址，汇率（美元 : 人民币）]。&lt;/p&gt;
&lt;p&gt;| 网址 | 汇率 |
| --- | --- |
| &lt;a href=&quot;https://api.kksj.org/&quot;&gt;https://api.kksj.org/&lt;/a&gt; | 1 : 0.9 |
| &lt;a href=&quot;https://api.gptai.cc/&quot;&gt;https://api.gptai.cc/&lt;/a&gt; | 1 : 1.5 |
| &lt;a href=&quot;https://api.nekoapi.com/&quot;&gt;https://api.nekoapi.com/&lt;/a&gt; | 1 : 1.5 |
| &lt;a href=&quot;https://gpt.0kk.top/&quot;&gt;https://gpt.0kk.top/&lt;/a&gt; | 1 : 1.5 |
| &lt;a href=&quot;https://api.oneabc.org/&quot;&gt;https://api.oneabc.org/&lt;/a&gt; | 1 : 2.1 |
| &lt;a href=&quot;https://aigcbest.top/&quot;&gt;https://aigcbest.top/&lt;/a&gt; | 1 : 3 |
| &lt;a href=&quot;https://aium.cc/&quot;&gt;https://aium.cc/&lt;/a&gt; | 1 : 3.5 |
| &lt;a href=&quot;https://sg.uiuiapi.com/&quot;&gt;https://sg.uiuiapi.com/&lt;/a&gt; | 1 : 3.37 |&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/gpt_collection-zh.webp"/><enclosure url="https://picr2.axi404.top/gpt_collection-zh.webp"/></item><item><title>周记 Week21</title><link>https://axi404.github.io/blog/week-21</link><guid isPermaLink="true">https://axi404.github.io/blog/week-21</guid><description>2024-11-18 ~ 2024-11-24.</description><pubDate>Thu, 21 Nov 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;可以说这一次的周记终于来得及在周中进行记录了，而不是拖延到这一周结束之后才开始姗姗来迟的记录。仔细回顾一下之前周记拖延的原因，主要还是因为 CVPR 的原因，以及随着经常的下雨以及天气转寒，我通常习惯的一边散步一边语音输入来记周记的习惯都泡汤了，所以说实在是没办法忙里偷闲，来记录这一段时间发生的事情。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;不过好在现在 CVPR 已经是结束了，也就剩出来了一点点的时间来做其他事情。虽然说，事实上 CVPR 的这个工作，有着更多的可行性以及空间，很可能在接下来的一个月的时间内依然会是我主要的工作内容，然而，显然的是，这些时间的消耗只能让工作变得更加 solid，而不能带来更进一步的产出。&lt;/p&gt;
&lt;p&gt;这究竟对于我来说是不是一个好消息呢？我说不准。假如说上海那边确实最后可以要我的话，那么这显然是不错的结果，我可以踏踏实实在这边做出一个具有一定影响力以及对于全局也有贡献度的工作，但是假如说将来被驱逐出去了，那么这篇工作对于其他学校来说，可能因为不需要类似的技术栈，所以说反而不需要我这样的人。&lt;/p&gt;
&lt;p&gt;我认识的其他的正在科研的同学大多数都在进行 learning based 的模型研究，无论是多模态还是具身智能，但是讲实话，在现在这个阶段中，我实在是无法体会到 learning based 的魅力所在。具身领域中的数据缺乏，使得 learning based 方法实在无法获得长足的进展，而大多数的工作虽然说找到了新颖的角度，但是解决问题的手段依然是增加一个 encoder 或者一些其他的模块来进行处理，给出一些形而上学的公式解释，但是这些公式大多数并没有实际的价值，模型的性能提升也是主要因为表征经过 designed 之后可以带来更具指向性的信息。我并不排除一些内容是有价值的，但是在当下的情况下，学界在具身领域暂时只能在 scaling up 以及 RL Based 里面多下功夫了。至于多模态，这段时间终于空闲下来了，可以找时间读一读论文，改天再来锐评。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;总的来说，这个星期依然没有什么好说的，因为这周又是两周一度的有作业的一周，所以说在熬夜的过程中还需要兼顾写作业，可以说是十分的折磨了，不过好在一切在我顽强的努力之下都赶上了。可以说最后两天又是经典的熬夜，虽然说我习惯于昼夜颠倒，但是这明显和熬夜还是不一样的，连续二十多个小时不睡觉，确实会让人的神智变得不清醒，而且头疼欲裂，假如还有下一次，一定不要做的这么极限了。值得一提的是，这次 CVPR 的正文和 sup 都是几乎 15:59 卡点提交的，可以说实在是有点极限了，令人忍俊不禁。&lt;/p&gt;
&lt;p&gt;这周是一个短暂的逗号，之后还有不少的事情需要做，无论是复习，还是继续科研，以及也需要 work paper balance 一下，拿出来一些时间来读论文，从长计议吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-21-zh.webp"/><enclosure url="https://picr2.axi404.top/week-21-zh.webp"/></item><item><title>周记 Week20</title><link>https://axi404.github.io/blog/week-20</link><guid isPermaLink="true">https://axi404.github.io/blog/week-20</guid><description>2024-11-11 ~ 2024-11-17.</description><pubDate>Mon, 18 Nov 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;上一周的周记，事实上写的时候这一周也已经过完了，所以有点故意卖关子的意思，事实上，CVPR 还是赶上了。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;在这一周里面的科研可以说是按照小时来计算时间的，基本上不允许任何一个小时出现错误，然而到了最后模型的训练还是出了差错，还好这篇工作不是主打的这个，所以暂时避重就轻地掠过了。除此之外可以说是惊心动魄，直到截止的最后一个小时，我们还在疯狂地写，以及检查一些问题，并且在截止前十分钟将第一版提了上去。然后就是进行了一些缝缝补补，又是一些别的修改，提交了别的版本。&lt;/p&gt;
&lt;p&gt;因为论文还没有发表，所以这些内容也不能说太多，那些在这里面吃的苦受的累，也许等将来论文发表了，至少是挂出去了，我会写一个回忆篇，回忆从开始接触这个到现在走过的路。&lt;/p&gt;
&lt;p&gt;在这个过程中，其实令我很触动的一件事情是，在最后的两天时间里面，mentor 找来了很多人来帮我，可以说是萍水相逢拔刀相助（事实上也不能这么说，毕竟在上海的时候，大家经常一起吃饭，也算是聊得来，虽然是上下级关系，但是氛围还是很扁平化的。不过确实对于这个课题来说，对他们确实是打扰，毕竟本来与他们无关），大家分别给论文写一些内容，然后修改语法以及提出建议，把故事理得更顺一些，都是付出了很多的精力。有一种类似的说法或者感觉，毕竟虽然大家关系很扁平，但是实际上好多都是百引或者千引的大佬，说得上一句，我何德何能，能够有此殊荣了。只能说这件事情的发生，更加坚定了我将来想去这边的想法，大家对我确实不薄，而且我也很喜欢那边，自然不能辜负。&lt;/p&gt;
&lt;p&gt;说起来也是单打独斗习惯了，无论是之前的工作，还是这个课题的前半段时间，大多数时候除了老师的讨论之外，都是一个个人在默默地写代码以及做其他事情，可以说是孤军奋战，然而而到了后面越来越多的人来帮助我，最后才让这个课题得以写成论文，颇有一种点点星光汇聚成耀眼的太阳那种感觉，也着实是温暖了我。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;除此之外也没有别的值得分享的了，或记录一下我和乐小姐的日常生活。周六深夜十一点多，她那边晚上考完试，然后和我说有点饿了，我向来就是一个行动派，就要带她去吃海底捞。结果到地方发现她把她的兔熊（一个毛绒玩具，是我送给她的，属于 Lolita 的搭配，六七百块钱）丢在了车上，然后伤心了好久。好在和司机打了电话，最后把东西送回来了，不过也从本来的吃海底捞改变成了去吃火炉旁烤肉。不过我们的印象确实没有错，这烤肉确实有一些柴，和之前的焰请旗鼓相当，也不知道是为什么，完全比不上齐齐哈尔烤肉（而且我确信这不是因为齐齐哈尔烤肉增加了添加剂，因为之前回齐齐哈尔的时候，家里人做的也是这样的嫩）。&lt;/p&gt;
&lt;p&gt;另外一件或许可以说一下的是，之前某一天晚上起夜有感，发现目前的算是睡衣的上衣还是十分可爱的，但是下装只能穿外裤。于是我自己比划了一下，购买了功能约等于睡裤+出去时候的安全裤的喇叭短裤，感觉十分可爱。&lt;/p&gt;
&lt;p&gt;就先这样吧，也算是追平了目前的时间，希望一切顺利，之后有越来越多的事情要做了。六级，课内考试，以及 CVPR 的附录，加上和女朋友打一些水赛，这都是时间开销，而我只是想懒惰地岁月静好。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-20-zh.webp"/><enclosure url="https://picr2.axi404.top/week-20-zh.webp"/></item><item><title>周记 Week19</title><link>https://axi404.github.io/blog/week-19</link><guid isPermaLink="true">https://axi404.github.io/blog/week-19</guid><description>2024-11-04 ~ 2024-11-10.</description><pubDate>Sun, 17 Nov 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;尽管说起来上一次，说的是记完上一次之后就很快要记这一周，然而因为各种各样的事情还是耽搁了，事实上确实是科研的压力太大了，而且还伴随着其他的琐事。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;现在距离 CVPR 也就剩下的寥寥无几的不到两周时间，科研压力也越来越大。一方面要做数据生成，这是我的科研内容的一部分，随着时间的推移，后面应该也可以慢慢更多的透露我在做什么，当然，等到工作发表之后，我会完整讲一讲。另一方面又有其他的很多工程内容需要去实现。要是满打满算来看，实际上这次科研我足足准备了三个月时间，不过从现在来看，还是有点太勉强了。主要的一个多月时间，我都在熟悉这个框架，剩下的时间有课内的压力，而且我所在的框架的生态并不友好。大量的内容，要是文档里有都已经谢天谢地了，有可能你需要的一些功能或者遇到的问题，都只能在论坛里面寻找答案，甚至要自己去看源代码，这毫无疑问极大的降低了我的开发效率。&lt;/p&gt;
&lt;p&gt;而且和那位大四的同学一起合作的过程中，另外一件事情，当然也是之前感慨过的，就是一些套路化的代码积累真的很重要。目前基本上每一个功能我都是手动实现的，这毫无疑问给我带来了巨大的负担，把大量的时间浪费家的不必要的地方。&lt;/p&gt;
&lt;h2&gt;北理工学术会议&lt;/h2&gt;
&lt;p&gt;这一周还值得一提的事情是，受到了辅导员的邀请，我去北京理工大学参加了一次学术会议。与其说是学术会议，其实就是北京理工大学那边的荣誉学院，和几个兄弟院校的荣誉学院（包括我们西安交通大学），邀请我们几个学院出一个人，和他们的一些人一起分享一下自己的科研成果。&lt;/p&gt;
&lt;p&gt;讲实话，我并不是很喜欢之前的工作，当然这并不是说我会把它看作是黑历史，实际上这篇工作相当不错，也在中稿之后给我带来了巨大的持续收益，不过相较于现在的我更加喜欢大规模的工作或者那些能够指向问题本质的工作来说，显然之前的这一篇实在是太儿戏了。&lt;/p&gt;
&lt;p&gt;因为要去参加这个会议，所以我又特意制作了 PPT，现在这一套成熟的 PPT 是根据之前去上海那边面试的 PPT 修改改过来的，经过了这几次迭代，想必将来也就可以在其他人再邀请我的时候，不需要耗费更多的精力。&lt;/p&gt;
&lt;p&gt;现在我非常看重积累这件事情，然而这不是一个可以分享的经验之谈，因为对于新手来说，即使他们知道也无从积累，而对于老手来说，在体验了几次更加熟练的人依靠积累建立的优势之后，也就自然而然会出现这种想法了。&lt;/p&gt;
&lt;p&gt;无论是学习资料，还是自己的一些经验分享总结，或者其他的一些东西，比如说水赛的课题，在我某一次耗费了大量的精力，将这个东西实现之后，我希望他可以持续地为我创造价值，而不是一次短暂的瞬时收益。&lt;/p&gt;
&lt;p&gt;这个看法，其实我觉得在当下尤其重要。事实上一名老练的科研工作者，相较于那些新手来说，本质的学术水平的差距已经不再是那么明显了。不同于数学存在大量需要彻底理解的公式，很多深度学习方面的见解都是浮于表面的，只要有一名尽职尽责的学长带路，不到一个月，这个人就可以醍醐灌顶。所以真正区分不同人的价值的区分点在哪里呢？关键就在于工作的效率。&lt;/p&gt;
&lt;p&gt;有的人只能主导一个课题，而有的人可以在主导自己课题的同时，为别人的课题提供数据，跑一些实验，以及写论文和画图，从而成为了二三四五作。当然这种看法并不是说，想要大量地添加无数的没有意义的论文到自己的名下，或者出于合作关系的挂名，而是确确实实出了一份力在有价值的课题里。&lt;/p&gt;
&lt;p&gt;每一个人的时间都是相同的，假如我们都同样聪明，也都同样努力，那么是什么让我们分出了三六九等，答案是效率，而为什么我能够在一段时间内做出更多的事情？是积累。&lt;/p&gt;
&lt;p&gt;这种积累体现在两个方面。一方面是知识的积累，比方说对于现在的我，我已经可以比较熟练的部署各种模型，我熟悉我的仿真平台，我熟悉调用 GPT，然而我不熟悉真机的使用，不熟悉模型训练以及集群使用，也就让我需要在这些任务上花费更多的时间（事实上这一点也是之后，我打算进行自我积累的），对于其他人来说，他们已经掌握，就不会像我一样花很多时间在处理简单的报错上面。另一方面就是代码的积累，可能我采集数据有一种自己的格式，而不同的模型有不同模型的格式，我对于某一个模型的格式写了一套转换的代码，而且我也熟悉了这个模型的训练流程，我就可以毫无压力的进行这个模型的丝滑训练。假如恰好别人需要这个模型，那么他把数据给我，数据格式转换以及模型运行的一体化代码，就可以快速地完成他的需求，从而成为合作者。&lt;/p&gt;
&lt;p&gt;这大概也是我最近的一些感受，也算是一些分享了，不过最近确实感觉，CVPR 貌似有点来不及了，我也不知道如何才能赶得上，但是还是尽到自己最后一次努力吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-19-zh.webp"/><enclosure url="https://picr2.axi404.top/week-19-zh.webp"/></item><item><title>周记 Week17</title><link>https://axi404.github.io/blog/week-17</link><guid isPermaLink="true">https://axi404.github.io/blog/week-17</guid><description>2024-10-21 ~ 2024-10-27.</description><pubDate>Tue, 05 Nov 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;老实说这周其实过得没什么波澜，就是正常的学习、熬夜以及生活。CVPR 其实已经在尽力地赶了，但是实际上还是收效甚微。现在的事情实在是太多了，有很多事情要做，也有很多人给了我很多期待，以及一些别的，更多更多的压力。现在在上海这边科研，可以说已经基本进入了平稳期了。可以预见的是，这个课题应该在 CVPR 之后还会继续持续一段时间，来达到一个比较理想的可以建立有影响力工作的一个程度。&lt;/p&gt;
&lt;h2&gt;轻薄本&lt;/h2&gt;
&lt;p&gt;一件值得记录的事情就该是因为之前一直觉得在课上只能睡觉（因为需要有一定量的时间来办公，要不然晚上熬夜办公白天睡觉，要不然白天办公，但是白天没电脑）实在是太过于浪费时间了，而且按理来说我的白天时间应该还是精力比较充沛的，结果还是因为必须熬夜办公，所以白天睡觉。甚至来说，这样做对于身体还是很不好的。因为种种原因，于是我决定自己自费来购买一个轻薄本，可以说把在上海这边实习赚到的钱又花在这边了。购买的是华硕得一个 228V 的 intel lunar lake 的电脑，但是讲实话，无论是续航还是性能确实没有到我的预期，而且价格高达六千多，不过感觉总归是够用的，续航虽然完全到不了想象中的十小时，加上我的充电宝，保持一天的使用倒是确实不是问题，所以一般吧，没有物超所值，但是值得。不过貌似现在来看，lunar lake 似乎还有不少的 bug，看看之后是不是会修复吧。&lt;/p&gt;
&lt;h2&gt;焰请&lt;/h2&gt;
&lt;p&gt;除此之外，貌似也没有什么过多的可以记录的内容，无聊的一周，除了和乐小姐在一起的时候可以带来一丝宽慰，其他的貌似就也是平平淡淡地度过了。值得一提的是，和乐小姐一起去吃了海底捞新开的一个烤肉店，叫做焰请，感觉性价比烂的一塌糊涂，而且在吃饭的时候，因为离某个音响比较近，他们又有什么土嗨 DJ 活动，巨大的声音让我的心脏很是不舒服，服务员都在摇摆，也没有人理。可以说没有海底捞的质量，没有海底捞的服务，没有海底捞的性价比，但是继承了土嗨和莫名其妙，不会再来了。至今为止在西安吃过的比较好吃的烤肉还是西交兴庆校区东南门的齐齐哈尔烤肉，但是貌似在今年的五月份过后就关门了，也算是一种遗憾。几十块就可以买到很多肉，可以让我和乐小姐两个人一起爽吃，而且肉质也很新鲜（焰请感觉肉有点柴）。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-17-zh.webp"/><enclosure url="https://picr2.axi404.top/week-17-zh.webp"/></item><item><title>周记 Week18</title><link>https://axi404.github.io/blog/week-18</link><guid isPermaLink="true">https://axi404.github.io/blog/week-18</guid><description>2024-10-27 ~ 2024-11-03.</description><pubDate>Tue, 05 Nov 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以这其实又是很久没有记周记之后，再一次迟迟才更新的一天。自从上次更新之后，又是隔了两周，然后才开始更新这一周的内容。隔两周显然不能让我很好地想起来这周发生的事情，如此看来今后要注意一下。&lt;/p&gt;
&lt;p&gt;总来说，这周还是忙碌的一周，或者换句话说可以说是抽象了。可以说是接连不断的事情不断地催促我熬夜，这一整周下来都没有一天是安宁的。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;一方面还是先说科研吧，这周课题里面增加了一位新同学，是一位浙大的大四学生，实力可以说是非常非常非常强，只能说在此之前我的进度都是由我自己一个人把控，而在他加入之后，第一次感受到了被 push 的感觉。且不说程序写的数量，在整体的构思以及论文的方面，包括说作图，可以说水准都是一流的，很快将一个只有表格的贫瘠论文变得比较有像样的模样，在这方面还是功不可没的。&lt;/p&gt;
&lt;p&gt;在此之前可以说我一股脑都将心思放在了代码上面，所以说在论文上面当时还没有下很多功夫，现在来看，如果当时没有这位同学的加入，我一个人在去做双线程可以说几乎不可能完成了，甚至直到 CVPR 结束很久，可能都到不了现在的进展。&lt;/p&gt;
&lt;p&gt;只能说大四同学确实厉害，相较于我还是懂很多其他的东西，而且也可以说是自己应该之前有很多 codebase 的积累，这就使得一些任务对他来说更加得心应手。这方面确实是我需要吸取的经验，在完成这篇工作之后，我也需要整理一下自己目前有的东西，然后做一个属于自己的完整的 utils 库，免得将来相同的功能还需要重复实现。&lt;/p&gt;
&lt;p&gt;总的来说，虽然进度依然很赶，但是可以说多了一丝胜利的曙光，尽管不多，但是也让我这样的飞蛾不禁扑火，去努力做一些成果出来，不愧对自己之前的付出。&lt;/p&gt;
&lt;p&gt;不过现在来看我其实对于这篇文章是否会中稿已经没有很多的执着了，尽管大三再有一篇 CVPR 是一个十分风光的事情，然而不得不说的是，我现在可能更加关注于在这个我才加入了的领域里面能否做一些自己有影响力的工作。&lt;/p&gt;
&lt;p&gt;尽管这篇的二分之一的立意是 mentor 给出的，但是在长时间讨论以及迭代之后，我认为里面还是表达了一些我自己的观点的，具体来说还是等论文 release 之后再说。而在之后我应该会去做一些更大的团队里面更大型的工作，这方面就自然有一些需要学习的新知识了，我已经找到了我需要学的内容，就差到时候闲下来开始沉淀。&lt;/p&gt;
&lt;h2&gt;课程安排&lt;/h2&gt;
&lt;p&gt;另一方面一样有必要吐槽的是学校的课程安排，虽然我可以理解学校设置课程是教书育人的本职工作，然而一个事实是，在当下的这个情况下，大多数课程的内容都不会对目前的科研有任何帮助，而只是在浪费时间。在这里的描述并不是对于老师的教学水平或者课程质量的贬低，而是这确实是一个现象，对于当代的大学生来说，课程除了给他们绩点帮助他们升学之外，可以说没有任何的价值，这些远离实际的实践以及科研一线的内容，除了在试卷上设计出几道弯弯绕绕的题目之外，并不能起到任何的良性作用。&lt;/p&gt;
&lt;p&gt;这种现象一直存在，倒也不能令我反感，然而实在是这个学期的作业实在太多，我确实没有时间投入，也就导致出现了周二赶周三的作业，周三赶周四的作业，周四赶周五的作业，这种滑稽的现象。一方面我更对科研有兴趣，并且自认为还是有能力在里面做出一些成果的，然而另一方面令人遗憾的是，在课内的诸多事情太过多的牵扯了我的精力，让我疲于奔命，而心中放不下一张平静的书桌。&lt;/p&gt;
&lt;p&gt;这周能够说的大概也就只有这些，一些希望和一些内耗，写到这里，之后有时间要抓紧写下周的周记，这次还是因为特意记下来了这两个话题，才能够顺利地展开讨论，从而水了一些字数，而下一次可就不好说了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-18-zh.webp"/><enclosure url="https://picr2.axi404.top/week-18-zh.webp"/></item><item><title>给乐小姐的教程</title><link>https://axi404.github.io/blog/tech4girlfriend</link><guid isPermaLink="true">https://axi404.github.io/blog/tech4girlfriend</guid><description>给女朋友的教程，所以从计算机完全零基础出发，教授一些学习生活中常用的技能。</description><pubDate>Mon, 28 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;GPT 使用指南&lt;/h2&gt;
&lt;p&gt;这部分主要讲一下如何使用转发站来无痛使用 GPT。&lt;/p&gt;
&lt;h3&gt;安装软件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;下载软件 NextChat，网址是 &lt;a href=&quot;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases&quot;&gt;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.1hs7tvsckm.webp&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在这个界面里面，存在一个选项，以 &lt;code&gt;_x64-setup.exe&lt;/code&gt; 结尾，这个是 64 位的安装包，单击下载。下载完成之后双击打开。&lt;/li&gt;
&lt;li&gt;可能显示阻止程序启动，点击更多信息，然后点击仍要运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;正常选择一些信息，比如说安装位置。创建一个桌面快捷方式，并且运行程序&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;配置 NextChat&lt;/h3&gt;
&lt;p&gt;NextChat 本质上是一个 GPT 套壳工具，我们可以使用它来调用 GPT 的服务，但是我们需要指定我们的 API Key 以及接口地址。简单理解一下，API Key 就像校园卡一样，接口地址是食堂窗口，每次向接口地址请求 GPT 的服务，就会记账到 API Key 对应的账号上面。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;点击设置&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;向下翻，可以找到接口地址以及 API Key 两项&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认情况下应该为 openai 的网址。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;修改接口地址为提供转发服务的服务商接口。&lt;/li&gt;
&lt;li&gt;将 API Key 修改为你的转发站的 API Key。&lt;/li&gt;
&lt;li&gt;在下方的 Model 中下拉并且找到 GPT-4o&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;开始聊天&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;点击新的聊天：&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;对于跳出的面具显示，选择不再展示，然后确认：&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;对于全部下方的按钮，只需要在意这个机器人图标，即使用的模型，可以看一下是否是 GPT-4o：&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;然后正常打字，打个招呼吧~&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;猫猫是一种可爱的生物！&lt;/p&gt;
&lt;h2&gt;录播下载&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;关于如何在 &lt;code&gt;http://class.xjtu.edu.cn&lt;/code&gt; 爬取视频。本教程对于电脑小白来说看上去吓人，但是实际上一步一步来就好。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先点击 &lt;code&gt;F12&lt;/code&gt;（关闭这个页面可以点击 &lt;code&gt;x&lt;/code&gt; 或者再次点击 &lt;code&gt;F12&lt;/code&gt;） 并且点击 Network，这可以让我们看到网页的请求信息（本网站的网页播放视频的逻辑是最为基础的，直接请求，也就是进入网页之后，会类似于在下载一样，从服务器中一直下载内容，而播放就是一边下载一边播放）。&lt;/p&gt;
&lt;p&gt;点击 Media 可以进行一下筛选，就只会看到目的是视频的请求了，这时候一般会出现四个请求，假如没出现的话，刷新一下课程页面。可以见到一般来说的四个内容，其中后两个（不是 &lt;code&gt;preview&lt;/code&gt; 开头的内容）是视频资源。&lt;/p&gt;
&lt;p&gt;众所周知录播平台有两个视频源以及两个音频源，其中视频是讲台摄像头视角以及电脑录屏视角，而音频则是一个话筒的麦克风以及一个电脑的麦克风，其中电脑的麦克风一般来说很炸。&lt;/p&gt;
&lt;p&gt;假如说希望看 PPT，可以两个都下载，不过还是建议拿到 PPT 课件，配上正常的讲台视角+电脑录屏来使用。&lt;/p&gt;
&lt;p&gt;点击某一个视频源，比如说这个 0 开头的，可以看到右侧的 Headers 里面有一项为 Request URL，后面跟着一大堆链接，复制这个链接，然后新建标签页，打开这个链接，会提示视频下载，下载即可。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/tech4girlfriend-zh.webp"/><enclosure url="https://picr2.axi404.top/tech4girlfriend-zh.webp"/></item><item><title>周记 Week16</title><link>https://axi404.github.io/blog/week-16</link><guid isPermaLink="true">https://axi404.github.io/blog/week-16</guid><description>2024-10-14 ~ 2024-10-20.</description><pubDate>Wed, 16 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;只能说是好险好险，差一点点就没有办法保持连载了，也可以说这一段时间确实十分的忙碌，所以说难以有比较多的时间来放在写周记里面，不过至少没有拖满一周，所以说姑且还是算是一节迟到的周记吧。&lt;/p&gt;
&lt;p&gt;总的来说，这周和上周其实依然很像，要是说主旋律是什么的话，毫无疑问依然是压力以及科研。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;因为安排的问题，所以这篇工作其实打算投稿 CVPR 2025，但是很明显时间并不一定来得及。实际上我的预期是在投稿截止后一周才可以完成这篇工作，但是既然大家都这么要求了，我可能也就需要赶一赶了。这篇工作出于入职时候的保密协议考虑（虽然貌似其实没多少人在意这个），暂时先不在周记中记录，但是等投出去之后会进行一下讨论以及宣传。&lt;/p&gt;
&lt;h2&gt;RoboMaster&lt;/h2&gt;
&lt;p&gt;除此之外，感觉可以简单记录一下的，应该也就只剩下 RoboMaster 的培训了。应该说总体来讲，我是比较失望的，今晚很多同学做得比当初我还要好，但是反观培训的针对力度，我认为依然是比较清晰的给出了学习的路线以及应该学什么，再加上现在有 GPT 的辅助，完成这些任务应该并不算难，但事实上效果并不算好。&lt;/p&gt;
&lt;p&gt;一个广泛存在的现象是，对于这个非常简单的任务来说，绝大都是同学到了第二周的最后一个周末才开始提交，我的预期实际上是在任务布置后的三天内，Version 1 的任务要求就已经能被大多数同学完成了。因为事实上之前已经经过了培训，完成了能量机关识别的任务。按理来说，识别任何一个物体，或者使用 OpenCV 来手动构建符合这个物体的特征，应该并非一件难事。&lt;/p&gt;
&lt;p&gt;因此，一个不假思索的答案就形成在了我的脑海里，那就是大多数同学依然等待了 DDL 的时候才进行了任务的完成。我们需要承认之前一段时间的培训以及任务确实会给同学们带来疲惫和压力，但最后一次任务依然期待这些新组员可以以比较积极的心态去完成。因为事实上反观剩下两个组别的任务，培训不是针对任务的培训，而任务也比这边难得多，但是同学们依然可以很积极的完成。&lt;/p&gt;
&lt;p&gt;如此，最后我还是舍弃了自己的名额（事实上下半年我可能也比较难以全力投入其中，因此也是半个隐退），然后拉了七名同学进来，但讲实话，这个结果我也并不是十分满意。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;和乐小姐的恋情还在正常发展，有时间就会她来找我或者我去找她，然后两个人在一起独处一段时间。可以说和乐小姐的生活已经成为了漫长时间中我努力的精神支柱了，乐小姐的拥抱也确实可以带给我一种心安。&lt;/p&gt;
&lt;p&gt;除此之外，一些计划也要准备开始实施，比如说之前一直想着和大家一起搭建一个 CS-BAYAN-Wiki，我使用 Astro 重构了目前的 Vitepress 页面，这使得组件的开发更加简单，而且本身我目前使用的主题也有很多不错的特性，后续可以安排一下做什么这件事。&lt;/p&gt;
&lt;p&gt;总的来说，这周其实主要做的事情还是科研，但是就是在一味地写代码以及推进度，所以说其实没什么好记录的，可能等到 CVPR 之后我会考虑大规模的更新一些自己的博客，我现在积攒了一些我读过的 paper list，但是还没有机会写下来一些看法。敬请期待吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-16-zh.webp"/><enclosure url="https://picr2.axi404.top/week-16-zh.webp"/></item><item><title>周记 Week15</title><link>https://axi404.github.io/blog/week-15</link><guid isPermaLink="true">https://axi404.github.io/blog/week-15</guid><description>2024-10-07 ~ 2024-10-13.</description><pubDate>Tue, 15 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说这其实又是很久以后的一次周记，可以说又是迟到了，但是感觉这也是迫不得已的事情。本来想着从意大利回来之后，可能伴随着日子进入正轨，一切都能好过一些，但是事实证明，这一切只是我个人的痴心妄想罢了。事实上，我对于我个人的能力还是有一些自信的，但也不知道是因为什么，事实上，我目前主导的课题确实不是十分顺利。&lt;/p&gt;
&lt;h2&gt;科研&lt;/h2&gt;
&lt;p&gt;一方面是因为我觉得这个课题的设定实在是有些太极端了，在数据十分匮乏的情况下，仅仅通过一些筛选就想获得到理想的结果，这件事情确实有很大的难度。而同时也让我比较难以接受的一件事情是，我在科研这一课外事项之外，课内依然给了我很大的压力。在我的大二时光中，事实上课堂中布置的作业并不算很多。而与此同时，这也给了我充分的时间进行自我的发展以及探索。我本来以为这是我们专业的优势，直到来到了大三之后，我基本上每一门主课都要布置漫长的作业，甚至一门计算机视觉的课程，还要布置繁琐的实验报告，动辄要写十页以上。这让我感到压力很大，也让我有些不知所措。&lt;/p&gt;
&lt;p&gt;反正因为种种原因，最后这次周记还是迟到了。而且在细想之下，这一周经历的事情除了反反复复的熬夜、通宵以及解决各种各样的问题和承受压力之外，好像也确实没什么可说的了。&lt;/p&gt;
&lt;h2&gt;趣事&lt;/h2&gt;
&lt;p&gt;不过，既然已经开启了这一篇章，我觉得还是有必要说一些有意思的事情。&lt;/p&gt;
&lt;p&gt;一方面，因为保研季节的结束，绿群的换届也已经准备开始了。其实这件事情并没有什么悬念，无外乎就是我因为在包括 Github 仓库的建设以及前一年的诸多管理的一系列内容的基础上，成功成为了群主。可以说之前绿群的一些机制实际上已经十分成熟了，假如说目前全部的新的管理员们都在积极地参与到群的管理中，那么应该维持基本的功能以及保持去年的水平肯定是没有任何的问题的。&lt;/p&gt;
&lt;p&gt;当然，就我个人而言，我还是有一些更多的计划的，因为自己之前搭建过比如说西安交大生存指南在内的一系列的教程网站，所以说，将 CSBAOYAN2025 的网站转变为一个类似具有 CS 保研 Wiki 功能的网站是一个十分值得考虑的事情。当然，这件事情也并不简单，因为网站的维护以及运营的成本都不小，按照之前的经验来说，说不定到了最后还是只有我一个人在孤军奋战。&lt;/p&gt;
&lt;p&gt;另一件事则是 RM 这边的面试。讲实话，我本人其实并不反感对于简历进行包装，但是假如将自己完全不了解的内容放到自己的介绍中，这种事情就显得有些过于过分了。起因是 RM 队伍里面说有一个人给我们队伍的指导老师发了邮件（尽管指导老师本身就并不对我们的队伍有任何的指导的作用，而是只是提供一些经费），而且说想要来视觉组。去年就有一位同学使用相同的伎俩发过邮件并且想要进入队伍，并且成功混了进来，但是最后给他安排的任务也没有完成，现在来说结果也就不了了之了。因此讲实话，我对于今年的这人本来就并不抱有任何的希望，加之以走后门的方式来进行面试，自然是要好好拷打一番。&lt;/p&gt;
&lt;p&gt;总的来说，对面的同学是计算机大三，说自己会 C++/Python/Java，并且有过计算机视觉的科研经历以及深度学习的经验，因此想来视觉组。众所周知，尽管放眼全国，我可能并不敢过多的说自己的科研水平如何，但是假如说仅仅在西交这一个学校之中，我个人还是很有把握拷打绝大多数同龄人以及比我小一些的同学的。不过尽管已经做好了这家伙会被我拷打到啥也不剩的心理准备，但是这位的简历造假程度还是令我大跌眼镜。不会用 Git/Linux/CMakeList；说自己会深度学习但是连反向传播都不知道，包装了个软著；说有图像处理经历，不知道最基本的概念，成像原理不通，滤波核没听说过；深度学习是不会的，传统的 OpenCV 也是不了解的，实在是令人难以理解。于是最后将这位驳回了面试。&lt;/p&gt;
&lt;p&gt;我认为我平时还是一个比较温和的人，尤其是在和大多数人交流的时候，但是在面对这种人的时候，我实在是很难耐下性子来给予他最基本的礼貌，更何况在造假的基础上走后门，而且面试也迟到。&lt;/p&gt;
&lt;p&gt;这周大概就是这样，总的来说绿群那边令我十分的开心，同时和乐小姐两个人的二人时光也十分的快乐（我们一起去逛了很多的商场），RM 的面试碰到了抽象人，而最主要的科研，则是依然再艰难但是顽强地推进进度。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-15-zh.webp"/><enclosure url="https://picr2.axi404.top/week-15-zh.webp"/></item><item><title>EndeavourOS 安装踩坑</title><link>https://axi404.github.io/blog/archlinux</link><guid isPermaLink="true">https://axi404.github.io/blog/archlinux</guid><description>品鉴 EndeavourOS 并且进行一些配置。</description><pubDate>Fri, 04 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;按照管理来说，来记录一下踩坑，这次主要是玩了一下 EndeavourOS，并且进行了很多的配置，其中自然也包括一些踩坑。&lt;/p&gt;
&lt;h2&gt;基础安装&lt;/h2&gt;
&lt;p&gt;首先，就是进行基本的安装，这其中还是建议进行联网安装。尽管大多数人都在使用 KDE，但是其实按照我个人的审美来说，最新的 GNOME 看起来还是很好看的，有一种高级感，所以说我选了 GNOME，当然这也带来了更多的踩坑。&lt;/p&gt;
&lt;p&gt;其中在分盘的时候，我删掉了之前的 Ubuntu 20.04 当时安装的单独的 EFI 以及挂载的 ext4。需要注意的是，读者假如说之前都是只用了一块 EFI，千万别删。我是存在一种习惯，每次安装系统都单独挂载一次。给 EFI 开一个 fat32，挂载 &lt;code&gt;/boot/efi&lt;/code&gt; 并且挂一个标签 &lt;code&gt;boot&lt;/code&gt;。剩下的都分配上 &lt;code&gt;brtfs&lt;/code&gt; 并且挂载到 &lt;code&gt;/&lt;/code&gt;，剩下的正常安装就好。&lt;/p&gt;
&lt;h2&gt;基础依赖安装&lt;/h2&gt;
&lt;p&gt;值得一提的是，EndeavourOS 不同于别的，基本上就是纯 Arch，所以说内容都可以查 &lt;a href=&quot;https://wiki.archlinux.org/title/Main_page&quot;&gt;ArchWiki&lt;/a&gt;，以及在我的踩坑过程中，很多内容也都参考了 &lt;a href=&quot;https://arch.icekylin.online/&quot;&gt;archlinux 简明指南
&lt;/a&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -Syu
sudo pacman -S sof-firmware alsa-firmware alsa-ucm-conf # 声 音固件
sudo pacman -S ntfs-3g # 使系统可以识别 NTFS 格式的硬盘
sudo pacman -S adobe-source-han-serif-cn-fonts wqy-zenhei #  安装几个开源中文字体。一般装上文泉驿就能解决大多 wine 应用中文方块的问题
sudo pacman -S noto-fonts noto-fonts-cjk noto-fonts-emoji noto-fonts-extra # 安装谷歌开源字体及表情
sudo pacman -S vim git zsh yay
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然之后也需要按照惯例配置以下 git，在这里就不进行赘述了。&lt;/p&gt;
&lt;p&gt;除此之外需要注意的是，正常安装之后，可能会导致找不到之前你安装的其他系统，这个其实就是因为 grub 里面默认关闭了 &lt;code&gt;os-prober&lt;/code&gt;，没啥大不了的：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -S os-prober
sudo vim /etc/default/grub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行修改&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;# Probing for other operating systems is disabled for security reasons. Read
# documentation on GRUB_DISABLE_OS_PROBER, if still want to enable this
# functionality install os-prober and uncomment to detect and include other
# operating systems.
#GRUB_DISABLE_OS_PROBER=false // [!code --]
GRUB_DISABLE_OS_PROBER=false // [!code ++]
GRUB_EARLY_INITRD_LINUX_STOCK=&apos;&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后重新生成 Grub 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo grub-mkconfig -o /boot/grub/grub.cfg
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;软件安装&lt;/h2&gt;
&lt;p&gt;正如上述说的，安装了 &lt;code&gt;yay&lt;/code&gt;，所以说可以安装一些常用的软件了：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -S code
yay -S nextchat-bin rustdesk-bin linuxqq wechat-uos-qt google-chrome obsidian
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;介绍一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;nextchat-bin&lt;/strong&gt;：一个调用 GPT API 的软件。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rustdesk-bin&lt;/strong&gt;：私有的远程桌面。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;linuxqq&lt;/strong&gt;：QQ。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;wechat-uos-qt&lt;/strong&gt;：微信。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;google-chrome&lt;/strong&gt;：chrome 浏览器。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;obsidian&lt;/strong&gt;：知识库类型的笔记软件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;一些自己使用的内容&lt;/h2&gt;
&lt;h3&gt;快捷指令&lt;/h3&gt;
&lt;p&gt;因为缺少了一些基本的配置，GNOME 一上来的使用体验并不是很好，包括说没有悬浮托盘，以及无法使用 &lt;code&gt;Ctrl+Alt+T&lt;/code&gt; 打开 Console，以及我在使用 &lt;code&gt;Super+L&lt;/code&gt; 的时候的锁屏，直接就黑屏了，没有办法再打开。&lt;/p&gt;
&lt;p&gt;进入设置-&gt;键盘-&gt;键盘快捷键，在系统中禁用锁定屏幕，并且在自定义快捷键中增加命令 &lt;code&gt;kgx&lt;/code&gt; 并快捷键 &lt;code&gt;Ctrl+Alt+T&lt;/code&gt;，增加命令 &lt;code&gt;systemctl suspend&lt;/code&gt; 并快捷键 &lt;code&gt;Super+L&lt;/code&gt;，其中名称可以任选。&lt;/p&gt;
&lt;h3&gt;悬浮托盘&lt;/h3&gt;
&lt;p&gt;也不知道叫什么比较好，应该是类似于悬浮托盘或者小图标，这在最新的 GNOME 里面并不存在，所以说需要进行安装，基本的思路是安装 &lt;a href=&quot;https://extensions.gnome.org/extension/615/appindicator-support/&quot;&gt;GNOME 插件&lt;/a&gt;，在里面引导并安装 &lt;a href=&quot;https://chromewebstore.google.com/detail/gnome-shell-%E9%9B%86%E6%88%90/gphhapmejobijbbhgpjhcjognlahblep&quot;&gt;GNOME Shell 集成&lt;/a&gt;，然后安装 &lt;code&gt;gnome-browser-connector&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -S gnome-browser-connector
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这时候插件应该就可以 turn on 了，没啥问题，直接开启。&lt;/p&gt;
&lt;h3&gt;ZSH&lt;/h3&gt;
&lt;p&gt;我安装了 ZSH，其中涉及我的 &lt;code&gt;.zshrc&lt;/code&gt; 的文件，可以进行一个分享：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;# Enable Powerlevel10k instant prompt. Should stay close to the top of ~/.zshrc.
# Initialization code that may require console input (password prompts, [y/n]
# confirmations, etc.) must go above this block; everything else may go below.
if [[ -r &quot;${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh&quot; ]]; then
  source &quot;${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh&quot;
fi


### Added by Zinit&apos;s installer
if [[ ! -f $HOME/.local/share/zinit/zinit.git/zinit.zsh ]]; then
    print -P &quot;%F{33} %F{220}Installing %F{33}ZDHARMA-CONTINUUM%F{220} Initiative Plugin Manager (%F{33}zdharma-continuum/zinit%F{220})Ã¢Â�Â¦%f&quot;
    command mkdir -p &quot;$HOME/.local/share/zinit&quot; &amp;#x26;&amp;#x26; command chmod g-rwX &quot;$HOME/.local/share/zinit&quot;
    command git clone https://github.com/zdharma-continuum/zinit &quot;$HOME/.local/share/zinit/zinit.git&quot; &amp;#x26;&amp;#x26; \
        print -P &quot;%F{33} %F{34}Installation successful.%f%b&quot; || \
        print -P &quot;%F{160} The clone has failed.%f%b&quot;
fi

source &quot;$HOME/.local/share/zinit/zinit.git/zinit.zsh&quot;
autoload -Uz _zinit
(( ${+_comps} )) &amp;#x26;&amp;#x26; _comps[zinit]=_zinit
### End of Zinit&apos;s installer chunk
# zinit
zinit light zsh-users/zsh-autosuggestions
zinit light zdharma/fast-syntax-highlighting
zinit snippet OMZ::lib/clipboard.zsh
zinit snippet OMZ::lib/completion.zsh
zinit snippet OMZ::lib/history.zsh
zinit snippet OMZ::lib/git.zsh
zinit snippet OMZ::lib/theme-and-appearance.zsh
zinit snippet OMZP::sudo/sudo.plugin.zsh
zinit ice depth&quot;1&quot; # git clone depth
zinit light romkatv/powerlevel10k

# To customize prompt, run `p10k configure` or edit ~/.p10k.zsh.
[[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;直接使用 &lt;code&gt;vim .zshrc&lt;/code&gt; 进行写入，然后运行 &lt;code&gt;zsh&lt;/code&gt; 即可，但是在此之前需要安装一下 nerd font，在这里我使用的是 FiraCode。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -S ttf-firacode-nerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;adb&lt;/h3&gt;
&lt;p&gt;因为我有使用 &lt;a href=&quot;https://alas.azurlane.cloud/&quot;&gt;ALAS&lt;/a&gt; 进行一个碧蓝航线的挂机，我使用了云手机，并且可以使用 ADB 进行远程打开 UI 界面，这使得我需要安装 ADB。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo pacman -S android-tools android-udev
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;NVM&lt;/h3&gt;
&lt;p&gt;因为需要使用 &lt;code&gt;npm&lt;/code&gt; 进行 Web 项目的构建，所以说进行了一个安装：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yay -S nvm
echo &apos;source /usr/share/nvm/init-nvm.sh&apos; &gt;&gt; ~/.zshrc
source ~/.zshrc 
nvm install node
npm install -g pnpm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可能会出现提示，&lt;code&gt;tput: unknown terminal &quot;xterm-256color&quot;&lt;/code&gt;，输入以下来解决：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;export TERMINFO=/usr/share/terminfo&quot; &gt;&gt; ~/.zshrc
source ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可能会出现 &lt;code&gt;nvm install node&lt;/code&gt; 包括 &lt;code&gt;nvm ls-remote&lt;/code&gt; 的时候都输出 &lt;code&gt;N/A&lt;/code&gt;，输入以下来解决：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;export NVM_NODEJS_ORG_MIRROR=http://nodejs.org/dist &quot; &gt;&gt; ~/.zshrc
source ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Anaconda&lt;/h3&gt;
&lt;p&gt;因为需要使用 Python，于是说安装了 &lt;code&gt;anaconda&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yay -S anaconda
source /opt/anaconda/bin/activate root
conda init zsh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后执行 &lt;code&gt;clear&lt;/code&gt; 会出现一些问题，输出 &lt;code&gt;terminals database is inaccessible&lt;/code&gt;，是因为 &lt;code&gt;clear&lt;/code&gt; 和 conda 的指令出现了冲突，可以执行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo mv $CONDA_PREFIX/bin/clear $CONDA_PREFIX/bin/clear_old
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;来解决这个问题。&lt;/p&gt;
&lt;h3&gt;ToDesk&lt;/h3&gt;
&lt;p&gt;因为之前使用的 rustdesk 使用的是同学的服务器，所以说要做好替代的准备，于是安装了 ToDesk：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yay -S todesk-bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是在打开之后，会发现网络出错，这是因为没有开它的一些服务，所以需要执行以下指令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo systemctl enable todeskd.service
sudo systemctl start todeskd.service
&lt;/code&gt;&lt;/pre&gt;</content:encoded><h:img src="https://picr2.axi404.top/archlinux-zh.webp"/><enclosure url="https://picr2.axi404.top/archlinux-zh.webp"/></item><item><title>大一回忆录</title><link>https://axi404.github.io/blog/uni-memoir1</link><guid isPermaLink="true">https://axi404.github.io/blog/uni-memoir1</guid><description>我的大一生活。</description><pubDate>Thu, 03 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;现在应该算是大三开学的第一周，属实是有很多的 ddl 缠身，一方面身上背负了两个科研任务，而且时间都很紧迫，而另一方面也有缓考考试的复习压力。不过好在是目前阶段性地完成了很多内容，于是奖励自己休息一下，写写流水账，回忆一下自己的大一生活，毕竟现在看来，当时的好多事情都已经回忆不清，假如再不提笔记下来的话，可能就确实被我遗忘了。&lt;/p&gt;
&lt;h2&gt;高考出分&lt;/h2&gt;
&lt;p&gt;故事大概还是得从高考成绩揭晓的那一刻讲起。那天，我低头看着那串数字，心中五味杂陈，说是垂头丧气，也不为过。如今我偶尔自嘲，甚至反过来调侃那些初出茅庐、踌躇满志的新生，但若回到那个盛夏的午后，我是真诚地以为，自己该是个“华五”遗珠。&lt;/p&gt;
&lt;p&gt;彼时我对数学有着纯粹的热爱，其次才是计算机。若能进入上交，甚至清北数学专业，倒也算圆了少年的心愿。现在看来，也许我的能力终究难以撑起那门恢弘深邃的学科——尽管在高中时，我在数理方面已算是鹤立鸡群。&lt;/p&gt;
&lt;p&gt;我在高考的时候报名的也是北大的强基计划。凭借一模二模的成绩，我的确摸到了那道门槛，甚至一度幻想，若能超常发挥，或许还能走上普招那条路。后来倒也的确“超常发挥”了，只是方向错了个彻底。&lt;/p&gt;
&lt;p&gt;清北无缘之后，我开始辗转于其他志愿之间。北航的分数线勉强可及，武大的录取尚有余地，而我当时最心仪的，是北理。如今看来，这份倾慕带着几分年少的执念。北理工是京籍考生心照不宣的归宿之一，它恰好位于那些既想冲击985，又不舍离京学子的心理区间内。再往下，是北工大，也是我许多高中同窗的落脚地。&lt;/p&gt;
&lt;p&gt;那阵子，我四处打听招生信息，甚至亲自找到了北理的招生组。可惜终究未能签约——没有承诺，就意味着无法预知未来的专业归宿。很多学校的专业组如同精心设下的棋局，把强势学科与边缘专业巧妙编排在一起，让人赌上一生的志愿表，却不知能否落在心仪的一格。&lt;/p&gt;
&lt;p&gt;印象深刻的是那次前往武大的行程。那时我尚未与乐小姐在一起，而是和前女友在一起，想着若能入读，便可在春日带她看那一树一树的樱花。那年，我大概是恋爱脑的年纪，也恰好是做梦的年纪。&lt;/p&gt;
&lt;h2&gt;孽缘纠葛&lt;/h2&gt;
&lt;p&gt;说到这里，也不得不提一段绕不开的旧事——那是我大学上半年，乃至下半年初的核心旋涡，关于我与前女友的故事。原本想将这部分细水长流地穿插在整体叙述中，但每每忆起，便如投石湖心，搅乱了原本平静的心绪。既如此，不如在此集中叙述，一并说完，也算为回忆录做个情感层面的补全。而我，也好继续后文那些更轻盈、更温柔的片段。&lt;/p&gt;
&lt;p&gt;我与她，是初中同学。那时我们都算是成绩拔尖的学生，也常在打闹中生出些许别样的情愫。青春期的情感，总有几分偷偷摸摸。家中自然反对早恋，于是便有了小纸条传情、有话不能当面说的“地下情节”。&lt;/p&gt;
&lt;p&gt;后来，她因户籍问题无法在北京中考，父母便一咬牙，决定将她送出国。家境虽非殷实，但也竭尽所能。在初二那个尚不懂离别为何物的年纪，我目送她远行，心中百般不舍。我们约定：十年之后再见，若彼此未变，便再次相守。从此，我们改用聊天软件保持联系。在旁人看来，这段感情仿若传奇。时光久远、感情未断，也许还能窥见我当年对感情的执着。而在我们之间，它更像是一段“网恋”，在字句中维系，在时差里延续。&lt;/p&gt;
&lt;p&gt;起初，两人仍在热恋的余温中。她初至异国，人生地不熟，对我多有依赖，而我也始终如一地陪伴她度过那段适应期。可渐渐地，一些变化悄然生根。我仍念念不忘“十年之约”，她却开始在微信上变得疏离，回复越来越少、越来越晚，总是“朋友来了”“有事要忙”……直到某日，一位初中老友告诉我，她其实已对身边的一位同学动了心。&lt;/p&gt;
&lt;p&gt;得知此事，我说不清是愤怒还是恶心。那一刻我忍住情绪，试着温和地向她求证，她坦白了。我提出让她自己选择，她却说仍然想和我在一起。可那之后，我们的关系终究变了。曾经的信任与默契，再也找不回来。可我仍然固执地坚持了我们的“十年之约”。&lt;/p&gt;
&lt;p&gt;大一上半年，我们的联系零零碎碎。我仍像从前那样愿意把她介绍给别人，给予她情感的安全感，而她却将我收起，仿佛藏在抽屉里的旧信件，鲜少提及。&lt;/p&gt;
&lt;p&gt;后来，我渐渐察觉我们的世界已不再交叠。也许是长期独处异乡，她的思想开始走向一种极端的“自由”幻象，不是那种对个体意志的珍视，而是一种近乎“魔怔”的自由幻想，有时像是从《进击的巨人》里艾伦的独白中摘出来的。我们谈论政治、社会、人生时，我越来越疲惫，而她愈发咄咄逼人。我努力维系着理解、体贴、包容的姿态，生怕争执伤了彼此，然而这段感情却像被风吹干的墨迹，越描越淡。&lt;/p&gt;
&lt;p&gt;终于，有一天，她又喜欢上了别人。她仍旧用她的“逻辑”安慰我，说那只是“喜欢”，而对我才是“爱”。这种文字游戏听多了，只剩下反胃。可我自认为问心无愧：她需要倾诉时，我仍聆听；她情绪崩溃时，我仍安抚。如今说来，也许这就是情绪价值的极致体现了吧。&lt;/p&gt;
&lt;p&gt;她提出了分手，两次。第一次，我拒绝了；第二次，我已遇见了乐小姐——一个真正让我从泥沼中走出的人。我没有立刻告诉前女友我心意已变，但心中的疲惫与厌倦早已盖过了年少的执念。当她再次提出分手时，我平静地答应了。&lt;/p&gt;
&lt;p&gt;这场带着引号的“和平分手”没有争吵，没有拉黑，微信也未删除，只是那句曾被我们反复念叨的“十年之约”，在那一刻，悄然化作尘埃，消散无声。我从未背叛过这段感情——我真的坚守到了最后，而终止它的，终究是她。&lt;/p&gt;
&lt;p&gt;再后来，我与乐小姐的感情逐渐明朗。有一天，那位老朋友又来找我，说是受前女友之托，来打听我的感情近况。我告诉他，我已遇到了很好的人，也正因此，我不会回头。朋友似乎误会了什么，说我似乎还“放不下”，我只笑了笑说，历史的车轮从不倒转，人生的轨迹也早已因一次次选择悄然分岔。&lt;/p&gt;
&lt;p&gt;后来得知，那段“复燃”的心思，不过是她向心仪对象表白被拒之后的一种回望。但我心中那段情感的句号，其实早已悄悄落下——或许就在第一次得知她变心的那个夜晚。那时它还隐晦模糊，如今终于落在纸上，再无续写。&lt;/p&gt;
&lt;h2&gt;报考西交&lt;/h2&gt;
&lt;p&gt;回到主线。那时四处奔走于招生办公室之间，原本抱有期待的武大，给我的却是一场彻底的失望。虽然归根结底，确实是我分数不够、实力未达，但让我心寒的，却是那位招生老师在面对我时轻描淡写地说“没有签约这回事”，转过身却将签约协议递给了邻座的同学——轻视写在脸上，我也便打消了再选它的念头。&lt;/p&gt;
&lt;p&gt;正是在这种情绪中，我偶然注意到西安交通大学。看到“C9”的名号，心想也许是捡了个“漏”，尤其北京考生多不愿西去，这让我有了意外的机会。西交招生老师的态度则截然不同，热情而坚定，愿意为我签约，并承诺前三志愿中必有一项可以落实。我在志愿上填写了“数学试验班”为首选，“人工智能”作为第二志愿。最终落入的正是人工智能。&lt;/p&gt;
&lt;p&gt;那时尚未有 ChatGPT 的横空出世，Stable Diffusion 也未掀起浪潮。我无法预知这一专业日后的风起云涌，但从此处开启的这条路，后来竟成为我最恰如其分的归宿。&lt;/p&gt;
&lt;p&gt;可以说，我几乎是以最小的代价，踏入了可及范围内最好的学校、最好的专业。当时或许未曾看出端倪，如今回望，却足以令我自信地说：我在的专业，是西交的王牌之一，而我做出了最明智的选择。&lt;/p&gt;
&lt;h2&gt;水群之旅&lt;/h2&gt;
&lt;p&gt;自高中起，我就是个“水群”爱好者。新生群中总少不了我的身影，活跃程度甚至让许多学长误以为我是他们的同届。&lt;/p&gt;
&lt;p&gt;毕业后的那个暑假，我的时间空前宽裕：和几位好友一起去了成都作毕业旅行——那次“齐聚”，至今未曾再现；然后是报复性地玩游戏，仿佛要把多年来压抑的时光一次性释放殆尽，但最后连游戏也变得无味；剩下的时间，全都献给了“水群”事业。&lt;/p&gt;
&lt;p&gt;是高中一位也被西交录取的同学拉我入群，自此一发不可收拾，结识了许多后来形影不离的朋友——尽管如今也已有些疏远，那段日子仍是青春中最温暖的一角。&lt;/p&gt;
&lt;p&gt;进入专业群之后，我成了最早一批拉新的人，也被推举为管理员，后来建了班级群，如今虽然我并非班委，但“群主”一职却由始至终未曾更替。&lt;/p&gt;
&lt;p&gt;与此同时，我也加入了一个极富个性的社团——“日本流行音乐社”，后来改名为“轻音音乐社”。那是个充满趣味、火花与改名史的组织，从“偶像文化研究社”到现在，为了回避争议，也为适应风潮，一次次变换着名字。&lt;/p&gt;
&lt;p&gt;“喵”这个我常挂在嘴边的尾音，也正是在这段社群狂欢中，嬉闹出来的习惯。不知不觉，它竟成了我的一种身份标志。&lt;/p&gt;
&lt;p&gt;那时的我，并无远大志向，只想着如何快乐地度过四年。保研、竞赛、升学——这些字眼还遥不可及。那些内卷、夜战与迷惘，暂且按下不表，不妨先回忆那段懵懂而美好的开端。&lt;/p&gt;
&lt;h2&gt;开学伊始&lt;/h2&gt;
&lt;p&gt;那时我仍沉迷游戏，特别是《守望先锋》。虽说《守望先锋2》对我心爱的角色“末日铁拳”做了改动，但我转为枪位后表现依旧不俗，体验感尚佳。直到暴雪国服关闭，我才慢慢淡出。&lt;/p&gt;
&lt;p&gt;入学之初，我也开始拍摄一些“女装”照片——最初是试水，后来渐渐成了习惯。那些照片有时也让我重新认识了自己。&lt;/p&gt;
&lt;p&gt;轻音音乐社的社长茗酱和配音部部长茜老师，对我颇为照顾。他们的互动氛围让我常觉得置身青春校园番剧之中。哪怕后面因学业和 RM 的压力逐渐远离社团，每当路上碰见，他们仍会亲切地打招呼。&lt;/p&gt;
&lt;p&gt;那时《艾尔登法环》刚刚上线，茜老师作为资深玩家，曾在游戏中送我不少好装备，可惜我没能坚持通关——原因还是因为 RM 的培训，这个我们后面再详说。&lt;/p&gt;
&lt;p&gt;还有一件事让我印象深刻：社团内“WOTA艺部门”的分裂事件。这本是动漫社遗留的部门，后来因招新政策与社内冲突而划归动漫社。我曾经为了参与活动，特意买了两百多块钱的荧光棒，但最终未能派上用场。这些人如今也少有再见，只在某次体育课上打过照面。&lt;/p&gt;
&lt;h2&gt;初入 RM&lt;/h2&gt;
&lt;p&gt;在高中时我就参与过 RoboMaster，大一自然也想继续这份热情。在专业群里询问时，被视觉组组长 WJH 学长带入 RM，正式开启一段漫长且密集的技术旅途。&lt;/p&gt;
&lt;p&gt;我最初连 C++ 也不熟，边学边做，后来通过考核入选。起初只是预备队员，结果 WJH 和另一位学长突然退出，名额空出，我阴差阳错转为正式队员，开始在社团里投入大量时间。&lt;/p&gt;
&lt;p&gt;写程序的风格也在此期间逐渐成型：我并不以缜密逻辑起步，而是先搭起结构，再不断调试。这个方式，或许不是最优，却极大提高了效率。&lt;/p&gt;
&lt;p&gt;这场偶然入选的转折，某种意义上，是我大学技术轨迹的真正起点。&lt;/p&gt;
&lt;h2&gt;学习小记&lt;/h2&gt;
&lt;p&gt;大一伊始，我便陷入一种“要赢”的心态。说是 peer pressure，也好，焦虑也罢，总之我不愿落于人后。&lt;/p&gt;
&lt;p&gt;舍友 CJ 是少年班的学生，沉默寡言，技术强悍。在我还沉浸在视觉工程里时，他已学习更深的算法知识。我们方向不同，结果自然无可比拟，后来我也就放下了那份竞争的执念。&lt;/p&gt;
&lt;p&gt;课堂上，我往往先行学习，因此上课常觉得内容浅显。不过张永怀老师的高数与线代，仍让我肃然起敬。他的课是我认真听完的少数课程之一。&lt;/p&gt;
&lt;p&gt;除了他，模电课的杨建国老师和计算机体系结构课的任鹏举老师，也令人印象深刻。他们的专业性与讲授风格，为我原本平淡的课堂记忆，添了些颜色。&lt;/p&gt;
&lt;h2&gt;RM 之旅&lt;/h2&gt;
&lt;p&gt;若要说我大一期间把最多的时间投注在何处，毫无疑问，是在 RoboMaster 社团中。那是一段令人身心俱疲、又收获满满的日子。而直到深入其中，我才逐渐察觉：这个社团的内部，远比表面要复杂得多。人际的牵扯、关系的错综，起初被我专注技术的视线所忽略。&lt;/p&gt;
&lt;p&gt;刚加入时，我并不能说有多擅长什么，但凭着一股韧劲，不断自学、完成任务、通过考核。那时视觉组由 WJH 学长带队，后来他因队伍运作的问题选择离开，大四的 LYZ 学长接任。原先大一的我们只能作为“梯队”，无法参与正式比赛，我本也就把重心转回课业，没打算为最后任务太多费心。&lt;/p&gt;
&lt;p&gt;却不曾料到，WJH 学长的突然退出连带另一位成员离去，使得原本只准备收一名正式队员的视觉组空出多个名额，我也因此顺利转正，最终与另外两位成为正式队员。&lt;/p&gt;
&lt;p&gt;后来回头看，这或许正是命运齿轮悄然转动的节点。很多事的发展，都在那次人事变动后悄然改变了轨迹。&lt;/p&gt;
&lt;p&gt;正式入选后，我将大量时间投入社团。那时疫情仍在持续，无法频繁外出，我们多在社团自习室工作。我第一个任务是相机取流，很快就完成了。也就是在这个过程中，我逐渐摸索出了自己写程序的风格：与其精心构思再落笔，不如先把主干草草搭起，再边调试边优化。虽然潦草，却极其高效。这种“先写后理”的方式，某种意义上也成为了我后来技术路径的一个隐喻。&lt;/p&gt;
&lt;h2&gt;学期之中&lt;/h2&gt;
&lt;p&gt;再把视角从社团拉回日常生活。选课时阴差阳错地选了一门名叫“艺术健美操”的体育课，原以为是类似广播体操的内容，结果上半年教的是啦啦操，下半年居然跳起了爵士舞，期末考核甚至是一段韩舞。&lt;/p&gt;
&lt;p&gt;因为内容设置的缘故，这门课的班级里几乎全是女生。我虽不算外向，却也能在人际关系中应对自如，高中那些年的社交经验教会了我一件事：如何与异性成为“自然的朋友”。&lt;/p&gt;
&lt;p&gt;说话要略带风趣，却不油腻；在全是女生的课堂上，若一本正经地发表些没人搭理的言论，场面会变得十分尴尬。我的外貌也帮了我一把——略长的头发削弱了性别带来的“侵略感”；而我发明的一种称呼法，也起到奇效——将女生唤作“哥”或“老师”，反倒拉开了暧昧的距离，反而更显亲近。这是一种奇妙的心理暗示：我们是朋友，而非暧昧关系的边缘。&lt;/p&gt;
&lt;p&gt;当然，还有一种终极屏障，就是反复强调“我已经有喜欢的人”，哪怕那时我正在一段并不顺利的感情中。&lt;/p&gt;
&lt;p&gt;大一时，我也曾帮助这门课上认识的朋友 ZRX 和几位同学（说实话，大多数是男生）解决过编程上的问题，关系处得还不错。只是进入大二后未再选相似的课程，加之学业压力骤增，交流也便日渐减少。倒是与她的同乡 LXW 走得较近——后来，我邀请他也加入了 RM。&lt;/p&gt;
&lt;p&gt;那年印象深刻的考试，应该是期中高数。我向来采用“看答案能看懂”的学习方式，目的是让自己在今后的学习中能迅速复盘旧知、接轨新知。这样的学习虽然高效，却不适合应试。所以临时复习后，我考了个八十分，略感遗憾，但也未太在意。&lt;/p&gt;
&lt;p&gt;当时的我并没有远大的目标，不曾想年级前列，也未动保研念头。取得中等偏上的成绩，对我而言算是“可接受”的范围。&lt;/p&gt;
&lt;p&gt;日子在混乱的作息中流转——学习、RM、偶尔一把《守望先锋》。现在想想，那时反倒颇有“工作—生活平衡”的味道。&lt;/p&gt;
&lt;p&gt;值得一提的是，我为《计算机程序设计》这门课写了一个复习大纲。这可算我正式步入“内容创作”领域的开始。之前虽然也有零星写点技术帖，比如 C++ 环境配置等，但这一次我系统地写下几千字，为大家梳理复习重点，还附上例题讲解，甚至被老师收录进了全班的复习资料包中，供后来的学弟学妹参考。&lt;/p&gt;
&lt;h2&gt;疫情与转折&lt;/h2&gt;
&lt;p&gt;时间进入学期末，疫情加重，期末考试取消，提前放假。地下自习室也随之封闭，只因检测出一例病例。&lt;/p&gt;
&lt;p&gt;那时视觉组正进行能量机关任务，我完成了视频拍摄后，便匆匆离校。本来计划的冬训也因防疫而取消。回家后，我曾斗志昂扬地准备“自学突击”，结果终究敌不过惰性，最后只学了些机器学习的基础内容，提前预习了第二学期的大半课程。&lt;/p&gt;
&lt;p&gt;虽然没有显著成果，但这个假期却成了我另一个重要的转折点。&lt;/p&gt;
&lt;p&gt;我写完取流程序后，被安排与 GYT 一起继续能量机关识别任务。由于前期整理得当，我很快从前人的屎山代码中抽丝剥茧，梳理出一套结构清晰的传统视觉方案，交由 GYT 去拟合。&lt;/p&gt;
&lt;p&gt;可惜规则更新，机关样式变更，前版程序作废。但得益于此前良好的模块化设计，新的适配版本也迅速成型。那时我已意识到传统视觉方案的一大致命问题：高度依赖光照和曝光。&lt;/p&gt;
&lt;p&gt;为此我写了一个可视化参数调节程序，便于在极短的比赛调试时间内快速校准曝光和二值化参数。尽管如此，仍属勉强应付。&lt;/p&gt;
&lt;p&gt;完成这版之后，我们也开始探索神经网络方案。&lt;/p&gt;
&lt;p&gt;这段时间，我阅读了不少技术书籍，比如《统计学习方法》，又比如冈萨雷斯的《数字图像处理》——后者几乎是传统视觉领域的“圣经”。它用清晰的图示和案例为我打开了视野，尽管现实中，我后来的研究方向更多落在深度学习，许多“图像信号处理”的底层逻辑早已封装在网络之中。&lt;/p&gt;
&lt;p&gt;但那时，我尚不知自己的路将通往何处。&lt;/p&gt;
&lt;h2&gt;下半年开始&lt;/h2&gt;
&lt;p&gt;大一下学期开始时，我已打下了扎实的数学与机器学习基础，RM 的视觉系统初步完成：从相机取流、电控通信，到能量机关的识别与预测，再到在线调参接口的封装，基本具备了可交付的原型。&lt;/p&gt;
&lt;p&gt;当然，这学期的第一件大事仍然是补上的期末考试。因疫情推迟，上学期期末改在这时进行——可以说是一种巧合，也像是命运的暗示。我以一个出人意料的成绩拿到了年级第二，甚至如果算上课程加分，理论上有望冲击第一。但我并不觉得自己在纸面实力上胜过那些真正扎实学习的同学，这次更像是一场运气之中的幸运。&lt;/p&gt;
&lt;p&gt;也是从那一刻起，“保研”这个词第一次在我脑海中变得清晰可触。那些曾遥不可及的目标，在这一瞬仿佛被拉近了许多。我意识到，自己要开始为未来真正准备了——科研、竞赛、资源积累，每一项都刻不容缓。&lt;/p&gt;
&lt;p&gt;而这段时间，另一件命运的馈赠，是我遇见了乐小姐。&lt;/p&gt;
&lt;p&gt;说来也奇妙，这场相遇起于一次轻音音乐社的聚餐。尽管我已因学业与 RM 的忙碌渐渐淡出社团活动，但那天还是意外地参与了聚会——也正因此，她与我相识。她的学长曾尝试加入 RM，却在培训中被刷下来，而我在饭后带大家参观地下室，展示了程序的运行过程。或许正是在那个瞬间，我的认真与专注打动了她——也可以说，是技术与命运的共同安排。&lt;/p&gt;
&lt;p&gt;从那天起，我们便渐渐走近。那时候我们只是朋友，吃饭都 AA 制。但她饭量小，常常一顿饭我吃得比她多得多，却还和我平分，倒也占了些便宜。我们最常去东南门外的小烤肉摊，我总是帮她烤肉夹菜，她也曾犹豫这是否“太亲密”，毕竟当时并未确定关系。&lt;/p&gt;
&lt;p&gt;而我的感情状态也如你在前文所知，一团乱麻。情绪的低谷与压力交织成阴霾，而她的出现，像一道温柔的光，照亮了我那段混沌的时光。&lt;/p&gt;
&lt;p&gt;与此同时，RM 项目的内容也开始向自瞄系统推进。我决定推翻原有的 ROS 框架，从头构建。原有系统安装复杂，结构臃肿，不利于新人成长；而旧有自瞄逻辑也已落后，我们开始尝试基于深度学习的自瞄方案，并借助沈航的开源实现完成初版。最终，在省赛中，我们凭借优异的哨兵表现与自瞄系统，成功夺冠。&lt;/p&gt;
&lt;p&gt;也是在这段时间，我与前女友的感情终于落幕。她提出分手——或许是一段拉锯故事的必然终章。那段关系破碎之时，我与乐小姐间的情愫也日渐明朗，尽管因此引发了一场误会——她一度以为自己是我们分手的导火索，想要退出我的生活，幸好我解释清楚了彼此的过去，也澄清了我们之间那段早已枯竭的情感。&lt;/p&gt;
&lt;p&gt;某天夜晚，我们一同散步回宿舍。临别之时，我鼓起勇气向她表白。她只是将食指轻轻点在我的嘴唇上，然后匆匆离开。那晚，她发来一张明信片的照片——上面写着：“我恰好也喜欢你。”&lt;/p&gt;
&lt;p&gt;从此，我们的故事，正式开始。&lt;/p&gt;
&lt;p&gt;她温柔、体贴，总是为我准备牛奶、面包等补给，只为让我在地下室的漫长学习中不至于挨饿。我也逐渐收敛社交，将重心回归于我们两人之间。她会因我与其他女生的联系而敏感，我也愿意为她调试生活的界限，给予她全部的安全感。&lt;/p&gt;
&lt;p&gt;感情逐渐升温，从最初牵手、同行，到如今我可以在街头轻轻亲她的脸、从身后抱住她，而她只会笑着回头。我们已无话不谈、亲密无间。&lt;/p&gt;
&lt;p&gt;学期渐入尾声，生活也不再起伏剧烈。我在 RM 的投入不断加深，社交圈收紧，日常多半围绕技术、几位志同道合的朋友，以及她。&lt;/p&gt;
&lt;p&gt;还记得那次 RM 去长沙参赛，是我们第一次长时间分别。比赛途中，她竟准确地说出了我回西安的时间，而我从未告诉她。后来她说，是在购票软件里一个个试着查的。那一刻，我突然鼻头一酸——除了家人，从未有人如此在意过我的归期。&lt;/p&gt;
&lt;h2&gt;结尾&lt;/h2&gt;
&lt;p&gt;下半年之后的很多细节，已模糊不清。&lt;/p&gt;
&lt;p&gt;一方面是因为我大部分时间都陷在 RM 的项目中，那些日夜的调试、调参、测试，都已被我记录在另一篇&lt;a href=&quot;rm-memoir&quot;&gt;RM 回忆录&lt;/a&gt;中，不再赘述。我参加了分区赛、国赛，逐渐走到了视觉组组长的位置。&lt;/p&gt;
&lt;p&gt;也正因投入太多，期末成绩滑落，再加上综测分的权重，我最终排名仅在年级前五左右——剔除综测后甚至落到第八或第九。到了大二，名次又进一步下滑。但那时我已开始主攻科研，视科研为第一竞争力，因此这些外在名次也慢慢失去了意义。&lt;/p&gt;
&lt;p&gt;回顾整个大一，绝大多数时间都在默默学习。学习并不值得书写，它像细水长流般渗入生活的缝隙，却又影响深远。&lt;/p&gt;
&lt;p&gt;若要简单总结，我在大一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自学完成了高数、线代、概率论；&lt;/li&gt;
&lt;li&gt;熟悉了 C++ 与 OpenCV，掌握了 Linux 常用操作；&lt;/li&gt;
&lt;li&gt;开始正式学习机器学习与深度学习，读完了《统计学习方法》《西瓜书》《图像信号处理》等约五六本经典教材；&lt;/li&gt;
&lt;li&gt;系统掌握了 PyTorch 框架，阅读了约 50 篇论文，完成科研的第一阶段准备；&lt;/li&gt;
&lt;li&gt;继续分享笔记资料，首次编写计算机程序设计复习大纲，为众人所用；&lt;/li&gt;
&lt;li&gt;加入了 AI 学辅项目，并被推荐进入“绿群”——这个计算机保研交流群，后续也将深刻影响我的科研路径与成长轨迹。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;正如开头所说，大一是成长的原点，是命运车轮的第一次转向。&lt;/p&gt;
&lt;p&gt;此处，暂且为这段旅程画下一个句号。&lt;/p&gt;
&lt;p&gt;倘若未来某天，我又忆起那时的微光与汗水，再回来补上几句，也未尝不可。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Uni-Memoir1-zh.webp"/><enclosure url="https://picr2.axi404.top/Uni-Memoir1-zh.webp"/></item><item><title>CUDA &amp; CUDNN &amp; Pytorch 安装</title><link>https://axi404.github.io/blog/torch</link><guid isPermaLink="true">https://axi404.github.io/blog/torch</guid><description>在 Ubuntu 22.04 安装 CUDA &amp; CUDNN &amp; Pytorch。</description><pubDate>Thu, 03 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;因为之前的 Ubuntu 系统又因为我自己的不小心所以坏掉了，于是又一次尝试重装系统，但是出现了很多的问题。&lt;/p&gt;
&lt;p&gt;我的系统是 Ubuntu 20.04.6，在清华大学镜像站下载的最新版，电脑显卡是 NVIDIA GeForce RTX 3070 Laptop，可以支持 CUDA 12.2，在本段内容书写的时候，Torch 的官网使用的最标准的 pytorch 是 CUDA 12.1 的，所以安装这个版本，以及 9.3.0 的 CUDNN。&lt;/p&gt;
&lt;h2&gt;安装 CUDA 与 CUDNN&lt;/h2&gt;
&lt;p&gt;首先给出下载 CUDA 和 CUDNN 的官网，其中 CUDA 12.1 为 &lt;a href=&quot;https://developer.nvidia.com/cuda-12-1-0-download-archive&quot;&gt;https://developer.nvidia.com/cuda-12-1-0-download-archive&lt;/a&gt;，CUDNN 9.3.0 为 &lt;a href=&quot;https://developer.nvidia.com/cudnn-downloads&quot;&gt;https://developer.nvidia.com/cudnn-downloads&lt;/a&gt;，之后依次选择自己的系统版本即可。其中 CUDA 的安装方法使用的是 &lt;code&gt;runfile (local)&lt;/code&gt;，并且在此之前运行了 &lt;code&gt;sudo ubuntu-drivers autoinstall&lt;/code&gt; 并重启以安装 driver。&lt;/p&gt;
&lt;p&gt;问题出现在，对于任何一个全新的最小安装的 Ubuntu 20.04 系统，在使用 runfile 的时候，均会报错，并说明在 &lt;code&gt;/var/log/nvidia-installer.log&lt;/code&gt; 中可以看到详情，为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;-&gt; Error.
ERROR: An error occurred while performing the step: &quot;Checking to see whether the nvidia kernel module was successfully built&quot;. See /var/log/nvidia-installer.log for details.
-&gt; The command `cd ./kernel; /usr/bin/make -k -j16  NV_EXCLUDE_KERNEL_MODULES=&quot;&quot; SYSSRC=&quot;/lib/modules/5.15.0-117-generic/build&quot; SYSOUT=&quot;/lib/modules/5.15.0-117-generic/build&quot; NV_KERNEL_MODULES=&quot;nvidia&quot;` failed with the following output:

make[1]: Entering directory &apos;/usr/src/linux-headers-5.15.0-117-generic&apos;
warning: the compiler differs from the one used to build the kernel
The kernel was built by: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
You are using:           cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
MODPOST /tmp/selfgz3405/NVIDIA-Linux-x86_64-530.30.02/kernel/Module.symvers
ERROR: modpost: GPL-incompatible module nvidia.ko uses GPL-only symbol &apos;rcu_read_unlock_strict&apos;
make[2]: *** [scripts/Makefile.modpost:133: /tmp/selfgz3405/NVIDIA-Linux-x86_64-530.30.02/kernel/Module.symvers] Error 1
make[2]: *** Deleting file &apos;/tmp/selfgz3405/NVIDIA-Linux-x86_64-530.30.02/kernel/Module.symvers&apos;
make[2]: Target &apos;__modpost&apos; not remade because of errors.
make[1]: *** [Makefile:1830: modules] Error 2
make[1]: Leaving directory &apos;/usr/src/linux-headers-5.15.0-117-generic&apos;
make: *** [Makefile:82: modules] Error 2
ERROR: The nvidia kernel module was not created.
ERROR: Installation has failed.  Please see the file &apos;/var/log/nvidia-installer.log&apos; for details.  You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经过检查，发现问题其实很简单，是因为 g++ 等版本为 9，太高了，设置为 7 即可。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt-get install gcc-7 g++-7
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 9
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 1 
sudo update-alternatives --display gcc
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 9
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 1
sudo update-alternatives --display g++
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后再次运行，获得输出：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;===========
= Summary =
===========
Driver:   Not Selected
Toolkit:  Installed in /usr/local/cuda-12.1/
Please make sure that
-   PATH includes /usr/local/cuda-12.1/bin
-   LD_LIBRARY_PATH includes /usr/local/cuda-12.1/lib64, or, add /usr/local/cuda-12.1/lib64 to /etc/ld.so.conf and run ldconfig as root
To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-12.1/bin
***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 530.00 is required for CUDA 12.1 functionality to work.
To install the driver using this installer, run the following command, replacing &amp;#x3C;CudaInstaller&gt; with the name of this run file:
    sudo &amp;#x3C;CudaInstaller&gt;.run --silent --driver
Logfile is /var/log/cuda-installer.log
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;设置环境变量：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo vim ~/.bashrc # or ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后在最后添加：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后再 &lt;code&gt;source&lt;/code&gt; 一下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;source ~/.bashrc # or ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;就可以正常的使用 CUDA 了：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;nvcc --version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Tue_Feb__7_19:32:13_PST_2023
Cuda compilation tools, release 12.1, V12.1.66
Build cuda_12.1.r12.1/compiler.32415258_0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后的 CUDNN 以及 torch 的安装就是按照提供的正常流程进行，完结撒花。&lt;/p&gt;
&lt;p&gt;全部的指令包括以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt update
sudo apt upgrade

sudo ubuntu-drivers autoinstall

reboot

sudo apt-get install gcc-7 g++-7
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 9
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 1 
sudo update-alternatives --display gcc
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 9
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 1
sudo update-alternatives --display g++

wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run
sudo sh cuda_12.1.0_530.30.02_linux.run

sudo vim ~/.bashrc # or ~/.zshrc

### add following in .bashrc ###

export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

################################

source ~/.bashrc # or ~/.zshrc

wget https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn-local-repo-ubuntu2004-9.3.0_1.0-1_amd64.deb
sudo dpkg -i cudnn-local-repo-ubuntu2004-9.3.0_1.0-1_amd64.deb
sudo cp /var/cudnn-local-repo-ubuntu2004-9.3.0/cudnn-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cudnn

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;安装 Pytorch&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n torch python=3.8
conda activate torch
pip3 install torch torchvision torchaudio
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后在 python 中 &lt;code&gt;torch.cuda.is_available()&lt;/code&gt; 返回为 &lt;code&gt;true&lt;/code&gt;。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/torch-zh.webp"/><enclosure url="https://picr2.axi404.top/torch-zh.webp"/></item><item><title>周记 Week14</title><link>https://axi404.github.io/blog/week-14</link><guid isPermaLink="true">https://axi404.github.io/blog/week-14</guid><description>2024-09-30 ~ 2024-10-06.</description><pubDate>Thu, 03 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;现在貌似周记迟到一两天已经是常态了，不过目前还是写了，而且也没有发现什么会无限拖延下去的威胁，所以说就暂且不去纠正这个坏习惯了。&lt;/p&gt;
&lt;p&gt;总的来说，这个星期基本上就是在意大利度过的，也就是过去参会 ECCV，这件事情我觉得还是比较有趣的，因此做一个简单的记录。&lt;/p&gt;
&lt;h2&gt;意大利&lt;/h2&gt;
&lt;p&gt;首先来说印象比较深刻的，就是意大利的天气了，可以带着引号的说，国外的空气确实「香甜」几分，出发的时候西安还比较炎热，起码在正午的时候是这样的，而到了晚上有些寒冷。不过在意大利的时候，下了飞机传来了就是一股恰到好处的凉爽，加上确实意大利米兰这个城市不算很发达，基本上属于是古朴的小镇，所以一路上视野开阔，确实有一种秋高气爽的感觉，让人身心愉悦。&lt;/p&gt;
&lt;p&gt;确实在这个城市里面有不少有意思的设定，是我之前没有想到我会看见的。包括有轨电车，无处不在的信用卡支付以及会吐出的大量发票，还有超乎我想象数量的带有欧式风格的建筑。以及不知道算是意外还是幸运，但是在米兰的时候并没有遇见治安混乱的情况，基本上都是平安无事。&lt;/p&gt;
&lt;p&gt;上篇博客里也给出了图片，我去参观了米兰大教堂，确实是和国内完全不同的一种华美，那种极具雕琢的艺术组成了一座庞大的建筑，那种视觉的冲击感确实难以言表。&lt;/p&gt;
&lt;p&gt;这几天主要的时间其实去了会场，然后晚上的时候在酒店里面写程序，准备之后的工作，所以说几乎没有太去逛他们的城市或者景点，倒是吃了一些所谓的当地美食，不过和国内确实相差甚远。&lt;/p&gt;
&lt;p&gt;我觉得或许比较有趣的一件事情是，我住的酒店里面有一个浴缸，一般来说在国内会是淋浴，因此在我用热水把浴缸反复清洗了五六次之后，我就开始了为期六天的泡澡生涯。我有些难以形容这是一种怎样的感受。&lt;/p&gt;
&lt;p&gt;在写代码写累了之后，整个人躺在浴缸的温水中，感受着浮力将你轻轻地托举着，然后卧室的窗半掩，微凉的风轻轻的吹拂进来，我将半个头埋到水下，能感受到那一抹凉微微刺痛我的皮肤，而水的温度又令人心安，让人的思绪不禁飘向远方。&lt;/p&gt;
&lt;p&gt;另外，再分享一些意大利时候的碎碎念。一方面会场的饭确实很难吃，我基本上吃了一次之后就没有再考虑了，但是其实还好，可以有糕点和巧克力吃，这个我倒是经常品鉴一番。一方面其实这出国的几天也提升了我的口语水平，虽然说我的口语确实不太好，但是因为现在有了使用的刚需，所以说也要硬着头皮去表达，这让我在和外国人交流的时候能在我所了解的学术领域中表达我的见解。当然最后我的 poster 顺利展示了，不过我 present 到一半就离开了，紧张是一方面的，更多的是精神状态不是很好，我到了米兰之后过了前两天的正常作息，又飞快的变成了熬夜的情况，导致那时候我实在头晕的厉害。&lt;/p&gt;
&lt;p&gt;在意大利的几天可以说是十分 work life balance 的，这导致在我周日回国之后立刻陷入了极大的不适应之中，后来思考了一下，大概是因为还有学校存在吧，这算是在两边的唯一变量，而且现如今又一次出现的 ddl，主要的提供者还是学校课程的作业，那些不会对我的未来有任何帮助的内容，在无尽地侵占着我的时间，浪费着我的精力，并且不会给我任何收获。&lt;/p&gt;
&lt;p&gt;然而显然我这样抱怨不会有什么结果，而且其实可以预见的是，这种现状在将来的很长一段时间内都不会有改变，所以也就只能闭上眼睛接受这一切了，希望可以尽早脱离苦海吧，但是至少当下还要熬夜，把没写完的作业写完。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-14-zh.webp"/><enclosure url="https://picr2.axi404.top/week-14-zh.webp"/></item><item><title>周记 Week13</title><link>https://axi404.github.io/blog/week-13</link><guid isPermaLink="true">https://axi404.github.io/blog/week-13</guid><description>2024-09-23 ~ 2024-09-29.</description><pubDate>Wed, 02 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这周的周记又是迟来的一次，不过还是发生了很多事情的，值得记录。&lt;/p&gt;
&lt;h2&gt;ECCV&lt;/h2&gt;
&lt;p&gt;这一周其实最主要的折腾就是前往意大利米兰去 ECCV，这耽误了很多的时间。不过说起来我对一出国参加会议这件事其实态度经常发生转变，一开始的时候肯定是十分激动的，非常向往这种学术交流；但是后来变成了一种惶恐，我其实知道 ai 领域中一些人做了一些很有意义的工作，虽然大多数工作还是以增量式为主，然而他们所在的领域至少看上去是光明的，然而我其实并不是很认为我的工作具有什么重大的意义，进而害怕被拷问；而后变成了厌倦，漫长的签证以及学校审批的流程实在是令我身心俱疲，心中暗道下次会议一定让人帮我贴个 poster 就完事了，我肯定懒得去；但是去了之后突然感觉很是新鲜，又有了之后继续去的动力。当然其实另一个不知道能不能说的，是在于独自一个人住一间酒店，然后女装。&lt;/p&gt;
&lt;p&gt;诚然我需要承认，可能相关的事情，我确实有一些癖好，但是不得不说的是，这确实很能缓解我的压力。具体来说对于我来讲，可能穿上比较好看的裙子以及丝袜，花上一个多小时拍摄几张自己很满意的照片，确实是一种在繁忙的任务中慢下来的好方法，并且可以让我身心愉悦。&lt;/p&gt;
&lt;p&gt;同时一方面女朋友在催促，一方面师兄也拉上我，在周日去了米兰的一些地方逛了一逛，主要还是米兰大教堂，以及买了一些纪念品之类的，也拍了一些照片。&lt;/p&gt;
&lt;p&gt;除此之外，我也完成了 ICLR 的工作，把应该跑的实验做完了，按照之前和本校周老师商量的，应该这也是本校这边我 lead 的最后一个课题了，之后更多的精力可能会放在上海那一侧的实习，本校这边老师将来打算在医学方面尝试做一个子刊，到时候我可能也会帮助一下，想想 idea，以及说不定还需要我帮忙写一些程序。周老师找了四名新同学组建了全新的医学影像小组，说来之前和我一起进组医学影像的两名本科生同学一名已经去了浙大直博，另一个则是应该要去考研，而只有我一个人有学术发表，如今新人换旧人，也让人不禁感慨时光荏苒。&lt;/p&gt;
&lt;p&gt;当然，不得不说的是这一批新人里面，大家的水平还是比较有限，甚至说连基本的读论文的能力，以及查找资料能力都比较有限。我之前是有了解一些向周老师陶瓷的同学，感觉都还不错，但是现在的一些同学，不知道是不是因为之前联系的比较早，不过感觉相对实力确实差点意思。&lt;/p&gt;
&lt;h2&gt;上海科研&lt;/h2&gt;
&lt;p&gt;上海这边实习的事情，则是有点不顺利。大概的话也不能说的太细节，可以理解为我在调用一个其他人的工作，然后把它部署到我的框架里面，但是坐标系转换的过程中出一些问题，类似于没有对齐。现在来看这个问题已经修复了，确实是在比较蠢的地方犯了错误，不过还是实打实的消耗了三天时间，时间肯定不在 mentor 那边的规划中，所以还是需要进一步的抓紧一下进度。&lt;/p&gt;
&lt;p&gt;除此之外的话，上海那边的 mentor 还推荐了我作为 ICLR 的审稿人，可以说这也是我第一次当审稿人，目前还无法接触到具身领域的审稿（因为自己之前的论文不是这个方向），但是我也会尽力去审稿的，尝试给一些自己的见解，自然也很期待别人的投稿是什么样子的。&lt;/p&gt;
&lt;p&gt;这周也没有发生更多的事情，作业之类的可以等到回去再写，下周可以稍微努努力的同时享受一下在意大利的生活，以及这和女朋友过的也很开心，直接进行一个贴贴。&lt;/p&gt;
&lt;p&gt;感觉虽然这一周依然很劳累，甚至在赶飞机的时候，在机场还要用电脑跑实验（因为我是游戏本，在没有电源的情况下，甚至完全开机就花了五六分钟），但是好在一些之前欠下的事情都开始尘埃落定，接下来还是有更美好的盼头的。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-13-zh.webp"/><enclosure url="https://picr2.axi404.top/week-13-zh.webp"/></item><item><title>周记 Week12</title><link>https://axi404.github.io/blog/week-12</link><guid isPermaLink="true">https://axi404.github.io/blog/week-12</guid><description>2024-09-16 ~ 2024-09-22.</description><pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;又是一周的周记了，写下这篇内容的时候是星期日，刚刚考完了复变函数，虽然不是特别特别顺利，但是感觉也还说得过去，之后的日子应该就是以科研为主了，加上和女朋友两个人的快乐生活了。曾经欠下的债总算是熬出了头，下个星期再努力写一下 ICLR 的论文，之后就可以专心投入我觉得有意思的科研内容了。&lt;/p&gt;
&lt;p&gt;尽管现在我写下这篇内容的心情十分的轻松愉快，但是这种心情却来自于长久的压抑，可以说这个星期的主旋律依然是抗压。&lt;/p&gt;
&lt;p&gt;这周也是开学的第二周了，假如是正常的同学，其实应该说的是自己也渐渐地适应了开学的节奏，然而很久以来的锻炼，早已经让我可以摆脱学校的节奏，而进行自己想要做的事情，因此这方面倒也无所谓了。&lt;/p&gt;
&lt;p&gt;基本上第二周的感觉和第一周还是很类似，每天在课上花点时间睡觉，然后下课之后待在电脑前狠狠学习。因为直到周日为止才有机会稍微自由一些，在此之前还是主要以课内为主了。&lt;/p&gt;
&lt;h2&gt;学习&lt;/h2&gt;
&lt;p&gt;我其实主要会有一种毛病，遇到紧张的时候越想做一些与 DDL 无关的事情，于是乎在这次复习的间隙中，我又开始筹备继续写西安交大生存指南的事情了。事实上我个人认为所谓的第一版本已经完全完成了，尽管我可以说想要加入更多的人文关怀，但是作为一个抒发我个人暴论的指南，这么做有必要，但是并非我的核心主旨。然而最近确实又遇到了不少的问题，因此感觉还是有必要再次抒发一下，包括对于前面一些内容的补充。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;此外可以在这周说的，大概是恋爱，和乐小姐两个人依然相处的十分愉快。其实从一种角度来看，我们似乎已经很久没有吵架了，当然这并非说我们之间不会有口角摩擦，然而事实上就是，我们两个人都知道这是一件可以被解决的问题，并且可以很快地解决掉。&lt;/p&gt;
&lt;p&gt;一方面因为实习的原因，多了一些闲钱；而同时一方面是因为，即使没有这些钱也会和乐小姐一起到处吃饭，所以说还是去吃了好多的饭，带领乐小姐到处觅食。因为饮食习惯之类的问题，我们两个基本上都是吃火锅，因此主要吃了海底捞以及巴奴，不过发现海底捞确实越来越让我们缺乏兴趣了，巴奴还是不错的。主要是我们可能吃了太多次了，基本上每次就是点一些鸭血、海带苗、竹笋之类的，不过因为我们喜欢吃的一些菜都已经下架了，比如说鸡胗卷以及黑鱼片。&lt;/p&gt;
&lt;p&gt;最后从结果上来说，感觉考试应该也还算顺利，虽然不是完全临时复习，但是只能说临时的成分依然是很大的，不过显然最后的努力没有白费，考了九十多分，挺不错的。&lt;/p&gt;
&lt;p&gt;这一些事情开始尘埃落定了，剩下的一件事情是 ICLR 需要抽出时间来收尾，之前确实因为复习，我还是比较怂，所以把网课之类的都看了好几遍，因此其实后续的时间对于 ICLR 来说就比较紧张了，因为之前来说大概完整的论文只有一层壳子，但是文字部分没有润色，而可视化的图也还没做。至于另外的上海的科研实习，则需要更多的时间投入了，目前计划是 CVPR，时间已经十分的紧张了。&lt;/p&gt;
&lt;p&gt;总的来说，这周放下了一些重担，也有好好生活，接下来要为了迈向更美好的生活而努力了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-12-zh.webp"/><enclosure url="https://picr2.axi404.top/week-12-zh.webp"/></item><item><title>周记 Week11</title><link>https://axi404.github.io/blog/week-11</link><guid isPermaLink="true">https://axi404.github.io/blog/week-11</guid><description>2024-09-09 ~ 2024-09-15.</description><pubDate>Mon, 16 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一次倒也没有像之前一样迟到太多，或者说，这一次周记其实是按时在写的，倒是也可以记录下来一些当时的感受。&lt;/p&gt;
&lt;p&gt;本来我是想要像上面一样写的，然而现实十分的不幸，第一段的几句话是我在星期三就提笔写下去的，然后到第二段的时候，又已经是下个星期一的凌晨了。&lt;/p&gt;
&lt;p&gt;这个星期可以说也是十分的忙碌，主要其实体现在这是第一个星期，很多事情还需要适应，有很多事情需要准备，而且身上的任务还很多，这些会在后面去说，然而依然可以说的是，总之来说，这个星期是幸运的。&lt;/p&gt;
&lt;h2&gt;医学影像项目&lt;/h2&gt;
&lt;p&gt;不知道看过之前周记的读者是否记得某一次我提及的，我所参加的一个医学项目，当时我在里面承担了主要工作，但是后续却被项目的负责人认为没有获奖是我们的问题（我们在答辩期间负责远程而设备出现了一定的问题，导致在当时我们起身并调节了一下设备），然后事实上当时专家提出的问题，现场答辩的同学也没有回答出来。&lt;/p&gt;
&lt;p&gt;尽管那位负责人在我的眼里，这件事情可以说做得十分不厚道，然后在平时此人还是比较和善的，尤其是作为同是口腔医学的学长，也是为我的女朋友提供了不少的学习方法的指导。然而最近发现这位学长又将我和我的女朋友拉黑了，虽然不知道是出于什么刺激，毕竟最近也没有联系许多，然后最后经过解释，双方的误会还是解除了，也算是一桩幸事。&lt;/p&gt;
&lt;h2&gt;上海科研&lt;/h2&gt;
&lt;p&gt;另外一件让我开心的事情其实体现在，上海那一侧科研的待遇问题，或许是因为我的贡献度确实不错，所以待遇也算不错，还是令我十分欣喜的。一方面，我感觉我的付出是值得的，受到了尊重，当然另一方面也是一种责任。尽管很多学生在求学的时候鼓励广撒网，然后我本人无论说是怂也好，还是说具有契约精神，终究是不太愿意无缘无故辜负别人，因此，或许我要更加努力在项目中尽自己的一份力，这很重要。&lt;/p&gt;
&lt;p&gt;不过主旋律还是和之前一样，依然是压力为主，这个星期过后，距离考试也就剩下了的一星期的时间，而距离 ICLR 的投稿也就剩下了两个多星期，所以其实说到底，时间还是很紧张的。&lt;/p&gt;
&lt;p&gt;目前考试的复习已经基本结束，可能还需要一些刷题之类的，论文那边的整体内容也都已经写完了，一些格式上的东西，以及实验还需要跑，这些都是时间的开销。&lt;/p&gt;
&lt;p&gt;下个星期结束的时候，我需要准备好论文可视化的内容，这些东西还需要比较细致的整理。&lt;/p&gt;
&lt;p&gt;不过目前来说，上海那边的项目进度其实不是很乐观，当然我也可以说在十月份投稿完毕之后加班加点的去进行工作，然而深度学习就是一件需要考虑绝对时间的事情，毕竟包括采集数据或者训练模型，他们的时间开销是不能压缩的，因此一些需要在之前完成的事情依然不能推迟。&lt;/p&gt;
&lt;h2&gt;感情&lt;/h2&gt;
&lt;p&gt;当然也可以谈一谈感情上的事情，现在和女朋友两个人可以说是十分和睦了，基本上只要两个人在一起的时候都十分的开心，也不会出什么争吵之类的，而且可能说两个人都已经十分信任彼此，颇有一番老夫老妻的意味。&lt;/p&gt;
&lt;p&gt;例如一个举例，在和乐小姐刚刚在一起的时候，我当时可能还算是比较热衷于社交，和大多数同学都聊得来（虽然现在也聊得来，但倒也不会选择主动进行没有意义的交流了），因为选课原因有认识同专业的几位女生，而当时可以说是，因为按照书院一起组队考试，和那几位女生有聊天，乐小姐就会十分警觉，并且让我断了联系。&lt;/p&gt;
&lt;p&gt;我现在来说可能在培训的时候，也会有一些女同学向我请教问题。而在我向她吐槽一些同学的水平有些堪忧（当然这种话题进行过很多次，主角是男生或者女生兼有之），她也不会在意我辅导的到底是男生还是女生了。&lt;/p&gt;
&lt;h2&gt;RoboMaster&lt;/h2&gt;
&lt;p&gt;然而说到培训，也就是 RoboMaster 这一边的事情，我到底还是负责一些内容的，或者说因为后续可能自己也难以兼顾很多东西，在前期的时候还是打算稍微多做一些事情的，也算是不辜负一些人一直在对我的期待吧。&lt;/p&gt;
&lt;p&gt;然而在培训的时候，或者说在面试的时候就已经展现了一些端倪，这届同学可能基础相较于上一届还是稍微薄弱了一些，当然一些同学是会使用包括 Linux 或者 OpenCV 在内的一些我们要求的内容的，不过比例确实也不高，还可能有大量的同学因为课业压力或者激情的衰退而选择退出，这都是很有可能的事。&lt;/p&gt;
&lt;p&gt;值得一提的是今年又有了几位女性同学选择加入视觉组。老实说我对于性别已经不是十分敏感，毕竟已经是有了女朋友的人。貌似一位女生向我请教过两次问题，我对她主要的印象还在于她用的是华为笔记本以及在 Windows 中使用了中文用户名（当然这也有我脸盲的原因在）。但是几年下来确实是，视觉组一直以来都只有男生存在，而经常被人调侃。&lt;/p&gt;
&lt;p&gt;虽然我不愿意抛出什么性别在内的观点，然而假如一个组别里面的男女比例适当，确实可以激发每一个人工作的积极性，虽然视觉组的同学在性别单一的情况下依然十分积极。&lt;/p&gt;
&lt;p&gt;不过很遗憾的是从目前的情况出发，这些有意向加入视觉组的女同学，被淘汰的概率依然很高。在工科中男女比例就已经失衡，而在这其中愿意去做机器人的同学，这一比例可能会更加的倾斜，而其中有水平的同学更是少之又少。在这样一个情况下，目前的视觉组中确实我没有见到比较具有技术能力的女同学。&lt;/p&gt;
&lt;p&gt;而尽管我设计的培训，我可以很有把握的说，一定可以让任何零基础的同学都能上手，而且从中学到东西，然而有基础的那些人的先发优势是不可忽视的，毕竟即使零基础可以上手，毕竟会存在大量的试错过程而消磨热情，长此以往，确实有基础的同学更有可能留下来。&lt;/p&gt;
&lt;p&gt;所以总的来说，这个星期还是在欣喜与抗压中度过，再继续往前走往前看的时候，压力或许会越来越小吧。不过目前来说，我和乐小姐两个人一起生活，同时在学习中提升自己，这段日子我还是乐在其中的。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-11-zh.webp"/><enclosure url="https://picr2.axi404.top/week-11-zh.webp"/></item><item><title>Paper Reading: Benchmark</title><link>https://axi404.github.io/blog/paper-reading-benchmark</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-benchmark</guid><description>一些和 Benchmark 相关的论文阅读。</description><pubDate>Sat, 14 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;因为将来要做 Benchmark 相关的一些内容，而且 Benchmark 延伸出来的数据飞轮在当下的我看来，比大多数的 Methods 和 Ideas 都要更加本质，所以说也要阅读相关的论文。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Paper-reading-Benchmark-zh.webp"/><enclosure url="https://picr2.axi404.top/Paper-reading-Benchmark-zh.webp"/></item><item><title>奇奇怪怪的 CS 小技巧</title><link>https://axi404.github.io/blog/littleskills</link><guid isPermaLink="true">https://axi404.github.io/blog/littleskills</guid><description>平时遇到的奇怪 CS 相关小技巧，记录并整理。</description><pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;在这里也算是开一个新坑，分享一些没什么用但是或许对于一些人来说比较有帮助的内容，主要来说是一些 CS 的一些装饰性的内容，或者说一些简单易懂的技巧，比如说某些主题设置，或者类似 &lt;code&gt;watch -n 1 nvidia-smi&lt;/code&gt; 这种几行就能说完的内容。&lt;/p&gt;
&lt;h2&gt;Github badges&lt;/h2&gt;
&lt;p&gt;准确的说，这种 badges 是可以在任何地方使用的，但是一般来说还是在 Github 里面见到的会多一些，所以干脆就在描述中添加一个 Github 的前缀。具体这个是个啥呢，在这里介绍的是我比较喜欢的一种描述一些 popular brands 的 badges，大概如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3776AB?style=for-the-badge&amp;#x26;logo=python&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/cpp-00599C?style=for-the-badge&amp;#x26;logo=cplusplus&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/NodeJS-5FA04E?style=for-the-badge&amp;#x26;logo=nodedotjs&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/TypeScript-3178C6?style=for-the-badge&amp;#x26;logo=typescript&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/google%20chrome-4285F4?style=for-the-badge&amp;#x26;logo=googlechrome&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/vivaldi-EF3939?style=for-the-badge&amp;#x26;logo=vivaldi&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/gnu%20bash-4EAA25?style=for-the-badge&amp;#x26;logo=gnubash&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/zsh-F15A24?style=for-the-badge&amp;#x26;logo=zsh&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/markdown-000000?style=for-the-badge&amp;#x26;logo=markdown&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/vim-019733?style=for-the-badge&amp;#x26;logo=vim&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/obsidian-7C3AED?style=for-the-badge&amp;#x26;logo=obsidian&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/git-F05032?style=for-the-badge&amp;#x26;logo=git&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/astro-BC52EE?style=for-the-badge&amp;#x26;logo=astro&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/android-34A853?style=for-the-badge&amp;#x26;logo=android&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/anaconda-44A833?style=for-the-badge&amp;#x26;logo=anaconda&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arc-FCBFBD?style=for-the-badge&amp;#x26;logo=arc&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/opencv-5C3EE8?style=for-the-badge&amp;#x26;logo=opencv&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/pytorch-EE4C2C?style=for-the-badge&amp;#x26;logo=pytorch&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/tensorflow-FF6F00?style=for-the-badge&amp;#x26;logo=tensorflow&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arduino-00878F?style=for-the-badge&amp;#x26;logo=arduino&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/ros-22314E?style=for-the-badge&amp;#x26;logo=ros&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/huggingface-FFD21E?style=for-the-badge&amp;#x26;logo=huggingface&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/hugo-FF4088?style=for-the-badge&amp;#x26;logo=hugo&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arxiv-B31B1B?style=for-the-badge&amp;#x26;logo=arxiv&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/latex-008080?style=for-the-badge&amp;#x26;logo=latex&amp;#x26;logoColor=ffffff&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/aseprite-7D929E?style=for-the-badge&amp;#x26;logo=aseprite&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/overleaf-47A141?style=for-the-badge&amp;#x26;logo=overleaf&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/bilibili-00A1D6?style=for-the-badge&amp;#x26;logo=bilibili&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/c-A8B9CC?style=for-the-badge&amp;#x26;logo=c&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/cmake-064F8C?style=for-the-badge&amp;#x26;logo=cmake&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/gnome-4A86CF?style=for-the-badge&amp;#x26;logo=gnome&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/godotengine-478CBF?style=for-the-badge&amp;#x26;logo=godotengine&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/javascript-F7DF1E?style=for-the-badge&amp;#x26;logo=javascript&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/nginx-009639?style=for-the-badge&amp;#x26;logo=nginx&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/nvm-F4DD4B?style=for-the-badge&amp;#x26;logo=nvm&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/openai-412991?style=for-the-badge&amp;#x26;logo=openai&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/ollama-000000?style=for-the-badge&amp;#x26;logo=ollama&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/pypi-3775A9?style=for-the-badge&amp;#x26;logo=pypi&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/rss-FFA500?style=for-the-badge&amp;#x26;logo=rss&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/vercel-000000?style=for-the-badge&amp;#x26;logo=vercel&amp;#x26;logoColor=FFFFFF&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;所以基本上可以发现，假如说你有一些需求，比如说想要展示自己的技能，就可以通过这种方法来展示这些技能对应的 badges，也算是一种十分炫酷的方法。本质上这个东西依然是通过经典的 &lt;a href=&quot;https://shields.io/&quot;&gt;shields.io&lt;/a&gt; 来实现的，具体的详情可以从链接进去来看文档，这里面给一种比较傻瓜式的调用方法：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;![](https://img.shields.io/badge/{title}-{color}?style=for-the-badge&amp;#x26;logo={logoname}&amp;#x26;logoColor={logocolor})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里面可以看到四个内容，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;title&lt;/strong&gt;：badge 的文字描述。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;color&lt;/strong&gt;：badge 的背景色，使用 hex 编码表示（不包括 &lt;code&gt;#&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;logoname&lt;/strong&gt;：badge 的 logo 名称。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;logocolor&lt;/strong&gt;：logo 的颜色，使用 hex 编码表示（不包括 &lt;code&gt;#&lt;/code&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这里面，logo 的名称可以在 &lt;a href=&quot;https://simpleicons.org/&quot;&gt;https://simpleicons.org/&lt;/a&gt; 中找到，在这里建议将 logo 颜色设置为白色，然后背景色设置为网页中推荐的那个 logo 的配色，会比正常设置要有质感一些，比如说显示 &lt;code&gt;vitepress&lt;/code&gt;，就可以使用：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;![](https://img.shields.io/badge/vitepress-5C73E7?style=for-the-badge&amp;#x26;logo=vitepress&amp;#x26;logoColor=FFFFFF)
&lt;/code&gt;&lt;/pre&gt;</content:encoded><h:img src="https://picr2.axi404.top/LittleSkills-zh.webp"/><enclosure url="https://picr2.axi404.top/LittleSkills-zh.webp"/></item><item><title>Paper Reading 0</title><link>https://axi404.github.io/blog/paper-reading-0</link><guid isPermaLink="true">https://axi404.github.io/blog/paper-reading-0</guid><description>在方向中探索之前，更加基础的一些内容，课程/书籍推荐以及基础论文阅读。</description><pubDate>Mon, 09 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;算是写在一切之前，在开始我的 LLM 以及 embodied 之前，自然还是下过不少的基本功的，在这里算是记录一下，后续的内容也会陆陆续续的更新。&lt;/p&gt;
&lt;p&gt;整体来说，这是一个 Paper Reading 主题的内容，在此之前曾经一度被我称之为 LLM Talk，但是思索了良久，一方面我自己的水平只是给出来一些 Paper 的浅显理解，而算不上 Talk；一方面我确实也有打算阅读更多的论文，也可能总结一些之前阅读的论文，这些都会横跨不少的领域，因此干脆就叫做 Paper Reading 吧，符合初衷，而且比较直观一些。全部的内容还是尽量按照我的阅读顺序来写的，所以说在时间线上并不能完全保证，或许某一天有更多的时间的时候，可以抽身出来好好整理一下，不过感觉也是需要等到很久以后了。&lt;/p&gt;
&lt;p&gt;我写的大多数的 paper reading 或者说 insight 分享，都是某一天想起来再写的。我估计我读过的论文，估计少说也有两百多，一篇一篇写是不太可能了，只能说慢慢读，慢慢写，想起来写，纯凭兴趣。&lt;/p&gt;
&lt;p&gt;正常的基本功内容以及之前的一些文章，可以说也有很多了，要是说写完，倒也不太可能，姑且作为一个长期的工作吧，希望能够有写完的一天。&lt;/p&gt;
&lt;h2&gt;机器学习&lt;/h2&gt;
&lt;p&gt;一开始是学习机器学习，在这里，大多数的知识点就是算法本身，更加偏向于数理之类的内容，不存在太多的 insight。要是真说是有的，估计是对于诸如熵/分布/采样等内容的理解与重视。中间看过几本书，推荐李航老师的《统计学习方法》以及周志华老师的《机器学习》。统计学习方法有&lt;a href=&quot;https://space.bilibili.com/406882224&quot;&gt;简博士的讲解&lt;/a&gt;，在我写这篇博客的时候，依然还在连载，不过事实上到了后面，一些内容很容易就看进去了，倒是不太需要视频。&lt;/p&gt;
&lt;h2&gt;深度学习&lt;/h2&gt;</content:encoded><h:img src="https://picr2.axi404.top/Paper-reading-0-zh.webp"/><enclosure url="https://picr2.axi404.top/Paper-reading-0-zh.webp"/></item><item><title>周记 Week10</title><link>https://axi404.github.io/blog/week-10</link><guid isPermaLink="true">https://axi404.github.io/blog/week-10</guid><description>2024-09-02 ~ 2024-09-08.</description><pubDate>Mon, 09 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;于是乎也算是写到了第十篇周记了，可以说是可喜可贺，当初创建博客的时候，还有回忆过之前的 WordPress 创作经历，可以说还是花钱不讨好，而现在居然已经坚持了十周的周记。&lt;/p&gt;
&lt;p&gt;这段时间里面自然也有一些技术类的分享，但是更多的还是欠下了很多的债，或者说挖开，但还没填好的坑。像是 Isaac Sim 的文章，我目前自己学习的进度已经远超越这篇文章，本来写下来是打算给同样入门的同学去看的，但是再这样下去，我已经无法设身处地地想到那些初学者会遇到的困难了。现在手头还有两个工作，可能后续处理完之后也就有时间把这些东西再整理整理了。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;p&gt;这也是在上海的最后一周了，也算是开始了我的正式工作，也就是一个应该算是我主导的课题，工程量总体来说还是很大的，而且也有一些时间成本。现在基本上代码是可以跑起来了，算是第一个小阶段告一段落，但是后续的事情还有很多，而 DDL 又不是很宽裕，还是要抓紧去做的。&lt;/p&gt;
&lt;p&gt;课内的一些事情也或许值得说一说，最近开始整理德育分，发现基本上自己的德育分都是 RM 这边给的。只能说，虽然大多数时候这边都把人累得够呛，但是关键时候确实能够帮上我不少事情，尤其是像是给工时之类的，免去了去做很多琐事的时间。&lt;/p&gt;
&lt;p&gt;然后或许就是社团的面试，因为又是新的一年招新季开始了，所以说作为视觉组组长，我在 RM 这边也要开始负责面试，以及后续的一些培训工作。到大三之后，可以说我已经成了名副其实的老东西。大二作为组长面试的时候还需要考虑对一些大三同学的尊敬或者什么的，当时甚至还有直系学长前来，而如今可以说，无论是在技术积累还是在培训阶段，目光所至的地方已经没有多少可以指导我的同学，对于大家对一些自我介绍，也可以比较准确的切中那些核心部分，而且那些包装的地方揭开真面目。&lt;/p&gt;
&lt;p&gt;一些现象确实也比较触目惊心，不少同学，其实在面试中都提到了自己有某某经历，然而我尽管学习的颇有广度，但假如他们真的在自己所说的领域中深耕许久，我必然没有他们所具备的深度，然而没有一个人能触及到我的知识盲区，甚至几个来回之下就已经说不出话了。&lt;/p&gt;
&lt;p&gt;这还不同于一开始我所担忧的，如同在西安交大生存指南中提到的，不少同学虽然努力，但是却将才华被浪费在了重复性的工作中，现在来看，不少人还没开始努力，也还没到，需要包装自己的时候，就已经开始说起了场面话，玩起了文字游戏。不过好在一些零基础的同学看上去还是未来可期，或许和当初的我也有些许相似，一些对技术的渴望，一些热爱以及数不尽的精力，只是不知道这种，是他们的故意表现还是确实如此。&lt;/p&gt;
&lt;p&gt;当然，最后新的学期要开始了，一些日常的学习也需要好好努力。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;终于又是和乐小姐见面了，还是很让人开心的，毕竟足足已经有一个多月没有相见，虽然平时都会在微信上聊天，但是还是不得不进行了一个狠狠的贴贴。&lt;/p&gt;
&lt;p&gt;和乐小姐一起去吃了海底捞，服务员也有问，好像我们很久都没有来了，我们之前经常一起出去吃饭，而她们那边校区旁边的海底捞是我们经常的选择。这里不得不说，海底捞之间也是有差距的，Momopark 店的服务十分的贴心到位，而反观兴庆这边就差点意思了。&lt;/p&gt;
&lt;p&gt;总的来说还是很顺利且幸运的一周，看起来一个假期的努力都是值得的，希望我和她能继续开心下去，享受学习享受科研享受生活吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-10-zh.webp"/><enclosure url="https://picr2.axi404.top/week-10-zh.webp"/></item><item><title>周记 Week9</title><link>https://axi404.github.io/blog/week-09</link><guid isPermaLink="true">https://axi404.github.io/blog/week-09</guid><description>2024-08-26 ~ 2024-09-01.</description><pubDate>Mon, 09 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这一周可以说也是过得平平淡淡，不过快要开学了，或许等到开学之后，有意思的事情就可以多一些。感觉这些东西甚至没有必要分成章节来说，还是简单的随意聊一聊吧，至少作为过去的这一周的记录。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;p&gt;这应该是我在上海这边线下实习的倒数第二周，一些阶段性的工作也都已经弄完了，可以进入下一阶段，并且更多的做一些事情。事实上我是非常想要一直呆在这边的，而且假如可能的话，这边应该是保研的唯一意向，双方可能都交流的比较好，而且这边的环境也属实不错。&lt;/p&gt;
&lt;p&gt;不过作为本科生，课内的学习确实是第一要务，所以还是需要回到学校的，参加一些实际对我已经没什么帮助的课程，然后在教室里浪费一些生命。&lt;/p&gt;
&lt;p&gt;本身来说，这周的事情还是很多，强行 push 自己做完了 ICLR 的 Method 部分，相关工作和实验部分都有其他的东西可以借鉴，创造性的工作已经几乎完成了，只是现在实验的结果不是很顺利，所以还要进一步的调参。实话这个内容并不是很顺利，所以剩下的也只有折磨了，在经历了一开始的几个成功案例之后，剩下的只有挫折，但终究还是要慢慢熬过去。&lt;/p&gt;
&lt;p&gt;同时补考的时间也开始临近，虽然之前也有学习过，但是老实说我也没有很大的把握，知识点已经差不多遗忘了，最近再看一看课，争取把沉睡的记忆唤醒，不然后续可能也会有大麻烦。&lt;/p&gt;
&lt;p&gt;其实事实上很大程度上，这一周以及下一周（因为写作这篇周记的时候已经是下一周），在学习中的主基调还是挫折。我确实又学到了很多新知识，但很遗憾，我会将大多数新知识归结为没有人带领入门，这也是很糟糕的事情。大量的内容我不知道应该向谁请教，很多时间我都在重复造轮子，而且一些相关工作身边的人也没有涉及，在摸索中大量的时间被消耗，而我做到的事情，可能其他人在有计划的指导下不到半天就可以完成。&lt;/p&gt;
&lt;p&gt;到头来这其实是传承的比拼，确实有很多人帮助了我，但是大多数时候还是我一个人在孤军奋战，不过这种自学的能力，我在一次次锻炼中也倒早已掌握，希望我最后的结果可以不负自己当初的期待。&lt;/p&gt;
&lt;p&gt;这段时间我又在想着新博客的事情，看到了一个框架，Astro，确实比我现在使用的 Valaxy 更加专业一些，可以个性化的定制自己的博客，而 Valaxy 本质上其实使用的还是主题，想要进行覆盖也不太优雅。&lt;/p&gt;
&lt;p&gt;不过现实是，就像我之前说的一样，这一周的主要基调还是挫折，在按照教程老老实实的学了之后，在代码高亮的设置上还是屡屡失败，最后放弃。哪有什么捷径可以走呢，想要用自己的几天甚至短短几个小时的努力，就达到别人努力了几年的积累，再怎么想也是天方夜谭。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;p&gt;在上海，因为也算呆了一段时间，可以说一些生活节奏也是慢慢地熟悉了，然而可以说还是比较遗憾的一点，就是我依然没有什么社交的活动，或者去参观一些上海其他地方（之前在同学的邀请下去了上交）。&lt;/p&gt;
&lt;p&gt;感觉就我个人性格而已，比起来户外的一些探索，可能更加适合在网上进行冲浪，或者和女朋友两个人窝在一起。&lt;/p&gt;
&lt;p&gt;说起来还有一周也要开学了，也快要见到女朋友了，还是十分期待的，又一个多月没有相见，希望她也要开开心心的。&lt;/p&gt;
&lt;p&gt;总的来说，这一周也没有经历太多的事情，也算是平稳过渡了，自己的一些能力有了很多长进，但是同时要做的事情还有很多，也需要仔细去准备，这都是必须要考虑的，还是先踏实下来好好做些该做的事情吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-09-zh.webp"/><enclosure url="https://picr2.axi404.top/week-09-zh.webp"/></item><item><title>周记 Week8</title><link>https://axi404.github.io/blog/week-08</link><guid isPermaLink="true">https://axi404.github.io/blog/week-08</guid><description>2024-08-19 ~ 2024-08-25.</description><pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;序言&lt;/h2&gt;
&lt;p&gt;实际上是直到今天，我才写完上周的周记，其实一直以来我都有些惶恐，生怕这个周记落下一周。一方面会遗忘一些经历的事情；另一方面，周记是我坚持写博客的一种象征，我生怕不写周记之后，下一个就轮到博客了，还好赶上了，也没有留太多的债。&lt;/p&gt;
&lt;p&gt;这一周经历的事情其实也不算多，基本上还是四平八稳的在实验室里做东西，同时也在学新的知识。事实证明学习还是永无止境的，个人感觉我目前的水平距离真正的高手还有很大的距离，只能说尚需努力吧，确实是还有很长的路要走。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;p&gt;还是简单说一下学业的情况，西交那边的课题组 Sanping Zhou 老师问起来进度的问题，所以说又推进了一下 ICLR 的论文进度。基本上性能可以说还算乐观，一些地方没有特别大的突破，但是故事还是很能讲下去的。现在把 Introduction 写完了，后续可能要再推几个公式，然后写一下方法部分。&lt;/p&gt;
&lt;p&gt;目前的计划可以说是，先把整体的论文的思路打通，这样子可以搞清楚需要多少实验，随后就可以并行跑实验以及继续打磨论文。老师给的建议是做一个自顶向下的故事线，而本身的故事没有什么大问题，不过这些东西都可以放在后面再说，先写方法跑实验吧，走一步看一步。&lt;/p&gt;
&lt;p&gt;在周老师这边的科研还是令我很有感触的，基本上有两件事情。第一件事情是和周老师谈起，我将来会去其他实验室的事，其实这肯定也是必然了，老师心里也清楚。我隐隐向老师说，将来可能会看一下其他的国内比较好的实验室，并且举例了 shailab 以及叉院，老师的第一反应居然不是问我的课题怎么办，或者一些劝说云云，而直接脱口而出的事可以给我写推荐信。回顾大二参与科研的这段时间，老师给我带来帮助不可谓不多，无论是和老师进行一些讨论，还是在论文方面接受指导，又或者是老师拜托其他师兄来帮助，这些事情确实没齿难忘。另一件事情就是在周日的组会上进行汇报，我去讲一些我的工作进度，以及将来的规划，可以说已经是井井有条了，相较于刚刚进组时的一窍不通，现在的我确实已经有了长足的长进。这是一年时间之内我的蜕变，那么再过一年会怎么样呢？还请诸君和我一起拭目以待。&lt;/p&gt;
&lt;p&gt;另外一边的事情，可以说还是在稳扎稳打的学习新知识，距离彻底成为这方面的高手，还有很长的距离。现在基本可以确定的应该算是将来的方向，可能主要还是在仿真里做一些事情。&lt;/p&gt;
&lt;p&gt;然而不得不说的是，确实因为一直以来忙碌于学习，导致一些博客之类的记录，其实没有很勤快了，现在其实基本的方向又一次稳固了下来，每天也是时候分一些时间去做别的有意义的事情。&lt;/p&gt;
&lt;h2&gt;参观&lt;/h2&gt;
&lt;p&gt;另外一件可能值得说一说的事情是，实验室里有一位同样是大二的同学，个人感觉能力比我强上很多，来自上交，在周末的时候邀请我去上交参观。&lt;/p&gt;
&lt;p&gt;说实话，假如不是别人的邀请，我可能除非保研到了上交，不然应该是不会踏进校园一步的，主要还是因为懒，然而不得不说的是，上交确实是每一个西交学子梦想中的学府。&lt;/p&gt;
&lt;p&gt;这次去参观的主要是闵行校区，也可以理解为西交的创新港，属于是在地价稍微便宜一些的地方开设了一个很大的校区。也有必要说明一下，虽然我说两个比较类似，然而闵行校区的附近依然是相对繁华的市区，而创新港真的是港村。&lt;/p&gt;
&lt;p&gt;闵行校区可以说是发展的比较完备的校区了，包括说各种图书馆、宿舍区以及教学楼，环境都十分的优美。就论图书馆这一点，虽然说西交的钱学森图书馆具有标志性的形状，然无论是功能性还是空间或者使用体验，明显远远逊色于闵行校区。虽然我不是去图书馆学习的那一类人，但是试想我在那样的环境之下读书，又怎能没有一个好心情呢？&lt;/p&gt;
&lt;p&gt;西交假如想在学校的实际上更进一步，基础建设是必不可少的，然而我目前目之所及，却没有见到什么有用的提升。人工智能专业作为钱学森书院的实验班，我们 2022 级住在几乎是学校最差的西十一，没有卫生间，没有阳台，走廊里挂满了潮湿的衣服，一股霉味。要说真的没有装修吗？那倒也冤枉人了。绕着思源活动中心的一圈路被拆了又装，虽然我没从中看出什么改进的地方。倒是我去地下室下楼的时候，有一段时间经常因为下雨，在挖空的路上踩了一脚泥。&lt;/p&gt;
&lt;h2&gt;开源&lt;/h2&gt;
&lt;p&gt;又是因为开源的事情，我似乎有必要再次拿出来说一说，只能说大学生中位数的素质之低，实在是超出了我的预期。&lt;/p&gt;
&lt;p&gt;首先必须要承认的是，在新生中是有懂得礼貌的佼佼者的，会询问清晰的问题，虚心求教，并且表达感谢。然而似乎是因为大多数人的知识越浅薄，就越不会表达感谢，也可能是因为疏于表达感谢，导致他们的知识浅薄。&lt;/p&gt;
&lt;p&gt;这期间又有不少的同学问了我很多的问题，一些是我会的，一些是我不会的。基本上对于全部的问题，我都会尽我所能的回答，或者去找一些我认识的其他同学询问一下，以确保回答的真实性和全面，然而事实上能够收获的感谢依然不多。&lt;/p&gt;
&lt;p&gt;与此同时，因为众所周知的原因，假如在简历上有一项不错的开源项目，可以说是一个很好的加分项了。假如读者看过我的自我介绍，应该倒也不难发现，我做过一个技术力不是很强，但是功能性还好的网站，cs-baoyan-ddl，因为确实切中了一些痛点，获得了 100+ 的 star。然而最近遇到了一些抽象事情，已经有人盯上了这个项目，通过很水的贡献，想要把这个项目加入自己的简历。&lt;/p&gt;
&lt;p&gt;我对一些开源的事业带有一种敬畏感，尤其是这种不大的分享，我认为更是需要远离获利这一概念，包括在绿群中成为管理员也是如此。在绿群中，我确实收获了很多友谊，也涨了很多见识，但是自认为和我在其中付出的时间/给出的帮助/做出的贡献相比，或许还是后者更多一些，不过帮助他人确实是一件美好的事情，我也获得过很多其他人的帮助，解决了一些棘手的难题，因此我倒也乐于做这件事。&lt;/p&gt;
&lt;p&gt;绿群作为一个非商业的交流群，无论是群主还是管理员，都没有进行任何的收费行为（为了维持QQ群的建立，每年的缴费需要大家众筹，但这完全不是管理员的报酬，而是建立大群需要对QQ进行的支付），我们自然也不希望任何的成员在其中进行打广告，然而确实有这样的成员存在，打着类似于收费保研辅导一样的幌子，在里面招摇撞骗。&lt;/p&gt;
&lt;p&gt;当然更加过分的是，同一个人，也私聊过我，说想对我的 cs-baoyan-ddl 进行贡献。&lt;/p&gt;
&lt;p&gt;我在之前关于 GitHub Actions 的博客中大概聊过这个项目是如何运作的，本质上通过一个仓库管理数据库，另一个仓库作为前端来运行一个网页，数据的同步通过定期的 Actions 执行，基本上是可以满足需求的。&lt;/p&gt;
&lt;p&gt;在此基础上，我为了更好的维护这个数据库，增加了一套通过 GitHub Issues 来提交数据的方案，具体来说你只需要创建一个 issue 并且 close 掉，只要 issue 中的内容符合规范，就会被自动提交为 json 来储存，甚至可以给院校自动打 tag，基本上不到 1 分钟就可以提交一条数据，更何况数据是从绿群的大文档里面直接拿的，只是大文档不太格式化，所以暂时是手动将其转换为格式化信息。&lt;/p&gt;
&lt;p&gt;这位群 u 直接直言不讳，告诉我想要攒一些 GitHub 贡献，然后一晚上提交了四个 issue，有两个不能用，剩下两个，我要进行的修改，效率还不如我从头自己做一遍。当时等待提交的信息大概三四十条，是我一个人提交完的。&lt;/p&gt;
&lt;p&gt;当然，之后和一些认识的朋友聊了这件事情，倒也有些释怀了，这个世界上的物种多样性还是很多的，在计算机领域尤其如此。我可以说我是一个一般的内容产出者，但是还算比较积极，不过确实有很多人既不产出内容，还想将功劳占为己有。先让他们占去吧，不属于自己的东西，永远不会属于自己，不说项目是如何搭建起来的，在面试的时候一旦被提问，含金量自然原形毕露。&lt;/p&gt;
&lt;p&gt;我自己的路，可以说还是自己选择的，既然选择了帮助其他人，索性做一些自己力所能及的事情，也不会就此停下来，但最近无论是学习还是其他，确实道也令我心身俱疲。不说需要停下来休息一会儿，但碰到这些烦心事，却免不了更加烦心，必须要写下来发发牢骚，倒是可能传递给读者一些负能量了。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-08-zh.webp"/><enclosure url="https://picr2.axi404.top/week-08-zh.webp"/></item><item><title>一种优化后的免费图床解决方案</title><link>https://axi404.github.io/blog/imagebed</link><guid isPermaLink="true">https://axi404.github.io/blog/imagebed</guid><description>仅需域名，无限空间，无限流量，国内快速访问，基于 vercel/PicX/webp。</description><pubDate>Sun, 25 Aug 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;最近在写博客的时候，发现图片上传的问题。由于博客使用的是 GitHub Pages，而 GitHub Pages 的图片上传收到访问速度的限制，因此需要寻找一种更好的解决方案。事实上，不只是图片，本身的博客依然有一定的体积，所以说找到一种很好的替代方案是很不错的。更何况把什么东西都扔到博客的仓库中，也太重了。我之前可能加上图片，博客已经将近 200 MB，而现在优化之后，只剩下了 1.4 MB。&lt;/p&gt;
&lt;h2&gt;图床现状&lt;/h2&gt;
&lt;p&gt;所以说就不得不说一下目前图床的现状了，无外乎就是付费以及免费的图床，一方面我都已经用 Github Pages 了，肯定还是尽可能寻求免费的解决方案，但是这种图床要不然十分不稳定，而且国内的连接情况不一定好，要不然可能直接就不支持 NSFW 内容。&lt;/p&gt;
&lt;p&gt;虽然本博客并非什么奇奇怪怪的系统，但是依然可能上传一些本人的图片等，会被检测为 NSFW，这为我带来了极大的困扰，那么有没有一种方案呢？答案是有的，那就是使用 Github。&lt;/p&gt;
&lt;p&gt;我们都知道，在把资源上传到 Github 之后，可以使用 Github 的 raw.githubusercontent.com 来访问这些资源，然而 Github 因为众所周知的原因，网速时有时无，很难指望作为一个稳定的图片存储源，而且操作起来也不是很直观。&lt;/p&gt;
&lt;h2&gt;解决方案&lt;/h2&gt;
&lt;p&gt;在进行了广泛的调研以及寻找之后，我找到了我的初步答案：PicX，网址是 &lt;a href=&quot;https://picx.xpoet.cn/&quot;&gt;https://picx.xpoet.cn/&lt;/a&gt;。PicX 是一个开源的图床工具，它可以可视化地进行图片的上传，虽然说本质上还是需要使用 Github 来存储图片，但是它提供了很多方便的功能，且不说做了一个网页这件事就已经神中神，其更是提供了自动转换 webp 压缩的选项。&lt;/p&gt;
&lt;p&gt;在这里介绍一下 webp，这种有损压缩的格式，压缩的性能十分离谱，一张可能好几 MB 的图片，在压缩之后只有几百 kb 或者更少，这一点在二次元图片（一般被用作博客文章的封面或者头图）上面更为明显，提供了十分离谱的压缩效率，而相应地，其压缩后的图片质量也并不差，几乎看不出区别。因此，使用 webp 格式来存储图片，可以极大地减少博客的体积，同时，由于 webp 格式的图片体积小，加载速度也会更快。&lt;/p&gt;
&lt;p&gt;PicX 本身有提供 Github 的图床方案或者一些不是很快的 cdn，但是事实上，虽然使用 webp 可以加快速度，但是并不是那么的理想，而且这个服务不是来自国内，所以还有优化的空间。&lt;/p&gt;
&lt;p&gt;这时候就不难想到 vercel 了，因为部署过博客，而且博客就是在 vercel 运行的，因此我之前就知道 vercel 具有一个在国内的 CNAME 服务器，于是不难给 vercel 加上 vercel。&lt;/p&gt;
&lt;p&gt;首先我们需要有一个域名，在这里直接使用 &lt;a href=&quot;https://www.namesilo.com/&quot;&gt;namesilo&lt;/a&gt;，比较老牌，而且可以支付宝支付。这里面购买就暂且不提了。&lt;/p&gt;
&lt;p&gt;然后前往 &lt;a href=&quot;https://dash.cloudflare.com/&quot;&gt;cloudflare&lt;/a&gt; 来获取 DNS 解析以及更多更强大的功能，进入 dashbroad 选择 Add a domain：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.5tqyhumx92.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;接下来输入你的域名，这里以一个不存在的域名为例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-1.7zqd3mel07.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;遗憾选择 Free 方案，毕竟是免费的解决方案：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-2.7p3jagzcuw.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;接下来添加一个 A，选择 vercel 的地址 &lt;code&gt;76.223.126.88&lt;/code&gt; 或者 &lt;code&gt;76.76.21.98&lt;/code&gt;，然后名称写 &lt;code&gt;@&lt;/code&gt;，点击保存。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-4.839z1c7nq2.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;接下来它要求你更改的 NameServer：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-3.6pnfxawlp5.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;并给出 NameServer：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-5.4uav4ok63f.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后就可以前往 namesilo 管理域名，在 My Account -&gt; Domain Manager -&gt; axi404.top -&gt; NameServers 删掉原来的东西，并且添加这些。&lt;/p&gt;
&lt;p&gt;之后回到 cloudflare，在快速入门中把 https 重写啥的都打开。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-6.3k7xyd26sb.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在 SSL/TLS 中选择完全或者完全（严格），否则会因为证书不匹配导致反复重定向而打不开网页：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-7.70a9qgbtul.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;接下来可以前往 &lt;a href=&quot;https://vercel.com/&quot;&gt;vercel&lt;/a&gt;，登录你的 Github 账号。PicX 会给你注册一个 picx-images-hosting 的仓库，在这个里面安装 vercel。Import 之后进入 Settings -&gt; Domains，这边就会提示你使用你添加的域名，比如说 pic.axi404.top，之后点击 add。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image-8.1023lq285x.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;再回到 cloudflare，在 DNS 中添加一条 CNAME 记录，在我的例子中 name 为 pic，内容为 &lt;code&gt;cname-china.vercel-dns.com&lt;/code&gt;，保存，即可。&lt;/p&gt;
&lt;p&gt;此时整个的方案就结束了，你在 PicX 上面上传图片部署网站之后，Vercel 会自动更新，同时你的图床访问图片的方法，类似于 &lt;code&gt;https://pic.axi404.top/image.webp&lt;/code&gt;，速度很快，完结撒花。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/ImageBed-zh.webp"/><enclosure url="https://picr2.axi404.top/ImageBed-zh.webp"/></item><item><title>周记 Week7</title><link>https://axi404.github.io/blog/week-07</link><guid isPermaLink="true">https://axi404.github.io/blog/week-07</guid><description>2024-08-12 ~ 2024-08-18.</description><pubDate>Sun, 25 Aug 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;所以说，这其实又是一次久违的周记，一方面确实上一周的事情很忙，没什么时间去写东西。假如有读者关注我的 RSS，不难发现，基本上都是大段内容的 Isaac Sim 分享，说白了也就是和科研紧密相关的技能的学习。&lt;/p&gt;
&lt;p&gt;我大概是周三来的上海人工智能实验室，然后周四入职，按照写周记的这天算，大概也一个星期了。这段时间基本上就是醒了学，然后累了就睡。&lt;/p&gt;
&lt;p&gt;说起来由于签了保密协议，所以其实做的一些事情和安排是不能说的，虽然听说没有过因为随手记录所以被惩罚的事例，但是毕竟按照我的计划来说，将来的很长一段时间都要在这边实习，所以还是小心使得万年船。在这里也立一个标准，以后相关的事情，只说自己学了什么，学了多久，不说干了什么，任务一类的一律不谈，直到产出为止。当然，我的一些学习分享的东西也会继续更新，这并不矛盾，或者说课题组里面貌似相关的东西也正好需要积累，那么何乐而不为呢？&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;p&gt;因为这一周发生的事情确实比较少，我也就不分条来概述了。这周的前半段的话是比较急的搞了一下签证，然后就坐火车来了上海，路上倒是没有发生什么事情。相较于回西安的时候买的是坐票，去上海还是稍微阔绰了一把，所以买了卧铺，个人感觉是硬卧的价格，虽然比不上软卧，但是貌似比我之前见过的那些卧铺要好很多，而且也很干净。来了这边之后住的也还不错，属于是酒店公寓了，和酒店一样，通勤骑车二十分钟。&lt;/p&gt;
&lt;p&gt;学习上的事情，我愿称之为三板斧，也就是三个很重要的事情。一个是要补考，之前复变函数缓考了，需要补，大概率还需要复习一下，毕竟我的记忆力只取决于平时的练习，所以还需要找回一下感觉；一个事情是本校那边的科研，因为打算投 ICLR，所以论文的工作现在就要开始了，也是要好好准备一下的；一个事情是上海这边的实习，目前的任务依然存在，而且还挺多。&lt;/p&gt;
&lt;p&gt;这个星期其实有一点久违的感觉，或者说这个假期至少没有荒废。之前的放假，总是会夹杂着一些自我感动，类似于自己平时已经那么努力了，假期休息一会儿也无所谓，然后就刷着没有营养的视频，又荒废了时间。现在来说确实好了不少，虽然说在假期前给自己定的 DDL 基本上一个也没有完成，但是参加科研实习，并且期间也收获了很多人的认可，也算是一件宽慰了。&lt;/p&gt;
&lt;p&gt;其实一直以来我都有一种暗自的自满，认为自己至少和同龄人相比，已经是很有水平了，但是实际上来了这边之后才发现，确实是强中更有强中手。一方面，比如说在绿群，可能我的水平已经算是很高了，甚至说西交的学历已经可以让我挺起胸膛，然而在目前的小组中，不说是科研经历，就连学历也已经成为垫底的了。&lt;/p&gt;
&lt;p&gt;有的时候我也想要提前向前看一步，类似于比我大一岁的人，我一年之后能不能超过他们，对于一些很厉害的人来说，我觉得只要足够努力还是有希望的，然而确实有的人我已经难以望其项背，也算是一种警醒，还需要继续高强度学习。&lt;/p&gt;
&lt;p&gt;基础来说，这一周学的东西还是很多的，后续能够放出来的，我也都会在博客中给出，算是一种积累和量化，顺便也给后来者一些力所能及的帮助吧。&lt;/p&gt;
&lt;h2&gt;聚餐&lt;/h2&gt;
&lt;p&gt;这个星期做的另一件事情，我觉得值得一提的，应该就是所谓的聚餐，或者说面基了。之前一直以来都是在线上和大家聊天，但是其实线下基本上都是和大家没有接触的，而和具有能力的人交流确实是一件愉快的事情，而且也可以增长自己的见识，再加上环境的改变，这种事情一定会越来越多。&lt;/p&gt;
&lt;p&gt;这周三也是受到了群友 Selen 的邀请，他和我以及谢老师三个人一起吃了个饭，然后晚上在太古里那边散步聊天。Selen 和我同样是大二升大三，谢老师是大三升大四，他们两个人都是西电的我们三个也可以说很有共同话题了。&lt;/p&gt;
&lt;p&gt;Selen 属于接触科研比较早的那一批，只是可能收运气或者什么原因，暂时还没有 publication，而谢老师是没有套瓷就进了清华的叉院和 AI 的人物，可以说不只是排名很好，而且科研能力远超于一般人，甚至让我将一年内的目标，可能就确定为想要到达他那种水平。&lt;/p&gt;
&lt;p&gt;互相交流的时候，因为我们也不太打游戏（虽然说我之前还有打游戏，但是如今可能也比较少了，唯一算是有的可能是碧蓝航线，然而用了 alas 之后可能几天都不用上去一次），一方面可能聊聊科研，一方面可能就是一些吐槽了。&lt;/p&gt;
&lt;p&gt;大家其实比较有共识的一点在于，西安确实想要发展是比较困难的，一方面高校虽然有，而且也算比较顶级，但是明显资源不多，而且内陆的企业也不算很多，基本肯来这种地方，假如是商业目的，实在是不如沿海城市。尤其是做人工智能，目前的一些领域里面，可能线下的实习会更加合适，而做出有影响力的工作又十分依靠资源和算力，因此在西安发展的自然也就举步维艰了。&lt;/p&gt;
&lt;p&gt;另一方面，被大家广泛吐槽的是，也就是学习和开源氛围了。讲实话，无论是西电还是西交，假如就说课内成绩，高手肯定还是很多的。日复一日年复一年的刷题和上课，这一批人又是好不容易卷出来的，能够将前排的绩点卷到一个匪夷所思的程度，自然也就不难理解了。但是尽管，说得好听一些，数学功底和计算机基础十分 solid 的人不在少数，但是在科研上有所长进的其实并不多。&lt;/p&gt;
&lt;p&gt;按照我们年级来说，我认为就课内学习而言，比我强的人少说也有七八个，然而就我所知，目前科研进度超过我的应该没有，甚至在人工智能班的历史上，能够和我大概在一个进度的也不算很多。谢老师更是属于提前规划的一批，早在大一就已经去商汤实习，这等前瞻在是令我望尘莫及，而 Selen 则是和我在同一节奏。&lt;/p&gt;
&lt;p&gt;不难发现的是有能力的人其实很多，然而大多数人在心态上就已经落后了不少，大概自己从初中高中就养成的做题家的舒适圈中，每天将课内的任务做完之后，以为复习的差不多滴水不漏，就已经知足。至于课外，大家也或多或少都听说过需要在课内的基础上再看看课外，然而依然不愿意自己去打破信息差。有的时候看到某某活动加工时/集体活动分，有的时候是某学生会干部，或者是某大创以及某创新创业竞赛，然后收到了别人的邀请，就义无反顾地投身其中，然而这些有点像是在青春小说里才会成为主要章节的内容，在现实中的功利主义面前实在上不了台面。或许有人说，自己水平可能还不够，也做不了科研，就并不是自己不想，而是不愿意给老师添麻烦。然而试问，这一类人又有没有了解过，有哪个课题组愿意招收课内成绩突出的学生，又有哪些是只要 self-motivated 就可以了。想着自己没有科研经历就不能科研，然而有没有尝试去了解需要学什么，而不是陷入没有科研就不能科研的死循环怪圈。&lt;/p&gt;
&lt;p&gt;或许可以说，有些人能力够了，但是心态还不够，但是从另一个角度，心态又何尝不是一种能力呢？敢于尝试，遇到事情先想，have a try，而非 give up，或许就能好上很多。&lt;/p&gt;
&lt;p&gt;当然，另一个更大的，让我非常有怨言的，也就是关于开源的事情。每一个计算机领域的人或多或少都听说过开源的传说，在 GitHub 上面用过那些炙手可热的工具，也或许有不少人都梦想成为一名开源事业的参与者。当然，从广义上来说，在大学中最常见的开源工作，还是要说学习资料分享和经验分享，将这些可以让自己的学习变好的内容分享出来，何尝不是一种开源的呢？&lt;/p&gt;
&lt;p&gt;然而事实上是，无论是国内的大学圈子，还是西交，开源氛围都差到了令人发指的程度。&lt;/p&gt;
&lt;p&gt;先谈西交，我无意否认那些学辅，优秀的诸如钱院学辅，一直在产出优秀的内容，然而确实存在不少的学习资料，以思政相关尤甚，为了做到滴水不漏，甚至一本资料上百页。当然，这也并非是在资料中给出了什么崭新的见解或者论述，而只是单纯将书中的大多数内容摘抄了下来。不少的同学只是为了工时，而在这里凑一些字数，有指标在，页数越多工时越多，而又如何做出小而美的开源？&lt;/p&gt;
&lt;p&gt;当然，将眼光聚焦在人智专业，开源的资料更是大多数都是上古时期流传的，而如今的内容，我可以说则可能有至少百分之五十是我贡献的，甚至我可以说，绝大部分。&lt;/p&gt;
&lt;p&gt;然而这是否说真的不存在开源的分享呢？一些还是有的，我们有 AI 学组，我之前建立过 Obsidian 的仓库，后来又建立过其他仓库去负责 AI 研讨会，到目前为止，PR 依然为 0，然而学组中的人数居然高达九人。&lt;/p&gt;
&lt;p&gt;前一段时间，学组又说要办 AI 研讨会，我不禁暗自称奇，且不说大家有没有能力去研讨，这我前些日子搭建出来的 AI 研讨会网站，也在专业群里进行了宣传，到现在还没有一个 PR，现在就想着自立门户了？所谓开源，难不成是创立山门，自立为王，然后等别人来丰富内容？&lt;/p&gt;
&lt;p&gt;这一研讨会还自称 NCAI，大概是南洋 AI 研讨会的意思，当时在群里征求意见，我确实一时间气的不行，写了大段的回复：&lt;/p&gt;
&lt;p&gt;—-&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最基础的问题，Nanyang Conference，这个署名权是否经过允许，钱学森书院并非南洋书院，这个名称是否需要商榷。&lt;/li&gt;
&lt;li&gt;科研虽然大家说门槛不高，是否真的不高，假如不是连续的讲座而是单次分享，是否真的可以将受众与深度兼得。&lt;/li&gt;
&lt;li&gt;所谓科研工具分享，除了 overleaf 和 chatgpt，科研工具真的存在必须必要而且实用的工具吗？文献整理/笔记/代码/SSH，各种工具我均有体验过，没有一个工具是真正值得推荐的，多半是差生文具多。假如介绍 LaTeX/SSH/conda 等的使用，分享者有多大的把握可以超过一个一概不知但是使用 ChatGPT 的人？教学的质量又有多大可能能超过那些经典的教程？&lt;/li&gt;
&lt;li&gt;当下流行的科研，有多少本专业本年级的同学仍沉迷于中文三大会的推文内容？有多少能准确说出当下最流行的几个方向以及其中的代表性工作？即使是炒作的工作，完整的理解公式的内涵而非朗读，又有多少人能做到？在我的印象里，此等人本专业本年级不超过三人。&lt;/li&gt;
&lt;li&gt;阅读论文，互联网上如李沐等大佬的视频均有之，分享者有何超越互联网内容的全新见解？假如如此，分享者论文阅读数量是否破百，是否真正了解某一领域的来龙去脉，而非人云亦云？&lt;/li&gt;
&lt;li&gt;思维碰撞，最忌讳 idea 抢跑，是否有足够的筛选机制，控制人员的安全性以及君子协定的严密？为什么有 idea 者不在组会分享（有更具经验的人指导，安全，而且假如可行可以提供算力，而研讨会甚至没有人有能力判断可行性），NCAI 有什么吸引力？&lt;/li&gt;
&lt;li&gt;开源社区的维护，早在一个多月之前，我已经搭建了 AI 研讨会的 GitHub 组织/仓库/静态网页，&lt;a href=&quot;https://xjtuaiseminar.github.io/&quot;&gt;https://xjtuaiseminar.github.io/&lt;/a&gt;，并在 AI 大群宣传，目前 pr 数量为 0。现如今的本专业甚至本校开源组织者，很大程度只是享受掌管开源社区的权力，而频繁另立山头，参与开源者寥寥无几。一个数据是，我目前维护七八个仓库，今年 commit 超过一千五百次，一个 star 破千，一个 star 破百，大量仓库访问量上万，而 star 不足五十，全部仓库活跃贡献者不超过十人。试问 NCAI 有什么把握可以维护活跃的社区？老实说，工时是西交仅次于大创第二没用的东西，用这种东西鼓励开源者甚至不如 GitHub 的一个 star 或者 follow，换句话说，是一种侮辱。&lt;/li&gt;
&lt;li&gt;领导者的选择，且不谈另立山头，一个会议就会有委员会，会有会长，谁能够担当这一职务？选择籍籍无名/学术成果不佳者，何以服众？选择成果累累者，由能用什么来吸引？开源是无偿的，但是不是别人的义务。&lt;/li&gt;
&lt;li&gt;举办会议，制作奖品，经费从何而来？如何管理？部分人自费终究只是部分人的自嗨，规模与影响力均无法扩大，而又没有足够的人手和精力处理一个复杂的经费管理体系。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;—-&lt;/p&gt;
&lt;p&gt;得到的回复则是乏善可陈：&lt;/p&gt;
&lt;p&gt;—-&lt;/p&gt;
&lt;p&gt;1.小问题，我也觉得要改，改个名字就行
2.4.实际上我自己的想法是开两种类型，一种是适合小白来学，一种是水平较高的同学互相讨论分享，也许是一种解决办法。
3.5.网上确实有优秀教程，但是自己学习的话动力没那么足，如果我们能够让大家我们紧密联系在一起集中一起学习，也许能够起到更好的效果。这就好比高数等学科，网上也有好的教程，但事实上很多同学还是跟着老师学。甚至我们不需要自己准备教程，对着李沐讲/放视频都行（？此点存疑），可以布置一些小作业/课上内容问答以活跃气氛等（但是这又是另一个问题，学校众多老师都没解决的课堂氛围不活跃、作业无法得到监督的问题，我们又如何解决？）
6.我也想到了这个问题，我认为有必要的时候，我们在分享之前需要向大家强调这一点，可以通过收集人员信息/让他们在保密协议上签字等方法进行约束，如果有人就是什么都不管抢跑了，我们可以找其导师进行反映。如果ta实在什么都不管不顾，我们也没办法，只能希望大家素质还是比较高的。
7.个人想法是如果我们能做出一个影响力较大的组织，那么可以让大家朝我们靠拢。
8.首先我不认同「会有会长」这个概念，相反我们需要的是「组织者」，这在一定程度上可以避免学术成果与组织的矛盾。另外，也完全可以采取集体领导的方式。
9.个人想法是，这个组织不需要多正式，实际上几个人之间的学术讨论确实可以理解成几个人的「自嗨」。&lt;/p&gt;
&lt;p&gt;以上是个人从建设性的角度针对上述问题提出的一些办法，不一定成熟且合理，只是在尝试着提出一些方法。以上不是何的意见，也不代表我反对高的意见（事实上相当多部分我是认同的）。其实问题大家或多或少都看在眼里，不过想办法解决总是对的。&lt;/p&gt;
&lt;p&gt;下面是个人的一些意见：
实际上虽然这件事做成了会是极好的一件事，但基于学校/专业目前的氛围，我对前景有些担忧。比如，且不提开源精神，我相信有些人github也不一定有。
我个人希望能够做到的（如果能做到），是给众多科研门外的小白提供一个入门的机会。诚然网络上教程相当多，但是如果对科研一点研究都没有的小白/其他专业的同学，他们根本都不知道这些资源、不知道去哪里找，甚至没有意识到科研这件事，那从何而来入门呢？&lt;/p&gt;
&lt;p&gt;我为什么那么关注科研小白，是因为两个原因。首先只有厚植土壤，形成良好的氛围，才能枝繁叶茂；其次，这类群体是最广大的群体，也是众多人群的最大公约数，我们想要做出有影响力的事情，我认为是应该面向他们起步的。
以上。&lt;/p&gt;
&lt;p&gt;—-&lt;/p&gt;
&lt;p&gt;这段话可以说是离奇至极，乍一看是冠冕堂皇的理想主义，但是细品之下却只有混乱的逻辑。一边说要帮助更多人，一边说是小部分人的自嗨，还说做开源，无论是研讨会还是之前的学组，又有多少人真正参与其中呢？现如今又是哪里来的把握，说自己一定可以再创辉煌。&lt;/p&gt;
&lt;p&gt;更何况做这种分享，搞一些网站也好，文章也罢，又或者是制作视频，完全是更好的载体，所谓的研讨会，也只不过是希望别人作为忠实的听众，给自己在台上侃侃而谈提供了一个平台。&lt;/p&gt;
&lt;p&gt;再回复，则是：&lt;/p&gt;
&lt;p&gt;—-&lt;/p&gt;
&lt;p&gt;不妨说的再不客气一些：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NCAI 对于水平高的同学有什么吸引力，有这个时间为什么不多看看自己的文章，写写代码，写写论文？而真正能参与理解和讨论，并非争论 Mamba out 或者 KAN 的 unfair 的人能有几位？&lt;/li&gt;
&lt;li&gt;NCAI 能办成有影响力的 xxx，事实上叙述已经前后矛盾，前面期待有影响力，后面又不指望多正式，那么这个组织的定位到底是什么？&lt;/li&gt;
&lt;li&gt;上课的课上问答和作业有谁喜欢？谁会愿意去多上一节课？徒增压力的事情对于每一个人都没有好处。在听说有作业之后，每一个本来有兴趣的小白都会放弃尝试。&lt;/li&gt;
&lt;li&gt;只是看视频或者寻找教程，大家又知道多少教程？不到一页 A4 规格 PDF 可以写完的内容，为什么还要开始讨论？&lt;/li&gt;
&lt;li&gt;和不同领域的人讨论，到底能有多少收获？假如说大家从跨领域的知识中可以获得理解，参加人机所之夏之后，是否有人受到启发已经开始写新论文了？假如大家没有参加人机所之夏，为什么又有把握参加 NCAI？假如说和同领域的人讨论，为什么不和同组的师兄交流？&lt;/li&gt;
&lt;li&gt;再说君子协定，到头来只能寄希望于每个人的素质/没有办法，而课题组几乎可以百分百确保这件事情不会发生，一个高风险低回报（不同领域的反馈），一个低风险高回报（同领域专家的反馈），为什么要分享？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;个人建议，假如大家真的想做这种交流与分享，可以先录制教学视频，制作文档，上传 B 站并搭建静态网页，并且在各种群中宣传，参与视频录制的人数以及播放量会告诉你答案。&lt;/p&gt;
&lt;p&gt;顺便，想要了解每个人的科研情况的话，不妨尝试建立以下的问卷，看看大家的科研到底到了什么阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;会使用 slrum&lt;/li&gt;
&lt;li&gt;会使用 huggingface&lt;/li&gt;
&lt;li&gt;处理过至少百万量级的数据集（处理，并非使用）&lt;/li&gt;
&lt;li&gt;阅读过超过 50 篇论文&lt;/li&gt;
&lt;li&gt;阅读过超过 100 篇论文&lt;/li&gt;
&lt;li&gt;主持过一项自己的课题（科研课题，非大创/腾飞杯）&lt;/li&gt;
&lt;li&gt;参与过超过三项课题（科研课题，非大创/腾飞杯）&lt;/li&gt;
&lt;li&gt;具有顶会一作的中稿（与前面的可以合计两分）&lt;/li&gt;
&lt;li&gt;在外校或者企业进行科研实习&lt;/li&gt;
&lt;li&gt;在港三/新二/美的顶尖学校进行科研实习（与前面的可以合计两分）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上内容各加一分。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正在主动参与一个大创项目&lt;/li&gt;
&lt;li&gt;正在使用非 Pytorch 的基础框架，或不会使用 Pytorch&lt;/li&gt;
&lt;li&gt;看过 KAN/xLSTM 等炒作工作，但是没听说过 RWKV&lt;/li&gt;
&lt;li&gt;只知道 CV 三大会，不知道机器学习三大会是什么&lt;/li&gt;
&lt;li&gt;不知道如何在服务器维护一个训练的线程，而不会在关闭的时候被 Kill&lt;/li&gt;
&lt;li&gt;没有使用过 GPT4（并非免费的 4o）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上内容减一分&lt;/p&gt;
&lt;p&gt;目前本专业本年级的同学大于等于 3 分的能有三位吗？&lt;/p&gt;
&lt;p&gt;当然，假如需要写文档或者录制视频，我肯定是会积极参与的，然而有必要提及的是，本人的另一项目，AI 自学指南，&lt;a href=&quot;https://aidiy.icu&quot;&gt;https://aidiy.icu/&lt;/a&gt;，同样为 0 PR。&lt;/p&gt;
&lt;p&gt;—-&lt;/p&gt;
&lt;p&gt;当然，我也并非说我的项目如此优秀，每个人都必须给我 PR，然而纵使我投入精力最多的西安交大生存指南，上万的访问量，却不足二十 star，也只有 2 PR，大多数人只是看，成为资料的收集者，并认为别人回答自己的问题是天经地义。不说满足提问的智慧，就连谢谢也不到 50%。&lt;/p&gt;
&lt;p&gt;我做了很多的项目，然而很少有人能够给出任何程度的反馈，而且我确信这些内容或许是有意义的，因为在我在网上的大量检索过程中，并没有发现同质项，然而本专业的一些同学却依然在热衷于挖新坑，等待别人去填，实在是令人难以接受。&lt;/p&gt;
&lt;p&gt;这实在是令人沮丧的，更不用说全国范围内，前不久也有同学来找到我，说想要参与 cs-baoyan-ddl 的项目，然而我制作的 workflow 可以通过 issue 直接更新数据库，并且 tag 也可以自己添加，更新一个学校不到一分钟。半天过后，我看到四个 issue，然后亲自把它们更改成了符合格式的样子，至于这个项目，按照他的话来说，「我想攒个 GitHub 的提交」，说不定已经进了自己的项目经历。&lt;/p&gt;
&lt;p&gt;好吧，一时间我实在不知道该说些什么，只是失语了，要是真的想帮助别人，之前写资料的时候大家又在哪里，现在怎么又只是热衷于提案/幻想/一触即走？&lt;/p&gt;
&lt;p&gt;这是这周的一些事情，权且当作牢骚和抱怨吧，开源的氛围和环境，这是一个长线的事情，我也早就不应该抱有过多的期望，每一届能有一两个人，或许我就应该要知足了，但至于我的路，暂且行一步看一步吧，希望能够在帮助别人这件事上走得更远。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-07-zh.webp"/><enclosure url="https://picr2.axi404.top/week-07-zh.webp"/></item><item><title>VitePress 贡献指南 &amp; 建站指南</title><link>https://axi404.github.io/blog/vitepress-tutorial</link><guid isPermaLink="true">https://axi404.github.io/blog/vitepress-tutorial</guid><description>如何对于 VitePress 项目进行贡献，快速看懂项目结构，并构建自己的项目。</description><pubDate>Fri, 23 Aug 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Aside } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;惯例的写一下前言，关于为什么要写这篇内容，以及这篇内容的主旨是什么。&lt;/p&gt;
&lt;p&gt;笔者最近开设了几个 VitePress 项目的网站，并且作为开源项目，开放给社区以及每一个人。毫无疑问，诸如 VitePress 类型的静态网页生成器，是一种极大的对于创作的便利，使得创作者无需关注于网站的构建，只需要专注于内容的创作。但是对于完全没有互联网基础的同学来说，这种内容甚至也已经超纲了，我们迫切需要一种类似于 Word 或者说 Markdown 编辑器这样子的开箱即得的记录方式，使得最不了解技术的创作者也可以尽情的创作。&lt;/p&gt;
&lt;p&gt;事实上这种内容是存在的，使用前后端的博客可以很轻易的达到这种效果，但是明显，目前因为种种原因而选择使用 VitePress 之后，一个为不了解技术的同学设计的 VitePress 贡献指南是有必要的，这可以帮助读者了解 VitePress 的基本结构，并且可以快速上手，对于 VitePress 项目进行贡献。&lt;/p&gt;
&lt;h2&gt;什么是 VitePress&lt;/h2&gt;
&lt;p&gt;VitePress 是一个基于 Vite 的静态网页生成器，它使用 Vue 作为其核心，并使用 Markdown 作为其内容格式。VitePress 的主要目标是提供一个简单而高效的方式来创建和维护静态网站，同时提供丰富的插件和主题来满足不同用户的需求。&lt;/p&gt;
&lt;p&gt;换句话来说，使用 VitePress，可以很轻易地通过 Markdown 格式的内容生成精美的静态网页，因此是很好的百科/博客类内容的载体。&lt;/p&gt;
&lt;h2&gt;项目结构&lt;/h2&gt;
&lt;p&gt;了解 VitePress 的项目结构是为 VitePress 做贡献的基本事项，一般来说，VitePress 的结构为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;├───.github
├───docs
│   ├───.vitepress
│   │   ├───cache
│   │   └───theme
│   ├───images
│   ├───public
│   ├───folders
│   └───index.md
├───node_modules
├───.gitignore
├───package.json
├───pnpm-lock.yaml
└───tsconfig.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于贡献者来说，仅需要关注 &lt;code&gt;docs&lt;/code&gt; 文件夹即可，&lt;code&gt;docs&lt;/code&gt; 文件夹下包含了 VitePress 的配置文件，以及所有的 Markdown 文件。其中作为初级的贡献者，需要了解的是 &lt;code&gt;docs&lt;/code&gt; 中的若干文件夹，并且对于新建的文档按照以下的步骤，在这里以项目 &lt;a href=&quot;https://survivexjtu.github.io/&quot;&gt;SurviveXJTU&lt;/a&gt; 为例。&lt;/p&gt;
&lt;h2&gt;贡献流程&lt;/h2&gt;
&lt;p&gt;关于从注册 Github 以及初始化 Git 开始的贡献流程，在 &lt;a href=&quot;https://survivexjtu.github.io/%E5%89%8D%E8%A8%80/%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97.html&quot;&gt;SurviveXJTU的贡献指南&lt;/a&gt; 中有具富文本与插图版本的说明，在这里给出转载。&lt;/p&gt;
&lt;h3&gt;注册 Github 账号&lt;/h3&gt;
&lt;p&gt;作为贡献者，首先需要注册 Github 账号，这一步十分的简单，前往 &lt;a href=&quot;https://github.com/&quot;&gt;Github 官网&lt;/a&gt; 并点击 &lt;code&gt;Sign Up&lt;/code&gt;，根据要求进行注册即可，在这里并不进行过多的讲解。&lt;/p&gt;
&lt;h3&gt;初始化本地 Git 并配置 SSH&lt;/h3&gt;
&lt;p&gt;在 &lt;a href=&quot;https://git-scm.com/&quot;&gt;Git 官网&lt;/a&gt; 选择下载 Windows 版本并按照提示进行安装。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/download_git.3goby6c027.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在安装中需要注意的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;code&gt;Select Components&lt;/code&gt; 中选择 &lt;code&gt;Git LFS&lt;/code&gt;，按需求安装其他组件。&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;Adjusting the name of the initial branch in new repositories&lt;/code&gt; 中可以选择 &lt;code&gt;Override the default branch name for new repositories&lt;/code&gt; 并将主分支命名为 &lt;code&gt;main&lt;/code&gt;（貌似是因为原默认名称 &lt;code&gt;master&lt;/code&gt; 涉及种族歧视，如今 Github 默认分支为 &lt;code&gt;main&lt;/code&gt;，最好保持一致）。&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;Adjusting your PATH environment&lt;/code&gt; 中选择 &lt;code&gt;Recommended&lt;/code&gt; 的选项。&lt;/li&gt;
&lt;li&gt;其他内容选择默认选项即可，或者在互联网进行查询。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;安装之后首先设置 Git 的基本信息：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git config --global user.name &quot;Your Name&quot;
git config --global user.email &quot;Your Email&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后需要配置 SSH，首先需要检查是否已经存在 SSH 密钥，如果存在则跳过此步骤，否则需要进行创建：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ssh-keygen -t ed25519 -C &quot;Your Email&quot;
# 或者使用 ssh-keygen -t rsa -C &quot;Your Email&quot;
cat ~/.ssh/id_ed25519.pub
# cat ~/.ssh/id_rsa.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将生成的密钥复制到 Github 中的 &lt;code&gt;Settings&lt;/code&gt; 中的 &lt;code&gt;SSH and GPG keys&lt;/code&gt; 中的 &lt;code&gt;New SSH key&lt;/code&gt; 并粘贴。&lt;/p&gt;
&lt;p&gt;此时本地理论上已经可以进行 Git 的 push 等操作到远程储存库了。&lt;/p&gt;
&lt;h3&gt;Fork 本仓库&lt;/h3&gt;
&lt;p&gt;进入本仓库的 &lt;a href=&quot;https://github.com/SurviveXJTU/SurviveXJTU.github.io&quot;&gt;Github 主页&lt;/a&gt;，点击 &lt;code&gt;Fork&lt;/code&gt; 按钮，即可将本仓库 Fork 到自己的 Github 账号下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/fork_1.1e8ja4df0x.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/fork_2.5mnqjy3nte.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Fork 操作本质上是复制了一份本仓库到自己的账号下，并在自己的账号下享有修改的权限，同时可以比较自己账号下的仓库与上游仓库之间的更改差别，Fork后的仓库可以在自己账号的 Repositories 中看到。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/find_fork.45m3svfps.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3&gt;克隆仓库&lt;/h3&gt;
&lt;p&gt;在 Fork 完成之后，在自己 Fork 的仓库中，找到并点击 &lt;code&gt;&amp;#x3C; &gt; Code&lt;/code&gt; 按钮，之后点击 &lt;code&gt;SSH&lt;/code&gt; 按钮，并复制链接。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/gain_ssh.m0632d03.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在本地找到适合保存本项目的文件夹，右键资源管理器，点击 &lt;code&gt;在终端中打开&lt;/code&gt;，并进行克隆操作。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone your_ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;The authenticity of host &apos;github.com (xxx.xxx.xxx.xxx)&apos; can&apos;t be established.
xxxxxxx key fingerprint is sHA256:xxx.
This key is not known by any other names.
Are you sure you want continue connecting(yes/no/[fingerprint])?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;需要明确输入 &lt;code&gt;yes&lt;/code&gt; 并回车，否则无法正确建立连接。&lt;/p&gt;
&lt;h3&gt;仓库文件结构&lt;/h3&gt;
&lt;p&gt;在克隆完成之后，可以使用 &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;VSCode&lt;/a&gt; 等编辑器打开文件夹并进行编辑，其中首先需要了解的是文件的结构：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;├───.github
├───docs
│   ├───.vitepress
│   │   ├───cache
│   │   └───theme
│   ├───images
│   ├───public
│   ├───folders
│   └───index.md
├───node_modules
├───.gitignore
├───package.json
├───pnpm-lock.yaml
└───tsconfig.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中例如 &lt;code&gt;.gitignore&lt;/code&gt;, &lt;code&gt;package.json&lt;/code&gt;, &lt;code&gt;pnpm-lock.yaml&lt;/code&gt;, &lt;code&gt;tsconfig.json&lt;/code&gt; 均为 Git 以及 Node.js 的相关配置文件，无需过于调整。&lt;code&gt;docs&lt;/code&gt; 文档中包含 &lt;code&gt;.vitepress&lt;/code&gt; 内容，此为 VitePress 的配置文件所在的文件夹，而其他的文件夹则按照文档的组织进行排序，其中本项目中全部的图片均维护在 images 文件夹中，而 logo 等资源则维护在 public 文件夹中。&lt;/p&gt;
&lt;h3&gt;撰写文档&lt;/h3&gt;
&lt;p&gt;在了解了文件结构之后便可以开始撰写文档了，确认自己想要撰写的文档所隶属于的类别，并进入该文件夹，新建一个 Markdown 文档，按照 Markdown 文档的语法进行撰写。&lt;/p&gt;
&lt;p&gt;与此同时值得注意的是，VitePress 支持部分的 Markdown 拓展语法，这些内容可以在 &lt;a href=&quot;https://vitepress.dev/zh/guide/markdown&quot;&gt;官方文档&lt;/a&gt; 中查阅。&lt;/p&gt;
&lt;p&gt;撰写文档之后进行保存即可。在这里需要注明的是，在 VitePress 中使用图片的插入，所使用的相对路径是相对于 Markdown 文档本身的相对路径，而非相对于项目根目录的相对路径。&lt;/p&gt;
&lt;h3&gt;修改 Sidebar&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;SurviveXJTU&lt;/code&gt; 的侧边栏使用人为的创建形式，这是为了更大限度的排版布局自由度，有的时候不同章节之间的内容，在写作的过程中存在顺序之分，而使用如 &lt;code&gt;vitepress-sidebar&lt;/code&gt; 等插件自动生成 &lt;code&gt;Sidebar&lt;/code&gt; 虽然快捷，但是很可能导致内容按照如字典序等方式进行排序，从而无法更好的符合写作者的意愿。&lt;/p&gt;
&lt;p&gt;前往 &lt;code&gt;docs/.vitepress/config.mts&lt;/code&gt; 中，可以在找到如下文所示内容，以下以其中的人生篇为例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-ts&quot;&gt;export default defineConfig({
    ...
    themeConfig:{
        sidebar: [
            {
                text: &apos;人生篇&apos;,
                link: &apos;/人生篇/&apos;,
                collapsed: true,
                items: [ // [!code focus]
                    ... // [!code focus]
                    { text: &apos;关于西交&apos;, link: &apos;/人生篇/关于西交&apos; }, // [!code focus]
                    { text: &apos;开源精神&apos;, link: &apos;/人生篇/开源精神&apos; }, // [!code focus]
                ] // [!code focus]
            }
        ]
    }

})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在其中找到你想要插入的位置，VitePress 会根据 items 中的顺序来排列 Sidebar，例如贡献者创建了文档 &lt;code&gt;人生思考&lt;/code&gt;，并认为在排版布局中应位于 &lt;code&gt;关于西交&lt;/code&gt; 与 &lt;code&gt;开源精神&lt;/code&gt; 之间，则加入一行即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-ts&quot;&gt;export default defineConfig({
    ...
    themeConfig:{
        sidebar: [
            {
                text: &apos;人生篇&apos;,
                link: &apos;/人生篇/&apos;,
                collapsed: true,
                items: [ // [!code focus]
                    ... // [!code focus]
                    { text: &apos;关于西交&apos;, link: &apos;/人生篇/关于西交&apos; }, // [!code focus]
                    { text: &apos;人生思考&apos;, link: &apos;/人生篇/人生思考&apos;} // [!code ++] // [!code focus]
                    { text: &apos;开源精神&apos;, link: &apos;/人生篇/开源精神&apos; }, // [!code focus]
                ] // [!code focus]
            }
        ]
    }

})
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;提交更改&lt;/h3&gt;
&lt;p&gt;在完成了文档的修改之后，可以使用 Git 进行更改的提交：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git pull origin main
git add .
git commit -m &quot;your commit message&quot;
git push origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后可以看到自己的更改已经提交到了自己的 Github 仓库中。&lt;/p&gt;
&lt;h3&gt;发起 Pull Request&lt;/h3&gt;
&lt;p&gt;假如说进行了成功的提交，可以注意到，自己的仓库中应显示如 &lt;code&gt;1 commit ahead of&lt;/code&gt; 的字样。点击 &lt;code&gt;Contribute&lt;/code&gt; 并点击 &lt;code&gt;Open pull request&lt;/code&gt; 即可发起一个 Pull Request，并等待管理员进行审核。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/pr_1.ic1uo3ql1.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;请确保 PR 的 title 中表意明确，同时 description 中清晰描述了自己添加的内容，之后点击 &lt;code&gt;Create pull request&lt;/code&gt; 即可，管理员在收到内容之后会进行审查并给出 &lt;code&gt;comment&lt;/code&gt; 或直接将你的 PR Merge 进主分支，即完成了贡献。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/pr_2.2rv2e5oh1y.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;VitePress 快速建站&lt;/h2&gt;
&lt;p&gt;本文接下来的内容用来讲解如何使用 VitePress 进行快速建站。&lt;/p&gt;
&lt;h3&gt;安装初始化&lt;/h3&gt;
&lt;p&gt;首先需要安装 npm，前往 &lt;a href=&quot;https://nodejs.org/zh-cn&quot;&gt;Node.js 的官网&lt;/a&gt;进行下载，之后按照指示安装即可，结束之后打开一个终端，输入 &lt;code&gt;node -v&lt;/code&gt; 以及 &lt;code&gt;npm -v&lt;/code&gt;，会提供 Node.js 以及 npm 的版本号，说明安装成功。&lt;/p&gt;
&lt;p&gt;接下来转用 pnpm，更加好用的包管理器：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;npm install -g pnpm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后使用 pnpm 安装 VitePress，新建文件夹，在目录下打开终端：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pnpm add -D vitepress
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后使用 VitePress 提供的快速初始化工具：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pnpm vitepress init
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在初始化的过程中，进行以下的选择：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;┌  Welcome to VitePress!
│
◇  Where should VitePress initialize the config?
│  ./docs
│
◇  Site title:
│  My Awesome Project
│
◇  Site description:
│  A VitePress Site
│
◇  Theme:
│  Default Theme + Customization
│
◇  Use TypeScript for config and theme files?
│  Yes
│
◆  Add VitePress npm scripts to package.json?
│  Yes
└
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后执行 &lt;code&gt;pnpm run docs:dev&lt;/code&gt; 即可在本地启动 VitePress 并进行预览。&lt;/p&gt;
&lt;h3&gt;Github 部署&lt;/h3&gt;
&lt;p&gt;在本地预览没有问题之后，就可以进行 Github 部署了，首先需要新建一个仓库，例如 &lt;code&gt;Example&lt;/code&gt;，然后在 &lt;code&gt;docs/.vitepress/config.mts&lt;/code&gt; 中添加如下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-ts&quot;&gt;export default defineConfig({
    ...,
    base: &apos;/Example/&apos; // 若仓库为 username.github.io，则 base 为 /
})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;与仓库建立链接（详细方法见本人 &lt;a href=&quot;https://axi404.github.io/Blog/p/git-%E7%9A%84%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/#%E5%85%B3%E8%81%94%E6%96%B0%E5%BB%BA%E4%BB%93%E5%BA%93&quot;&gt;关于 Git 的博客&lt;/a&gt;）之后，在根目录下创建一个 &lt;code&gt;.github/workflows/deploy.yml&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yml&quot;&gt;# 构建 VitePress 站点并将其部署到 GitHub Pages 的示例工作流程
#
name: Deploy VitePress site to Pages

on:
  # 在针对 `main` 分支的推送上运行。如果你
  # 使用 `master` 分支作为默认分支，请将其更改为 `master`
  push:
    branches: [main]

  # 允许你从 Actions 选项卡手动运行此工作流程
  workflow_dispatch:

# 设置 GITHUB_TOKEN 的权限，以允许部署到 GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# 只允许同时进行一次部署，跳过正在运行和最新队列之间的运行队列
# 但是，不要取消正在进行的运行，因为我们希望允许这些生产部署完成
concurrency:
  group: pages
  cancel-in-progress: false

jobs:
  # 构建工作
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # 如果未启用 lastUpdated，则不需要
      - uses: pnpm/action-setup@v4
        with:
          version: latest
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: pnpm # 或 pnpm / yarn
      - name: Setup Pages
        uses: actions/configure-pages@v4
      - name: Install dependencies
        run: pnpm install # 或 pnpm install / yarn install / bun install
      - name: Build with VitePress
        run: pnpm docs:build # 或 pnpm docs:build / yarn docs:build / bun run docs:build
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs/.vitepress/dist

  # 部署工作
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    needs: build
    runs-on: ubuntu-latest
    name: Deploy
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 Github 仓库中，找到 &lt;code&gt;Settings&lt;/code&gt; -&gt; &lt;code&gt;Pages&lt;/code&gt; -&gt; &lt;code&gt;Build and deployment&lt;/code&gt; -&gt; &lt;code&gt;Source&lt;/code&gt;，选择 &lt;code&gt;Github Actions&lt;/code&gt;，之后进行：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git add .
git commit -m &quot;Initial Commit&quot;
git push origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;稍等片刻之后即可看到部署成功。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;限于篇幅以及内容的设计，本篇内容可能暂时截止于此，更多的内容会在后续选择性地在此处更新，或者新建新的博客进行分享，希望本博客可以帮助读者更好的了解 VitePress 的写作流程，并为相关的开源项目做出更多的贡献。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/VitePress-Tutorial-zh.webp"/><enclosure url="https://picr2.axi404.top/VitePress-Tutorial-zh.webp"/></item><item><title>校园 VPN 连接实录</title><link>https://axi404.github.io/blog/vpn-setting</link><guid isPermaLink="true">https://axi404.github.io/blog/vpn-setting</guid><description>EasyConnect + SSH 校外链接服务器。</description><pubDate>Fri, 16 Aug 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;简单来说，这是一篇成分复杂的文章，阅读到这篇文章的读者，多半并不符合这篇文章所属的条件。简单来说，需要是，你来自西安交大 + 你在校外出差 + 你在校内有跳板机 + 你需要使用校内服务器跑程序，不知道会不会对一些人有帮助。&lt;/p&gt;
&lt;p&gt;本人电脑小白一枚，所以使用的方法极有可能绕了远路，而且我没有 sftp 需求，也就懒得研究更加优雅的方式了，欢迎大家在下方评论进行补充。&lt;/p&gt;
&lt;h2&gt;下载 Easy Connect&lt;/h2&gt;
&lt;p&gt;学校的 SSH 分为两种，一种是 WebVPN，只能活在浏览器里面，本质上是在一个浏览器里面套了个壳子，在里面访问校内网；另一种是 SSLVPN，通过 SSL/TLS 访问内部资源的方法。&lt;/p&gt;
&lt;p&gt;使用 SSLVPN，首先需要前往学校的官网，下载一种叫做 EasyConnect 的玩意，之后打开软件，会卡在一个获取登录配置的地方，在浏览器中进入 &lt;a href=&quot;https://sslvpn.xjtu.edu.cn&quot;&gt;sslvpn&lt;/a&gt; 的官网，然后在学校账号认证界面登录，就可以成功进入某种内网了，此时可以连接跳板机了。&lt;/p&gt;
&lt;h2&gt;SSH&lt;/h2&gt;
&lt;p&gt;之后就可以进行正常的 SSH 了，在这里因为是使用跳板机，对于 &lt;code&gt;C:/Users/user_name/.ssh/config&lt;/code&gt; 进行修改：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;Host *
    ServerAliveInterval 60
Host jump_server
    HostName host_name
    User user_name
    Port port
    IdentityFile C:/Users/34064/.ssh/serect_key
Host j67
    HostName host_name
    User user_name
    Port port
    ProxyJump jump_server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中前一个里面是类似组里的跳板机，于是使用组里面提供的地址以及端口和密钥来登录，之后的是正常的服务器，多了一个 &lt;code&gt;ProxyJump&lt;/code&gt; 来表示使用跳板机。&lt;/p&gt;
&lt;p&gt;之后使用 &lt;code&gt;ssh j67&lt;/code&gt; 就可以登录了。&lt;/p&gt;
&lt;h2&gt;二次 SSH&lt;/h2&gt;
&lt;p&gt;由于跳板机只有特定端口的转发，而组里的跳板机连接的是一个 4*2080Ti 的服务器，我现在有一个能够用单卡 V100 的服务器，所以说要连接别的服务器。&lt;/p&gt;
&lt;p&gt;于是选择了比较愚蠢的方法，因为我当前这个服务器已经在校园网内，约等于我拥有了一个校园网内的终端，那么直接进行二次的 SSH 即可。在这里不得不提到 tmux，确实是十分实用的工具，不仅可以避免自己的程序被没有心跳信号杀死，也可以在一个 SSH 里面多开窗口，可以说十分的方便了。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;感觉自己的这一套流程笨笨的，一套操作猛如虎，最后 SSH 确实很卡，毕竟套了好几层，不知道有没有更好的方法。&lt;/p&gt;
&lt;h2&gt;Updates&lt;/h2&gt;
&lt;p&gt;事实上发现自己可能确实笨完了，按照我们实验室的手册来说，确实是根据上述的流程才没问题的，但是事实上貌似只要开启了 sslvpn 之后就进入了内网。我使用的 4*2080Ti 的服务器是使用跳板机进行转发的（之前我应该也配置过，但是忘记了），然而假如说是正常的服务器，是不需要进入跳板机之后再二次 SSH 的，直接进行进行 SSH 连接即可，注意关闭自己的 VPN 程序即可。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/vpn-setting-zh.webp"/><enclosure url="https://picr2.axi404.top/vpn-setting-zh.webp"/></item><item><title>周记 Week6</title><link>https://axi404.github.io/blog/week-06</link><guid isPermaLink="true">https://axi404.github.io/blog/week-06</guid><description>2024-08-05 ~ 2024-08-11.</description><pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;尽管做过很多的期待，但是第六周的故事实际上确实枯燥无趣，几乎没什么可说的，不过为了记录生活，还是勉强记录一下吧。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;h3&gt;RoboMaster&lt;/h3&gt;
&lt;p&gt;RM 一直打到周六，最后也算是尘埃落定，虽然没有晋级八强，但是在十六强的时候也赢了一局，也算是破除了之前的魔咒了，姑且也算是队史的最好成绩了。&lt;/p&gt;
&lt;p&gt;可以说的是，这一周以来实际上我做的事情不算很多，一方面算法的稳定性确实很不错，另一方面英雄的主要任务确实也不是打前哨站。我在视觉组的组员兼同班同学 LXW 确实在自瞄方面做得很不错，无论是哨兵还是无人机，都是可以推掉前哨站的，而英雄又在吊射方面如此出色，前哨站的任务也就不主要交给英雄了。不过到了后面，自瞄体系的不稳定还是时常发生（指体系，如机械电控等问题导致视觉算法的效果不好，也很有可能），而且也伴随着战术的调整，英雄的反前哨站也就再次被提上日程了。&lt;/p&gt;
&lt;p&gt;姑且也算是一种释然吧，最后自己写的东西还算是派上了用场，虽然说最后输了，但是前一天晚上依然查出来电控那边的一个奇怪问题，也算是为后面做了一点技术积累。&lt;/p&gt;
&lt;p&gt;其实我也不是很确定自己是不是要继续打下去，但是既然自己已经是组长了，假如说没有人打算接任这个位置的话，大概率还是我继续顶上去。&lt;/p&gt;
&lt;p&gt;一些反思也会后续写在 RM 回忆录里面，但是在这里也姑且说一下。事实上今年的视觉组安排的不是特别好，一开始我的想法是，视觉组的任务太无聊了，我希望大家做一些更有意思的东西，甚至说讨论一下学术也是可以的，但是一方面确实任务还是很紧，而且在西交这样一个课内压力大的环境下，本科科研就已经很过分了，更别说在这样一个社团里面，大家聚在一起做一些讨论。&lt;/p&gt;
&lt;p&gt;本来我一直想的是，将自瞄这件事情基本上都由我负责，然后我拉进来的同学 LXW 负责协助，这样子两个人压力可以少很多。然而事实上因为上半年深度参与科研的原因，自瞄这部分很显然又出现了去年的情况，LXW 一个人做得太多，导致别人已经无法跟上他在做的东西了（这并不是指算法的困难，工程代码我从未要求过规范性，现在基本上还是以能跑通优先，所以每个人的风格不一样，也就很难看懂），这下压力就到了他那边，而我也就换到英雄反前哨站的工作了。&lt;/p&gt;
&lt;p&gt;但是说到底，这一年来我的工作其实还差不多，假如继续连任的话，一方面大三下确实磨不开时间，包括实习的一系列事情，会让我难以参加国赛，然而另一方面，我的重心可能会放在对于代码规范的整理以及文档的编辑中，之前的程序实在是太过于野蛮生长，急需一次收束。从这个赛季的结果上来看，可以说的是，视觉组的全部任务基本都完成了，后续的完善固然可以让这些老队员继续负责，然而我想将之转化为更加规范的工作。现在存在的一些缺陷，其中有部分，我依然有必要怀疑电控的水平（并非专业素养，而是代码中确实存在 corner case），这些都需要后面缓慢的对齐。&lt;/p&gt;
&lt;p&gt;如何如何，反正一年的劳作，终于也算是尘埃落定了，用心写的算法拿到了对应的结果，也算是给了那些新队员一个交代。&lt;/p&gt;
&lt;h3&gt;签证&lt;/h3&gt;
&lt;p&gt;这个星期另一件在忙的事情就是签证，因为这个事情我确实没有自己做过，尽管现在是和两个师兄一起进行团体签，但是心里确实还是没底，截止到周日晚上，材料也算是准备好了，就是不知道后续具体是不是还会少东西，这里还是要打一个问号。&lt;/p&gt;
&lt;p&gt;假如一切顺利的话，我后续应该也会单独写一篇文章来记录一下，对于本校本专业的学生应该是有帮助的，毕竟给我的感触，难点之一固然是材料的繁琐，但是另一大难题，还是在于学院的老师踢皮球，让我无从下手。&lt;/p&gt;
&lt;h3&gt;科研&lt;/h3&gt;
&lt;p&gt;这周的 offer 也算是正式发下来了，把一些资料填写了一下，之后也就进了浦江实验室的 oa 流程，车票订的是下个星期二，也就是签证的下一天，然后星期三到，计划是和绿群的群友晚上一起吃个饭（我在考虑是否要赠送女装照作为见面礼），然后第二天去入职。不过需要在这里提醒我自己的是，入职需要的材料最好别忘了，尽量在西安的时候处理完，学校里的打印机还算比较多，不然去了上海之后就麻烦了。&lt;/p&gt;
&lt;p&gt;因此，顺便加了老师的飞书，老师也给我分享了文档，让我去读一些文章，基本上来看，老师就是 Yilun Chen 了，老师本身的实力也很强，也算是希望能做一些有影响力的工作吧。不过老师竟然说要讨论一下，可能我还是要看完这些论文，我论文的积累量确实不多，现在还剩下八篇，但是基本上已经轻车熟路了，希望可以按时完成。&lt;/p&gt;
&lt;p&gt;在 Sanping Zhou 那边的课题来说，计划是投 ICLR，不过我目前发现了模型的一些问题，可能基础的代码还要再跑一跑，来看看最后的效果，希望没问题。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-06-zh.webp"/><enclosure url="https://picr2.axi404.top/week-06-zh.webp"/></item><item><title>RoboMaster 回忆录</title><link>https://axi404.github.io/blog/rm-memoir</link><guid isPermaLink="true">https://axi404.github.io/blog/rm-memoir</guid><description>我的 RM 故事。</description><pubDate>Sat, 03 Aug 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import { Aside } from &apos;../../../components/user&apos;&lt;/p&gt;
&lt;p&gt;跟着队伍去深圳打 RoboMaster，本来是想好好静下心来读读论文，奈何着天气太燥热潮湿，让我心头也浮躁，总想写点什么东西，于是想起来之前说的，正好现在时间也合适，不如写一篇 RoboMaster 回忆录，记录一下，这贯穿我人生五年的比赛。&lt;/p&gt;
&lt;h2&gt;高中概况&lt;/h2&gt;
&lt;p&gt;说起来是比较抽象的，大多数人参加 RoboMaster，还是只有一年或者两年，自然也不难理解，大一或者大二加入，打到大三大四，最近可能会有四年的，但本身大一的梯队留下来的就不多，而直接成为正式队员又太难，所以还是以两三年为主。&lt;/p&gt;
&lt;p&gt;那假如说广义的 RoboMaster，那么我已经从高一就开始接触了，大概是由于什么北京的所谓素质教育，而且高中里面就存在着一些科技向的社团，有着很优秀的老师，也让我有了接触这些机器人比赛的途径。不过说到头，这些比赛真的是素质教育吗？我看也未必。&lt;/p&gt;
&lt;p&gt;感觉到了最后，就算我拿了省冠军，倒也没有什么作用，毕竟确实技术难度很低，远远远远低于奥赛一类的竞赛，而又需要投入成本，大多数学校也负担不起，不够亲民。&lt;/p&gt;
&lt;p&gt;所以说到底，机器人比赛还是一些富裕地区的富裕学校小打小闹的产物，让学生们体验一下一些创造感，虽然貌似这些技能在大学中的同样类型的比赛中还能起到作用，然而个人的感觉是，相较于大学中的那些专业竞赛，此类比赛依然算是小打小闹。&lt;/p&gt;
&lt;p&gt;高中的时候我还觉得我自己多少有些天赋，自认为是要裸分上清北的，平时和那一帮学霸一类有说有笑，虽然成绩不是最顶尖那一批，但是课余时间的松弛感还是和他们对齐了。我和我的胞弟是同等岁数，在大学期间加入的社团也并非机器人社，而是自己出钱捣鼓一些桌游，创办了一个桌游社。说起来这个社团现在应该依然存在，我们当初留下来的桌游估摸着价格也有大几百元，对于一个月生活费只有五十元的高中生来说，可以说是一笔慷慨捐赠了。&lt;/p&gt;
&lt;p&gt;我所在的高中有一种实验班，叫做项目实验班，其实有点类似于培养学生的科创能力那种感觉，而是事实上，聚集的就是那一帮想考理科实验班，但没进去的学生。&lt;/p&gt;
&lt;p&gt;我记忆里面真正有项目能力的同学，可能不超过五个，其他的人基本还是经典的做题家，到了毕业，说不定也不会工程制图，代码也写不出几行，当时还没有 GPT，那大伙的水平还要再降一等。&lt;/p&gt;
&lt;p&gt;项目实验班有一系列的创客课程，类似于物化生信，选择一个感兴趣的方向，几个同学弄一个小的发明创造，当然也不全是创造，基本就是在网上已经有的东西，大家用一个学期跑一遍流程。&lt;/p&gt;
&lt;p&gt;别的不说，我从小应该还算对电脑接触得很多，不是那种单纯的手机用户。在这里并没有鄙视的意思，然而个人感觉，在同时具有使用电脑和手机的条件下，基本只使用手机，而从未接触过电脑，这种人到了大学里面，能力都很差。不少人是本来电脑接触的就不多，开始接触了之后也就了解了其中魅力，更是变成了很厉害的水平，这种自然不在上述范围之内。&lt;/p&gt;
&lt;p&gt;我之前也对电脑感兴趣，加上网瘾少年的游戏制作梦想，所以说就选择了机器人的项目，然后用 Arduino 写写东西。基本的 Arduino 确实没什么好说的，就是很基础的技术，随便来个 GPT 都能写出来比我当时好的程序，然而正是那时候，认识了机器人社团里的两位老师，Q 老师和 S 老师。&lt;/p&gt;
&lt;p&gt;Q 老师貌似是北大计算机毕业，很是厉害，管理机器人社和天文社两个社团，平时自己还搞搞书籍翻译，感觉有点像那种已经精神富足了的计算机大佬，而且符合那种搞计算机的刻板印象，幽默风趣，有的时候偏好发滑稽，和学生们打成一片。S 老师我接触的更多，本人非常好说话，非常的和蔼可亲，不过关于老师的其他细节，我倒是不是很清楚，大多数时候和老师聊天，要不然是日常，要不然是比赛相关，聊老师自己几乎没有。&lt;/p&gt;
&lt;p&gt;尤其我这个人比较脸盲，而且其实大多数时候比较内向。我非常乐于助人，而且可以和很多人打成一片，然而事实上，我并不愿意结识新的朋友，或者说这令我恐惧且疲惫。可能只有在新环境中，我才会尝试扩展自己的社交圈，而在此之后，这个圈子多半不会有很大的变动。&lt;/p&gt;
&lt;h2&gt;初识 RM&lt;/h2&gt;
&lt;p&gt;其实本质上，高一的时候我并没有接触 RM，然而和机器人社还有很多的往来，尤其是用机器人社的电脑偷着打游戏，这件事我乐此不疲，而且老师虽然嘴上严厉教育，但实际上管得不严。&lt;/p&gt;
&lt;p&gt;到了高二的时候，有一次我去机器人社，碰到了我的好朋友 CX，CX 是机器人社的主力之类的。我们学校对于那些类似提前批进入学校实验班的同学，会有一个类似于夏令营的东西，他在里面已经提前做了一些机器人相关的内容了，然后在后续，也一直在里面打比赛。&lt;/p&gt;
&lt;p&gt;CX 当时说自己在写 RM 的循线程序，在之前的时候，我就已经知道机器人社搞来了几台 RoboMaster S1，麦轮的机器人可以平移，看上去确实现代感十足。我当时说要一起去看看，所以去了我们的另一个场地，看他写这个程序。&lt;/p&gt;
&lt;p&gt;高中的 RM，只有工程机器人需要自己写程序，而且写的也不算多，其他的机器人基本上就是图形化编程，我看那些说明也不算难，提供的接口都很简单，所以就干脆写了写，出现了第一版的跑图程序。&lt;/p&gt;
&lt;p&gt;随后我就开始了我的 RM 生涯，有点像是说之前的一些同学似乎不打了，反正我可以写步兵的程序，同时作为步兵的操作手。&lt;/p&gt;
&lt;p&gt;像是之前说的一样，RM 的程序不算很难，但是图形化接口也意味着很难去使用自己的一些算法，包括说像是 OpenCV 一类的程序更是别谈。现在回忆起来，当初有一位 ZQ 同学在项目班的另一个项目里，貌似学了 OpenCV，当时我问能不能写一个装甲板检测出来，他展示的那个效果很不错，我还问能不能放到我们的机器上，现在来看，好像只是一个单纯的 threshold，甚至没有 findContours，难以评价。&lt;/p&gt;
&lt;p&gt;甚至说当时 Dji 没有开放装甲板识别的接口，只有数字识别，所以我们也没指望自瞄。当时的任务是击打能量机关和基地，能量机关是从小到大打一到五的数字，Dji 会提供数字识别的接口，返回数字以及图像坐标系坐标；基地的击打则是需要跑图到对面半场，然后自瞄。&lt;/p&gt;
&lt;p&gt;这里面有必要解释一下，RM 高中组的赛制，和大学有一些区别，加上读者可能也不了解 RM，干脆从头解释。&lt;/p&gt;
&lt;p&gt;RM 本质上是一个类 DOTA 游戏，其实说角色变成了纯物理世界的机器人，可以用键鼠控制。高中组有限制对机器人的改装幅度，不像大学组是自己做机器人，我们都是用大疆的 S1 机器人。&lt;/p&gt;
&lt;p&gt;既然是类 DOTA，肯定是涉及击杀以及血量等等的，RM 具有一套所谓的皮肤系统，说白了就是几个装甲板，或者说传感器，有人说是压力传感器，也有人说是声音传感器，具体也不得而知。高中组的机器人分为步兵/工程/无人机，步兵可以发射子弹，是那种水弹，所以打在装甲板上就可以造成伤害；工程有机械臂或者其他的机构，可以获得弹药瓶，步兵机器人的发弹量是被系统限制的，用完了之后只能等待系统发放的低保，或者用相机模块扫描弹药瓶上的标志获得补充，弹药瓶的获得自然有不同难度，区分度在于高度或者位置，其扫描后可以获得的发弹量也自然不同，最多的貌似是三百，可以爽打一局；无人机也是一个大疆提供的无人机，只需要控制它飞到对面基地的特定位置，让对面的摄像头扫到，基地就会破甲。比赛有补给区，补给区里面有一个标志，扫到就可以回血。&lt;/p&gt;
&lt;p&gt;比赛主要分为两个阶段，一阶段是自动的，二阶段是手动的，现在貌似划分更多了，但我只说我当时经历的。基地的护甲默认是五十，假如对面机器人阵亡了就会扣二十，最低是零，但是无人机可以给它扣到负的，一发子弹的伤害是十，护甲与伤害有一种计算关系，假如满护甲，那打一下好像才扣两点血。&lt;/p&gt;
&lt;p&gt;另一个机制是能量机关，可以理解为是一个在场地中心的机构，有五个可击打的传感器，显示数字一到五，需要按顺序打，就可以获得攻击增益 ATK。自动阶段激活是一个永久的 1.5，手动阶段激活是暂时的，但貌似是 2。能量机关有冷却。&lt;/p&gt;
&lt;p&gt;所以说，不难理解的是，自动阶段需要步兵先激活能量机关，再去打基地；工程拿弹药瓶，而且要跑得快一些；无人机不需要做什么。到了手动阶段，大家就开始 FPS，然后攻打对面的基地。&lt;/p&gt;
&lt;h2&gt;冠军之路&lt;/h2&gt;
&lt;p&gt;我加入了 RM 队伍之后，和大家一起努力，程序可以说完成的差不多，不过需要说的是，确实还是接口好用，技术含量不高。&lt;/p&gt;
&lt;p&gt;甚至说因为曝光问题，我们在坡上无法识别对面的基地，所以需要循线准确一些，然后再撞墙矫正位置，之后手动抬到一定高度，完全是开环。&lt;/p&gt;
&lt;p&gt;工程机器人确实十分给力，我们的速度很快，所以说不至于打低保，弹药脸足够的情况下，基本上是稳赢的。&lt;/p&gt;
&lt;p&gt;高中的队伍其实就是打打闹闹，大伙都没什么含金量，不过确实有的学校，本身教育水平可能差点意思，所以打算把这个作为宣传招牌，也是我们当时的劲敌，民大附，不过到最后这个队伍有一些唐，到时候再说。&lt;/p&gt;
&lt;p&gt;当时我其实发现了一个问题，这些队伍普遍操作能力不太强，RM 的机器人，毕竟是物理控制，操作有一些粘滞感，一般人很难适应，基本上不能跑打，打移动的目标也不太行，尤其是在没有自瞄的时候（我们当时没人有自瞄）。而我可能说恰好有一些天赋，当时队伍里面的训练，和两个高一的同学加上一个老师打，我可以一个人打三个，而且地图不大，我可以倒着跑全图，一边逃跑一边还手。另一位操作手是 CX 同学，也很厉害，我们属于是强强联合。&lt;/p&gt;
&lt;p&gt;当时我们发现了一个简单清晰的盲点，大家貌似都很笨，在基地护甲没有到 0 的时候就在打基地，两点两点的扣，根本无济于事（基地貌似三千还是多少血量，反正很多），而对战欲望很低，让我一度怀疑我在打 RM 单机版。&lt;/p&gt;
&lt;p&gt;战术的前置是，我们有比较充足的弹量，两个厉害的步兵操作手，这些恰好我们都符合，于是理论如下：我们在自动阶段获得充足弹药，并且获得永久 ATK，基地的击打其实无所谓，在比赛的前期专注 PVP 而非打基地，杀掉对面三辆车，同时无人机破甲，激活临时能量机关。这时候我们一发子弹对基地可以造成 50 点伤害，两辆车一起不到十秒钟就可以结束比赛。&lt;/p&gt;
&lt;p&gt;靠着这套战术，我们很轻易的就在比赛里面披荆斩棘，当然，写的一些程序可能也是有帮助的。&lt;/p&gt;
&lt;p&gt;正像是前面所说的，大多数的队伍的操作手都没有灵性，基本上在站桩，所以就一路杀出重围，到了决赛。决赛的队伍就是之前说的民大附，在这里不得不好好诋毁一下，相关问题也无需抵赖，只要是当时那一届参赛的队员都会有印象。&lt;/p&gt;
&lt;p&gt;民大附这支队伍，我个人感觉是没有什么含金量的，貌似是请了外援还是什么的，写了些程序，我是不知道这些程序有啥必要找外援，然后还找了赞助之类的，最后实际上比赛没什么意思，基本上轻松取胜，主要说一些小插曲。&lt;/p&gt;
&lt;p&gt;当时印象很深刻的是，他们的无限火力机关枪。在 RoboMaster 中对于发射的限制主要分为弹量限制和热量限制。弹量限制很好理解，你只能发射那些你赚到的子弹，更多的不能发；热量则是说，你每段时间只能发一定数量的子弹，这被量化为热量，热量会不断下降，同时开火会热量上升，热量满了就不能开火了。&lt;/p&gt;
&lt;p&gt;抛开没有明确证据的，在和他们对战的过程中，他们貌似就已经开了无限火力模式一样，就算三百的弹药瓶不在他们这边，依然可以全场一直在打，甚至没有热量的感觉；民大附的战队在比赛的自动阶段打出过 1700 伤害，按照理论计算，最高伤害也不到 1500，所以就有点匪夷所思了；之后还有过民大附机器人失控实录，在自动阶段演都不演，直接启动无限火力，无论是射速还是弹量，都比正常的高无数倍，获得「机关枪」美誉，最后紧急暂停。他们自然也有解释，认为是 BUG 之类的，但是这种现象频繁出现在他们身上，而其他队伍从来没出现过，是不是他们自己动的手脚，自然也就不言而喻了。&lt;/p&gt;
&lt;p&gt;同时还有一位仁兄，胖胖的，感觉凶神恶煞。上文说道他们队伍有赞助商，这个赞助并非用来建设队伍，而是打钱之后大家平分，当然，也有一个前提，那就是获得冠军。一共一万多块钱的赞助，每个队员都能分到小几千块，自然是对高中生来说的一笔巨款。然而我们队伍的强势打破了他们这一幻想，基本上我们夺冠是势在必得的，这位仁兄就守在我们队伍旁边聆听我们的战术，不知道是想要帮我们指点一二还是什么。&lt;/p&gt;
&lt;p&gt;在别的队伍的备赛区逗留本身就是违规，我们尝试驱逐无果，他依然硬着脖子说要站在这里，然后又开始打电话，装作放狠话的样子。不知道是不是民大附就是这种职高氛围，还是怎样，反正一股子小混混的味道。随后更是忽然暴起，把我们备场区的一张木头的桌子用手砸裂了。我不太理解这个行为是什么意思，可能类似于混混在街头打架斗殴前，先把玻璃瓶子在自己脑袋上砸一下，显得自己一脸血很勇敢。有一种脑子不好使的感觉，最后搞得满手是血，还溅到我们同学的衣服上了，最后是骨折了还是怎样，他们就弃权比赛，把这位仁兄送到医院了。&lt;/p&gt;
&lt;p&gt;综合来看，几场比赛都没什么悬念，我们就拿下了北京市和华北赛区的双冠军，也没啥难度和压力。之前比赛之前设想过很多的情况，高手如云等，但是事实上到了比赛才发现，原来我们才是那个唯一的高手。&lt;/p&gt;
&lt;p&gt;随后就是备赛国赛，准备了半天，最后因为疫情也成为了线上评分。我们队伍的一大优势在于操作手，最后纸面实力排了一个国家二等奖，倒也说得过去。&lt;/p&gt;
&lt;p&gt;基本上 RM 比赛可以算得上高中最后的疯狂了，紧接着的暑假里面还可以有一些新生培训，算是夏令营，事实上新生的素质也有一些堪忧，机器人队成为了打游戏的地方，后面也就禁止游戏了。&lt;/p&gt;
&lt;p&gt;事实上我们几个老队员也有打游戏，也可以说这种行为，或者说不好的表率是我们先开始的，但是老队员们一是早已经完成了测试，任务都做完了，二是当下确实没什么要紧的事；新的这些同学，倒是觉得机器人社是玩游戏的避风港了，实在是令我头疼不堪，更是有人说出了「要不是因为可以玩游戏，谁来机器人社」这种荒谬的言论。&lt;/p&gt;
&lt;p&gt;第二年 RM，我基本上也就是有时间回来看看，算不上深度参与，因为高考所以也没有时间和他们一起去比赛。据说这一年民大附他们又有技术的提升之类的，我们输了，但是具体我也没有了解太多。&lt;/p&gt;
&lt;p&gt;高中的 RM 就在这种不知不觉之间结束了，唯一留给我的只有两个冠军奖杯，数不尽的回忆，以及当初的热血沸腾。&lt;/p&gt;
&lt;p&gt;在这里再补充一些内容，以免将来忘记，由于我的记录是以事件为主的，所以说对于人的记录其实甚少，甚至说在前面提到的 Q 老师和 S 老师在后面也没有提及，当然很大程度也是因为大多数事情我已经记不清了。&lt;/p&gt;
&lt;p&gt;我们队伍的主力一共有四个人，都是和我一个年级的同学，除了我，CX 同学，还有 HY 同学和 TR 同学，这两位同学共同负责的工程机器人。&lt;/p&gt;
&lt;p&gt;后来加入了大学的 RoboMaster 比赛，有一定原因就是因为 HY 同学是 RMUC 的忠实粉丝，在高中期间就不断地说比赛里的一些情况，也让我对大学组的比赛有了了解，否则我可能连大学组的比赛都不知道。&lt;/p&gt;
&lt;p&gt;除此之外，还有几个高一的同学，名字我也都记着，但是就缩写而言，和后面的一些人产生了冲突，所以就不一一写了，然而依然有必要说的是，这些高一的梯队同学在比赛中也做出了卓越的贡献。因为疫情原因，全国赛改为了线上比赛，此时我们这些主力队员都已经高三了，需要准备高考，而线上比赛又需要录制视频，这些都是他们做出来的。&lt;/p&gt;
&lt;h2&gt;大学概况&lt;/h2&gt;
&lt;p&gt;不少读者应该知道我在大学的情况，但是为了回忆的完整性，还是重新说明一次，把内容进行完全记录。&lt;/p&gt;
&lt;p&gt;大学的时候，我进入了人工智能专业，老实说我之前是想选数学的，可以理解为某些对于自己数学天赋的自信，虽然这种天赋近期貌似遗失了。然后进了人工智能专业之后，我也就开始想要了解这个专业里的东西了，当时我能想到的，无论是和编程还是人工智能，唯一能擦上边的就是 RoboMaster。&lt;/p&gt;
&lt;p&gt;于是我就在专业群里面发问，当时我在专业群里还算活跃，所以大家的回复也很有效，大概的意思就是说，我们专业里面有 RM 视觉组的组长，也是我的学长，比我大两年。&lt;/p&gt;
&lt;p&gt;于是后来我就联系了这位学长，即 JH 学长，然后他带我进了 RM 招新群。&lt;/p&gt;
&lt;p&gt;在大学一开始的时候，事实上我对一些学习之类的事情不是很上心，我的目标就是保研而已，而且是保研本校。我刚开始的时候做过很多心理建设，类似于既来之则安之，因此没有去想再追赶上我那帮高中的同学，当然后来的前进的理由，也已经不是追赶。&lt;/p&gt;
&lt;p&gt;尽管我现在在一些新生指南中说，大家可以在假期把高数/线代/概率论全学完，然而这并非我当初做到的事情，准确的说，我没学概率论。在此基础上更加糟糕的是，我对于编程的学习知之甚少。&lt;/p&gt;
&lt;p&gt;尽管我之前说过我有一定的编程基础，但是事实上，也只是在一些算法竞赛中做过最粗浅的学习，使用的也是 Dev C++，同时做过项目和写过算法题的同学，应该自然有所了解，基本可以说除了他们的语言相同之外，很少可以见到共同点，包括一些 C++的特性，算法中也是少有涉及，而是主要因为 C++ 的速度。&lt;/p&gt;
&lt;h2&gt;视觉梯队&lt;/h2&gt;
&lt;p&gt;我当时基本上也没有做什么准备，因为之前和 JH 学长说的也是，对于大一同学基本上没什么要求，但是可能也无法成为正式队员。当时我没有做过多的了解，记得当时面试的时候，是一个非常简单的问题，如何在一串数组里面找到最大的元素，需要手写代码，可以说十分的水。&lt;/p&gt;
&lt;p&gt;当时在视觉组里面的有四位学长，分别是人工智能专业的 JH/MD/JXY 以及电气的 YZ，都是很厉害的人。&lt;/p&gt;
&lt;p&gt;RM 拥有一套属于自己的培训与筛选的体系，这套体系的根本目的并非培训，而是进行筛选。之前的队伍其实倒也还好，然而最近随着内卷的风气越发严重，而最近我们的成绩很不错，有了一个加智育分的招牌，也就吸引得不少学生呼啸而来。&lt;/p&gt;
&lt;p&gt;老实说我非常痛恨这些人，因为 RM 本来就并非像是腾飞杯一样的水赛，不是说来几个人拿一个实验室里的项目，自己包装包装，就可以混一个奖项加分的。倒也不是说这类为了分数来的人没有水平，只是说他们确实很难坚持下去，可以说混这个字已经刻在了一些人的骨子里。一开始在队伍里的培训的时候，大家也都是知道轻重缓急的，都是一副很卷的样子，后面一旦发现自己进入了梯队，也就全都兴致全无了，又或者呆在主力队员的位置上，但不太做事。&lt;/p&gt;
&lt;p&gt;我大一时候的培训更加偏向于压力，培训的内容和考核的内容基本上关联不大，当时可能说培训的是计算机视觉的理论，但是考核是让你写 C++ 的 OpenCV。&lt;/p&gt;
&lt;p&gt;OpenCV 的本体其实是 C++为主，然而因为 Python 的易用性，导致网上的教程基本都是和 Python 相关，当时 ChatGPT 也还没有横空出世，所以代码基本上都是在没有人教的情况下慢慢摸索。&lt;/p&gt;
&lt;p&gt;其中当时教学中的几个很大的坑，包括说 OpenCV 和 C++的一些配置，以及 ROS 等等，在后面我也都有慢慢去自己学习，当时的任务也算基本完成了，然而由于组长告知大一同学只能是梯队，完成最后的任务没有什么意义，我在简单看出了思路之后也就没有再继续写代码。&lt;/p&gt;
&lt;p&gt;当时的那段时间是疫情期间，基本上也都是网课，所以说对于点名之类的问题，要求也不算很高，加上我更加倾向于自学，那段时间的作息可以说全乱套了。当时我基本上一个循环是六个小时，可能学习四到五个小时，然后剩下的时间睡觉，之后又醒来，接着学习，尽管从那段时间对我后来的提升很明显，包括说对于编程的一些基础/思维方面/计算机视觉的理解。&lt;/p&gt;
&lt;p&gt;大概也就是在国庆结束之后，我们刚刚结束了培训，准备公布正式队员的名额，此时 JH 学长和 MD 学长忽然就离队了。由于本人并不是非常热衷于社交，而且也并不是十分八卦，这其中一大部分的原因来自于本人的脸盲，剩余的可能是懒惰。因此假如读者想在本篇回忆中，找到一些关于队伍历史上的秘密，那么可能就失望了。事实上关于这两位学长离队的原因，我也没有太多打听，有人说是因为加分，有人说是因为压力，具体我也不得而知。&lt;/p&gt;
&lt;h2&gt;正式队员&lt;/h2&gt;
&lt;p&gt;由于两位学长的离队，我也从之前的只能成为梯队，变成了一名正式队员，所以接下来理论来说就是完成剩余的任务，这里面我首先负责的是相机取流的任务。&lt;/p&gt;
&lt;p&gt;我从头去阅读海康相机的 sdk，然后去写取流程序，当时的疫情已经越来越严重，基本上能够开展工作的时间不算很多，大多数时候我就呆在地下室（也就是我们社团的活动场地），然后看一些代码，顺便学习一些课内知识。&lt;/p&gt;
&lt;p&gt;后来因为更多的疫情，期末考试也取消了，基本上整个学校全部封禁，本来我们说 RM 要办冬训，这也算是一个传统，为了下半年的正式比赛打一下基础，把大多数的事情都做好，但是也被迫转到线上了。&lt;/p&gt;
&lt;p&gt;现在还有印象的是，当时听说了地下室也要封，我们的能量机关被放在另一个地方，当时我和视觉组的组长 YZ 学长骑车去一个挺远的楼里面，用相机拍视频，这样用来到时候在家里进行调试。虽然貌似最后，因为能量机关的改版，以及楼道里的反光太过于严重，所以说这些视频并没有很派上用场。&lt;/p&gt;
&lt;p&gt;之前我的任务是负责相机的取流，直到后来，在冬训的时候，开始和 YT 一起负责能量机关的识别和预测，我负责识别，预测则由 YT 负责。YT 适合我同年加入队伍的大二学长，比我大一个年级，预测当时主要就是通过拟合，然后弹道模型算出来飞行时间，去求出来云台需要转动的角度。比较朴素的预测是通过迭代法进行的，我们当时建立了一个模型，发现这个最终的角度虽然是超越的，但是是存在一个方程的，所以后面直接可以用牛顿法进行解决，然后求出来 pitch 和 yaw。&lt;/p&gt;
&lt;p&gt;寒假的时候我主要在读之前的能量机关代码，之前的代码主要使用了 ROS 框架，但是讲实话并不好评价，因为这套框架对于 ROS 的运用仅限于把串口通信和运算分成了两个线程，我们后面觉得在这里使用 ROS 是完全浪费的，而且也会被迫要求新人也重新学习 ROS，这是巨大的教学开销，也没什么必要。&lt;/p&gt;
&lt;p&gt;大多数的代码有很多的嵌套，而且有一些算法，如今来看，可能确实十分的精妙，但是有的耗时太多，有的效果不太明显，而且并非 clean code。&lt;/p&gt;
&lt;p&gt;当时我把能量机关识别的流程完整的写了出来，然后一个一个梳理，写了一套新的比较简单的流程，跑起来也没有什么问题，可以说十分的流畅，而且也没有误识别的现象；在培训的时候也有这个识别任务，当时我用了漫水处理，这并非一个主流的写法，但我也把它作为一个方法封装进去了。&lt;/p&gt;
&lt;p&gt;当时我们商量的是，因为自瞄还在老代码的框架里面，但是我已经新写了一套取流+能量机关的框架，把两个融合在一起并不简单，而且我当时也不是很会 ROS，所以说留一辆车给新代码，其他的都是老代码的。&lt;/p&gt;
&lt;p&gt;自瞄当时还处于需要检测装甲板的这个阶段，最大的难度其实是灯条匹配，车辆的每一块装甲板都有两个灯条，怎么确定这两个灯条是属于同一块板子还是两块，这是一个难题。通过几何上的特征，可以保证大多数情况不出错，然而这并非全部，后面有不少的队伍出了一些异形车，使得这种朴素方法的误识别率更高了。&lt;/p&gt;
&lt;p&gt;当时已经把能量机关做的差不多了，所以想着做一下自瞄，一开始是 yolo 识别一个 bbox，然后加一个灯条匹配，后面看到了沈航的开源，他们做了一个四点回归，直接求了装甲板的四个顶点，我的手比较快，直接把这套流程缝合到了新框架里面。&lt;/p&gt;
&lt;p&gt;记忆中比较深刻的是当时的一次交流赛，是西安联盟里面的几个学校一起打，当时队长的意思是需要稳定性，所以说不允许换新代码，我偷偷在一个车上面放了新代码，可以说是效果拔群，事后队长来找我说，这个代码效果确实好，把每个车上都放上这个代码吧。&lt;/p&gt;
&lt;p&gt;还有其他的事情是，能量机关也换了样子，所以说老的传统视觉不好用了，当时已经有了训练神经网络的技术，其实说是训练，主要是包括标注在内的一套框架，以及最后部署的代码，所以说我们对能量机关也做了一个神经网络去识别，同时我也写了一套新的视觉逻辑，也没什么问题。&lt;/p&gt;
&lt;p&gt;后面就是联盟赛，联盟赛办在本校，肯定是要打出气势的。当时队伍里面除了组长 YZ，还有我、YT、同样是新人的 SY 以及 JXY 学长。JXY 学长招来了 ZH，他们两个人一起做了这个赛季的另一个重大项目，哨兵的 SLAM 和导航。&lt;/p&gt;
&lt;p&gt;我们在联盟赛打的还算不错，最后拿了冠军，其中很大一部分原因就是哨兵出力了，同时当时的操作手也很有水平。那时候我的课余精力还很充沛，所以也报名了操作手选拔，当时说的是大家一起打一打，然后看看水平，具体叫谁去打会在群里说，结果我就被叫了两次，一共才打了两局，其他人貌似就已经十多局了，后面操作手出来了，我甚至都没有第一时间知道。再之后有人说，压根就不打算让视觉组当操作手，这句话是不是真的我就不得而知了。&lt;/p&gt;
&lt;p&gt;再之后，组长 YZ 和其他的学长们，因为都已经大四了，所以开始要准备毕设，我是其他人里面代码写的最多的，所以说隐隐约约间有一种要成为组长的架势，也开始负责一些东西。&lt;/p&gt;
&lt;p&gt;值得一提的是，当时的视觉圈子里面出了一个很厉害的开源，忘了是哪个大学了，作者叫陈君，所以大家称之为君瞄，用的也是 ROS 框架。我们的代码后面把他们的一些程序解耦了，然后做了修改，放进了自己的框架里面，我这个人特别手快，对这种解耦的事情非常擅长，后续调了调也就没啥大问题，现在我们的程序基本上沿用的还是这一套框架。&lt;/p&gt;
&lt;p&gt;虽然说君瞄确实很不错，但是整体的氛围却让我感到非常反感，有点造神的感觉，当时陈君把他们录制的视频放到网上，大家都很吹捧，但是好像也没什么人在实战中打出来这个效果。&lt;/p&gt;
&lt;p&gt;后来作者说是因为和人吵架，被人嘲讽了，所以把自己的程序删库跑路了，但是据传说，在此之前他用这个几百个 star 的 repo 拿到了大疆的 offer，所以是懒得搭理开源社区，还是真的恼羞成怒，还是要画一个问号的。&lt;/p&gt;
&lt;p&gt;当时分区赛，陈君还和我聊过，拿着他们那个库的贴纸，问我要不要，然后就像是在推销一样，我说不用了，他依然说个不停。令我印象很深刻的是道具训练，这个环节大概就是每一个队伍调试一下能量机关以及飞坡之类的，他们队伍能量机关不行，就在场地里面调自瞄。这种行为其实有点行为艺术，因为自瞄完全在任何地方都可以，这个地方光线也不一定正确，完全就有种秀肌肉的感觉，事实上我印象里命中率也不高，车离的还很近。&lt;/p&gt;
&lt;p&gt;有读者可能好奇，我们既然拿了他们很多程序，为什么我还对他如此诋毁，岂不是吃饱了骂厨子，这还要从比赛本身开始说起。&lt;/p&gt;
&lt;p&gt;事实上，就像是我之前说的一样，我对于陈君的反感主要来自于造神以及饭圈的氛围。我们小组赛和他们分散了一个组，当时我们在他的算法上做了不少改进，自然要碰一碰，当时我们打他们，打到了一比零，我们落后，因为裁判系统的故障，比赛暂时暂停。那时候没什么人看好我们，毕竟君瞄威名远扬，这倒也可以理解，更何况他们已经先下一城。然而陈君在视觉群里面直接说，假如他把西交打败了，他就去无偿给每一个队伍调车，群里立刻席卷了一片西交必输的恶毒言论。&lt;/p&gt;
&lt;p&gt;当时我印象很深刻的深圳大学依然支持我们，我们现在和深圳大学关系也很不错，在分区赛也是和他们一个场地。这个事情也就导致我们之间确实结下了梁子，不过因为抽签的运气好（说起来，当时还是队长和我两个人去抽的签），我们在一比一平了之后，小组第二出线，反而战胜了劲敌晋级八强以及四强，遇到的都是状态不太好的队伍，而陈君则止步十六强，去打复活赛了。&lt;/p&gt;
&lt;p&gt;另一件印象深刻的事情是哨兵，当时两位学长都来不了现场，而队长在申请建图的时候，因为疏忽没有通过申请，我当时负责哨兵的维护，也是出了不少的状况，最后基本上哨兵是通过在家里的巡逻获得了一点点的贡献。&lt;/p&gt;
&lt;p&gt;后面到了国赛的时候，视觉组这边一直负责雷达的 LXY 接管了哨兵的工作，但令我印象深刻的是他把工控机一拿到手之后就让我做了格式化，出于谨慎，我对工控机中的内容进行了备份，不然估计程序都要消失了。&lt;/p&gt;
&lt;p&gt;事实上，在第一个赛季的时候，视觉组主要起到作用的还是能量机关的激活，而且因为各种各样的原因，最后激活的效果其实不尽如人意，自瞄因为只能识别不能预测，或者说没有调过预测，所以说不能打高速旋转的小陀螺，在实际的赛场中作用不是很大，主要是一个辅助。&lt;/p&gt;
&lt;p&gt;当然，我们做的工作还是很多的，我们完全地重新写了一套框架，包括说串口的通信/相机取流/识别和预测/能量机关，在这里面值得一提的是串口的一个奇怪的 bug。&lt;/p&gt;
&lt;p&gt;很久以前，我们的通信就已经可以使用了，但是事实上还有不小的问题。因为 Linux 系统的特性，串口的通信本质上就是对文件的读和写，然而在细节上来说，还有不少的内容需要设置，我们使用了网上的开源程序，封装之后放到了我们自己的框架里面，但是出现了一个很奇怪的问题。我们的通信协议的长度是 64，也就是说，只有在我们接到一段长度是 64 的内容之后，我们才会对其进行解码，然而事实上我们经常收到长度为 63 的内容。&lt;/p&gt;
&lt;p&gt;这个问题我们想过很多方法解决，包括说是串口线的问题/串口的问题，甚至到了最后，我们感觉将线稍微弯曲一下，就会持续地发出 63，而将其恢复，通信就正常了。这种感觉就好像我们将那一个字节捏在了手里一样，让我们百思不得其解。最后是视觉组的 YT 发现了问题所在，在看了几篇博客之后，修改了代码，这时候我们才知道，原来是因为串口的收发会将回车不认为是字符而是真回车，导致这个字符不会被记录，从而少了一个字节。&lt;/p&gt;
&lt;p&gt;到了国赛的时候是去深圳，在这里顺便说一下两个城市的住宿条件。我们出去比赛主要是学校出钱，或者使用社团的经费，当然我们每个人也都垫付了一些，其中甚至有的没要回来，这是后话。为了省钱，我们每次出行肯定都不是那种豪华酒店，往返一般是硬卧，酒店也很难安排到每人一张单人床，可能要很多人挤在一起。&lt;/p&gt;
&lt;p&gt;长沙的条件一直都很好，我们住的是 LOFT，在市区里面，旁边也有吃饭的地方，车辆调试一般在晚上进行，我们会租一个篮球场，也在住的地方不远处；深圳在当时则是一个酒店，因为长沙的时候我已经实在无法忍受住宿，所以干脆特立独行，让家里人给我单开了一间房，酒店的条件也很舒适，市区里面，旁边还有商场。&lt;/p&gt;
&lt;p&gt;去深圳之后，我们是在一个羽毛球馆里进行调试，当时我负责反前哨站，基本可以做到百发百中，但是有些玄学，而且因为机械装配的问题，在离得比较远的时候，会出现怪异的情况：我发给电控一个坐标，希望他瞄准，瞄准的时候会抬高枪管，我就看不到目标了，导致在离的很远的时候不能正常的自瞄，这个问题在下一个赛季通过修改机械结构解决了。&lt;/p&gt;
&lt;p&gt;国赛的时候，在调试的时候，视觉能做的事情已经不多了，能量机关差不多的打，自瞄的识别很稳，反前哨站则因为机械结构而爱莫能助。除了哨兵，由于上述的问题，导致本来还算能用的框架又出了不少问题，LXY 需要通宵调车。其他的我们几个视觉组的，前半夜把已经没问题的程序跑上几遍，然后就在一边聊天，等夜宵，我经常去场地边上的便利店里买几包酒鬼花生，很是好吃。&lt;/p&gt;
&lt;p&gt;国赛倒是没什么好记录的，我们的水平，老实说，在当时并不配得上群魔乱舞的国赛，但是运气好，分在的小组竞争并不算很激烈。我们第一赛季的全部比赛，可以说能够胜出，都是因为有运气在里面，而这一点在国赛体现的尤为明显。&lt;/p&gt;
&lt;p&gt;小组赛的三个对手，要不然机器人出了常规问题，要不然哨兵出去了没有回来，导致我们几个在观赛席的反应是：「Nice，他们哨兵出去了，这下应该回不来了」或者「果然没回来，基地已经展开了，该去偷家了吧」，因为相较于今年，去年的对手普遍没有击杀哨兵或者上环高打基地的能力，或者是因为被我们主动的盯防而阻止的。&lt;/p&gt;
&lt;p&gt;在这样的情况下，我们开始了两场比赛全部胜利，然后莫名其妙的，就出线小组赛了。从观赛席回到备厂区的时候，我们几个人问彼此，这就国一了？显然是有点不太相信这件事情的发生。&lt;/p&gt;
&lt;p&gt;国赛的后续碰到的真高手，自然也就赢不下去了，我们的名次也就止步十六强了，但是好歹是在很多年后再次追平了队史。&lt;/p&gt;
&lt;h2&gt;组长之路&lt;/h2&gt;
&lt;p&gt;事实上，在第一个赛季结束之前，我们就已经基本确定了视觉组组长的人选，也就是我，抛开别的不谈，我可能确实做的工作比较多。&lt;/p&gt;
&lt;p&gt;然而在国赛之后，LXY 也对组长的位置起了觊觎之心。大概是因为加分的缺乏之类的，他在国赛负责的是哨兵，可能认为加上了组长的身份，就可以获得不止两分的加分，于是便开始和我竞争。&lt;/p&gt;
&lt;p&gt;我因为有事情，比赛结束之后就回了西安，大多数的队员还留在深圳，因为有大疆举办的青工会。据说是因为当时，之前定好的队长去打别的比赛，耽误了这边的进度，老队员对一大堆定好管理层都有一些意见，所以打算大洗牌。当时是队长和老组长找到我，我也记不清先后顺序了，包括 YT 也问我，视觉组组长是不是要换人。我心里倒觉得奇怪，我的培训教程都已经写得差不多了，为什么忽然有人提换人的事情？&lt;/p&gt;
&lt;p&gt;后来大概了解了一下才知道，估计是视觉组在深圳期间没做什么事情（开始的时候需求就是那么多，我们都做完了，我之前还说过要不要调试一下防陀螺，队长也不让，我们还能做什么呢），估计是看 LXY 做的事情多（毕竟工控机都格式化了），所以打算把组长的位置换人。&lt;/p&gt;
&lt;p&gt;LXY 其人，倒也不坏，但是干活的积极性确实不好说，当时我们几个其实关系都不错，但是这件事情确实把我气得不轻。之前他做的是雷达，来队里的次数就不算很多，雷达是老代码，相较于之前据说是需要删，都不需要写什么。后面接手了哨兵，也就需要经常来一段时间了。&lt;/p&gt;
&lt;p&gt;我们当时拉了一个群，讨论组长人选的事情，现在来看，我其实对于组长是谁没什么所谓，只是说对于莫名有人说我贡献度什么的，心里确实不满。包括说后面有一段让我气愤的话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;唉，其实我说一下就是我看其他组的同学每天通宵熬夜调车感觉很心疼，感觉视觉组就这样走了很对不住其他组，所以我一般选择了陪伴。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;视觉组工作做完了，留一个人守夜，其他人走，不也很正常吗？难道说让大家在这里空耗才是正确的吗？再说陪伴，怎么不见你分区赛陪伴了，我因为 RM，这一年来少说也熬夜了五六十天，估计上百，那时候你怎么不心疼了，不陪伴了？&lt;/p&gt;
&lt;p&gt;这是我第一次隐约对 RM 产生了一丝失望。我高中就是打 RM 的，给我留下的都是快乐的回忆，但是大学的 RM 显然并非如此，越往后，越不是一个安心搞技术的地方。人情世故，责任推诿，勾心斗角，层出不穷。只是说我打了一年，一是舍不得一些朋友们，二是对得起一些人的期望，三是确实还抱有改变一些事情的念头，所以打算留下来。在今天来看，我还是不知道自己做的是否是对的，我不需要加分，第一赛季的分数早就够了，更多的分数也没有任何价值；科研的性价比远高于 RM，要我操心的事情也没有那么多，但是我还是留下来了，虽然可能没人因此感动。为什么呢，我会问自己，有太多人给了我太多的期待，我想，至少不要辜负太多。&lt;/p&gt;
&lt;p&gt;考虑了许久，YT 打圆场，说暂时就先考察我，假如没问题，我继续当组长，我也和前队长保证，好，那就没问题，我就正式成为组长了，虽然说事实上我履行这个职责已经将近半年之久。&lt;/p&gt;
&lt;p&gt;比赛也结束了，下一任管理层也都定下来了，那这个赛季基本上也就告一段落了，收拾收拾再出发，要准备下一个赛季了。&lt;/p&gt;
&lt;p&gt;我们的招新工作其实有点混乱，早在暑假开始的时候，我们办理过一个综能课，当时就是说培训之类的，但是最后讲了一点点就不了了之，后面就要进行训练了，综能课快速完结。到了比赛结束，已经八月十多号，也已经到了需要开始下一轮招新的时候了。&lt;/p&gt;
&lt;p&gt;我一向认为自己是有一些新人亲和力的，在招新的环节也比较下功夫，一方面我大二的时候确实更多时间会投身于科研，当初写了一些程序，可能还是需要新队员来继续开发，另一方面，我也确实认为，新鲜血液才是个队伍的未来所在。&lt;/p&gt;
&lt;p&gt;之前的招新工作其实一个重点在于压力培训，我们培训的内容和讲的内容并不是很相关，而最后做的事情和培训的内容关系也不是很大，只能说确实是作为筛选所设计。而今年培训的难点，另一方面则在于人工智能的兴起，不像我当初的情况，大多数的代码都需要我顺着其他人的博客去查，而且质量也参差不齐，如今只需要和模型说上几句话，大多数问题就迎刃而解。&lt;/p&gt;
&lt;p&gt;因此一方面，视觉组的培训应该更着重于帮助大家快速掌握一些基础技能，这些事情是人工智能也不能真正速成的，所以我在家的时候，大概录制了八期课程，去讲解视觉所需要用到的 C++知识，本身长度也不长，这样可以在线上就完成这最费时间的一步。在之后的培训，我也是主要从计算机视觉的基础说起，讲了一些通俗易懂的概念，然后就带大家上手代码。&lt;/p&gt;
&lt;p&gt;事后来看，这种选择不一定是正确的，培训的效果确实很好，对于任务，大家也可以比较出色的完成，但是事实上视觉组的主要工作在于后期的长期调试，代码上的一些东西，一是没有必要再去找轮子，二是那些需要写出来的难度都不算很高，导致任务缺乏区分度。&lt;/p&gt;
&lt;p&gt;一些同学确实是三分钟热度，很快就离开了，剩下的不少人都可以完整的跟下来整个培训。然而一方面出于一些交情，因为培训和新人走得太近，不太好意思通过打卡时间将他们开除；另一方面，大家的实力确实尚可，导致之后选拔不出来可以坚持下来的人，按照现在来看，当时选拔下来估计七八个人，现在只留下来了三个人。&lt;/p&gt;
&lt;p&gt;要是将来还是由我来办培训，双盲的打分是肯定要存在的，而且估计任务的难度还要进一步提升。&lt;/p&gt;
&lt;p&gt;当了视觉组组长之后，确实有参与一些行政相关的事情，然而到了赛季的后半程，这些事情又莫名其妙的消失了。我们队伍本来打算使用飞书，然而从结果上来看，除了项管还在坚持，大多数人其实用的不多，这种特别小型的团队，确实没有必要使用飞书进行管理的地步。同时也是因为熟人，物资管理到最后做的也不是很好，尽管我三令五申过，视觉组的物资，在使用之前要和我说一下，但从结果上来看，不只是我的键盘和设备丢的差不多，像是相机之类的东西，其他组拿走之后，我也要花不少时间才能溯源，还好最后没丢什么。&lt;/p&gt;
&lt;p&gt;第二年的技术发展也算是日益完善，之前没有的视觉兑矿，以及更好的反陀螺自瞄，更好的能量机关算法，更好的 SLAM（最后是交给了电控组的同学，LXY 因为到勤时间不够被移出队伍了），也都陆陆续续地出来了，总体上发展还算平稳。我把去年我做的一些开发，陆陆续续的都交出去了，分配给其他同学，他们在我的基础上也有做很多的调整，或者独立做了很多的开发。&lt;/p&gt;
&lt;p&gt;成为组长之后的一个明显的体验是，没必要再去亲力亲为的进行调试了，虽然说因为我还负责反前哨站的工作，所以和实际工作还有一些接轨，但是绝大多数时候，我只需要了解大家大致的情况就已经可以了。&lt;/p&gt;
&lt;p&gt;这个赛季之中还有一些人员变化，但是因为我也不太了解具体情况，可能也就不会过多的介绍了，万一说错了，可能反而还要被其他人说。不过可以说的是，从体感来讲，我身边的队员普遍对于管理层都不太满意，而且一些进度上确实也有太过于 push 的嫌疑，现在比赛还没有结束，所以说换届还不会开始，事后怎么清算我自然也就不太清楚了。&lt;/p&gt;
&lt;p&gt;不过事实上我倒是观感不是很大，一方面现在的这些人基本都和我同届，而且我也都已经有了加分之类的，也可以说是死猪不怕开水烫，反正我事先定的什么目标，我就按照这个目标去执行，也不管别人催什么的，定的 ddl 之前肯定也可以完成。&lt;/p&gt;
&lt;p&gt;一开始因为本身的自瞄，对于远距离的目标的识别和建模也都不太准确，所以说我的调试也不是很顺利，但是后面考虑到前哨站的特性，使用了更加 simple yet effective 的策略，总体来说就还算准确了。之前说的是命中率百分之七八十，后面我基本可以做到百分之百，但是因为发射之类的原因，有的时候可能卡一下，就会有一定偏差。&lt;/p&gt;
&lt;p&gt;这种问题也不是某一方的责任，确实是长期需要改进的。现在另外的问题是通讯，我发出去的信息在另一边不能很好的接受，似乎是因为电控的总线上挂了太多电机导致的，现在随着头越来越重，控制的死区也开始增大，这一点我暂时也没有想到很好的解决方法。&lt;/p&gt;
&lt;p&gt;这些策略在学校的时候，还算很不错，一开始用的是大疆的 Gen1 弹丸，基本上可以说很稳，到了后面用 Gen2，事实上弹道就和电控的弹道模型有了很大的差距，这个事情在视觉这边可以用算法比较方便的解决。在电控端重新标定一个弹道模型，玄学成分太大，我也没有指望。后续的基本全部操作，都是为了弥补弹道上的问题，对弹道做了一阶多点的标定，不过显然的是，在没有解决本质问题之前，视觉的方案终究只是治标不治本。&lt;/p&gt;
&lt;p&gt;这个赛季依然选择的是去长沙参加分区赛，然后再晋级国赛。相较于去年来说，一些视觉的方案确实已经被落地了，加上我们也派上了一些如平衡以及吊射英雄的一些比较前沿的兵种，所以说最后打出来的效果很是不错。&lt;/p&gt;
&lt;p&gt;去年如此来看，确实运气不错，最后还拿了一个分区赛四强，今年就不是那么顺利了，但是还是打出了自己的水平，晋级国赛。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;一开始打算写回忆录，还是比较激情澎湃的，想着自己至少也打了这么多年，一定也有很多有意义的东西可以记录，然后我到后面却发现，好多事情早已无法让我兴奋起来。一些内容我记不清了，一些内容我也不太确定，太多的琐事太多无趣的瞬间，让我开不了口，也落不下笔，记叙也从顺叙变成的插叙，索性将故事收拾一下，等将来想写了，再做一些补充。&lt;/p&gt;
&lt;p&gt;大学的 RM，一开始做技术的时候是很有趣的，不断自我提升的感觉也很棒，然而出一些自尊或者其他的情结，想要接任组长之后，我是反而慢慢地对这个比赛失去了一开始的激情，而回忆路越往后写，反而越觉得没什么事情好写。一些事情我已经记不清了，时间也仿佛飞快的流逝，那些古老的回忆，反而在我的印象十分深刻。&lt;/p&gt;
&lt;p&gt;两年的 RM 大学生涯，我好像做了很多，但是对一切又似乎没什么改变。机械依然是比赛的焦点，视觉组从重构到各项技术都达到新标准，我确实也或多或少都有参与，然而组员们的功劳，我倒也不能全都一个人拿下。&lt;/p&gt;
&lt;p&gt;我忽然间想起来，在第一年去国赛的时候，YZ 学长和我说的话。他说自己虽然是组长，但是好像这一个赛季也没有帮上什么忙，我一个人向前做得太快了，大家都没有追上很多。YZ 学长的技术能力是我至今见过最顶级的一批人，一开始的很多探索也都是他牵的头，那时的他想的会不会和如今的我想的一样呢？&lt;/p&gt;
&lt;p&gt;我或许给一切开了个头，然后每名组员就向前冲刺了，而我还呆在原地，缠身于生活中的琐事，回过神来的时候，自己却已经帮不上忙了。&lt;/p&gt;
&lt;p&gt;好在结果不差，也算是没有辜负大家一路以来的努力，尽管我的参与也不算很多，但或许也还值得厚着脸皮说上一句，没有辜负当时大家的嘱托。&lt;/p&gt;
&lt;p&gt;未来的比赛我还会不会打，我想了很久，假如没有人赶我走的话，大概还是会再厚着脸皮待上一段时间，不过未来还是属于新人的，我嘛，暂且日拱一卒吧。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/RM-Memoir-zh.webp"/><enclosure url="https://picr2.axi404.top/RM-Memoir-zh.webp"/></item><item><title>周记 Week5</title><link>https://axi404.github.io/blog/week-05</link><guid isPermaLink="true">https://axi404.github.io/blog/week-05</guid><description>2024-07-29 ~ 2024-08-04.</description><pubDate>Wed, 31 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;由于迟到的第四周周记，许多在第五周发生的事情，在第四周里没有来得及表达，因此也就给了我的表达欲更多的动力，在匆匆忙忙写上几个第四周的周记之后，也就开始了这周的内容。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;h3&gt;科研&lt;/h3&gt;
&lt;p&gt;首先是跟进一下之前的实习申请，在大老板面试之后，又是小老板的面试，大老板那里只能说确实压迫感很强，虽然老板本身和蔼可亲，但总体看上去，是对于我的工作比较不在意的。可能说投稿只是一个必备项，能证明我是优秀一些的学生，但本身考虑更多的还是我对于将来的领域的理解，以及一些基础技能的掌握。这确实也是一点，事实上我的这篇论文确实是是在一个小领域中的小贡献，对那些大型的工作来说，可以说只是我有科研的经历而已，剩下的其实也不能证明什么。&lt;/p&gt;
&lt;p&gt;小老板的面试倒是细节了不少，但是也没有拷打的意味，这说起来我的感受也不清楚，假如说是一个非一作的同学来回答，会不会对他的一些问题回答不上来呢？但是至少我来说，感受并不明显。&lt;/p&gt;
&lt;p&gt;大概是很舒适的和小老板一起过了一遍我的工作，然后我又聊了一下关于 embodied ai 的一些看法。这其中自然也有一些提问，多半是关于一些，无论是多模态也好，还是具身智能相关的一些论文的内容。只能说确实是一分耕耘一分收获，一方面我展示一些之前的博客内容，老板也知道了我的积累量，而且后续的一些解释和理解，也基本在我的了解范围之内，因此没有太多的问题，观感应该也很不错。&lt;/p&gt;
&lt;p&gt;面试结束的时候，小老板说我对于领域的理解在同龄人里面应该是比较多的，实在是令我受宠若惊，然而，好吧，先虚心接受，再继续努力吧。&lt;/p&gt;
&lt;p&gt;之后 HR 很快给了反馈，两次专业面试都通过了，之后和 HR 打了一个电话，了解了一下将来的一些情况，之后就说开始跑 HR 流程了，估计一周左右就可以有 offer，希望顺利。&lt;/p&gt;
&lt;h3&gt;焦虑&lt;/h3&gt;
&lt;p&gt;我已经走了很多的路，走了很远很远，但是回头看向身后，总感觉那些被我拉开距离的人不一会便会追赶上，而向前望去，在我前面的人又是距离我那么远。&lt;/p&gt;
&lt;p&gt;加入了一个小群，里面应该也算是一批目前全国非常厉害的科研高手，只能说还是人外有人天外有天，可笑我之前嘴里还说着无所谓，但是依然在心里是有一点沾沾自喜的。&lt;/p&gt;
&lt;p&gt;年纪轻轻就已经有了一篇顶会，貌似这在人工智能学院是前所未有，是不是也可以说是天才了呢？然而我的成绩不算好，别的内容也不算突出。女朋友总问我，有一篇不就够了吗，为什么还要继续努力？我也想问自己，是不是贪得无厌了。&lt;/p&gt;
&lt;p&gt;我总是能在一些人里面混到中游偏上，然而总是做不到顶尖。&lt;/p&gt;
&lt;p&gt;小学的时候，我是数学课代表，初中的时候，我是数学最顶尖的几个人，高中的时候，几乎大半个我们年级的数学组老师都认识我。高中的时候年级组长也是数学老师，我们模拟考试，数学卷子有的时候我提前一小时就可以做完，然后就借口说上厕所，就出来闲逛。年级组长语重心长地和我说，你是咱们年级最聪明的学生，当然，还有经典的：「需要更多的努力」。这句话后来几乎成了梗，像是差学生专属的 pua，但是我大概能确认我并非此类，因为老师也跟别的不少老师提过。高一的时候我还参加过数学竞赛，拿了省一的末尾，不过后面学校里面竞赛训练聊胜于无，又赶上了疫情，最后也就不了了之。&lt;/p&gt;
&lt;p&gt;不过考得好我是一向不擅长，语文作文有的时候能拿个范文，但是整体上大多数科目属于看着都会做着都错的情况。不少问题我看到红叉子之后立刻就能反应过来，然而有什么用呢？老师也总是说我粗心，但是我已经尽力检查了。&lt;/p&gt;
&lt;p&gt;我能在那帮学霸里面混的开，但是一到考试就和大家拉开差距了。小学的时候有推优，能去更好的学校，我没去成，在初中里面也成了很厉害的人；中考去了不错的学校，又是新的环境，更厉害的人，我还是混到了靠前的位置；高考发挥失利，来了西交，人工智能专业（如今来看，这或许是很好的结果，本来我一直是对数学感兴趣），又是年级前列，但也不是最顶尖。&lt;/p&gt;
&lt;p&gt;有人说，你这是不擅长考试，所以去的地方人都不如你，你自然就到中游偏上了，我却倒也觉得自己是有几分天赋的：我是北京考生。&lt;/p&gt;
&lt;p&gt;我其实有一套歪理，辩解一下，北京考试卷子简单但是考验心细云云，在这里就不说了。从现实来看，我虽然排名不高，也算是年级前列了，论文发表也是自己老老实实做。所以要不然我是有混到中游偏上的天赋的，要不然北京考试就真是地道了，还是能整出来两个像我这样的做题家的。&lt;/p&gt;
&lt;p&gt;天赋的话题就到此为止，那么现在呢？我已经到了前列了吗？插入了太多的回忆，现在暂且回到之前的内容，焦虑。&lt;/p&gt;
&lt;p&gt;我是总很容易安于现状的，一千米考试的时候，到了快结束，会想着跑慢些，反正分数差不多得了，中考高考考完之后，说是沮丧，倒也没有很多情绪在里面。然而科研岂是中游偏上即可？要是安于现状，有怎么有安稳的生活？&lt;/p&gt;
&lt;p&gt;我的理想不算远大，我喜欢学习喜欢技术，学习本身的开销不大，一张桌子一台电脑足矣；女朋友喜欢裙子，倒也不是名牌，jk 或者 lolita 足矣；我们经常出去一起吃饭，也不是豪华饭店，一顿饭贵则一百多足矣，否则小几十块钱也能将就；住房不用太大，不逼仄就好，能晒晒太阳。虽然这种生活已经难得，然对于我当下来说，不算很困难。&lt;/p&gt;
&lt;p&gt;可是我总会多走一步，免得遇到意外，但多走了这一步，又是新的风景，又有新的高手，又要我更加多走一步了。&lt;/p&gt;
&lt;p&gt;于是乎焦虑油然而生，越往上，越是高手如林，越是觉得后怕，高手如此之多，假如我没往前这一步，那当初那个层面的厉害人士岂不是更多？只是我不了解罢了。&lt;/p&gt;
&lt;p&gt;唯有学习，继续努力，日拱一卒吧。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;h3&gt;火车上&lt;/h3&gt;
&lt;p&gt;由于在火车上，陷入了极度的人生的不平静之中，周围都是小孩的声音，偏偏自己的事情又一团乱麻，让人心烦。&lt;/p&gt;
&lt;p&gt;坐火车去深圳，队伍里只能报销硬卧，这劳什子火车，晃来晃去，逼仄的车厢，穷鬼的我，还有什么比这更让人烦心呢？&lt;/p&gt;
&lt;p&gt;一直在筹划时间问题，首先需要到深圳之后去照相馆照个证件照，然后正常的仁至义尽调调车，RoboMaster 对我来说，功利主义的价值约等于零，然而仍有情义在。老组长将视觉组托付给我，我总不能放下不管，不然其实可以连比赛都不去，就没这么多事情了。至于事后假如说不让我当组长了，那再说吧，我已经无所谓。&lt;/p&gt;
&lt;p&gt;现在团体签证需要八号或者九号到西安，家里不出钱的话，便宜一些的是二百块硬座坐上两天。硬座本身我倒是无所谓，感觉甚至可能比硬卧好上不少。硬卧直不起腰，我在课堂上练了两年坐着睡觉，总该派上用场。&lt;/p&gt;
&lt;p&gt;先和队里请辞，然后签证，然后去上海实习，一切串成线自然最好，否则还是有罪受了。&lt;/p&gt;
&lt;p&gt;另一方面，这西交的系统真是不好用。由于我的问题，一些材料上传错误，外出申请驳回，需要重新提交，现在电脑深埋箱子里，又不差这一天，想着不如等等，到了地方再说。然而最简单的方法显而易见，材料我都已经准备好，只欠登入系统更新材料，然而手机版的系统不允许切换到电脑版，也没有修改或者重新提交的界面，真是见鬼。&lt;/p&gt;
&lt;h3&gt;杂谈&lt;/h3&gt;
&lt;p&gt;和队伍一起出去打 RM，去了深圳，不得不说，住宿条件非常的不好，不过对物理的经费有限也是可以理解的。这一次可以说基本上住在了山里，按照之前来说，应该是自己住一个房间，不过这次的条件实在太过于艰巨了，也就没有最后自己住，睡的地方床硬得像木板。&lt;/p&gt;
&lt;p&gt;有太多的琐事可以简单说一说，但是又说不了太多，所以干脆直接列成一个大的章节，或者也可以叫做山区往事。&lt;/p&gt;
&lt;p&gt;这几天的调试基本上没出什么大问题，但是需要说的是，由于通信和发弹延迟的问题，尽管设想很美好，但是我的程序没有发挥的十分出色。假如说到时候输了某一局，我不知道是否会存在一个甩锅环节，不过我希望并没有。事实上我的程序很早以前有一个十分优雅的版本，假如弹道根据监控提供的模型来运行的话，准确率会很高，然而事实往往并不尽如人意。在这里表述，也并不是为了苛责什么，英雄的本职工作应该是吊射，反前哨站实属是锦上添花，并不会被放在工作的首位，吊射的工作完成得很出色，我要是再去说前哨站并非我的责任，未免显得有些不粘锅的意味。&lt;/p&gt;
&lt;p&gt;不过从技术上来说，事实确实如此，我频繁发出的开火信号，现在电控甚至难以正常的接收；接收之后也频繁面临卡弹；同时远距离的高精度瞄准对于控制的死区要求严格，目前的死区已经超过了半个前哨站的距离，能够维持瞄准已经很是不易；而弹道的散布并不能说明弹道的轨迹始终不变，我还需要频繁的调整参数。目前的技术方案，可以说从一个简单的角度切入，建立了一套具有一定保质期的火控逻辑，但是假如真的追求完美的效果，后续电控与机械的优化依然缺一不可，这并非仅仅视觉方案可以解决的：发射延迟的波动在一百毫秒，就可以让命中率完全不受视觉组控制。&lt;/p&gt;
&lt;p&gt;然后就是这几天在写的 RoboMaster 回忆录，这篇文章依然可以在我的博客看到，只能说一开始忽然想要去写，但是到了后面，反而一些东西不太愿意去写得清楚，等我离开了利益相关，或许会更好。我更加希望这份回忆录是给我自己写的，而读者们与我共同观赏那些被我遴选出的我所喜爱的回忆，而不是捡起一些灰暗的回忆，让大家看我发牢骚，这份内容暂时完结了，不过后续肯定还有不少的补充。&lt;/p&gt;
&lt;p&gt;在山里的这些天，可以说是让我十分的心烦意乱，完全没有心思静下来做一些事情，也搁置了不少的计划，好在周末开组会的时候，目前的工作老师说也可以尝试一下 ICLR，也不知道最后的结果如何。&lt;/p&gt;
&lt;p&gt;大概就先写这些吧，这样的折磨可能还会持续一个星期，所以我们可以拭目以待下周的周记，是否依然是乌云密布。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-05-zh.webp"/><enclosure url="https://picr2.axi404.top/week-05-zh.webp"/></item><item><title>周记 Week4</title><link>https://axi404.github.io/blog/week-04</link><guid isPermaLink="true">https://axi404.github.io/blog/week-04</guid><description>2024-07-22 ~ 2024-07-28.</description><pubDate>Mon, 29 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;这里是迟到的第四周周记，因为学习等原因，最终还是太忙了，所以说在这周的中间没有来得及写。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;h3&gt;实习&lt;/h3&gt;
&lt;p&gt;这周做得比较多的一件事情便是学习，不过有一说一，这也不是本人的某一个星期的特点，基本上我的大多数时间都是在学习的，只不过这一周在基础的内容之上，增加了更多的压力。&lt;/p&gt;
&lt;p&gt;简单来说，ECCV2024 的中稿只是暂时性的胜利，想真正的迎来解放，还是要联系进组，并且进行科研实习。所以说我的目标是联系浦江实验室（上海人工智能实验室）里面的 OpenRobot 实验室，然后做一些具身智能相关的研究，所以说就需要读大量的论文。&lt;/p&gt;
&lt;p&gt;当然，在这期间，我也可以说是有很多收获的，大多数的内容都准备在博客上进行连载，也就是 Paper Reading 系列的内容。具身智能在我看来还是一个很长远的方向的，大多数的内容并不能在一时半会儿之内就解决掉，所以说仍需要努力吧，也希望自己可以在这个领域中做出有影响力的工作。&lt;/p&gt;
&lt;p&gt;在星期四的晚上，我联系了 OpenRobot 实验室的主任进行套瓷，总体来说还算顺利，把简历发过去之后，对面也很快给了回复，安排 HR 在周末进行面试。&lt;/p&gt;
&lt;p&gt;当时说的内容是，会要求我讲一下自己之前的工作，同时的话讲一下自己对这个领域的理解，以及自己将来的计划。我做了一个 PPT，然后又看了两篇综述，之后就参加面试了。&lt;/p&gt;
&lt;p&gt;面试也不算很难，一方面，论文确实最后的去向还行，所以说老师也没有很多的锐评，只是说我也承认这个是增量式工作，然后表示，希望在实验室中做更多有影响力的工作。老师也问了几个问题，还是比较经典的一些基础知识，分别是交叉熵/ softmax/attention 的公式，也不是很难。&lt;/p&gt;
&lt;p&gt;最后老师说的是，会安排手下的小导师来进行一下面试，周六晚上结束的面试，目前 HR 还没有消息，预计感觉大概率是因为周末不会上班，等过几天之后再看看，实在不行给老师发邮件。&lt;/p&gt;
&lt;p&gt;这段申请还算顺利，可能也是因为论文发表的原因，所以说还是受到了认可的，同时貌似这个实习是那种正式的实习，而且允许远程，看上去还是十分的友好的。&lt;/p&gt;
&lt;h3&gt;科研&lt;/h3&gt;
&lt;p&gt;在人机所这边的科研进展也还算顺利，我的方法其实是作效了的，我在我 ECCV2024 的论文的那篇方法上跑了一下目前的新方法，事实证明，新方法作为一个可插拔的模块来说，确实能够提升性能，但是总是感觉工作量不太大，不知道最后的论文应该怎么写。&lt;/p&gt;
&lt;p&gt;另一方面是，我之前的那篇工作的代码，优化就不算很好，而新的这篇工作又偏偏很吃算力，这也就导致，一方面，我现在需要一个很大的显卡（因为目前多卡训练有问题）来跑我的实验，另一方面，即使是很好的显卡，效率依然很慢，所以说我需要大量的时间，目前还在跑实验的过程中。&lt;/p&gt;
&lt;p&gt;现在可能这方面里面头疼的地方还是在于工作量以及故事线的问题，如何水出来大量的内容来符合篇幅的需求，这是一个问题。同时，是否要在其他领域的论文中使用我的方法，这也是一个问题。难也。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;h3&gt;一些胡想&lt;/h3&gt;
&lt;p&gt;现在又是一个深夜，在外面闲逛，想起来第四周的周记还没有写完，而现在已经是第五周周三的凌晨。&lt;/p&gt;
&lt;p&gt;假如已经到了下一周，周记的写作未免有些束手束脚，一方面，这确实是我拖延导致的，但同时，我也不敢发散说太多，不然显而易见的是下一周又没什么好说的了。&lt;/p&gt;
&lt;p&gt;我现在这里说一下周记这个事情本身。在之前内容中应该也提到过，我之前有写日记，也有尝试写周记，最后在尝试写月记，然后无限期推迟，最后让我的上一个博客服务器关停了。&lt;/p&gt;
&lt;p&gt;我其实总是想写下一些什么东西的，但是到落笔的时候，又感觉浑身不自在。&lt;/p&gt;
&lt;p&gt;很久很久之前的时候，我的梦想是成为一名游戏制作人，感觉是每一个网瘾少年都会有的想法。那时候我已经接触了不少优秀的独立游戏，但是我所喜爱的游戏类型，可以理解为一种要素丰富的大模拟类游戏（说白了其实有点像爽游，人生模拟，模拟的人生自然比现实要好很多），却没有优秀的品类，我的需求甚至是简单的一款纯文字游戏也可以满足，但是对于内容量的需求很高，所以并没有被充分满足。&lt;/p&gt;
&lt;p&gt;后来由于年少的无知，具体来说学了几天编程就老实了，毕竟当时才上初中，还没有编程的底子，当时用的最多的还是图形化编程，看了 Unity，就自觉地放弃了，然后开始想写作。&lt;/p&gt;
&lt;p&gt;写作确实是一种风格化很强的表达形式，能够表达一些优秀的内容，当然也可以用来写小说。当时我对西幻很感兴趣，当然现在也是，西幻和赛博朋克，这两个题材的设定我尤其喜欢，而且它们可以被设计得真实得像是另一个世界。&lt;/p&gt;
&lt;p&gt;所以很多内容就开始在笔头打磨，然后慢慢练习，说实话，一开始的一些写作确实是幼稚的，但后来慢慢初窥门径了，能写出来不少有意味的文字。这直接的导致了我的高考作文写得一直是记叙文，所以得分不太稳定，一看手感，二看题材，我不太愿意背素材，还因此被老师批评了很多次，最后语文的滑铁卢可能也与此相关。&lt;/p&gt;
&lt;p&gt;记不清楚去年还是今年了，我翻出了一些过去的文字，看了以后沉默良久，然后对我的女朋友说，我感觉自己再也写不出来那样的文字了。我的内心当时十分悲伤，女朋友只是安慰我，我猜她也不知道为什么这种小事会有这么大的反应。&lt;/p&gt;
&lt;p&gt;这有点像和过去的自己渐行渐远，我曾经想做游戏，我曾经想成为一名作家，然而这些都离我远去了，如今成为了一名科研工作者，我能否继续走下去呢？我也不知道。&lt;/p&gt;
&lt;p&gt;写出一些有意义的文字，这个本事我不知道什么时候才能再拾起来，时间太少，我又实在太忙，很难有时间静静打磨一下，想想有价值的输出，想想有设计的情节。&lt;/p&gt;
&lt;p&gt;然而或许，或许至少记录一下自己的生活还是可以的，所以就开始动笔写，记录我的所思所想，或许将来看起来有些幼稚，但是也能回忆起这时候的时光。&lt;/p&gt;
&lt;p&gt;我在西安交大生存指南里面有说，记笔记是一种很好的学习方法，这不是因为记笔记什么复习总结的效果，而是因为记笔记是一种量化学习成果的方式，我记下了多少笔记，我就学了多少东西，看着过去的积累，正反馈就油然而生。&lt;/p&gt;
&lt;p&gt;我想写日记是否也是如此呢？我用笔丈量自己的每一周，然后回头看看自己已经走出了多远，看看相较上一周，或者上上周，我又是否做了一些有意义的突破呢？&lt;/p&gt;
&lt;p&gt;有很多的朋友找我谈天，说说自己的心事，也算是一种诉苦。我是一位很不错的聆听者，至少自诩是如此的（当然也或许因此，我的表达欲只能在周记中释放，越多的聆听，越少的表达，不能喧宾夺主，就是这么个道理）。大家会讲讲自己的经历，一路上都发生了什么，我这个人虽然有很多不如意，但在别人看来或许还算顺风顺水，盖因我对自己有过太多的期待，然而别人可能并不指望我成就如此，所以我很能和大家共情。&lt;/p&gt;
&lt;p&gt;我开始的时候是打 RoboMaster 的，现在也还是视觉组的组长，这个比赛当前的统治者是上海交通大学的交龙，他们的口号我很喜欢，叫，「日拱一卒，功不唐捐」，我常把这句话送给每一个人，也说给自己听，写周记大概也是这样的理由，看看自己一个小卒，又向前拱了多少。&lt;/p&gt;
&lt;h3&gt;女装其二&lt;/h3&gt;
&lt;p&gt;关于女装的事情也已经不知道是多少次提及了，但暂时来看，我确实乐在其中。上周就已经说过，我买了一条女式的牛仔短裤，然而貌似由于一些发货的错误，给我发成了那种牛仔热裤，且是那种破的风格，导致之前设想的白衬衫加牛仔裤的穿搭风格，从中性偏女，彻底沦为了女装。&lt;/p&gt;
&lt;p&gt;然后即使是热裤，我依然有解释的空间，毕竟不是裙子，说说太热了什么的，走在街上最多引人侧目，倒也不会被认为过于奇怪。&lt;/p&gt;
&lt;p&gt;暂时来看，本人腿型还算不错，所以，还算不错：&lt;/p&gt;
&lt;p&gt;某种程度上来说，这也确实可以说是我的伪街初体验，虽然距离裙子还差不少，但是我不会化妆，所以那种还是暂且搁置吧。&lt;/p&gt;
&lt;p&gt;暴露度较高的热裤确实是一种很奇怪的感受，而且不知道是由于奇装异服还是确实有一定的吸引力，还是不少的回头率的，这也导致了羞耻感增加了。我不太敢去想那些路人看到我之后的感受，是觉得如此一双腿，结果居然是一个男的，还是说更多的是欣赏，谁知道呢。&lt;/p&gt;
&lt;p&gt;不过这条裤子确实立了功，之前下大暴雨，只有这条裤子不会被打湿，所以穿着它和凉鞋，才能够去到学校的各个地方。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-04-zh.webp"/><enclosure url="https://picr2.axi404.top/week-04-zh.webp"/></item><item><title>OpenVLA 代码笔记</title><link>https://axi404.github.io/blog/openvla-reading</link><guid isPermaLink="true">https://axi404.github.io/blog/openvla-reading</guid><description>OpenVLA 纯小白代码阅读记录。</description><pubDate>Tue, 23 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;因为要开始入门具身智能，所以说要阅读代码，显然选择了开源的 OpenVLA，于是在这里记录一下代码的阅读过程。&lt;/p&gt;
&lt;p&gt;本人代码水平为，掌握 Pytorch 大多数语法，对于 Hugging Face 不太了解。故部分内容会省略，尽量做到大多数内容均详实。&lt;/p&gt;
&lt;h2&gt;OpenVLA&lt;/h2&gt;
&lt;p&gt;OpenVLA 是一个具身智能大模型，Open 在这里就是 Open Source 的意思，于是使用其开源代码，开源网址为 &lt;a href=&quot;https://github.com/openvla/openvla&quot;&gt;https://github.com/openvla/openvla&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;代码结构&lt;/h2&gt;
&lt;p&gt;直接运行一个 &lt;code&gt;tree&lt;/code&gt;，看一下代码结构：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;├───prismatic
│   ├───conf
│   ├───extern
│   │   └───hf
│   ├───models
│   │   ├───backbones
│   │   │   ├───llm
│   │   │   │   └───prompting
│   │   │   └───vision
│   │   ├───vlas
│   │   └───vlms
│   ├───overwatch
│   ├───preprocessing
│   │   └───datasets
│   ├───training
│   │   └───strategies
│   ├───util
│   └───vla
│       └───datasets
│           └───rlds
│               ├───oxe
│               │   └───utils
│               └───utils
├───scripts
│   ├───additional-datasets
│   └───extern
└───vla-scripts
    └───extern
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中首先关注如何从头训练，于是关注 &lt;code&gt;vla-scripts/train.py&lt;/code&gt; 这个文件。&lt;/p&gt;
&lt;h2&gt;模型训练&lt;/h2&gt;
&lt;h3&gt;主文件&lt;/h3&gt;
&lt;p&gt;简单让 GPT4-o 生成了 &lt;code&gt;vla-scripts/train.py&lt;/code&gt; 的逐行注释，如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&quot;&quot;&quot;
train.py

Training script for Vision-Language-Action (VLA) Policies, built on top of pretrained VLMs, trained using mixtures of
the Open-X Embodiment dataset. Performs training in native PyTorch, using Fully-Sharded Data Parallel (FSDP) to run
distributed across GPUs (and nodes). By default, assumes that CUDA toolkit is &gt;= 11.0 (to support BF16 mixed precision).

Notes &amp;#x26; Prerequisites:
    - If you want to set a custom location for all HF / TIMM artifacts --&gt; `export HF_HOME=&quot;&amp;#x3C;PATH&gt;&quot;` *before* running!
        =&gt; For example (add to end of .bashrc): `export HF_HOME=&quot;/mnt/fsx/skaramcheti/cache&quot;`
    - If you want to suppress random Tensorflow logs --&gt; `export TF_CPP_MIN_LOG_LEVEL=3`

Run with:
    - [Single Node One-GPU (Debug)] : torchrun --standalone --nnodes 1 --nproc-per-node 1 vla-scripts/train.py
    - [Single Node Multi-GPU (= $K)]: torchrun --standalone --nnodes 1 --nproc-per-node $K vla-scripts/train.py
&quot;&quot;&quot;

import json  # 导入json模块，用于处理JSON数据
import os  # 导入os模块，用于与操作系统交互
import re  # 导入re模块，用于正则表达式操作
from dataclasses import dataclass, field  # 从dataclasses模块导入dataclass和field，用于定义数据类
from pathlib import Path  # 从pathlib模块导入Path，用于文件路径操作
from typing import Optional, Tuple, Union  # 从typing模块导入一些类型提示

import draccus  # 导入draccus库，用于配置管理
import torch  # 导入torch库，用于深度学习
import torch.distributed as dist  # 导入torch.distributed模块，用于分布式训练
import yaml  # 导入yaml模块，用于处理YAML文件

from prismatic.conf import VLAConfig, VLARegistry  # 从prismatic.conf导入VLAConfig和VLARegistry
from prismatic.models import load, load_vla  # 从prismatic.models导入load和load_vla函数
from prismatic.overwatch import initialize_overwatch  # 从prismatic.overwatch导入initialize_overwatch函数
from prismatic.training import VLAMetrics, get_train_strategy  # 从prismatic.training导入VLAMetrics和get_train_strategy
from prismatic.util import set_global_seed  # 从prismatic.util导入set_global_seed函数
from prismatic.vla import get_vla_dataset_and_collator  # 从prismatic.vla导入get_vla_dataset_and_collator函数
from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics  # 从prismatic.vla.datasets.rlds.utils.data_utils导入save_dataset_statistics函数

# 设置合理的默认值
os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;  # 禁用分词器的并行处理

# 初始化Overwatch =&gt;&gt; 包装`logging.Logger`
overwatch = initialize_overwatch(__name__)  # 初始化日志记录工具

@dataclass  # 使用dataclass装饰器定义数据类
class TrainConfig:
    # fmt: off

    # VLAConfig (`prismatic/conf/vla.py`); override with --vla.type `VLARegistry.&amp;#x3C;VLA&gt;.vla_id`
    vla: VLAConfig = field(
        default_factory=VLAConfig.get_choice_class(VLARegistry.DINOSIGLIP_224PX_MX_OXE_MAGIC_SOUP_PLUS.vla_id)
    )  # VLA配置，默认使用VLARegistry.DINOSIGLIP_224PX_MX_OXE_MAGIC_SOUP_PLUS.vla_id

    # 目录路径
    data_root_dir: Path = Path(  # Open-X数据集目录的路径
        &quot;datasets/open-x-embodiment&quot;
    )
    run_root_dir: Path = Path(&quot;runs&quot;)  # 存储日志和检查点的目录路径

    # 恢复运行参数
    pretrained_checkpoint: Optional[Path] = None  # 预训练检查点的绝对路径
    is_resume: bool = True  # 是否继续之前的训练
    resume_step: Optional[int] = None  # 恢复的全局步骤
    resume_epoch: Optional[int] = None  # 恢复的训练周期

    # 运行参数
    run_id: Optional[str] = None  # 用于日志记录的运行ID
    run_id_note: Optional[str] = None  # 用于日志记录的额外注释
    save_interval: int = 2500  # 保存检查点的间隔（以步骤为单位）
    image_aug: bool = False  # 是否启用图像增强
    seed: int = 7  # 随机种子（用于可重复性）

    # HF Hub 凭证（用于任何受限模型）
    hf_token: Union[str, Path] = Path(&quot;.hf_token&quot;)  # 环境变量或HF Token的路径

    # 跟踪参数
    trackers: Tuple[str, ...] = (&quot;jsonl&quot;, &quot;wandb&quot;)  # 初始化的跟踪器
    wandb_project: str = &quot;openvla&quot;  # W&amp;#x26;B项目名称
    wandb_entity: str = &quot;stanford-voltron&quot;  # W&amp;#x26;B实体名称

    def __post_init__(self) -&gt; None:
        &quot;&quot;&quot;提升优化参数的可用性，并验证`expected_world_size`&quot;&quot;&quot;
        self.epochs = self.vla.epochs  # 设置训练周期数
        self.max_steps = self.vla.max_steps  # 设置最大训练步骤数
        self.global_batch_size = self.vla.global_batch_size  # 设置全局批次大小
        self.per_device_batch_size = self.vla.per_device_batch_size  # 设置每个设备的批次大小

        self.learning_rate = self.vla.learning_rate  # 设置学习率
        self.weight_decay = self.vla.weight_decay  # 设置权重衰减
        self.max_grad_norm = self.vla.max_grad_norm  # 设置最大梯度范数
        self.lr_scheduler_type = self.vla.lr_scheduler_type  # 设置学习率调度器类型
        self.warmup_ratio = self.vla.warmup_ratio  # 设置预热比率

        self.train_strategy = self.vla.train_strategy  # 设置训练策略

        # [验证] 断言`expected_world_size`
        assert (
            self.vla.expected_world_size == overwatch.world_size()
        ), f&quot;Expected World Size = {self.vla.expected_world_size} but Found {overwatch.world_size()} GPUs!&quot;  # 验证期望的世界大小是否与实际一致

    # fmt: on


@draccus.wrap()  # 使用draccus.wrap装饰器定义训练函数
def train(cfg: TrainConfig) -&gt; None:
    overwatch.info(&quot;OpenVLA Training :: Warming Up&quot;)  # 记录训练开始的信息

    # 注意 =&gt; 在`torchrun`下初始化`overwatch`会自动设置`torch.distributed`
    torch.cuda.set_device(device_id := overwatch.local_rank())  # 设置CUDA设备
    torch.cuda.empty_cache()  # 清空CUDA缓存

    # 配置唯一的运行名称和保存目录
    vla_id = cfg.vla.vla_id  # 获取VLA ID
    cfg.run_id = (
        f&quot;{vla_id}+n{cfg.vla.expected_world_size // 8}+b{cfg.per_device_batch_size}+x{cfg.seed}&quot;
        if cfg.run_id is None
        else cfg.run_id
    )  # 如果运行ID为空，则生成唯一的运行ID
    if cfg.run_id_note is not None:
        cfg.run_id += f&quot;--{cfg.run_id_note}&quot;  # 如果有运行ID注释，则添加到运行ID中
    if cfg.image_aug:
        cfg.run_id += &quot;--image_aug&quot;  # 如果启用了图像增强，则添加到运行ID中

    # 开始 =&gt;&gt; 创建目录并设置随机性
    overwatch.info(&apos;&quot;Do or do not; there is no try.&quot;&apos;, ctx_level=1)  # 记录日志信息
    hf_token = cfg.hf_token.read_text().strip() if isinstance(cfg.hf_token, Path) else os.environ[cfg.hf_token]  # 读取HF Token
    worker_init_fn = set_global_seed(cfg.seed, get_worker_init_fn=True)  # 设置全局随机种子
    os.makedirs(run_dir := (cfg.run_root_dir / cfg.run_id), exist_ok=True)  # 创建运行目录
    os.makedirs(cfg.run_root_dir / cfg.run_id / &quot;checkpoints&quot;, exist_ok=True)  # 创建检查点目录

    # 保存配置 =&gt;&gt; 另外保存一个JSON版本以供以后HF集成
    if overwatch.is_rank_zero():
        draccus.dump(cfg, open(run_dir / &quot;config.yaml&quot;, &quot;w&quot;))  # 保存配置到YAML文件
        with open(run_dir / &quot;config.yaml&quot;, &quot;r&quot;) as f_yaml, open(run_dir / &quot;config.json&quot;, &quot;w&quot;) as f_json:
            yaml_cfg = yaml.safe_load(f_yaml)
            json.dump(yaml_cfg, f_json, indent=2)  # 保存配置到JSON文件

    # 加载VLA检查点（如果从训练中恢复）或基础VLM（从`cfg.vla.base_vlm` ID或路径）
    #   =&gt;&gt; 注意::验证所有参数在加载时都以FP32加载！
    overwatch.info(f&quot;Loading Base VLM `{cfg.vla.base_vlm}` from ID/Path&quot;)  # 记录日志信息
    if cfg.pretrained_checkpoint is not None:
        # [验证] 预训练检查点的`step`和`epoch`应与`resume_step`和`resume_epoch`匹配
        #   =&gt;&gt; 注意::我们要求开发人员传递`resume_*`参数作为额外的健全性检查！
        if cfg.is_resume:
            assert int(re.search(&quot;step-(.+?)-&quot;, cfg.pretrained_checkpoint.name).group(1)) == cfg.resume_step
            assert int(re.search(&quot;epoch-(.+?)-&quot;, cfg.pretrained_checkpoint.name).group(1)) == cfg.resume_epoch

        vlm = load_vla(cfg.pretrained_checkpoint, hf_token=hf_token, load_for_training=True)  # 加载VLA检查点

    else:
        vlm = load(cfg.vla.base_vlm, hf_token=hf_token, load_for_training=True)  # 加载基础VLM

    # [验证] 模型应为全精度！
    for param in vlm.parameters():
        assert param.dtype == torch.float32, f&quot;Loaded VLM parameter not in full precision: {param}&quot;  # 验证模型参数类型

    # 根据冻结与未冻结的参数确定训练“阶段”--&gt;支持不同的微调方案！
    if not cfg.vla.freeze_vision_backbone and not cfg.vla.freeze_llm_backbone:
        stage = &quot;vla-full-train&quot;  # 完全微调
    elif cfg.vla.freeze_vision_backbone and not cfg.vla.freeze_llm_backbone:
        stage = &quot;vla-train&quot;  # 冻结视觉编码器
    elif not cfg.vla.freeze_vision_backbone and cfg.vla.freeze_llm_backbone:
        assert cfg.vla.unfreeze_last_llm_layer, &quot;You should unfreeze at least the last layer of your LLM!&quot;
        stage = &quot;vla-sandwich-train&quot;  # 微调视觉编码器、投影器和LLM最后一层
    elif cfg.vla.freeze_vision_backbone and cfg.vla.freeze_llm_backbone:
        assert cfg.vla.unfreeze_last_llm_layer, &quot;Need to unfreeze at least last LLM layer to train!&quot;
        stage = &quot;vla-last-layer-train&quot;  # 仅微调LLM最后一层
    else:
        raise ValueError(
            &quot;Weight freezing configuration not supported. VLA config has the following parameters: &quot;
            f&quot;freeze_vision_backbone: {cfg.vla.freeze_vision_backbone}&quot;
            f&quot;freeze_llm_backbone: {cfg.vla.freeze_llm_backbone}&quot;
            f&quot;unfreeze_last_llm_layer: {cfg.vla.unfreeze_last_llm_layer}&quot;
        )  # 如果配置不支持，则引发错误

    # [显式] 调用`freeze_backbones`以提高清晰度 =&gt;&gt; 将准确记录哪些被冻结
    overwatch.info(f&quot;Invoking `VLM.freeze_backbones()` for `{vla_id}` =&gt; Stage: `{stage}`&quot;)  # 记录日志信息
    vlm.freeze_backbones(stage)  # 冻结模型参数

    # 打印总参数和可训练参数的数量
    num_params = sum(p.numel() for p in vlm.parameters())
    num_trainable_params = sum(p.numel() for p in vlm.parameters() if p.requires_grad)
    overwatch.info(
        f&quot;# Parameters (in millions): {num_params / 10**6:.3f} Total, {num_trainable_params / 10**6:.3f} Trainable&quot;
    )  # 记录参数数量

    # 获取VLA数据集和collator
    overwatch.info(f&quot;Creating VLA Open-X Dataset with Mixture `{cfg.vla.data_mix}`&quot;)  # 记录日志信息
    vla_dataset, action_tokenizer, collator = get_vla_dataset_and_collator(
        cfg.data_root_dir,
        cfg.vla.data_mix,
        image_transform=vlm.vision_backbone.get_image_transform(),
        tokenizer=vlm.llm_backbone.get_tokenizer(),
        prompt_builder_fn=vlm.llm_backbone.prompt_builder_fn,
        default_image_resolution=vlm.vision_backbone.default_image_resolution,
        shuffle_buffer_size=cfg.vla.shuffle_buffer_size,
        image_aug=cfg.image_aug,
    )  # 获取VLA数据集和collator

    # 保存数据集统计信息以便在推理时去归一化
    if overwatch.is_rank_zero():
        save_dataset_statistics(vla_dataset.dataset_statistics, run_dir)  # 保存数据集统计信息

    # 创建训练策略
    overwatch.info(f&quot;Initializing Train Strategy `{cfg.train_strategy}`&quot;)  # 记录日志信息
    train_strategy = get_train_strategy(
        train_strategy=cfg.train_strategy,
        vlm=vlm,
        device_id=device_id,
        stage=stage,
        epochs=cfg.epochs,
        max_steps=cfg.max_steps,
        global_batch_size=cfg.global_batch_size,
        per_device_batch_size=cfg.per_device_batch_size,
        learning_rate=cfg.learning_rate,
        weight_decay=cfg.weight_decay,
        max_grad_norm=cfg.max_grad_norm,
        lr_scheduler_type=cfg.lr_scheduler_type,
        warmup_ratio=cfg.warmup_ratio,
        enable_gradient_checkpointing=cfg.vla.enable_gradient_checkpointing,
        enable_mixed_precision_training=cfg.vla.enable_mixed_precision_training,
        reduce_in_full_precision=cfg.vla.reduce_in_full_precision,
        worker_init_fn=worker_init_fn,
    )  # 初始化训练策略
    train_strategy.run_setup(run_dir=run_dir, n_train_examples=len(vla_dataset))  # 设置训练策略

    # 创建度量工具 =&gt;&gt; 动态跟踪，记录到指定的跟踪器（例如JSONL，Weights &amp;#x26; Biases）
    overwatch.info(f&quot;Creating Metrics with Active Trackers =&gt; `{cfg.trackers}`&quot;)  # 记录日志信息
    metrics = VLAMetrics(
        cfg.trackers,
        cfg.run_id,
        run_dir,
        draccus.encode(cfg),
        wandb_project=cfg.wandb_project,
        wandb_entity=cfg.wandb_entity,
        resume_step=cfg.resume_step,
        resume_epoch=cfg.resume_epoch,
    )  # 创建度量工具

    # 运行VLA训练
    overwatch.info(&quot;Starting VLA Training Loop&quot;)  # 记录日志信息
    train_strategy.run_vla_training(
        vla_dataset,
        collator,
        action_tokenizer,
        metrics,
        save_interval=cfg.save_interval,
    )  # 运行VLA训练

    # 完成
    overwatch.info(&quot;Done with Training =&gt;&gt; Finalizing Metrics&quot;)  # 记录日志信息
    metrics.finalize()  # 完成度量工具

    # 完成所有操作
    overwatch.info(&quot;... and that&apos;s all, folks!&quot;)  # 记录日志信息
    dist.barrier()  # 同步所有进程
    dist.destroy_process_group()  # 销毁进程组

if __name__ == &quot;__main__&quot;:
    train()  # 如果是主模块，则运行训练函数
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这里暂时不用关注太多的事情，我第一件关心的事情是，一开始 &lt;code&gt;import&lt;/code&gt; 的那么多的库里面，他们分别起到了什么作用。&lt;/p&gt;
&lt;p&gt;假如说前往 OpenVLA 的 &lt;a href=&quot;https://github.com/openvla/openvla&quot;&gt;Github 仓库&lt;/a&gt;，可以发现其 fork 了另一个库，也就是 &lt;a href=&quot;https://github.com/TRI-ML/prismatic-vlms&quot;&gt;prismatic-vlms&lt;/a&gt;，在这里我只想关注 OpenVLA 的实现，所以我想要知道，相较于 prismatic-vlms，OpenVLA 有什么改动。&lt;/p&gt;
&lt;h3&gt;prismatic-vlms&lt;/h3&gt;
&lt;p&gt;在 prismatic-vlms 中，同样运行一下 &lt;code&gt;tree&lt;/code&gt;，看一下文件结构：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;├───prismatic
│   ├───conf
│   ├───models
│   │   ├───backbones
│   │   │   ├───llm
│   │   │   │   └───prompting
│   │   │   └───vision
│   │   └───vlms
│   ├───overwatch
│   ├───preprocessing
│   │   └───datasets
│   ├───training
│   │   └───strategies
│   │   └───strategies
│   └───util
└───scripts
    └───additional-datasets
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;conf&lt;/code&gt; 里面，可以发现的是，其中包括 &lt;code&gt;datasets.py&lt;/code&gt; 以及 &lt;code&gt;models.py&lt;/code&gt; 这两个文件，OpenVLA 增加了一个新的 &lt;code&gt;vla.py&lt;/code&gt;，也是同样一个代码风格。&lt;/p&gt;
&lt;p&gt;以 &lt;code&gt;vla.py&lt;/code&gt; 为例，具有一个 &lt;code&gt;VLAConfig&lt;/code&gt; 的类：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;@dataclass
class VLAConfig(ChoiceRegistry):
    # fmt: off
    vla_id: str                                     # Unique VLA Policy ID that fully specifies a configuration variant
    base_vlm: Union[str, Path]                      # Base VLM as ID/Path to Run Directory (e.g., `prism-dinosiglip+7b`)
    freeze_vision_backbone: bool                    # Freeze Vision Backbone Parameters (akin to pretraining)
    freeze_llm_backbone: bool                       # Freeze LLM Backbone parameters
    unfreeze_last_llm_layer: bool                   # Unfreeze final layer of LLM (only takes effect if LLM is frozen)

    # Data Mixture Parameters
    data_mix: str                                   # Open-X Embodiment Dataset =&gt;&gt; Unique Mixture ID (e.g., `bridge`)
    shuffle_buffer_size: int                        # Size of Shuffle Buffer (100K for Bridge, 1M for OXE)

    # Optimization Parameters
    epochs: int                                     # Epochs to Run (in case `max_steps` is not specified)
    max_steps: Optional[int]                        # [Optional] Max Gradient Steps to Run (overrides `epochs`)

    expected_world_size: int                        # Expected # of GPUs =&gt;&gt; allows us to gate training on hardware
    global_batch_size: int                          # Global Batch Size (divided across processes / world size)
    per_device_batch_size: int                      # Per-Device Batch Size (per-process / individual GPU)
                                                    #   =&gt;&gt; # of accumulation steps is auto-computed

    learning_rate: float                            # Peak Learning Rate (`lr_scheduler_type` sets warmup/decay)
    weight_decay: float                             # Weight Decay for AdamW Optimizer
    max_grad_norm: float                            # Max Grad Norm (for global gradient clipping)
    lr_scheduler_type: str                          # LR Scheduler (usually: &quot;constant&quot; | &quot;linear-warmup+cosine-decay&quot;)
    warmup_ratio: float                             # Fraction of Steps to Warmup (for warmup LR schedulers)

    train_strategy: str                             # Train Strategy (default &quot;fsdp-full-shard&quot;)

    # Enable Gradient/Activation Checkpointing (for the LLM Backbone)
    enable_gradient_checkpointing: bool = True      # Enable Gradient/Activation Checkpointing during Training

    # Mixed Precision Training via Torch Native AMP (`autocast`)
    enable_mixed_precision_training: bool = True    # Enable Traditional BF16 Mixed Precision
    reduce_in_full_precision: bool = True           # Accumulate/Reduce All-Gather Gradients in FP32 Full Precision

    # fmt: on
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这等于说是全部的需要的配置信息了，接下来就需要在里面塞入一些配置就好了，之后在创建的时候，使用类似于 factory 的东西进行调用就可以了。&lt;/p&gt;
&lt;p&gt;于是就使用一个配置即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;@dataclass
class Exp_SigLIP_224px_Bridge(VLAConfig):
    vla_id: str = &quot;siglip-224px+mx-bridge&quot;
    base_vlm: Union[str, Path] = &quot;siglip-224px+7b&quot;

    freeze_vision_backbone: bool = False
    freeze_llm_backbone: bool = False
    unfreeze_last_llm_layer: bool = False

    # Data Mixture Parameters
    data_mix: str = &quot;bridge&quot;
    shuffle_buffer_size: int = 256_000

    # Optimization Parameters
    epochs: int = 1000
    max_steps: Optional[int] = None

    expected_world_size: int = 8
    global_batch_size: int = 256
    per_device_batch_size: int = 32

    learning_rate: float = 2e-5
    weight_decay: float = 0.0
    max_grad_norm: float = 1.0
    lr_scheduler_type: str = &quot;constant&quot;
    warmup_ratio: float = 0.0

    train_strategy: str = &quot;fsdp-full-shard&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于其他的配置来说的话，相较于这个原来的配置文件，只需要进行少量的修改，于是直接进行继承就好：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;@dataclass
class Exp_FreezeVIT_SigLIP_224px_Bridge(Exp_SigLIP_224px_Bridge):
    vla_id: str = &quot;siglip-224px-icy+mx-bridge&quot;
    base_vlm: Union[str, Path] = &quot;siglip-224px+7b&quot;
    freeze_vision_backbone: bool = True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后实现一个枚举：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# === Define a VLA Registry Enum for Reference &amp;#x26; Validation ===
@unique
class VLARegistry(Enum):
    # Sanity Check Configurations =&gt;&gt; BridgeV2
    SIGLIP_224PX_MX_BRIDGE = Exp_SigLIP_224px_Bridge
    DINOSIGLIP_224PX_MX_BRIDGE = Exp_DinoSigLIP_224px_Bridge

    # SigLIP Frozen Backbone Experiment
    FREEZE_SIGLIP_224PX_MX_BRIDGE = Exp_FreezeVIT_SigLIP_224px_Bridge

    # [OpenVLA v0.1 7B] SigLIP 224px + OXE Magic Soup
    SIGLIP_224PX_MX_OXE_MAGIC_SOUP = Exp_SigLIP_224px_OXE_Magic_Soup

    # [OpenVLA 7B] DINO + SigLIP 224px + OXE Magic Soup++
    DINOSIGLIP_224PX_MX_OXE_MAGIC_SOUP_PLUS = Exp_DinoSigLIP_224px_OXE_Magic_Soup_Plus

    # === TDROID Fine-tuning Configs ===
    SIGLIP_224PX_MX_TDROID_CARROT_IN_BOWL = Exp_SigLIP_224px_TDROID_CarrotInBowl
    SIGLIP_224PX_MX_TDROID_POUR_CORN_IN_POT = Exp_SigLIP_224px_TDROID_PourCornInPot

    SIGLIP_224PX_ICY_MX_TDROID_CARROT_IN_BOWL = Exp_SigLIP_224px_Icy_TDROID_CarrotInBowl
    SIGLIP_224PX_LASTLAYER_MX_TDROID_CARROT_IN_BOWL = Exp_SigLIP_224px_LastLayer_TDROID_CarrotInBowl
    SIGLIP_224PX_SANDWICH_MX_TDROID_CARROT_IN_BOWL = Exp_SigLIP_224px_Sandwich_TDROID_CarrotInBowl

    # === DROID Fine-tuning Configs ===
    SIGLIP_224PX_MX_DROID_WIPE = Exp_SigLIP_224px_Droid_Wipe

    @property
    def vla_id(self) -&gt; str:
        return self.value.vla_id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后批量将这些内容注册成 &lt;code&gt;subclass&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Register VLAs in Choice Registry
for vla_variant in VLARegistry:
    VLAConfig.register_subclass(vla_variant.vla_id, vla_variant.value)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;虽然现在 prismatic-vlms 我还没有看完，但是我已经急了，所以对一些内容进行了跳过，接下来再次回到 &lt;code&gt;train.py&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;run_vla_training&lt;/h3&gt;
&lt;p&gt;简单检查一下训练的代码，不难发现，前面的大多数内容都是类似的，除了一些获取数据集之类的操作之外，主要还是正在设置各种的配置文件，但是在这里暂时先不关心这些，而是直接跳到 &lt;code&gt;run_vla_training&lt;/code&gt;，换句话说，我想要知道其论文中的训练是如何实现的。&lt;/p&gt;
&lt;p&gt;在这里简单再次复述一下 OpenVLA 的训练过程，&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/openvla-reading-zh.webp"/><enclosure url="https://picr2.axi404.top/openvla-reading-zh.webp"/></item><item><title>周记 Week3</title><link>https://axi404.github.io/blog/week-03</link><guid isPermaLink="true">https://axi404.github.io/blog/week-03</guid><description>2024-07-15 ~ 2024-07-21.</description><pubDate>Tue, 23 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;于是我履行诺言，在今天开始写第三周内容，第一次提笔的时候应该是星期四的凌晨，准确的说是五点多，今天上午有专业实习的参观，大概在车上可以睡会儿觉。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;h3&gt;RoboMaster 培训&lt;/h3&gt;
&lt;p&gt;记录的第一件事情是培训，之所以开篇就是这个，是因为本周的学习，除了看论文和代码之外，倒也没有做特别多其他内容，于是便讨论一下这个，以及我的感想。&lt;/p&gt;
&lt;p&gt;我作为我们学校的 RoboMaster 战队的视觉组组长，也自然负责了招新工作以及培训，由于去年就进行过相关的培训，所以倒也不算非常生疏。&lt;/p&gt;
&lt;p&gt;第一次的培训在上上周末，貌似是这个时间，也可能是我的记忆出了差错，当时大约来了四十多个人。我不清楚这是因为第一次培训是电控和视觉一起进行，还是因为第一次培训过于的无趣，或者第二次的培训，我通知的太着急，但是最后，第二次培训只来了十多个人，在结束的时候只剩下了十个人。&lt;/p&gt;
&lt;p&gt;这其实倒也可以理解，一方面第一次培训讲解的是 C++，我当时进行过相关的调查，便觉得没什么好讲的，培训面向的都是以老生为主，所以大伙多多少少都有基础，同时编程语言这种事情，一是不能速通，二是不如看我之前的速通教程，没有仔细备课后的讲解，感觉比较混乱，也可能引起了一些不满。&lt;/p&gt;
&lt;p&gt;但是确实可以发现的是，电控组的同学比视觉组多一些，所以出于自我安慰的角度来说，说不定视觉组确实有这么些人，所以第二次来看的人不多。&lt;/p&gt;
&lt;p&gt;讲实话，很早以前，我是打算只培训两次的，第一次讲计算机视觉的导论，第二次简单说一下程序的实现。&lt;/p&gt;
&lt;p&gt;事实上上次的招新就是这样的流程，因为大多数的内容只看文档就可以解决，而且手把手教到位了，又应该如何进行选拔呢？视觉组的内容，我认为并不是很难，尤其是在如今有 ChatGPT 的情况下，基础的任务更是信手拈来，因此过于详细的详解，貌似没有什么必要。私以为这种教学只需要指出需要了解的内容和学习的路径，同学们便可以自行学完需要学的内容，更何况这些内容确实算不上困难。&lt;/p&gt;
&lt;p&gt;但是由于想要写博客的原因，所以说干脆多留了一些课时，也顺便为后续的下下次招新积累一些材料，算是对于视觉组的一种积累。&lt;/p&gt;
&lt;p&gt;我不知道大家有没有听进去，同时队里的希望是招入一些大一升大二的同学，我不否认这种看法，但是还是认为新生更有潜力。视觉组已经不再是需算法创新的地方了，目前基础的框架已经牢固，剩下来只是一些维护，或者几个熬夜的努力完成的某个模块的大型更新，这些内容需要更多的心气去完成，而入学了一年同学，加入社团很难说是因为热爱，更大的可能是因为功利。&lt;/p&gt;
&lt;p&gt;我在队伍里面也呆了两年了，要是粗糙的算，从第一年快结束就已经当了组长，也见识过招新的场面，很多同学兴致勃勃地向我询问队伍的各种细节，然而问的最多的就是加分，而很多同学在听到没有加分后（这里指梯队队员不加智育分）便兴致缺缺。能为一个人带来十分德育分加分的比赛又有多少了呢？大抵还是和队伍里的正式队员的福利待遇对比之下，心里出现了不平衡。更何况视觉组从来不需要天才。&lt;/p&gt;
&lt;p&gt;我其实最希望的是把视觉组培养成一个类似于技术讨论小组的组织，内容也不局限于一个机器人比赛的技术，事实上大多数的同学都有人工智能背景，或者对这些内容感兴趣，相关的讨论完全是我所期待的，而讲实话，这个比赛的任务过于无聊，又太工程，大多数人不应该将生命浪费在这种地方。但是即使对于技术不感兴趣，至少也应该热爱这个比赛，而不是纯粹的功利，不然确实很难坚持下来。&lt;/p&gt;
&lt;h3&gt;西安交大生存指南&lt;/h3&gt;
&lt;p&gt;最近又抽空看了一下西安交大生存指南的网站数据，使用了谷歌的统计系统，对于信息进行了统计，发现用户数高达 700，然而这下不得不反思了，大多数用户仅停留在首页便匆匆离去，是因为内容过于无趣，还是说大伙暂时还没有心情静下心来慢慢阅读？我无从得知。&lt;/p&gt;
&lt;p&gt;从我个人的角度出发，还是希望这个网站能够帮到尽可能多的人的，但是目前来看情况并不明了，希望顺利吧。&lt;/p&gt;
&lt;p&gt;往好的角度去想，上海交大生存指南的用户数据应该也没有好到哪里去（这里指活跃用户），毕竟这只是一本书而已，也不会有人闲来无事反复观看，最多在读完之后，推荐给自己的学弟学妹，就仅此而已了。&lt;/p&gt;
&lt;h3&gt;企业实习&lt;/h3&gt;
&lt;p&gt;事实证明，周记开始向流水账的形式进行发展了，我不知道这是好还是坏。星期四去电信的什么云计算中心进行了实习，说是实习，其实就是参观，由一位讲解员带着在园区里面转了转，然后讲了一下各个设施的用处。老实说，我并不知道这有什么意义，这种课程的设置难道不是纯纯的浪费时间吗？假如说大三的专业实习也是如此，或者打发大家在工位上消磨生命，这种做法除了和众多夏令营营造冲突之外，我想不出来什么其他的意义。由此来看，教务处确实很不情愿放本科生离校，因此可以说半点机会也不给创造。&lt;/p&gt;
&lt;p&gt;想到这种事情便不免有所抱怨，也包括之前的选课系统开放，由于大二的偷懒，我还有两门选修课尚未修全，于是打算放到大三上来进行。选修课也是西交的一大鸡肋之一，讲实话，我并不对什么量子科学或者工程伦理感兴趣，而同时但凡是一个正常人，并不会想到去用科学创造来危害社会（假如一个人确实有这种打算，也不是一节课可以打消的），我不否认这些领域可能有所高深之处，但是显然不在课程教授的范围之内，目之所及，课程只不过是兜着圈子，逼着大家每节课打卡/回答问题/参加考试，然后抱着一些对自己的将来没有任何意义的资料，在考场上翻来翻去。&lt;/p&gt;
&lt;p&gt;我并不反感选修课的存在，据我所知也有若干大学的选修课，确实十分有趣，但一旦将选修课和奖学金挂钩，和老师的绩效挂钩，或者和什么毕业要求挂钩，某种本不应该存在的驱动力推着你参加这些课程，那么唯一的选择就是找个水课赶紧结束。&lt;/p&gt;
&lt;p&gt;比起来在这种课上虚度光阴，不如让我睡个好觉来的有用，至少能让我将来猝死的概率降低一些。&lt;/p&gt;
&lt;p&gt;更何况大多数的课程设立，内容全都不明所以，一些假大空的选题，在里面说着没有营养的车轱辘话，大谈未来/产业/趋势/大环境，但凡哪个学生信了这番话，我估计将来是要栽个跟头了。&lt;/p&gt;
&lt;p&gt;这又不免让我又向上讨论，想到了当今的本科生的压力。假如说你是摆烂一派倒也还好，每天打卡上课玩玩游戏，看看小说，刷刷视频，然后一天便过去了，让自己不至于被劝退，等着考研或者就业就好了（不过值得一提的是，即使对于这种空余精力很多的人来说，选修课的存在依然十分滑稽，创造的情绪价值不如在电脑前面打开一把排位赛）；但假如某人不幸是内卷一派，那可又是陷入了一片泥沼之中，什么绩点竞赛科研轮番上阵，就算是铁打的学生也恐怕招架不来，分分钟便要头晕眼花，更何况还需要一点点的个人兴趣拓展，这由哪里来的时间分给选修课呢？到头来也只能把讨论换成一个匆匆的句号，然后将矛头指向某些抽象的环境或者氛围，真是苦海。&lt;/p&gt;
&lt;p&gt;在讲完这些内容之后，在交大门中，又与門友进行了讨论。准确地说也不能是讨论，只是我对于他人提问的一个回复又被人回复了而已，具体的主题是关于推荐大三参加的选修课程，而从我的了解来看，大多数我所认识的课程都是必修课，因此相关内容并不在学校要求学分范围之内。&lt;/p&gt;
&lt;p&gt;令我触动了一点是，在大学中确实有人依然执着于课堂的学习，在我看来其实是一件好消息。尽管说我并不认可大学中大多数教学的质量，或者说我本身是厌恶听课的，听课的知识密度太低，而又要照顾全班的同学，很容易便让我听得昏昏欲睡。当然，有必要指出的是，昏昏欲睡的一大元凶很有可能是我自己的熬夜习惯，我个人的能力并没有自诩为鹤立鸡群。但是由此看来，可能是因为我走得太快或者太远，导致已经无法共情大多数同学的感受，既然有人能有激情于课内的选课，这也意味着我们的培养方案终归还是有可取之处的，这才让人有继续选择的欲望，希望可以越来越好吧。&lt;/p&gt;
&lt;h3&gt;科研&lt;/h3&gt;
&lt;p&gt;这个星期继续履行我的承诺，要补习具身智能相关的知识，而后申请课题组，于是在埋头苦学。在企业实习之后，剩下的三天时间仿佛过得飞快，几乎就是一睁眼一闭眼，也忘记了继续写周记记录，还好，最后也算赶上了。&lt;/p&gt;
&lt;p&gt;我之前所进行的领域，且不说大语言模型或者 agent，甚至连多模态也没有半点接触，而如今却要开始学习具身智能，确实压力很大。尽管说我认为我的论文阅读还是很广泛的，但是对于不少的大语言模型中的细节依然了解不多，而上手实操更是毫无经验，若用这番姿态去申请，恐怕立刻被拷打的遍体鳞伤。于是我先从最基础的 BOW 模型开始，然后一路向后学习，现在总算开始看具身智能了，不过一些内容还是看得太急，所以缺少一些个人的总结，但是数量至少上去了。&lt;/p&gt;
&lt;p&gt;另一边，我目前在周三平老师课题组的工作，现在还欠缺进一步的实验，然而显卡成了问题。我写的程序不知道为何，在并行的时候总会导致性能的暴跌，因此每次只能使用一张显卡进行训练，而在结构的数量堆起来之后，自然对于显卡的需求也就高了，现在需要足足 28GB。对于如何搞来这样的免费显卡，我还没有头绪，实在不行也只能自费租卡了。然而悲伤的是，这种显卡也要训练几十小时，也就是几百元的开销，更何况我不知道结果如何。&lt;/p&gt;
&lt;p&gt;最好还是问问老师有没有更高显存的显卡吧。&lt;/p&gt;
&lt;h3&gt;医学比赛&lt;/h3&gt;
&lt;p&gt;在星期六的时候，我参加了一次医学的竞赛，或者说是医学相关的项目竞赛，因此人工智能或者计算机的专业相关同学也就有了用武之地。&lt;/p&gt;
&lt;p&gt;因为熬夜的习惯，我甚至没有敢去睡觉，直接通宵，然后来到了那边，准备线上的会议，结果问题不断。&lt;/p&gt;
&lt;p&gt;首先是会议的延迟，一直拖到下午才开始，然后进了会议之后，软件出了问题，放不出声音，又要起身去调整，貌似这一点不太符合规矩，也可能扣掉一些分数。最后比赛没有获奖，这时候令我气愤的事情发生了，我甚至可以说是认为，这是十分可耻的背叛。&lt;/p&gt;
&lt;p&gt;参加这个项目是因为女朋友需要一些加分，正好我手头有现成的技术，那么和她一起刷一个竞赛也就成为了一个想当然的想法，我主力，女朋友辅助，相辅相成。&lt;/p&gt;
&lt;p&gt;一开始加入这个竞赛，是因为宣传的时候他说这个竞赛是 A 类竞赛，当时我便有所质疑，因为在学校的智育分加分名单里面并没有这个，在项目进行许久之后我又询问，迎来的只有道歉，还有什么牵强的其他的竞赛明目。让我说实在是可笑，每一个交大学生，都只知道一种 A 类竞赛，哪里来的这么多弯弯绕绕？&lt;/p&gt;
&lt;p&gt;后来又是这个项目的问题，整个项目，我和女朋友加入之前一共六个人，只有一堆标注的数据（一张影像标注七个数字，人力成本低到令人发指）以及一个 R 语言写的模型（调用库函数实现的传统机器学习算法），让我说这种东西有半点含金量吗？值得六个人做一个假期。换做我，毫不夸张地说，多于一天完成，都算是我没有认真，代码是 GPT 分分钟完成的水平，而标注的数据又是苦力活。于是乎整个项目的完成内容几乎全交给了我一个人（一开始说需要给这个项目套个壳子，结果发现整个项目简陋不堪，又需要从根本上修改方案），我用的是半监督技术，所以说也没有要求他们标注太多数据集，甚至说大多数的标注都出了问题，还是我一个人补上的。我完全有理由说这个项目完全由我一个人完成，其他人的贡献绝对小于百分之五。&lt;/p&gt;
&lt;p&gt;现在比赛没有拿奖（在腾飞杯还获得了校一），倒是过来兴师问罪了，因为我没有精力维护这种项目，所以交给了我的女朋友，然而后续呢，老师又过来说当天的问题主要出在线上。我本人是不愿意骂人的，尤其是在我的博客中，我希望这里是一片净土，不过我确实很难控制自己的情绪。&lt;/p&gt;
&lt;p&gt;去现场答辩的，评委问了三个问题，我们声音恢复之后听到了两个，一个问数据有限的问题（他们只提供了这么多数据，也没有人力更多的标注，一个项目挂了那么多人，除了提供错误数据之外我不知道都在做什么），一个问这个项目本身的意义问题（项目的主题也是离奇无比），项目的负责人同学，一个问题也没回答上来，只能说好好好，然后留给老师自问自答。&lt;/p&gt;
&lt;p&gt;PPT 的制作也是一言难尽，我用的是 SOTA 的医学影像分割模型，PPT 里面提的不多，却拿着自己做的二分类准确率 80% 的随机森林奉若至宝：貌似在开启项目之前，他们已经用这个东西水了论文和软著，现在有了更好的技术引入，也不愿意更新图片或者文字内容。&lt;/p&gt;
&lt;p&gt;再说回到这个项目本身的主题，更是可笑。说要用医学影像的图像，具体来说是上颌窦，来判断一个人的性别和年龄，用于某种法医的检测。我想着是，都能获得上颌窦的影像了，难道还不能知道一个人是男还是女？假如一个人真是半点DNA也没有留下，无法进行 DNA 鉴定，那么又应该怎么把这个人的骨骼拼起来，然后送进CT机里面，来照出来一个影像？这个设定应该不难理解，读者看后便也会会心一笑，这完全是一个南辕北辙的产物。&lt;/p&gt;
&lt;p&gt;综上所述，一个项目，先是画饼把人骗进来，该有的比赛都是虚构的，加分也没有；然后项目本身的主题也是毫无意义，项目的分工由我一个人完成，其他的也就剩些琐碎的文书内容；躺在自己之前的产出上复制粘贴，不愿意多做什么，新的图片也是我产出，实验结果也是我在跑，也不愿列一个表格之类的；答辩的时候也是一句话也说不出来，被老师问的哑口无言。最后没有获奖，倒是怪起来，我们线上简单起个身了。然后负责人和老师一起来兴师问罪，又让我的女朋友愧疚了好久，真是好大的威风。&lt;/p&gt;
&lt;h3&gt;其他&lt;/h3&gt;
&lt;p&gt;另外，在这里作为备忘录的，有一些想要介绍的内容，打算在将来写成博客，然而我的记性并不算好，因此记录一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;傲梅分区助手：老实说之前我一直是 DiskGenius 的忠实用户，而傲梅分区助手无论从名字还是网站，都看上去像是某种流氓套壳软件，然而其一是提供绿色版的压缩包，二是有诸多极为逆天的强大功能，让我大受震撼，不得不推荐。&lt;/li&gt;
&lt;li&gt;LLM 教程：我从 BOW 开始重新学习了 LLM，直到最近的 VLM 以及 embodied ai，这些内容有必要整理为系列的博客。&lt;/li&gt;
&lt;li&gt;LLM 代码阅读：同样，我需要阅读一些 LLM 的代码，然后整理为博客，以便于自己回顾。&lt;/li&gt;
&lt;li&gt;奇妙的报错指南：一些我在日常生活中收集到的奇怪问题，可能与相关内容的文档不符，在这里记录一下。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;h3&gt;Choker 与腿环&lt;/h3&gt;
&lt;p&gt;由于一些奇奇怪怪的原因，本人的画风也开始变得奇奇怪怪，明明之前没有如此明显的症状的，也可能是因为最近一直在学习的原因吧，压力太大了之类的情况，总而言之，开始购入一些奇怪的东西。总的来说购入了两件东西，一件是 Choker，一件是腿环。除此之外其实还有一个腿套，但是因为上衣以及裤子，上衣是衬衫的搭配，但是全是褶子，等待熨烫中；裤子买了牛仔短裤，但是还没到，所以无法展示。&lt;/p&gt;
&lt;p&gt;Choker 搭配上长发确实有一番韵味，但是一是本人不想露脸，而是现在疏于打理，十分憔悴，所以没有照片，但是幻想中的场景，用 GPT 生成了一段：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;昏暗的房间里，只有一盏小灯散发出柔和的光芒，勾勒出他纤细的轮廓。阿汐站在镜子前，微微低着头，轻柔的呼吸让他脖颈上的黑色choker一紧一松。那条细细的带子紧贴着他的肌肤，与白皙的颈部形成鲜明的对比。他的呼吸逐渐急促起来，细腻的肌肤上泛起一层淡淡的红晕。每一次呼吸，锁骨处的曲线便愈发明显，灯光下，他的脸颊微微泛红，显得格外迷人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但是显然我没有此等魅力，我倒是希望，但是条件不允许。&lt;/p&gt;
&lt;p&gt;同时腿环还是比较有趣的，有一张比较昏暗的照片可以给大家一看，但是由于本人可能腿比较细一些，所以腿环没有勒肉感，不是很涩。&lt;/p&gt;
&lt;p&gt;完整的内容等待我买的熨烫机和裤子到了之后，再来尝试吧。&lt;/p&gt;
&lt;h3&gt;恋爱&lt;/h3&gt;
&lt;p&gt;开开心心的，每一天都想要和乐小姐在一起，十分的充实。&lt;/p&gt;
&lt;p&gt;除此之外，乐小姐最近开始沉迷乙游，于是我也注册了一个账号，目前五星貌似比她多一个，不过她貌似还没有察觉，希望不要因此不开心。&lt;/p&gt;
&lt;h3&gt;博客&lt;/h3&gt;
&lt;p&gt;除此之外，打算最近整理一下博客的内容，这也暂时列在日程里面吧。博客里面有很多的内容都是语音生成的，因此需要稍微规整一些，文字功底自然是越好越好，另，也是恢复一下本人的文笔。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-03-zh.webp"/><enclosure url="https://picr2.axi404.top/week-03-zh.webp"/></item><item><title>周记 Week2</title><link>https://axi404.github.io/blog/week-02</link><guid isPermaLink="true">https://axi404.github.io/blog/week-02</guid><description>2024-07-08 ~ 2024-07-14.</description><pubDate>Tue, 16 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;事实上，果然不出所料，第二次的周记还是晚了许多，当然这其中我是可以辩解的，在周日的白天迎来了噩耗，要回家奔丧，于是做一周总结的时间便减少了，自然而然的，加之以一些任务清单的存在，周记的撰写自然被排在最后面，也自然而然的推迟了。&lt;/p&gt;
&lt;p&gt;现在在写这篇内容的时候，以周记的日期来算，已经是第三周的星期三了，不过现在还能勉强回忆起来上周发生过的种种事情，因此倒也还算说得过去。&lt;/p&gt;
&lt;p&gt;不过说到底，这些内容终归还是借口，还是因为我的时间规划问题导致的，正常来说，每天应该有一些时间留给任务清单，有一些时间留给英语单词，最后在一天快要结束的时候，在周记中记下今天的内容，从之后开始吧，记完第二周的周记，我也会同步第三周内容。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;h3&gt;一些被遗忘的事&lt;/h3&gt;
&lt;p&gt;首先还是先说一下关于学习的事情，总的来说，忘记了一些之前记录的小细节。&lt;/p&gt;
&lt;p&gt;在小学期的时候有一门课程，叫做理论计算机基础，这门课有一个教材，属于是计算机经典的纯英文教材，不过是老师自己写的。&lt;/p&gt;
&lt;p&gt;在计算机领域中使用英文教材确实是十分常见的操作，包括说使用英文的 PPT，但是对于我这种英文苦手来说明显就不是很友好了。学组群里面有同学立刻用 DeepL 做了一个 pdf 翻译，但是我不喜欢。老实说，一方面我并不认可 DeepL 宣传的自己是最好的翻译工具这一说法，而这种宣传在 GPT4-o 出现之后更是显得十分滑稽，GPT 比它更快，而且更好。&lt;/p&gt;
&lt;p&gt;但是我当时并没有精力去写一个对于 pdf 的批处理程序，互联网上对 GPT 进行的 pdf 优化莫名又已经消失的差不多了，于是一个简单粗暴的方法出现了，自己一个一个去截图（直接复制或者复制并提取数学公式，前者不准，后者太慢），然后把一切交给 GPT4-o 即可。&lt;/p&gt;
&lt;p&gt;不过遗憾的是，显然不是我故意的，这个文件在考试前我忘记发到群里了，导致最后我只打印了一份，其他同学并没有用上，但是所幸低年级的同学有福了。&lt;/p&gt;
&lt;h3&gt;沉淀&lt;/h3&gt;
&lt;p&gt;最近正在盘算加入课题组的事情，但是因为我之前所处于的领域是 CV，而更准确的说，集中在少样本学习里，但是这一方向的课题组太少，我也并非完全感兴趣，于是我挑选了 embodied ai 作为将来的方向，但是就意味着我需要学习一些新内容。在和前辈学长交流了之后，获得了一些学习的方向，于是最近正在看论文和看代码，不过只能说还是任重而道远，之前只看过经典论文，对于其他内容的了解，并不算很多。&lt;/p&gt;
&lt;h3&gt;开源&lt;/h3&gt;
&lt;p&gt;至于开源相关的事业，就不像上次一样设立一个单独的其他类了，干脆和学习放在一起，主要是因为太懒。&lt;/p&gt;
&lt;p&gt;最近还在维护西安交大生存指南，但是有几点感受，一是热度并不高，二是由于目前基本内容已经施工完毕，还差一个类似于快速问答的内容。概念大概就是，瞬间回答一下一些常见问题，比如说补卡/宿舍/社团，这些内容都没有必要单独开一个章节去讲解，一段话足矣。&lt;/p&gt;
&lt;p&gt;同时开源相关的另一件事情，是关于绿群，目前基本上对于项目的维护也已经趋于稳定了，这一点没什么好说的。&lt;/p&gt;
&lt;h2&gt;生活&lt;/h2&gt;
&lt;h3&gt;恋爱&lt;/h3&gt;
&lt;p&gt;最近一段时间，和乐小姐的感情经历不能算是顺利，可以说有一些小摩擦，这大概是因为一些焦虑导致的。我可能有的时候埋头做的事情太多了，没有停下来回头等她，也就没有顾及她的感受，这一点之后或许需要注意一下，我其实很能理解这种被抛在身后的感觉。&lt;/p&gt;
&lt;p&gt;希望感情顺利吧，希望可以和她一直在一起。&lt;/p&gt;
&lt;h3&gt;水群&lt;/h3&gt;
&lt;p&gt;应该大概也是上周，正式的在新生群中进行了一些水群，讲实话，我并不是很喜欢新生群，大多数人充斥着一种摆烂的气息，这倒也不是鄙视，但是部分的人对我就暗含了一些攻击性，这让我比较不舒服。&lt;/p&gt;
&lt;p&gt;每一个人都有自己的选择，所以我还是尊重别人的选择的，而且长远来看，就我目前的体验来说，假如说我目前做的事情不是我所喜爱的，我难以想象走到现在我需要承受多大的痛苦，所以说每一个人的选择或许属于他自己的原因，尊重。更何况我也不知道我目前走的这条道路是否正确，说不定在大学好好的享受生活，才是那个正确答案。&lt;/p&gt;
&lt;h3&gt;女装&lt;/h3&gt;
&lt;p&gt;不知道这种事情是否需要在博客里写，但是也算是近些日子以来我的一个思考，虽然说博客可能会被别人看到，但是我觉得这是我的多元性中的一点，或许别人也有必要了解。&lt;/p&gt;
&lt;p&gt;我在博客中也有发布过一些女装照片，事实上有必要澄清的是，我并非男娘或者药娘，也并非具有类似的意愿。一方面，据说服药会降低寿命，而我比较惜命；另一方面，我十分确认我的心理性别为男。所以充其量，此等打算算是异装癖的一种，但是我并无法分析其成因，或许是出于压力或者其他原因，我不知道，但是类似的行为的确可以使我心情愉悦，于是我这样做了，这似乎是一个很直接的理由。&lt;/p&gt;
&lt;p&gt;我不打算让这些内容影响我的正常生活，也希望读到此篇内容的人可以提及，但不要过分的用类似内容叨扰我，这会令我十分苦恼。&lt;/p&gt;
&lt;p&gt;我喜爱自由的开心的生活，这种微薄的请求在我的生活中应该不算过分。我希望我可以快乐，所以我有时会尝试女装，但也没有打扰到别人，我希望别人可以快乐，因此我在做一些开源的事情，这或许能让别人收到帮助。至于其他，我不清楚，结果我也不清楚，希望船到桥头自然直吧。&lt;/p&gt;
&lt;h3&gt;交大门&lt;/h3&gt;
&lt;p&gt;交大门是一个目前为西交毕业学子（即門中的 admin）搭建的论坛，也可以说是目前西交的活跃论坛了。我在一年以前曾经在上面活跃过，最近又开始在里面发帖，一方面，我的现实生活越来越少，基本在现实中只和自己的女朋友交流；另一方面，我的线上生活则越来越多，包括说绿群，或者是門，或者是目前的新生群，线上认识的朋友们往往更加随和，也是易于交流的，于是我开始依赖这种感觉，成为了一种互联网著民。&lt;/p&gt;
&lt;p&gt;門的氛围和绿群不一样，绿群中的大家是一种积极向上的氛围，大家在为了保研努力，同时可能还有一些彼此之间的谦虚，和开源精神的协作，这种氛围无疑是让人感到温暖的。而門则不一样，某种程度来说，大家的观点差异很大，而且事实上，在论坛中存在着一些「崇洋媚外」的声音。当然，有必要在这里打上引号，事实上門友并非二极管，而我也并非二极管，对于不同事物，关于推崇国内还是国外，这均是有待商榷的（尽管这么说会显得我有些理中客）。大多数时候其实我会站在国内这一方，可能因为本人尚未接受过社会的毒打，诱惑者是这个社会向我展露的友好已经足够多。然而国内外不同的环境中，确实各有各的不幸，而同时，我也支持大多数門友所说的，对于外国教材与教学的推崇，但因为我的英语苦手，于是便无福消受了。但是本人依然粗浅地浏览过部分的内容，确实比我见过的国内内容要更加深刻且有用。&lt;/p&gt;
&lt;p&gt;同时引人深思是，论坛确实是一个能让人静下心来发表长篇内容的载体，很多的话语，在使用论坛发帖时，便会仔细斟酌并给出全面答复，这让人有一种满足感，同时也带来了更好的讨论氛围。&lt;/p&gt;
&lt;p&gt;然而不幸的是，一方面存在着校园集市这样的低质量「论坛」，一方面門的活跃用户并不算多，因此，大多数时候，开展一个美好的讨论氛围的打算还是落空了。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;对于上周的回忆，我已经大多记不清，能够想起来的也就只有这些了，一些属于第三周的内容，我便放到了第三周里面去说，于是没有在本篇内容中过多提及，其他的便随他去吧。&lt;/p&gt;
&lt;p&gt;假如有读者的话，希望你们看得开心。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-02-zh.webp"/><enclosure url="https://picr2.axi404.top/week-02-zh.webp"/></item><item><title>RoboMaster 视觉组第一次培训</title><link>https://axi404.github.io/blog/rm-tutorial-section-1</link><guid isPermaLink="true">https://axi404.github.io/blog/rm-tutorial-section-1</guid><description>关于 C++ 的快速入门以及基础概念讲解。</description><pubDate>Wed, 10 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;欢迎来到视觉组&lt;/h2&gt;
&lt;p&gt;欢迎大家来到视觉组，在这里简单的介绍一下视觉组的情况。&lt;/p&gt;
&lt;p&gt;众所周知，在 RoboMaster 中存在着若干的组别，其中比较关键的是机械组以及电控组，比较不关键的是视觉组。RoboMaster 作为一个机器人比赛，机器人的稳定性往往大于功能性，而在此基础之上，由于机器人的设计十分的复杂，加之以场上的频繁碰撞，即使是最坚固的机器人也面临着 Robust 的考验，因此比起让机器人开着自瞄在场上大杀四方，机器人能够活着走下赛场明显更为重要。&lt;/p&gt;
&lt;p&gt;很不幸，我们的队伍已经摆脱了机器人无法活着的难题了，因此压力有的时候会来到视觉组。&lt;/p&gt;
&lt;p&gt;给出视觉组的一个定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在 RoboMaster 比赛中，在基础的车辆搭建以及控制的基础之上，为了在比赛之中起到更好的效果，计算机视觉被在车辆上使用，而视觉组（一称算法组）便是在工控机上使用计算机视觉等方法在比赛实现一些效果的组别。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;目前来看，视觉组主要包括几大经典任务，如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自瞄：自瞄，也就是自动瞄准，是指视觉组通过程序获取相机图像，经过处理之后获得敌方车辆装甲板的信息（包括但不限于三维坐标、位姿、速度、击打所需的云台角度），并且将信息发送给电控，进而使得电控可以控制云台旋转而对车辆进行自动瞄准。&lt;/li&gt;
&lt;li&gt;能量机关激活：识别某一种具有特定特征的标靶，并且预测其运动状态，在远距离进行击打，假如击打成功就可以获得一定的增幅（详情见规则手册）。此过程因为机械延迟等原因，操作手很难直接手动操作进行击打，所以需要视觉进行识别并将信息发送给电控进行击打。&lt;/li&gt;
&lt;li&gt;哨兵导航：哨兵使用激光雷达对于比赛地图进行 SLAM 建图，进而通过导航技术在比赛场地中自动巡航，实现自动的导航/避障等功能。&lt;/li&gt;
&lt;li&gt;视觉兑矿：在比赛中，工程机器人被要求将矿石通过机械臂送进一个角度刁钻的矿仓中，这一过程仅凭操作手的操作，一方面难度较大，另一方面则耗时较多。视觉的工程自动兑矿旨在通过视觉方案对矿仓的位姿进行估计，实现更加快捷且准确的兑矿流程。&lt;/li&gt;
&lt;li&gt;雷达：雷达是RoboMaster比赛的特殊兵种，在赛场外的较高位置，通过识别敌对车辆在场地中的位置，为己方队员提供视野，并为敌方带来减益。视觉方案的雷达通过计算机视觉或激光雷达方案，对车辆进行识别、定位。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中部分的知识具有较高的学习成本，在完成了统一的基础培训之后，将通过任务分流，并进行专项的培训。&lt;/p&gt;
&lt;p&gt;目前计划中，视觉组的基础培训主要包括以下安排：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/timeline.2krugonqmx.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;格式&lt;/h2&gt;
&lt;p&gt;在了解如何写文章之前，要先了解标点符号，对于编程也是如此，C++与其它语言一样，都具有其特有的格式（虽然 C++所使用的格式也被广泛用在大量语言上），在这里需要重点说明。&lt;/p&gt;
&lt;p&gt;本段中会使用一些代码片段，你无需了解他们的含义，因为我们只需了解代码的格式，这对于代码的含义的改变至关重要。&lt;/p&gt;
&lt;p&gt;在 C++中，两个比较关键的标点符号是空格以及 &lt;code&gt;;&lt;/code&gt;，同时在使用 C++进行编程的时候，需要注意除了文本、注释等内容，一切符号均要使用英文的半角符号。&lt;/p&gt;
&lt;p&gt;其中空格起到了划分的作用，将两段字符隔开，这一点上和英语中的划分是一样的，所以并无数量限制，也就是说 &lt;code&gt;int a&lt;/code&gt; 与 &lt;code&gt;int      a&lt;/code&gt; 的含义是一样的，不会有任何的区别。同时，需要注意的是，回车在这其中可以起到和空格一样的作用。&lt;/p&gt;
&lt;p&gt;值得一提的是，诸如 &lt;code&gt;=&lt;/code&gt;、&lt;code&gt;+&lt;/code&gt; 、&lt;code&gt;-&lt;/code&gt;、&lt;code&gt;*&lt;/code&gt; 等符号同样具有划分的意义。&lt;/p&gt;
&lt;p&gt;而作为另一部分，&lt;code&gt;;&lt;/code&gt; 的使用则重要许多，&lt;code&gt;;&lt;/code&gt; 的唯一用法就是使用其分割不同的语句，也就是说两句话之间假如使用 &lt;code&gt;;&lt;/code&gt; 隔开，则意味着这是两句话而不是一个整体。&lt;/p&gt;
&lt;p&gt;另外需要介绍的是注释，注释的意思是，注释中的内容在程序编译（一种将代码变成可以跑起来的程序的步骤）以及运行的时候都不会被看到，但是在日常的编程中，这些内容是可视的，因此可以起到解释代码的作用。&lt;/p&gt;
&lt;p&gt;注释分为行注释 &lt;code&gt;// text&lt;/code&gt; 与段注释 &lt;code&gt;/* text */&lt;/code&gt;，以下给出示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// 这是行注释，这一行都可以作为注释，但是下一行不可以

/*这是段注释
所以只要被这两边括起来的内容都是注释
我在里面可以随意书写
这里也能写*/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;于是你能否理解，这两段代码的含义是一样的：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// 第一段代码

int a = 0;
std::cout&amp;#x3C;&amp;#x3C;a&amp;#x3C;&amp;#x3C;std::endl;

// 第二段代码

int           a =
0

;

std::cout&amp;#x3C;&amp;#x3C;
a  &amp;#x3C;&amp;#x3C;
   std::endl;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同时，还有一个需要提及的概念是代码块，代码块使用 &lt;code&gt;{}&lt;/code&gt; 表示，平行的代码块之间相互独立。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;{
	// 代码块1
	{
		// 代码块2，与代码块1相关
	}
}

{
	// 代码块3，与1和2均无关
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;变量&lt;/h2&gt;
&lt;p&gt;程序的本质就是对于数据的处理，这句话是我说的，但是多少有一点道理。&lt;/p&gt;
&lt;p&gt;一般来说我们粗略地区分程序，会认为程序分为两部分，也就是代码以及数据，其中代码也就是那些具备一定功能的工具，而数据则被存放在名为变量的容器中。&lt;/p&gt;
&lt;p&gt;那么首先我们需要做的事情是选取容器。这个事情也比较好理解，比如说比较基础的字符集合，可以使用 ASCII 码表示，如下：&lt;/p&gt;
&lt;p&gt;|二进制|十进制|十六进制|字符/缩写|解释|
|---|---|---|---|---|
| $00000000$ | $0$ | $00$ |NUL (NULL)|空字符|
| $00000001$ | $1$ | $01$ |SOH (Start Of Headling)|标题开始|
| $00000010$ | $2$ | $02$ |STX (Start Of Text)|正文开始|
| $00000011$ | $3$ | $03$ |ETX (End Of Text)|正文结束|
| $00000100$ | $4$ | $04$ |EOT (End Of Transmission)|传输结束|
| $00000101$ | $5$ | $05$ |ENQ (Enquiry)|请求|
| $00000110$ | $6$ | $06$ |ACK (Acknowledge)|回应/响应/收到通知|
| $00000111$ | $7$ | $07$ |BEL (Bell)|响铃|
| $00001000$ | $8$ | $08$ |BS (Backspace)|退格|
| $00001001$ | $9$ | $09$ |HT (Horizontal Tab)|水平制表符|
| $00001010$ | $10$ | $0\mathrm A$ |LF/NL (Line Feed/New Line)|换行键|
| $00001011$ | $11$ | $0 \mathrm B$ |VT (Vertical Tab)|垂直制表符|
| $00001100$ | $12$ | $0\mathrm C$ |FF/NP (Form Feed/New Page)|换页键|
| $00001101$ | $13$ | $0\mathrm D$ |CR (Carriage Return)|回车键|
| $00001110$ | $14$ | $0\mathrm E$ |SO (Shift Out)|不用切换|
| $00001111$ | $15$ | $0\mathrm F$ |SI (Shift In)|启用切换|
| $00010000$ | $16$ | $10$ |DLE (Data Link Escape)|数据链路转义|
| $00010001$ | $17$ | $11$ |DC 1/XON  (Device Control 1/Transmission On)|设备控制 1/传输开始|
| $00010010$ | $18$ | $12$ |DC 2 (Device Control 2)|设备控制 2|
| $00010011$ | $19$ | $13$ |DC 3/XOFF  (Device Control 3/Transmission Off)|设备控制 3/传输中断|
| $00010100$ | $20$ | $14$ |DC 4 (Device Control 4)|设备控制 4|
| $00010101$ | $21$ | $15$ |NAK (Negative Acknowledge)|无响应/非正常响应/拒绝接收|
| $00010110$ | $22$ | $16$ |SYN (Synchronous Idle)|同步空闲|
| $00010111$ | $23$ | $17$ |ETB (End of Transmission Block)|传输块结束/块传输终止|
| $00011000$ | $24$ | $18$ |CAN (Cancel)|取消|
| $00011001$ | $25$ | $19$ |EM (End of Medium)|已到介质末端/介质存储已满/介质中断|
| $00011010$ | $26$ | $1\mathrm A$ |SUB (Substitute)|替补/替换|
| $00011011$ | $27$ | $1\mathrm B$ |ESC (Escape)|逃离/取消|
| $00011100$ | $28$ | $1\mathrm C$ |FS (File Separator)|文件分割符|
| $00011101$ | $29$ | $1\mathrm D$ |GS (Group Separator)|组分隔符/分组符|
| $00011110$ | $30$ | $1\mathrm E$ |RS (Record Separator)|记录分离符|
| $00011111$ | $31$ | $1\mathrm F$ |US (Unit Separator)|单元分隔符|
| $00100000$ | $32$ | $20$ |(Space)|空格|
| $00100001$ | $33$ | $21$ |!||
| $00100010$ | $34$ | $22$ |&quot;||
| $00100011$ | $35$ | $23$ |#||
| $00100100$ | $36$ | $24$ |$||
| $00100101$ | $37$ | $25$ |%||
| $00100110$ | $38$ | $26$ |&amp;#x26;||
| $00100111$ | $39$ | $27$ |&apos;||
| $00101000$ | $40$ | $28$ |(||
| $00101001$ | $41$ | $29$ |)||
| $00101010$ | $42$ | $2\mathrm A$ |*||
| $00101011$ | $43$ | $2\mathrm B$ |+||
| $00101100$ | $44$ | $2\mathrm C$ |,||
| $00101101$ | $45$ | $2\mathrm D$ |-||
| $00101110$ | $46$ | $2\mathrm E$ |.||
| $00101111$ | $47$ | $2\mathrm F$ |/||
| $00110000$ | $48$ | $30$ |0||
| $00110001$ | $49$ | $31$ |1||
| $00110010$ | $50$ | $32$ |2||
| $00110011$ | $51$ |33| $3$ ||
| $00110100$ | $52$ | $34$ |4||
| $00110101$ | $53$ | $35$ |5||
| $00110110$ | $54$ | $36$ |6||
| $00110111$ | $55$ | $37$ |7||
| $00111000$ | $56$ | $38$ |8||
| $00111001$ | $57$ | $39$ |9||
| $00111010$ | $58$ | $3\mathrm A$ |:||
| $00111011$ | $59$ | $3\mathrm B$ |;||
| $00111100$ | $60$ | $3\mathrm C$ |&amp;#x3C;||
| $00111101$ | $61$ | $3\mathrm D$ |=||
| $00111110$ | $62$ | $3\mathrm E$ |&gt;||
| $00111111$ | $63$ | $3\mathrm F$ |?||
| $01000000$ | $64$ | $40$ |@||
| $01000001$ | $65$ | $41$ |A||
| $01000010$ | $66$ | $42$ |B||
| $01000011$ | $67$ | $43$ |C||
| $01000100$ | $68$ | $44$ |D||
| $01000101$ | $69$ | $45$ |E||
| $01000110$ | $70$ | $46$ |F||
| $01000111$ | $71$ | $47$ |G||
| $01001000$ | $72$ | $48$ |H||
| $01001001$ | $73$ | $49$ |I||
| $01001010$ | $74$ | $4\mathrm A$ |J||
| $01001011$ | $75$ | $4\mathrm B$ |K||
| $01001100$ | $76$ | $4\mathrm C$ |L||
| $01001101$ | $77$ | $4\mathrm D$ |M||
| $01001110$ | $78$ | $4\mathrm E$ |N||
| $01001111$ | $79$ | $4\mathrm F$ |O||
| $01010000$ | $80$ | $50$ |P||
| $01010001$ | $81$ | $51$ |Q||
| $01010010$ | $82$ | $52$ |R||
| $01010011$ | $83$ | $53$ |S||
| $01010100$ | $84$ | $54$ |T||
| $01010101$ | $85$ | $55$ |U||
| $01010110$ | $86$ | $56$ |V||
| $01010111$ | $87$ | $57$ |W||
| $01011000$ | $88$ | $58$ |X||
| $01011001$ | $89$ | $59$ |Y||
| $01011010$ | $90$ | $5\mathrm A$ |Z||
| $01011011$ | $91$ | $5\mathrm B$ |[||
| $01011100$ | $92$ | $5\mathrm C$ |||
| $01011101$ | $93$ | $5\mathrm D$ |]||
| $01011110$ | $94$ | $5\mathrm E$ |^||
| $01011111$ | $95$ | $5\mathrm F$ |_||
| $01100000$ | $96$ | $60$ |`||
| $01100001$ | $97$ | $61$ |a||
| $01100010$ | $98$ | $62$ |b||
| $01100011$ | $99$ | $63$ |c||
| $01100100$ | $100$ | $64$ |d||
| $01100101$ | $101$ | $65$ |e||
| $01100110$ | $102$ | $66$ |f||
| $01100111$ | $103$ | $67$ |g||
| $01101000$ | $104$ | $68$ |h||
| $01101001$ | $105$ | $69$ |i||
| $01101010$ | $106$ | $6\mathrm A$ |j||
| $01101011$ | $107$ | $6\mathrm B$ |k||
| $01101100$ | $108$ | $6\mathrm C$ |l||
| $01101101$ | $109$ | $6\mathrm D$ |m||
| $01101110$ | $110$ | $6\mathrm E$ |n||
| $01101111$ | $111$ | $6\mathrm F$ |o||
| $01110000$ | $112$ | $70$ |p||
| $01110001$ | $113$ | $71$ |q||
| $01110010$ | $114$ | $72$ |r||
| $01110011$ | $115$ | $73$ |s||
| $01110100$ | $116$ | $74$ |t||
| $01110101$ | $117$ | $75$ |u||
| $01110110$ | $118$ | $76$ |v||
| $01110111$ | $119$ | $77$ |w||
| $01111000$ | $120$ | $78$ |x||
| $01111001$ | $121$ | $79$ |y||
| $01111010$ | $122$ | $7\mathrm A$ |z||
| $01111011$ | $123$ | $7\mathrm B$ |{||
| $01111100$ | $124$ | $7\mathrm C$ ||||
| $01111101$ | $125$ | $7\mathrm D$ |}||
| $01111110$ | $126$ | $7\mathrm E$ |~||
| $01111111$ | $127$ | $7\mathrm F$ |DEL (Delete)|删除|&lt;/p&gt;
&lt;p&gt;这些 ASCII 码不需要背诵，但是不难理解这个 ASCII 码的集合只有 128 种。但是同理，我们不难发现，实际上的数字，比如说整数，本身的范围可以说是无限，在计算机领域，规定的整数范围（这里指 C++中的 int），则是从 $-2^{31}\sim 2^{31}-1$。从计算机的角度来说，八组 $01$ 组成一个字节，则 ASCII 码集合中的字符只需要一个字节，而整数则需要四个字节，尽管 ASCII 码构成了字符与数字的一一对应关系，使得通过数字也可以表示字符，但是假如说使用整数表示一个字符，还是会导致三个字节的空间浪费。&lt;/p&gt;
&lt;p&gt;这种浪费无疑是需要避免的，一种在计算机语言中常用的方法就是让编程者规定容器的种类（变量类型），将这个判断交给编程者。&lt;/p&gt;
&lt;p&gt;同时，假如说创建了一个容器（也就是变量），那么对于其他的也是存放这种类型的数据的容器，他们之间必须要有区分，这种区分通过为变量命名来实现。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// 整型，也就是整数
int a = 1;

// 单浮点数，小数
float b = 1.0F; // F 表示单浮点，但是不写也没事

// 双浮点数，小数
double c = 1.0; // 双浮点相较单浮点占用空间多但精度高

// 字符
char d = &apos;a&apos;; // 字符使用&apos;&apos;括起来，其中不能含有多个字符

// 布尔值
bool e = True; // 布尔值表示真或假

// 字符串
std::string f = &quot;hello world&quot;; // 字符串与前面不同，后续会讲解
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里需要注意的一共有两点：&lt;/p&gt;
&lt;p&gt;第一是命名规则，对于变量来说，明明需要满足以下规则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标识符可以包含字母、数字和下划线。&lt;/li&gt;
&lt;li&gt;标识符必须以字母或下划线开头，不能以数字开头。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是更多时候，在此基础之上，我们希望每一个变量的表意明确，就像 &lt;code&gt;sum&lt;/code&gt; 总会比 &lt;code&gt;a&lt;/code&gt; 让人看代码的时候便于理解代码的含义。这一系列的标准我们会在后面提及。&lt;/p&gt;
&lt;p&gt;第二是变量之间存在一种转换，分为显式转换以及隐式转换。&lt;/p&gt;
&lt;p&gt;其中显式转换主要通过以下格式进行 &lt;code&gt;value_name = (Type) value&lt;/code&gt;，这里面比较常见的操作是将字符以及其对应的 ASCII 码进行转换：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int a = (int)&apos;a&apos;; // a = 61
char b = (char)61; // b = &apos;a&apos; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;而隐式转换则是 C++自动实现的一种机制，约等于实现了一些默认的转换，这里给出一些例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;double a = 1; // 自动将整型转为双浮点
float b = 1.1; // 自动将双浮点转为单浮点
double c = 3 / 2; // 此时 c 等于 1.0，整数相除保留结果的整数位
double d = 3 / 2.0; // 此时 c 等于 1.5，整数与浮点数相除结果为浮点数
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;逻辑语句&lt;/h2&gt;
&lt;p&gt;变量与算法在程序中缺一不可，而逻辑语句就是算法的底层固件。&lt;/p&gt;
&lt;p&gt;我们通常使用逻辑语句进行程序的编写，实际上，基本上 C++全部的后面的特性都是建立在变量与逻辑语句的基础之上，只是对于一些功能进行了一些的拓展。&lt;/p&gt;
&lt;p&gt;首先在这里简要说明一下运算符，一般来说我们使用的运算符主要包含两种，分别是算数运算符以及逻辑运算符，其中算术运算符就像是大家之前在日常通常会使用的，诸如 &lt;code&gt;+-*/%&lt;/code&gt;，分别的含义是加减乘除以及取模；而逻辑运算符则是诸如大于小于之类的操作：&lt;/p&gt;
&lt;p&gt;|运算符|含义|
|---|---|
|&gt;|大于|
|&amp;#x3C;|小于|
|==|等于|
|!=|不等于|
|&gt;=|大于等于|
|&amp;#x3C;=|小于等于|
|!|非|
|&amp;#x26;&amp;#x26;|与|
||||或|&lt;/p&gt;
&lt;p&gt;一般来说算数运算符的返回值是一个数字，而逻辑运算符则是一个布尔值，但是在这里其实也没有必要完全分开这些概念，因为本质上，一个非零的数字就可以隐式转换为布尔值中的 &lt;code&gt;True&lt;/code&gt;，而零则被转换为 &lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;逻辑语句主要包含以下几种：&lt;/p&gt;
&lt;h3&gt;条件语句 - if&lt;/h3&gt;
&lt;p&gt;条件语句 &lt;code&gt;if&lt;/code&gt; 用于在满足给定条件时执行一段代码块。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;if (条件)
{
    // 如果条件成立，执行这里的代码
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int num = 10;
if (num &gt; 5)
{
    cout &amp;#x3C;&amp;#x3C; &quot;Number is greater than 5&quot; &amp;#x3C;&amp;#x3C; endl;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;条件语句 - if-else&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;if-else&lt;/code&gt; 语句在条件成立时执行一个代码块，否则执行另一个代码块。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;if (条件)
{
	// 如果条件成立，执行这里的代码
}
else
{
    // 如果条件不成立，执行这里的代码
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int num = 3;
if (num &gt; 5)
{
    cout &amp;#x3C;&amp;#x3C; &quot;Number is greater than 5&quot; &amp;#x3C;&amp;#x3C; endl;
}
else
{
    cout &amp;#x3C;&amp;#x3C; &quot;Number is not greater than 5&quot; &amp;#x3C;&amp;#x3C; endl;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;多重条件语句 - if-(else if)-else&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;if-(else if)-else&lt;/code&gt; 结构用于在多个条件之间做选择。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;if (条件 1)
{
    // 如果条件 1 成立，执行这里的代码
}
else if (条件 2)
{
    // 如果条件 2 成立，执行这里的代码
}
else
{
    // 如果以上条件都不成立，执行这里的代码
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int num = 7;
if (num &amp;#x3C; 5)
{
    cout &amp;#x3C;&amp;#x3C; &quot;Number is less than 5&quot; &amp;#x3C;&amp;#x3C; endl;
}
else if (num == 5)
{
    cout &amp;#x3C;&amp;#x3C; &quot;Number is equal to 5&quot; &amp;#x3C;&amp;#x3C; endl;
}
else
{
    cout &amp;#x3C;&amp;#x3C; &quot;Number is greater than 5&quot; &amp;#x3C;&amp;#x3C; endl;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;循环语句 - while&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;while&lt;/code&gt; 循环在满足条件时重复执行一段代码块。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;while (条件)
{
    // 只要条件成立，重复执行这里的代码
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int count = 0;
while (count &amp;#x3C; 5)
{
    cout &amp;#x3C;&amp;#x3C; &quot;Count: &quot; &amp;#x3C;&amp;#x3C; count &amp;#x3C;&amp;#x3C; endl;
    count++;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;循环语句 - for&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;for&lt;/code&gt; 循环用于指定初始值、终止条件和迭代步长，然后重复执行一段代码块。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;for (初始值; 终止条件; 迭代步长)
{
    // 在每次迭代中执行这里的代码
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;for (int i = 0; i &amp;#x3C; 5; i++)
{
    cout &amp;#x3C;&amp;#x3C; &quot;i: &quot; &amp;#x3C;&amp;#x3C; i &amp;#x3C;&amp;#x3C; endl;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;循环语句 - do-while&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;do-while&lt;/code&gt; 循环与 &lt;code&gt;while&lt;/code&gt; 循环类似，不同之处在于它会至少执行一次代码块，然后根据条件决定是否继续执行。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;do
{
    // 先执行一次这里的代码
} while (条件);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int num = 0;
do
{
    cout &amp;#x3C;&amp;#x3C; &quot;Num: &quot; &amp;#x3C;&amp;#x3C; num &amp;#x3C;&amp;#x3C; endl;
    num++;
} while (num &amp;#x3C; 5);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;break 与 continue&lt;/h3&gt;
&lt;p&gt;在 C++中，&lt;code&gt;break&lt;/code&gt; 和 &lt;code&gt;continue&lt;/code&gt; 是两种控制流程的关键字，用于在循环语句中改变程序的执行顺序。它们通常用于 &lt;code&gt;for&lt;/code&gt;、&lt;code&gt;while&lt;/code&gt;、&lt;code&gt;do-while&lt;/code&gt; 等循环语句中，以便在特定条件下跳出循环或跳过当前迭代。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;break&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;break&lt;/code&gt; 用于立即终止当前所在的循环，并跳出该循环，继续执行循环外的代码。它的主要作用是在满足某个条件时提前退出循环，从而避免不必要的迭代。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;#x3C;iostream&gt;

int main()
{
    for (int i = 1; i &amp;#x3C;= 5; ++i)
    {
        if (i == 3)
        {
            std::cout &amp;#x3C;&amp;#x3C; &quot;Breaking the loop at i = &quot; &amp;#x3C;&amp;#x3C; i &amp;#x3C;&amp;#x3C; std::endl;
            break;  // 当 i 等于 3 时，跳出循环
        }
        std::cout &amp;#x3C;&amp;#x3C; &quot;Current i: &quot; &amp;#x3C;&amp;#x3C; i &amp;#x3C;&amp;#x3C; std::endl;
    }

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Current i: 1
Current i: 2
Breaking the loop at i = 3
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;continue&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;continue&lt;/code&gt; 用于跳过当前循环中余下的代码，直接进入下一次迭代。它主要用于在循环中某些条件不满足时，跳过当前迭代，继续下一次迭代。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;#x3C;iostream&gt;

int main()
{
    for (int i = 1; i &amp;#x3C;= 5; ++i)
    {
        if (i == 3)
        {
            std::cout &amp;#x3C;&amp;#x3C; &quot;Skipping iteration at i = &quot; &amp;#x3C;&amp;#x3C; i &amp;#x3C;&amp;#x3C; std::endl;
            continue;  // 当 i 等于 3 时，跳过当前迭代
        }
        std::cout &amp;#x3C;&amp;#x3C; &quot;Current i: &quot; &amp;#x3C;&amp;#x3C; i &amp;#x3C;&amp;#x3C; std::endl;
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Current i: 1
Current i: 2
Skipping iteration at i = 3
Current i: 4
Current i: 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意：&lt;code&gt;break&lt;/code&gt; 和 &lt;code&gt;continue&lt;/code&gt; 只影响最内层的循环，如果嵌套了多个循环，它们只会作用于包含它们的最近的那个循环。&lt;/p&gt;
&lt;h2&gt;地址与指针&lt;/h2&gt;
&lt;h3&gt;地址与指针&lt;/h3&gt;
&lt;p&gt;在 C++中，地址是内存中的位置，每个变量都在内存中有一个唯一的地址。指针是一个变量，其存储的值是另一个变量的地址。通过指针，我们可以直接访问或修改其他变量的值。&lt;/p&gt;
&lt;h4&gt;定义指针&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int main()
{
    int num = 42;
    int *ptr; // 定义一个整型指针
    ptr = &amp;#x26;num; // 将ptr指向num的地址
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这个例子中，&lt;code&gt;ptr&lt;/code&gt; 是一个指向整数的指针，通过 &lt;code&gt;&amp;#x26;num&lt;/code&gt; 可以获取 &lt;code&gt;num&lt;/code&gt; 的地址，然后将这个地址赋值给 &lt;code&gt;ptr&lt;/code&gt;。&lt;/p&gt;
&lt;h4&gt;使用指针&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int main()
{
    int num = 42;
    int *ptr;
    ptr = &amp;#x26;num;
    
    // 通过指针访问变量的值
    cout &amp;#x3C;&amp;#x3C; &quot;Value of num: &quot; &amp;#x3C;&amp;#x3C; *ptr &amp;#x3C;&amp;#x3C; endl;
    
    // 修改变量的值
    *ptr = 100;
    cout &amp;#x3C;&amp;#x3C; &quot;Updated value of num: &quot; &amp;#x3C;&amp;#x3C; num &amp;#x3C;&amp;#x3C; endl;
    
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过 &lt;code&gt;*ptr&lt;/code&gt; 可以访问指针所指向的变量的值，同时，修改 &lt;code&gt;*ptr&lt;/code&gt; 的值也会影响到原始变量 &lt;code&gt;num&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;数组与指针&lt;/h3&gt;
&lt;p&gt;数组名可以被视为指向数组首元素的指针，这使得我们可以通过指针来遍历数组。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int main()
{
    int arr[5] = {1, 2, 3, 4, 5};
    int *ptr = arr; // 数组名作为指针使用
    
    for (int i = 0; i &amp;#x3C; 5; ++i)
    {
        cout &amp;#x3C;&amp;#x3C; *ptr &amp;#x3C;&amp;#x3C; &quot; &quot;;
        ptr++; // 移动指针到下一个元素
    }
    
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但同时需要解释一个概念：语法糖。&lt;/p&gt;
&lt;p&gt;语法糖（Syntactic Sugar）是编程语言中的一种特性，它指的是一些语法上的便利性或简化写法，虽然并没有引入新的功能，但却能让代码更易读、更方便编写。&lt;/p&gt;
&lt;p&gt;其中数组的定义便是使用了语法糖，通过定义了 &lt;code&gt;a[i] = *(a + i)&lt;/code&gt;，使得对于数组这一具有连续地址的数据结构拥有了更加便捷的访问方法。&lt;/p&gt;
&lt;h3&gt;引用&lt;/h3&gt;
&lt;p&gt;引用是 C++中的另一个重要概念，它允许我们使用变量的别名来操作该变量。引用在声明时没有自己的存储空间，它只是给已存在的变量创建了一个别名。引用一旦与变量绑定，就无法重新绑定到其他变量。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int x = 5;
int &amp;#x26;ref = x;  // ref是x的引用
ref = 10;  // 修改ref也会修改x的值
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;引用与指针的主要区别在于，引用必须在声明时被初始化，并且一旦初始化后不能再引用其他变量。&lt;/p&gt;
&lt;h3&gt;new 与 delete&lt;/h3&gt;
&lt;p&gt;C++提供了 &lt;code&gt;new&lt;/code&gt; 和 &lt;code&gt;delete&lt;/code&gt; 运算符来动态分配和释放内存，这对于在程序运行时创建变量和数据结构非常有用。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int main()
{
    int *ptr1 = new int; // 动态分配一个整数大小的内存
    *ptr1 = 10;
    
    cout &amp;#x3C;&amp;#x3C; &quot;Value: &quot; &amp;#x3C;&amp;#x3C; *ptr1 &amp;#x3C;&amp;#x3C; endl;
    
    delete ptr1; // 释放内存
	
	int *ptr2 = new int[10]; // 动态分配一个整数数组的内存
	
	delete[] ptr2; // 释放整数数组的内存
	
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但务必要注意，在不再需要动态分配的内存时，使用 &lt;code&gt;delete&lt;/code&gt; 将其释放，以防止内存泄漏。&lt;/p&gt;
&lt;h2&gt;函数&lt;/h2&gt;
&lt;p&gt;当我们在写程序的时候，我们有的时候会发现，一些功能会被我们反复使用，但是假如说我们每一次都重写这个功能，写在 &lt;code&gt;int main&lt;/code&gt; 中，则对于代码的可读性以及书写量都是一件不好的事情。&lt;/p&gt;
&lt;p&gt;一种想法是将这些重复使用的功能变成一个工具，也就是函数。&lt;/p&gt;
&lt;h3&gt;什么是函数？&lt;/h3&gt;
&lt;p&gt;函数是 C++编程中的基本构建块之一，用于执行特定任务或操作。它可以接受输入（参数）并返回输出（返回值）。函数有助于将代码分割为可重用和模块化的部分，从而使代码更易于理解和维护。&lt;/p&gt;
&lt;h3&gt;函数的声明与定义&lt;/h3&gt;
&lt;p&gt;在使用函数之前，需要先声明（declare）它。函数声明告诉编译器函数的名称、参数类型和返回类型。函数定义（define）则提供了函数的实际实现。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// 函数声明
返回类型 函数名(参数类型 参数名);

// 函数定义
返回类型 函数名(参数类型 参数名)
{
    // 函数实现
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假如没有函数的声明，函数的定义既是定义也是声明，但是不能只有声明没有定义，会出现编译错误。&lt;/p&gt;
&lt;h3&gt;函数的参数与返回值&lt;/h3&gt;
&lt;h4&gt;参数&lt;/h4&gt;
&lt;p&gt;函数可以接受零个或多个参数，参数在函数声明和定义中指定。参数允许你向函数传递数据。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int add(int a, int b)
{
    return a + b;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;返回值&lt;/h4&gt;
&lt;p&gt;函数可以返回一个值，用于向调用者提供计算结果。返回值的类型在函数声明和定义中指定。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;double divide(double numerator, double denominator)
{
    return numerator / denominator;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于已经定义了返回值的函数，该函数必须在 &lt;code&gt;return &lt;/code&gt; 中给出返回值，同时，存在一种返回值 &lt;code&gt;void&lt;/code&gt; 意为无返回值，可以不写返回值 &lt;code&gt;return&lt;/code&gt;，其等价于编译器在函数结尾自动补充 &lt;code&gt;return;&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;形参与实参&lt;/h3&gt;
&lt;p&gt;实际上，在函数中，存在形参与实参这一概念，意思是形式参数与实际参数。以下给出一个经典的例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;void swap(int a, int b)
{
	int temp = a;
	a = b;
	b = temp;
	return;
}

int main()
{
	int x = 10;
	int y = 20;
	cout &amp;#x3C;&amp;#x3C; x &amp;#x3C;&amp;#x3C; &quot; &quot; &amp;#x3C;&amp;#x3C; y &amp;#x3C;&amp;#x3C; endl;
	swap(x, y);
	cout &amp;#x3C;&amp;#x3C; x &amp;#x3C;&amp;#x3C; &quot; &quot; &amp;#x3C;&amp;#x3C; y &amp;#x3C;&amp;#x3C; endl;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行以上的程序之后，发现 &lt;code&gt;x&lt;/code&gt; 与 &lt;code&gt;y&lt;/code&gt; 的值并没有变化，这就是因为此时 &lt;code&gt;swap&lt;/code&gt; 传入的变量，其本质上意思是：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int main()
{
	int x = 10;
	int y = 20;
	cout &amp;#x3C;&amp;#x3C; x &amp;#x3C;&amp;#x3C; &quot; &quot; &amp;#x3C;&amp;#x3C; y &amp;#x3C;&amp;#x3C; endl;
	{
		int a = x;
		int b = y;
		int temp = a;
		a = b;
		b = temp;
	}
	cout &amp;#x3C;&amp;#x3C; x &amp;#x3C;&amp;#x3C; &quot; &quot; &amp;#x3C;&amp;#x3C; y &amp;#x3C;&amp;#x3C; endl;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这也就是为什么 &lt;code&gt;x&lt;/code&gt; 与 &lt;code&gt;y&lt;/code&gt; 的值均没有改变，这是因为本质的传参出现了问题。&lt;/p&gt;
&lt;p&gt;所以根据我们之前学习的指针与引用，我们得到了两种可以修改传入变量的方法：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// 通过指针
void swap(int* a, int* b)
{
	int temp = *a;
	*a = *b;
	*b = temp;
	return;
}
// 通过引用
void swap(int&amp;#x26; a, int&amp;#x26; b)
{
	int temp = a;
	a = b;
	b = temp;
	return;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;具体的解释可以如上方一样将函数本身展开到 &lt;code&gt;main&lt;/code&gt; 函数中，就易于理解了。&lt;/p&gt;
&lt;h3&gt;调用函数&lt;/h3&gt;
&lt;p&gt;要使用函数，需要在代码中调用它。函数调用通过提供参数值来触发函数的执行，并且可以使用返回值。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int sum = add(5, 3);
double result = divide(10.0, 2.0);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;函数重载&lt;/h3&gt;
&lt;p&gt;C++允许你定义具有相同名称但不同参数列表的多个函数，这称为函数重载。编译器根据提供的参数类型和数量来确定要调用的函数。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int square(int x)
{
    return x * x;
}

double square(double x)
{
    return x * x;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;默认参数&lt;/h3&gt;
&lt;p&gt;函数参数可以有默认值，这使得在调用函数时可以省略这些参数。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int power(int base, int exponent = 2)
{
    int result = 1;
    for (int i = 0; i &amp;#x3C; exponent; ++i)
    {
        result *= base;
    }
    return result;
}

int main()
{
    int square_result = power(5);       // 默认使用指数为2
    int cube_result = power(2, 3);      // 指定指数为3
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;函数返回多个值&lt;/h3&gt;
&lt;p&gt;尽管函数只能返回一个值，但可以通过引用或指针参数实现返回多个值的效果。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;void minMax(int arr[], int size, int&amp;#x26; minValue, int&amp;#x26; maxValue)
{
    minValue = maxValue = arr[0];
    for (int i = 1; i &amp;#x3C; size; ++i)
    {
        if (arr[i] &amp;#x3C; minValue)
        {
            minValue = arr[i];
        }
        if (arr[i] &gt; maxValue)
        {
            maxValue = arr[i];
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;局部变量与作用域&lt;/h3&gt;
&lt;p&gt;函数内部声明的变量称为局部变量，它们只在函数内部可见。局部变量在函数调用结束后会被销毁。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int multiply(int x, int y)
{
    int result = x * y;  // result是局部变量
    return result;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;结构体&lt;/h2&gt;
&lt;p&gt;在 C++中，我们基础使用的数据结构只有诸如 &lt;code&gt;int&lt;/code&gt;、&lt;code&gt;float&lt;/code&gt;、&lt;code&gt;double&lt;/code&gt; 等表述正常内容的数据内容，但是假如说我们想要统计一系列同学的身高体重，进而计算这些同学的 BMI 指数，一种想法是设置两个数组：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int main()
{
	double num = 0;
	cin &gt;&gt; num;
	double* height = new double[num];
	double* weight = new double[num];
	for(int i = 0; i &amp;#x3C; num; i++)
		cin &gt;&gt; height[i] &gt;&gt; weight[i];
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是这种写法并不优美，于是一种想法是，我们能否创建一种变量类型来专门储存学生的身高体重以及 BMI 指数，也就是一种可以储存三个值的变量，实际上我们确实可以这么做，这种被我们人为创建的变量类型被称为结构体。&lt;/p&gt;
&lt;p&gt;在 C++中，结构体（struct）是一种用于组合不同数据类型的用户自定义数据类型。它允许你将多个不同的变量打包成一个单一的数据结构，从而方便地管理和操作这些数据。&lt;/p&gt;
&lt;h3&gt;定义结构体&lt;/h3&gt;
&lt;p&gt;结构体通过定义一个新的数据类型来表示，其中可以包含多个不同的数据成员。定义结构体的方式如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct Person
{
    std::string name;
    int age;
    double height;
}; // 注意这里的分号
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在上面的示例中，我们定义了一个名为 &lt;code&gt;Person&lt;/code&gt; 的结构体，其中包含了 &lt;code&gt;name&lt;/code&gt;、&lt;code&gt;age&lt;/code&gt; 和 &lt;code&gt;height&lt;/code&gt; 三个不同类型的&lt;strong&gt;成员变量&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;创建结构体对象并访问结构体成员&lt;/h3&gt;
&lt;p&gt;可以使用结构体定义的数据类型来创建结构体对象，就像创建基本数据类型的变量一样，同时，可以通过 &lt;code&gt;.&lt;/code&gt; 来访问结构体内部的数据：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Person person1;  // 创建一个Person结构体对象
person1.name = &quot;Alice&quot;;
person1.age = 25;
person1.height = 165.5;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这种访问除了赋值当然也可以输出。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;std::cout &amp;#x3C;&amp;#x3C; &quot;Name: &quot; &amp;#x3C;&amp;#x3C; person1.name &amp;#x3C;&amp;#x3C; std::endl;
std::cout &amp;#x3C;&amp;#x3C; &quot;Age: &quot; &amp;#x3C;&amp;#x3C; person1.age &amp;#x3C;&amp;#x3C; std::endl;
std::cout &amp;#x3C;&amp;#x3C; &quot;Height: &quot; &amp;#x3C;&amp;#x3C; person1.height &amp;#x3C;&amp;#x3C; std::endl;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;结构体作为函数参数&lt;/h3&gt;
&lt;p&gt;结构体可以作为函数的参数传递，从而方便地将多个相关数据一起传递给函数：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;void printPerson(const Person&amp;#x26; person)
{
    std::cout &amp;#x3C;&amp;#x3C; &quot;Name: &quot; &amp;#x3C;&amp;#x3C; person.name &amp;#x3C;&amp;#x3C; std::endl;
    std::cout &amp;#x3C;&amp;#x3C; &quot;Age: &quot; &amp;#x3C;&amp;#x3C; person.age &amp;#x3C;&amp;#x3C; std::endl;
    std::cout &amp;#x3C;&amp;#x3C; &quot;Height: &quot; &amp;#x3C;&amp;#x3C; person.height &amp;#x3C;&amp;#x3C; std::endl;
}

int main()
{
    Person person2 = {&quot;Bob&quot;, 30, 180.0};
    printPerson(person2);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;结构体初始化&lt;/h3&gt;
&lt;p&gt;可以使用初始化列表来初始化结构体对象：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Person person3 = {&quot;Charlie&quot;, 22, 170.0};
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;结构体嵌套&lt;/h3&gt;
&lt;p&gt;结构体可以嵌套在其他结构体中，从而构建更复杂的数据结构：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct Address
{
    std::string street;
    std::string city;
};

struct Contact
{
    std::string name;
    Address address;
    std::string phone;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;结构体指针&lt;/h3&gt;
&lt;p&gt;同样，正如正常的数据结构可以使用指针，我们人为创建的结构体也可以使用指针。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Person* personPtr = &amp;#x26;person;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 C++中，通过使用结构体的指针来访问其成员时，可以使用箭头操作符（&lt;code&gt;-&gt;&lt;/code&gt;）来简化操作。这种语法糖使得通过指针访问成员的代码更加清晰和简洁。&lt;/p&gt;
&lt;p&gt;如果有一个指向 &lt;code&gt;Person&lt;/code&gt; 结构体的指针，假设命名为 &lt;code&gt;personPtr&lt;/code&gt;，要访问 &lt;code&gt;name&lt;/code&gt; 成员，可以使用以下两种方式：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;(*personPtr).name;  // 使用括号和点号
personPtr-&gt;name;    // 使用箭头操作符
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里，&lt;code&gt;(*personPtr).name&lt;/code&gt; 表示先解引用 &lt;code&gt;personPtr&lt;/code&gt; 指针，然后使用点号访问 &lt;code&gt;name&lt;/code&gt; 成员，而 &lt;code&gt;personPtr-&gt;name&lt;/code&gt; 使用箭头操作符直接访问了 &lt;code&gt;name&lt;/code&gt; 成员。&lt;/p&gt;
&lt;p&gt;因此，&lt;code&gt;personPtr-&gt;name&lt;/code&gt; 是对 &lt;code&gt;(*personPtr).name&lt;/code&gt; 的一种更简洁的表达方式，它更易读、易懂，并且在处理指向结构体的指针时更方便。&lt;/p&gt;
&lt;h2&gt;类&lt;/h2&gt;
&lt;h3&gt;什么是类？&lt;/h3&gt;
&lt;p&gt;在 C++中，类（class）是一种用户自定义的数据类型，它允许你将数据成员和成员函数组合在一起，形成一个单一的实体，以便更好地表示现实世界中的对象。类提供了一种创建自己的数据结构，以及定义操作这些数据的方法。&lt;/p&gt;
&lt;h3&gt;定义类&lt;/h3&gt;
&lt;p&gt;定义类的方式如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class ClassName
{
public:
    // 成员函数和成员变量声明
private:
    // 私有成员声明
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;public&lt;/code&gt;、&lt;code&gt;private&lt;/code&gt; 等是访问控制关键字，用于定义成员的可访问性。&lt;/p&gt;
&lt;h3&gt;成员函数和成员变量&lt;/h3&gt;
&lt;p&gt;类可以包含成员函数和成员变量。成员函数是在类中定义的函数，它们用于操作类的数据成员。成员变量是类的数据成员，用于存储对象的状态信息。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class Circle
{
public:
    double radius;  // 成员变量

    double calculateArea() // 成员函数
    {
        return 3.14 * radius * radius;
    }
};
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;创建对象&lt;/h3&gt;
&lt;p&gt;可以使用类定义的数据类型来创建对象，就像创建基本数据类型的变量一样：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Circle myCircle;  // 创建Circle类的对象
myCircle.radius = 5.0;  // 访问成员变量
double area = myCircle.calculateArea();  // 调用成员函数
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;构造函数和析构函数&lt;/h3&gt;
&lt;p&gt;构造函数在创建对象时自动调用，用于初始化对象的数据成员。析构函数在对象被销毁时自动调用，用于释放资源。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class Book
{
public:
    std::string title;

    Book(const std::string &amp;#x26;t) // 构造函数
    {
        title = t;
        std::cout &amp;#x3C;&amp;#x3C; &quot;Book &quot; &amp;#x3C;&amp;#x3C; title &amp;#x3C;&amp;#x3C; &quot; is created.&quot; &amp;#x3C;&amp;#x3C; std::endl;
    }

    ~Book()
    {  // 析构函数
        std::cout &amp;#x3C;&amp;#x3C; &quot;Book &quot; &amp;#x3C;&amp;#x3C; title &amp;#x3C;&amp;#x3C; &quot; is destroyed.&quot; &amp;#x3C;&amp;#x3C; std::endl;
    }
};
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;访问控制&lt;/h3&gt;
&lt;p&gt;C++中的访问控制关键字 &lt;code&gt;public&lt;/code&gt;、&lt;code&gt;private&lt;/code&gt; 和 &lt;code&gt;protected&lt;/code&gt; 用于控制类成员的可访问性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;public&lt;/code&gt; 成员可以在类的外部访问。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;private&lt;/code&gt; 成员只能在类的内部访问。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;protected&lt;/code&gt; 成员类似于 &lt;code&gt;private&lt;/code&gt;，但派生类可以访问。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;类的声明和定义分离&lt;/h3&gt;
&lt;p&gt;通常，类的声明（包含成员函数和成员变量的声明）会放在头文件（. h 或 .hpp），而类的定义（成员函数的实现）会放在源文件（. cpp）中。&lt;/p&gt;
&lt;p&gt;其中，对于成员函数来说，其实现的写法为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Typename Classname::Function(/*v*/)
{
	// code
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;初始化列表&lt;/h3&gt;
&lt;p&gt;虽然类的声明以及定义可以分离，但是对于一些类中的成员来说，其必须需要一个初始值，但有的初始值在 &lt;code&gt;.hpp&lt;/code&gt; 中无法赋值（如初始值是某一函数的返回值，&lt;code&gt;.hpp&lt;/code&gt; 并不具备执行函数的能力），于是需要在构造函数中赋值，但是又因为构造函数开始时一切成员变量均已经创建完毕，于是会导致报错。&lt;/p&gt;
&lt;p&gt;所以需要一种方法，在声明与定义分离的情况下，起到等效于直接在 &lt;code&gt;.hpp&lt;/code&gt; 中赋值的效果，这种写法就是初始化列表。&lt;/p&gt;
&lt;p&gt;在 C++中，初始化列表形式的构造函数是一种特殊类型的构造函数，用于在创建对象时对成员变量进行初始化。它在构造函数的参数列表之后使用冒号来定义，用于显式地指定成员变量的初始值。&lt;/p&gt;
&lt;p&gt;初始化列表构造函数可以帮助避免使用构造函数体内的赋值操作，从而提高代码效率并减少可能的错误。&lt;/p&gt;
&lt;p&gt;以下是一个示例，展示了如何使用初始化列表形式的构造函数：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class Person
{
public:
    // 初始化列表形式的构造函数
    Person(const std::string &amp;#x26;n, int a) : name(n), age(a)
    {
        // 构造函数体内没有赋值操作
    }

private:
    std::string name;
    int age;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这个示例中，构造函数的初始化列表 &lt;code&gt;: name(n), age(a)&lt;/code&gt; 指定了成员变量 &lt;code&gt;name&lt;/code&gt; 和 &lt;code&gt;age&lt;/code&gt; 的初始值。使用初始化列表的好处是，它可以直接将初始值赋值给成员变量，而不需要在构造函数体内执行赋值操作。&lt;/p&gt;
&lt;p&gt;初始化列表还可以用于初始化常量成员、引用成员和调用基类构造函数等情况。&lt;/p&gt;
&lt;h3&gt;this 指针&lt;/h3&gt;
&lt;p&gt;在 C++中，&lt;code&gt;this&lt;/code&gt; 是一个特殊的指针，它指向当前对象的实例。它被用来在类的成员函数中引用调用该函数的对象本身。&lt;code&gt;this&lt;/code&gt; 指针的存在使得在类的成员函数中能够准确地访问到调用该函数的对象的成员变量和成员函数，尤其在存在同名的局部变量和成员变量时，它能够帮助解决歧义问题。&lt;/p&gt;
&lt;p&gt;比如说在以上 &lt;code&gt;Person&lt;/code&gt; 类中，创建构造函数：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Person::Person(std::string name, int age)
{
	name = name;
	age = age;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时函数出现了歧义，因为类中已经有名为 &lt;code&gt;name&lt;/code&gt; 与 &lt;code&gt;age&lt;/code&gt; 的变量，但是输入的参数中也有名为 &lt;code&gt;name&lt;/code&gt; 与 &lt;code&gt;age&lt;/code&gt; 的变量，此时严格来说，因为作用域问题，这里面的 &lt;code&gt;name&lt;/code&gt; 均代表输入的变量，于是带来了表意不明。&lt;/p&gt;
&lt;p&gt;此时我们可以如下写：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Person::Person(std::string name, int age)
{
	this-&gt;name = name;
	this-&gt;age = age;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时构造函数中的两个左值便准确地指向了类中的成员变量，而非构造函数的输入值。&lt;/p&gt;
&lt;p&gt;也就是说，this 指针具备以下的特性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;隐式使用：&lt;/strong&gt; 当你在类的成员函数内部使用成员变量或成员函数时，编译器会自动地插入 &lt;code&gt;this-&gt;&lt;/code&gt;，即使你没有显式地写出它。例如，&lt;code&gt;this-&gt;someVariable&lt;/code&gt; 就是隐式使用 &lt;code&gt;this&lt;/code&gt; 指针来访问成员变量 &lt;code&gt;someVariable&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;显式使用：&lt;/strong&gt; 在需要显式指明当前对象时，可以使用 &lt;code&gt;this&lt;/code&gt; 指针。比如，你可以在成员函数内部返回当前对象本身，例如 &lt;code&gt;return *this;&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;解决歧义：&lt;/strong&gt; 当成员函数的参数名与类的成员变量同名时，使用 &lt;code&gt;this&lt;/code&gt; 指针可以帮助解决歧义，明确地指出你想要使用成员变量而不是参数。例如：&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class Example
{
private:
    int value;

public:
    void setValue(int value)
    {
        this-&gt;value = value; // 使用 this 指针明确访问成员变量
    }
};
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;&lt;strong&gt;静态成员函数：&lt;/strong&gt; 在静态成员函数中，由于没有当前对象的实例，所以不能使用 &lt;code&gt;this&lt;/code&gt; 指针。静态成员函数是与类本身相关联，而不是与具体对象相关联的。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class Example
{
public:
    static void staticFunction()
    {
        // 无法使用 this 指针
    }
};
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;封装、继承与多态&lt;/h2&gt;
&lt;p&gt;封装/继承/多态是 C++ 面向对象编程的三大核心，在这里进行简短的介绍。&lt;/p&gt;
&lt;h3&gt;封装&lt;/h3&gt;
&lt;p&gt;封装是 C++面向对象思想中最重要的一个思想。&lt;/p&gt;
&lt;p&gt;对于类来说，或者说对象，我们对其的一个共识是，其是一个独立的个体。在程序流程中，我们往往仅关心对象在获得了输入之后能否得到我们期望的输出，于是需要我们设置为 &lt;code&gt;public&lt;/code&gt; 的函数以及值并没有那么多。&lt;/p&gt;
&lt;p&gt;实际上，假如暴露过多的函数接口在外部，反而会给另一位这个类的使用者（没有参与编写）以困惑，而且随意的调用往往意味着不安全。&lt;/p&gt;
&lt;p&gt;于是就体现到了封装的思想，也就是仅暴露需要使用的接口，并且不暴露一切的变量，对于需要访问的变量来说，则使用诸如以下的写法实现：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;std::string Person::getName()
{
	return this-&gt;name;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这种写法可以确保对于外界来说，大多数的内容是只读的。&lt;/p&gt;
&lt;p&gt;进行合理封装的类会体现为其仅包含必要的接口，因此对于一个非开发者使用该类的时候，仅需要注意对象的每个方法其传参与效果即可，不需要在意类对于功能内部实现的逻辑。&lt;/p&gt;
&lt;h3&gt;继承&lt;/h3&gt;
&lt;h4&gt;成员属性&lt;/h4&gt;
&lt;p&gt;对于对象中的变量以及方法，具有其自身的属性，决定了其调用的访问等级，分别为 &lt;code&gt;public&lt;/code&gt;、&lt;code&gt;protected&lt;/code&gt; 以及 &lt;code&gt;private&lt;/code&gt;，分别意味着在类内外都可以访问、只能在类内访问且不继承给子类以及只能在类内访问但是可以继承给子类。值得一提的是，不进行声明，类中的成员属性均为 &lt;code&gt;private&lt;/code&gt;。&lt;/p&gt;
&lt;h4&gt;父与子&lt;/h4&gt;
&lt;p&gt;继承作为一种面向对象的高级用法，其更好的描述了面向对象对于事物抽象描述并且加以定义的流程，其中继承的语法为 &lt;code&gt;class Son : 继承属性 father&lt;/code&gt;，实现继承操作的类被称为子类或者派生类，而被继承的则被称为父类或者基类。&lt;/p&gt;
&lt;p&gt;其中继承属性指 &lt;code&gt;public&lt;/code&gt;、&lt;code&gt;protected&lt;/code&gt; 以及 &lt;code&gt;private&lt;/code&gt;，意味着将父类中继承的比当前级别更松内容放到哪个级别中，也就是说 &lt;code&gt;public&lt;/code&gt; 会将 public 内容放入 public，protected 内容放入 protected，&lt;code&gt;protected&lt;/code&gt; 会将 public 和 protected 内容放入 protected，而 &lt;code&gt;private&lt;/code&gt; 会将 public 以及 protected 内容放入 private，给出一个实例：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// 定义一个类，人，其必然拥有一些人具有的属性，如下
class Person
{
public:
	int age;
	int height;
	int weight;
	string name;
};
// 定义一个类，男性，其继承自 Person，也就是说其具备一切人具备的特征，同时还有一些作为男性的特征，比如说自己是一名男性
class man : public Person
{
public:
	void speak ()
	{
		cout &amp;#x3C;&amp;#x3C; &quot;I&apos;m a man, my age is&quot; &amp;#x3C;&amp;#x3C; this-&gt;age; // this 指针指向当前的类，使用-&gt;符号，后面填写当前类中的成员或者方法，进行调用
	}
};
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;多态&lt;/h3&gt;
&lt;p&gt;多态是 C++乃至大多数面向对象的程序语言都拥有的一个特性，可以用来增加程序的拓展性，更加灵活的编写程序。&lt;/p&gt;
&lt;p&gt;简单讲解一下一个最为基本的多态的使用场景：假如说有以下一个类，Animal，其提供一种方法，叫做 speak，会输出「动物 speak」，而 Animal 是 Cat 以及 Dog 两个类的父类，而我们希望 Cat 以及 Dog 类各自实现一种 speak 的方法，分别输出「猫 speak」以及「狗 speak」。假如说有这样的一个场景，希望其中输入一个动物，然后调用其 speak 方法，一种较为复杂的方法是依次实现参数列表中为 Cat 以及 Dog 的方法，进行函数的重载，但是还有另一种解决方案，如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;class Animal
{
public:
    void speak ()
    {
        cout &amp;#x3C;&amp;#x3C; &quot;Animal Speak&quot;;
    }
};

class Cat : public Animal
{
public:
    void speak ()
    {
        cout &amp;#x3C;&amp;#x3C; &quot;Cat Speak&quot;;
    }
};

class Dog : public Animal
{
public:
    void speak ()
    {
        cout &amp;#x3C;&amp;#x3C; &quot;Dog Speak&quot;;
    }
};

void doSpeak (Animal &amp;#x26;animal)
{
    animal.speak ();
}

int main ()
{
    Cat c;
    doSpeak (c);
    system (&quot;pause&quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不难看出，这个程序的执行会将 doSpeak 函数中传入的 Cat 类当作 Animal 类并调用其 speak 方法，这样做的底气在于，因为 Cat 是 Animal 的子类，所以 Cat 中必然包含 Animal 的方法，但是这样做，因为其&lt;strong&gt;静态多态函数地址早绑定&lt;/strong&gt;的原因，所以只会输出 Animal Speak，但是可以预见的是，假如说我们预想的，因为 Cat 中重新写了相关的 Speak 函数，假如说有一种方法可以调用子类的方法，而不是父类的方法，必然可以解决我们的需求，而且让整体的程序十分的简单。&lt;/p&gt;
&lt;h3&gt;VOB&lt;/h3&gt;
&lt;p&gt;VOB 是多态的常见元素，一般来说多态一定会有这三个元素，来达成其多态的效果，而因为这其中的一些硬性的关键字等，主要出现在其他语言，以及 C++更加新的标准中，在 C++98 等中或许没有，但是其依然作为一种概念，规范着多态程序的书写。&lt;/p&gt;
&lt;p&gt;VOB，也就是虚函数 (virtual)、重写 (override)以及父类 (base)，是多态实现的三要素。&lt;/p&gt;
&lt;p&gt;首先是 virtual 关键字，对于父类中的方法，添加了 virtual 关键字之后，会将其由本来的函数转化为一种函数指针，之后就可以实现，在调用的时候链接到子类之上。&lt;/p&gt;
&lt;p&gt;对于子类中的方法，既然要进行多态操作，也就是要进行完全的对于本来方法的覆盖。不同于函数重载中，对于参数列表的不同，重写的要求更为极端，要求一切与原函数完全一致 (对于协变来说并不是如此，但是因为不在考核范围之类，请对其感兴趣的同学自行了解)，对于一些语言，在子类的重写函数之前需要添加 &lt;code&gt;override&lt;/code&gt; 关键字，而 c++11 的特性中也添加了 &lt;code&gt;override&lt;/code&gt; 关键字，作为对于程序的规范，不过这都不在考虑范围内，override 这个单词本身并不必须，但是可以提醒我们对于重写这一点严格的遵守。&lt;/p&gt;
&lt;p&gt;最后是 base，这一点在诸如 &lt;code&gt;C#&lt;/code&gt; 等语言可以调用父类中本来应该被重写掉的函数，但是在 C++中这一点并没有实现，所以这里的 B，只是为了提醒我们其代表着当前子类与父类的某种覆盖关系。&lt;/p&gt;
&lt;h3&gt;虚、纯虚与抽象&lt;/h3&gt;
&lt;p&gt;在一些项目的架构中，以及一些设计中，诸如上面的 Animal 案例，虽然我们已经使用了虚函数，对其进行了改进，但是事实上，并不存在一种没有准确名字的动物，可以用到输出的「Animal Speak」，也就是说，在某种程度上，虽然有这一句话没有问题，但是假如程序真正说出了「Animal Speak」，却恰恰意味着程序出了问题，所以对于一些更为「极端」的设计，当然，也是为了保证程序正常运行没有疏漏的常规操作，存在这样一种函数，其本质上完全没有任何的实现，所以假如不是通过虚函数链接到了别的函数，而是其本身直接执行，就会报错，甚至在编译阶段，编译器就会给出报错，这种函数就叫做&lt;strong&gt;纯虚函数&lt;/strong&gt;，而包含了纯虚函数的类被称为&lt;strong&gt;抽象类&lt;/strong&gt;，因为其中有一些方法是尚未被实现的，所以&lt;strong&gt;不能被实例化&lt;/strong&gt;，而是只是作为一种程序框架中的抽象的概念而存在。&lt;/p&gt;
&lt;p&gt;纯虚函数的写法是，不像一般的具有 &lt;code&gt;virtual&lt;/code&gt; 的函数一样进行实现，而是写如 &lt;code&gt;virtual void speak () = 0;&lt;/code&gt;，这样就是一个纯虚函数了。&lt;/p&gt;
&lt;p&gt;给出一个完善的使用纯虚函数写的上述 Animal 案例供参考：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;class Animal
{
public:
    virtual void speak () = 0;
};
class Cat : public Animal
{
public:
    void speak ()
    {
        cout &amp;#x3C;&amp;#x3C; &quot;Cat Speak&quot;;
    }
};
class Dog : public Animal
{
};
void doSpeak (Animal &amp;#x26;animal)
{
    animal.speak ();
}
int main ()
{
    Cat c;
    // Dog d; 不能被执行，因为 Dog 没有实现 speak 方法，所以为抽象类，不能被实例化
    doSpeak (c);
    system (&quot;pause&quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;这一节主要讲解了 C++ 中的一些基础，以及类、继承、多态以及虚函数等，这些内容是 C++ 中最为基础的部分，也是 C++ 中最为核心的部分，希望读者能够理解并掌握。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/RM-Tutorial-Section-1-zh.webp"/><enclosure url="https://picr2.axi404.top/RM-Tutorial-Section-1-zh.webp"/></item><item><title>周记 Week1</title><link>https://axi404.github.io/blog/week-01</link><guid isPermaLink="true">https://axi404.github.io/blog/week-01</guid><description>2024-07-01 ~ 2024-07-07.</description><pubDate>Mon, 08 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;之所以要写周记，大概是记录一下过去的这一个星期内我都做了什么，这是一种很好的总结，你可以较为直观的对我的成果进行回顾。&lt;/p&gt;
&lt;p&gt;粗略来算的话，这应该是我建立这个博客的第三个星期，也会是我坚持写周记的第一个星期，简单记录一下生活吧，这样在将来回忆的时候也能有所凭证。&lt;/p&gt;
&lt;p&gt;虽然说名字叫做周记，但是事实上有的内容可能并不是上个星期发生的事情，随心所欲地写，有感而发地说，大抵是这样的主旨。&lt;/p&gt;
&lt;p&gt;总的来说，在建立这个博客之前，我也有维护过另一个博客，当时还傻乎乎的租了一个服务器，搭建了一个 Wordpress。但是最后发现，Wordpress一是性能不行，二是其使用的富文本编辑，对于经常使用 Markdown 的我来说，并不是十分的习惯。最后本来从勤快地更新技术类博客，到写周记，然后干脆把周记的名字直接改成月记，最后连月记都懒得写。说起来大概也是当时对于新知识的追求懈怠了的原因吧，希望这次可以打起精神，避免重蹈覆辙。&lt;/p&gt;
&lt;p&gt;回顾大学的两年时间，可以说这一周也是经历了令我印象深刻的事件了。&lt;/p&gt;
&lt;h2&gt;日常生活&lt;/h2&gt;
&lt;p&gt;首先简单回顾一下日常生活，最近和女朋友在一起的时光还是十分的快乐。&lt;/p&gt;
&lt;h3&gt;Lolita&lt;/h3&gt;
&lt;p&gt;女朋友最近入坑 Lolita 了，说起来我之前早期的服装里面也有一件是 Lolita，但是当时因为没有裙撑而惨淡收场，或许改天可以再尝试一下。&lt;/p&gt;
&lt;h3&gt;巴奴火锅&lt;/h3&gt;
&lt;p&gt;最近两个人尝试了巴奴火锅，之前一直以来我们两个人吃饭都是以吃海底捞为主，现在甚至已经 20000 海底捞积分了，主要是因为海底捞可以说是最实惠的火锅之一了。我们两个人吃海底捞，大多数时候是不点肉的，一般是吃血旺以及各种杂七杂八的食物，涮一个四分之一的辣锅，每次基本上原价不超过一百五，还有学生优惠/黑海送菜，基本一百元解决。价格在这之下的火锅，几乎没有，更何况海底捞的味道也很不错，还有一流的服务。&lt;/p&gt;
&lt;p&gt;上次吃巴奴火锅是和轻音音乐社（相关内容可以改天写一篇回忆录）的同学，cn 为雨宫铃子，一起吃。当时貌似已经是大一上了，四处寻找海底捞无果（如今发现海底捞在赛格顶层）之后吃的这个，花了很多钱但是两个人都没吃饱，而且服务员一直在边上盯着吃饭，所以印象不是很好，甚至可以说有了心理阴影。&lt;/p&gt;
&lt;p&gt;这次吃巴奴的原因是因为父亲请客，总的来说可以说是因为不差钱，之前一直和女朋友笑谈巴奴火锅的传说，正好钱多，故打算带女朋友吃一顿。&lt;/p&gt;
&lt;p&gt;吃完之后，不得不说，收回之前的成见，巴奴万岁。简单评价一下，基本上是海底捞里面有的菜品巴奴基本也都有，虽然说他们招牌的毛肚，涮起来不是很容易，不像海底捞一样，涮到脆的容错空间大一些。但是不得不说，他们调的这个蒜香的毛肚蘸料确实一绝，很好吃。女朋友很喜欢喝巴奴的菌汤，我这种平时不喝汤的也喝了不少，确实浓郁好喝。然后就是点了牛肉很不错，绣球菌很入味口感很好，羊肉感觉稀碎，入锅就散了，到处捞肉沫，血旺则也很好吃。&lt;/p&gt;
&lt;p&gt;大概以上这些，感觉还是很不错的。&lt;/p&gt;
&lt;h2&gt;学业&lt;/h2&gt;
&lt;p&gt;接下来来回忆一些令人兴奋的学业相关内容。&lt;/p&gt;
&lt;h3&gt;考试&lt;/h3&gt;
&lt;p&gt;对于考试，我只能说我这个人并不是很擅长应试考试吧，也可能是因为科研之类的乱七八糟的内容，反正最后感觉烂完了。还好总体来说对于排名貌似影响不大，有一些科目还可以，但也就差不多得了。&lt;/p&gt;
&lt;h3&gt;ECCV 中稿&lt;/h3&gt;
&lt;p&gt;好耶！终于讲到最喜欢的一集了。总的来说，用一句简短的话来概括，就是 ECCV 中稿了。&lt;/p&gt;
&lt;p&gt;这是一个让我既兴奋又释然的消息，早在本学年的开始大约是十月份的时候，我便已经提前进组参与科研，当时老师给的是一个半监督医学影像的论文，让我以及两个大三学长看一看，之后进行汇报。简单地跑了一下程序，并且做了一些实验之后，在组会中idea以一种极为巧合的方式突然冒出来，好在实现起来并不困难，于是写了写程序，然后发现性能 SOTA 了。&lt;/p&gt;
&lt;p&gt;可以说这是一个戏剧性的结果，我一个没有任何科研经验的人，之前只是看过若干的论文，写过若干的程序，结果一瞬间便莫名完成了科研的第一步。也可以说这是来自于大一一年以来的积累，但是我即使再怎样自负，也会将运气放在这一产出的第一位。&lt;/p&gt;
&lt;p&gt;当时还没有到十一月份，所以说和老师定下了 deadline，想要尝试赶一下 CVPR。CVPR 可以说是每一位计算机视觉初学者的耶路撒冷，我也可以说是拼尽全力的做了实验，写了论文，提交了补充材料，除了和老师探讨了论文写作和作图方面的内容之外，几乎是一个人完成的全部内容，走下来了一次论文投稿的完整流程。&lt;/p&gt;
&lt;p&gt;接下来便是焦急的等待了，然后在新年之前，审稿结果出了，113，可以说是晴天霹雳。&lt;/p&gt;
&lt;p&gt;可以说是天道好轮回了，运气式的 idea 产出，加上没有仔细打磨的论文——审稿人光是指出内容错误便占据了好几条，终究换来了报应。所以没有任何悬念的，我选择转投，备战 ECCV。&lt;/p&gt;
&lt;p&gt;我为我在 CVPR 的失败做了很全面的总结，一方面是打磨不充分，无论是故事线的塑造，还是一些诸如表头等内容的细节，还是 pipeline 的作图，可以说都十分的草率。甚至说由于我的初稿是我自己写的，后面再和老师的协商之下更换了方法的名字，而在一些表头中，原方法的名字依然残留在上面，被审稿人指出，并表示了困惑。另一方面则是对于科研领域的一些规则的不了解，最经典的例子便是我在对比实验中对其他人的每一个方法都进行了 K 折验证。我可以问心无愧地说，进行上述的操作，完全没有打压其他人方法的意思，对于性能上的不足，我向来是采用对自己方法的调参解决的，然而由于某一篇论文使用了 K 折验证，而其他论文没有，于是我便选择了最公平最稳定的测试方法，也就是全在我的设备上跑一次 K 折。结果出乎意料，复现的性能与原论文相差甚至十个点，但我还是头铁地将结果汇报了上去，不出所料，遭到了审稿人的强烈质疑。&lt;/p&gt;
&lt;p&gt;于是我按照领域中的常见规范打磨论文，重新跑实验，并且对方法进行了略微的调整，最后形成了新的稿件，在 CMT 上面进行了投稿。&lt;/p&gt;
&lt;p&gt;投稿之后，这件事情便被我抛之脑后了，当时事后和老师讨论，老师跟我说，即使这篇论文无缘 ECCV，将来头一个 PR 也是比较稳的，这给我带来了极大的安慰，毕竟升学的压力始终伴随着我。&lt;/p&gt;
&lt;p&gt;到了 ECCV 出结果的那一天，我还是熬夜了，但是老实说我并不对 ECCV 的结果抱有任何的期待，实在是 CVPR 给我带来的打击太大了，我当时心里想的是，只要有一个积极的评价，我便已经知足了，也就有了后续继续做科研的勇气。&lt;/p&gt;
&lt;p&gt;然后就是开奖，那一瞬间，我几乎从座位上跳了起来，我反复地查看这个评分，以及评分的标准，最后才相信我看到的都是现实，443，我做梦也想不到自己可以获得这样的分数。&lt;/p&gt;
&lt;p&gt;我急忙和老师汇报了这一喜讯，然后将审稿意见看了又看，很快便拟出一份 rebuttal 的临时稿件，当然，肯定是超页数了。&lt;/p&gt;
&lt;p&gt;第二天我便前去找老师进行讨论，并且定下了 rebuttal 的基调。三个审稿人都比较温和友善，提出的问题也不算特别尖锐，可以说只有几个疑惑需要解答，并且认为论文的写作中有一些笔误和不清晰。于是任务便定下来了，重新作图，完善 motivation 以及 pipeline 的表达，老师请来了两位师兄师姐，与我共同完成这些内容。&lt;/p&gt;
&lt;p&gt;不得不说，本人的作图审美可以说是一塌糊涂，两位师兄师姐则是颇有水平，一方面在他们的指导下，一方面他们亲自上阵帮忙，仅仅是一天时间，图片变已经从原来的莫名其妙变为了颇具顶会风格的插图。然后便是我较为擅长的内容，打磨文字，删减字数，顺便和老师讨论表达有何不妥，最后提交了 rebuttal，一切告一段落，顺便进入了期末周的复习阶段。&lt;/p&gt;
&lt;p&gt;可以说这个初审的分数给我带来了极大的希望，甚至说这种希望近乎已经成为了一种折磨，把我从原来对于 ECCV 已经不抱有任何幻想的状态，拉进了每天的煎熬中。&lt;/p&gt;
&lt;p&gt;那段时间几乎每天我都会看知乎相关的话题回复，看看又有多少人超过了我的分数，还有多少人逊色于我。我的分数确实处于一个领先地位，这是一个安慰，但是随后更大的焦虑便袭来了。万一我所在的领域分数都是如此的高；万一审稿人看见了彼此的评价，并降低了分数；或者干脆就是 meta reviewer 看我不顺眼，于是直接将我 reject。毕竟这些内容在网上都能找到前科，而我对于我的运气下来不是十分自信，当时甚至立下了，假如中稿便拍摄二十张精心设计的女装照的对赌。&lt;/p&gt;
&lt;p&gt;在煎熬中，时间就这样流逝，然后便到了公布结果的那一天，我本来是做好了熬夜的打算，然而在前一天的晚上，结果便提前公布了。我的双手几乎可以说是颤抖着，将链接复制下来然后输入浏览器，紧接着 ctrl f，查询我的编号。搜索框中出现 not found 的时候我的心脏几乎停跳了一下，然后检查我的编号，发现是记错了一位，最后终于找到，连忙保存 html 留作副本（万一他们反悔怎么办），然后截图，紧接着我瘫坐在椅子上的时候，一种欣喜才迟迟地涌上我的心头。&lt;/p&gt;
&lt;p&gt;我一时间似乎有些想哭，但是哽咽就卡在嗓子中，哭不出来，一路上可以说经历了太多的坎坷，绝对不是这一篇博客，几段文字，简简单单便可以概括的。好在一切都有了结果，好在努力得到了回报。接下来我开始向各路关心我的人报喜，然后发了一条 QQ 的说说，如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ECCV中稿了，也算是给近一年以来的科研画了一个迟来的逗号。&lt;/p&gt;
&lt;p&gt;一开始这篇工作是在CVPR投稿期间做出来的，idea的形成以及实验都还算顺利，但是投稿的过程却颇为坎坷，无论是论文的写作还是最后的表达都有所欠缺，最后转投ECCV。&lt;/p&gt;
&lt;p&gt;也许是运气的眷顾，或者是努力终究会有回报，好在最后的结果还算顺利，积累了经验，提升了能力，收获了许多。&lt;/p&gt;
&lt;p&gt;一时间有些不知道该说些什么，一路坎坷走到现在，未来还有多远呢？好在眼下还有不错的激励，能让我强打精神再出发。&lt;/p&gt;
&lt;p&gt;刚接触科研的时候晚上常常想多，想着万一自己中了论文，如何如何，当时就想着发这样一条说说，文案也想好了，就叫「春风得意马蹄疾」。&lt;/p&gt;
&lt;p&gt;感谢一路走来的一切，感谢老师和师兄师姐的指导，女朋友的陪伴，群u们的支持与鼓励，以及种种。现在若干时间过去了，现在想想，用一句诗来结尾的话，或许我会选择。&lt;/p&gt;
&lt;p&gt;「轻舟已过万重山。」&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;是啊，轻舟已过万重山，我等这一刻究竟已经等了多久？为此熬了多少夜？现在终于可以松了一口气。仿佛一瞬间一切的烦恼，一切的忧愁，一切的烦心事，在那一刻看来都好似过眼云烟，结束吧。&lt;/p&gt;
&lt;h2&gt;其他&lt;/h2&gt;
&lt;h3&gt;绿裙相关&lt;/h3&gt;
&lt;p&gt;本人在计算机保研交流群，也就是俗称的绿群中，一直以来都是一名活跃群友，并且自认为还是干过几件大事的。&lt;/p&gt;
&lt;p&gt;首先是在今年四月份的时候，当时群众便已经聚集了 21 级的大量同学，大家都对自己的命运琢磨不透，并因此焦虑。绿群中一个常见的话题，便是俗称的 bg 定位，也就是某人对于自己的背景情况（background，即 bg）进行阐述，并且邀请群中对于保研更加了解的前辈或者同学，对于自己可能的去向给出建议。&lt;/p&gt;
&lt;p&gt;我是一个无比喜爱信息差这个概念的人，并且热衷于将分散的信息进行整合，于是一个有趣的想法便自然而然的形成了，建立一个匿名的 bg 统计表格，每一个人都可以将自己的信息进行上传，而且这些匿名信息事实上并不会导致过多的信息泄露，因为一方面这些内容的填写是可选的，另一方面，每个人也是自愿的提供这些内容，更何况这些内容本身也会在群中被发表出来。这些信息包括了学校/排名/科研与竞赛的经历等等，并且开设了提问板块以及建议栏，可以让群友们自行对于已经公布信息的题目给出建议。&lt;/p&gt;
&lt;p&gt;在这个之后，我又增加了序号功能，并且表格可以根据回复的人数进行排序，让靠后的人也有机会被看到，最后更是直接转战谷歌，这是因为保研中介的引入带来了大量的举报，文档经常就被封禁，于是我使用了谷歌的统计表以及表格的联动，并且选择将表格内容通过外挂网站的方式进行公布，避免了这一现象，并且便捷了提交流程。&lt;/p&gt;
&lt;p&gt;在当时这一创造可以说是现象级的，自动化保研交流群，甚至也都在讨论这一举动，并且光速推出了他们自己对于这一表格的模仿版，但是显然无论是热度还是内容量，又或者是维护的先进性，与我推出的表格都有着明显的差距。&lt;/p&gt;
&lt;p&gt;不过遗憾的是，这一表格的投稿人很多，但是给出建议的群友，向来只有不几个，我也因此甚至自己请教了大量的前辈们，并且尝试给出自己的意见。也正是因为这个原因，加之以时间的流逝，和真正的保研的临近，这一表格后来也就慢慢被荒废了。&lt;/p&gt;
&lt;p&gt;接下来便是最近设计的又一创举，也是受到了广泛的传播，并且拿下了一百多个 star，也就是本人实现并维护的 &lt;a href=&quot;https://cs-baoyan.github.io/CS-BAOYAN-DDL/&quot;&gt;cs-baoyan-ddl&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;起因是因为几位活跃群友在群中的讨论，想要构建一个项目，酷似 &lt;a href=&quot;https://ccfddl.github.io/&quot;&gt;ccfddl&lt;/a&gt;，可以实现对于夏令营截止日期的统计，而我看到了这一想法之后，由于之前对于网页制作稍有经验，并且更关键的是精通面向 GPT 的编程，因此只是短短的两个小时时间，这个网页便被搭建起来了，同时被我挂到了 Github Pages 上。&lt;/p&gt;
&lt;p&gt;大家的反响依然十分激烈，给出了许多的赞美，但是我深知这一网站并非完美的，无论是对于数据的管理，我需要人工大量地从已经形成的 CSSummerCamp 的 Markdown 文档中进行人为的提取，这是大量的工作量，同时在后续进行仓库的维护，也并不是很便捷，最后使用原生的 web 三大件大建出来的网页维护起来也是十分麻烦，并且有很多功能难以实现，于是我便踏上了漫长的重构旅程。&lt;/p&gt;
&lt;p&gt;首先我又一次学习了 Github Actions，通过建立了 &lt;a href=&quot;https://github.com/CS-BAOYAN/BoardCaster&quot;&gt;BoardCaster 仓库&lt;/a&gt; 模拟了一个后端数据库的存在，并且可以使用 Action 让不同的仓库都订阅这一仓库，从中获取需要的数据。同时，我设计了一套可以根据 Issue 直接更新数据库的流程，这使得数据库的更新变得更加方便。&lt;/p&gt;
&lt;p&gt;然后，我紧接着学习了 Vue3，并且和 GPT 一起完成了对于整个网站的重构，并且添加了更多的功能。再一次开始宣传的时候，可以说这一网站的功能已经完美地达到了 2.0 的版本，并且支持了更多的数据库的引入。&lt;/p&gt;
&lt;h3&gt;汐学组&lt;/h3&gt;
&lt;p&gt;与此同时，在这里同样被我维护的开源项目还有另一个项目，便是汐学组。关于汐学组的创立，这是一段曲折的经历。&lt;/p&gt;
&lt;p&gt;本人可以说是一个十分热衷于开源的人，我从大一上开始便已经开始制作各种各样的教程，其中比较知名的是一个关于 C++程序设计的复习指南，根据学弟透露的情报，在最新的期末复习课的资料中，我当初所写的资料已经赫然位列其中。能够成为所谓官方的复习资料，本人还是十分荣幸的。&lt;/p&gt;
&lt;p&gt;能够产出复习资料，除了出于对于开元社区做贡献的热爱之外，也是因为我在 RoboMaster 时期便已经收到了良好的编程训练，这其中很重要的一项就是 Markdown 文档的编写，在此之后我又自学了 $\LaTeX$，使得我的相关功底更上一层楼。顺带一提的是，关于 RoboMaster，我也计划写一期回忆录，暂且列在待办事项中，毕竟博客不只是用来分享技术的，也是我随便写写东西的场所，而其中很重要的一点便是对于过去的回忆记载。&lt;/p&gt;
&lt;p&gt;在此之后我也进行了诸多的创作，产出了大量的复习资料，或者说与其说是复习资料，不如说是某种复习大纲，或者是 cheatsheet 格式的内容，可以帮助使用者在考前快速的回顾知识点，但是或许并不适用于那些对于知识压根不了解的同学。同时我还热衷于进行PPT转述，将PPT中的全部重点以及内容公式使用 Markdown 进行转述，一方面这种资料的整理可以帮助我更快的巩固知识点，同时转述后的PPT也很便于立刻形成成体系的复习大纲。&lt;/p&gt;
&lt;p&gt;老实说我一直对开源社区是抱有希望的，我一直认为我将 PPT 转述的内容发布之后，便会有其他同学，紧跟我的脚步，将内容进行二次压缩，但是很遗憾的是，这种事情往往不会发生。&lt;/p&gt;
&lt;p&gt;整个大一的学年中，从我之外分享复习资料的同学屈指可数，而且部分的资料，当然也可能是我记忆错误，事实上来自于大二，通过类似于手写扫描或者电子笔记的方式进行分享。一方面，这种载体的资料分享并不便于他人在此基础上进行二次加工，而与此同时，在使用了 Markdown 以及 LaTeX 等工具之后，我对于文档的格式化表达形成了一种近乎洁癖的偏执，而很明显的一点是，我所在专业同年级的绝大多数同学，可以说完全没有接受过相关的训练。甚至我敢打保票，直到大二结束的现在，仍然有超过50%的同学不知道如何书写 Markdown。&lt;/p&gt;
&lt;p&gt;于是显然，开源之路是孤独的。&lt;/p&gt;
&lt;p&gt;到了大二的时候，我创作的资料已经足够使用一个知识库进行维护了，但是当时我对于网站的创立等内容尚且不太熟悉，于是使用了我常用的笔记软件，Obsidian，将我的资料进行了开源，但是貌似直到目前为止，相关仓库在我的宣传后，也没有一个 PR，而显然，大多数同学更不会使用 Git。&lt;/p&gt;
&lt;p&gt;另一件怪事是，在大二的时候，我们忽然变成立了一个所谓的AI学组。老实说，在西交中确实存在着无数的学组，诸如钱学组/仲英学组/彭康学组等，虽然他们的一些资料为了确保没有知识点的遗漏，而导致过于的冗长，但是总的来说，这些资料具有极高的价值，同时他们还整理了往届的考试原题，这几乎是我期末复习周的救命稻草。但是令我困惑的是，AI学组究竟因何诞生。正如我之前所提到的，除了我之外，我并未在年级中听说过任何一位其他的资料分享的积极活跃者，难道说终于有同学开窍了？打算加入开源的事业？&lt;/p&gt;
&lt;p&gt;刚开始的时候，我对于学组的建立是十分激动的，无论是对于资料的分享，共同维护一个硕大的 Github 资料库，还是说在学习之余，进行一些科研相关的讨论会，交流一下彼此的见解，这对于年级的水平提升和共同进步的促进都是十分明显的。然而在我加入其中，兴致勃勃地分享了自己创建的仓库，并且积极更新资料同时，其他同学的资料则难以见到其踪影，直到后来临近期末考试，在考试开始的前几天，终于出现了电子笔记形式的资料，至此，我对于学组已经多半失望了，或许还是自己来比较合理，正好我有的是精力。建立优质的资料，为大家树立标准，或许可以激励学组和其他的开源行为变得更好，于是在大二下的时候，我自己的开源网站以及组织，汐学组成立了。&lt;/p&gt;
&lt;p&gt;大多数的学组均是如此，挂靠在学校的诸如书院的团委名下，并且做出的贡献都会根据贡献量给予工时等奖励。然而不同于这些内容，汐学组是完全为爱发电的形式，没有工时奖励，没有赞助窗口（或许将来我会建立一个，用于购买咖啡或者维护服务器），只有我分享的资料以及网站，并且目前我的 Github 账号正以每天十条以上的 commit 的频率活跃着。&lt;/p&gt;
&lt;p&gt;现如今，汐学组名下主要维护着两个网站，分别是 &lt;a href=&quot;https://xistudygroup.github.io/&quot;&gt;汐学组的主页&lt;/a&gt;，以及 &lt;a href=&quot;https://survivexjtu.github.io/&quot;&gt;西安交大生存指南&lt;/a&gt;。其中主页的内容包括了学组的介绍，以及一些复习资料的分享，通过 Github 的 raw，可以提供直接使用链接进行点击下载的功能。与此同时，西安交大生存指南则是我受到了上海交大生存指南的启发，从而自己建立的一个生存指南，涵盖了包括在大学中看待事物的方法，入学时候的须知事项，一些基础的学习技巧，一些恰当的学习路线，以及其他的可能在正常途径中无法获得的信息。&lt;/p&gt;
&lt;p&gt;在将来的一个打算是，或许可以推出一种周刊或者月刊形式的学习资源分享，当然这个分享并非课内的，而分享的内容也并非我自己制作的，大概的名字叫做，学点没用的。分享一些可能在正常的专业学习路径中无法遇到的知识，并且学习它们，拓宽自己的知识面，并且维持自己对于新知识的热情。&lt;/p&gt;
&lt;p&gt;总的来说，作为一个开源社区的贡献者，我自认为在本年级中，暂时还没有同学可以超过我，包括无论从呈现的形式上，还是从工作量上来说，均是如此。&lt;/p&gt;
&lt;p&gt;目前暂时来看，关于开源工作的推进，一方面是寻找一些其他专业的成绩优异的同学，为西安交大生存指南提供优质稿件；另一方面则是仍然需要培养本专业 24 级同学的参与开源的能力与热情，同时积极把握 23 级同学。但是至于 22 级，好吧，暂时来看我是随缘了。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;说来说去，作为我的第一篇周记来说，写了太多的东西，已经有点类似于我的前两年的回忆录的感觉了，虽然前两年经历的事情远不止如此，之后的每一次周记，我预计会包含大概四到五条，但是内容量肯定不如本次一样丰富，毕竟这次还融合了许多之前发生的事情，而之后的话，想要多写内容，可能需要托我的联想能力多多发挥功效了。&lt;/p&gt;
&lt;p&gt;大概就是这样子，接下来应该继续投身于科研，以及更多的开源事业中去了。&lt;/p&gt;
&lt;p&gt;简单立下一些 flag，下周首先要将西安交大生存指南新增大约四到五篇文章，然后开始学习计算机图形学，同时需要提醒的是，要将 ECCV 的稿件顺利投稿，这是很重要的。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/week-01-zh.webp"/><enclosure url="https://picr2.axi404.top/week-01-zh.webp"/></item><item><title>Obsidian 快速上手指南</title><link>https://axi404.github.io/blog/obsidian-tutorial</link><guid isPermaLink="true">https://axi404.github.io/blog/obsidian-tutorial</guid><description>Obsidian 快速上手，概念辨析 &amp; 基础使用。</description><pubDate>Sat, 06 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;根据惯例写一下前言，关于这篇博客为什么要写，以及其中的内容。&lt;/p&gt;
&lt;p&gt;笔者曾经不止一次在各种场合推荐过记笔记的好处，以及笔者所使用的 Obsidian 这一笔记软件。关于记笔记，每个人可能存在不同的理由，在这里笔者给出的理由是，笔记的记录是一种对于学习的正反馈，是对于自身学习内容的一种定量描述，更多的论述见 &lt;a href=&quot;https://survivexjtu.github.io/%E7%A0%94%E5%AD%A6%E7%AF%87/%E8%AE%B0%E7%AC%94%E8%AE%B0%E6%98%AF%E4%B8%80%E7%A7%8D%E6%AD%A3%E5%8F%8D%E9%A6%88.html&quot;&gt;SurviveXJTU 的对应章节&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;目前市面上存在着若干的笔记软件，国内的诸如语雀/flowus 等，而国外的则诸如 Notion/Obsidian 等。在这里有必要指出，尽管国内的笔记软件的功能均在提升，但是其功能性均不如 Notion，而 Notion 则是基于网络的内容，你记载的笔记均不是本地内容，同时伴随着较高的学习成本。&lt;/p&gt;
&lt;p&gt;相较起来，Obsidian 则是一款本地笔记软件，你的全部内容可以均保存在本地，也可以通过 Github 等方式进行备份与同步。Obsidian 使用 Markdown 语言进行笔记的撰写，这使得其具有较低的上手难度，而同时其双向链接功能以及诸多插件则可以覆盖基本全部的日常笔记记录的需求，最后其仓库的概念则使得笔记之间被优雅地组织在一起。&lt;/p&gt;
&lt;h2&gt;安装与下载&lt;/h2&gt;
&lt;p&gt;Obsidian 的安装与下载十分简单，你可以在 &lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsidian 的官网&lt;/a&gt; 进行下载，之后按照提示进行安装即可。&lt;/p&gt;
&lt;h2&gt;仓库&lt;/h2&gt;
&lt;p&gt;一如 VSCode 中的工作区，在 Obsidian 中，一系列的笔记内容（一个知识库）被组织在一个名为仓库的单位中。仓库实际上就是一个文件夹，其根目录下包括一个 &lt;code&gt;.obsidian&lt;/code&gt; 文件夹，其中含有一些 Obsidian 自身的配置以及日志文件，而其他的文件夹以及内容则全部都是笔记的内容。&lt;/p&gt;
&lt;p&gt;仓库的特点在于，在仓库中的全部文件，可以由 Obsidian 自动进行索引，这使得你可以通过文件名进行搜索，而无需担心文件路径的问题，而当文件的名称改变之后，这些链接也会自动更新。&lt;/p&gt;
&lt;h2&gt;双向链接&lt;/h2&gt;
&lt;p&gt;Obsidian 不同于专业的 Markdown 编辑器，如 Typora 等软件的最大特点便是其使用的双向链接功能。&lt;/p&gt;
&lt;p&gt;理解双向链接是一个很简单的过程，其可以在不同的 Markdown 文件之间建立联系。这一过程像是网页中的链接功能，在每一个文档的右上角打开 &lt;code&gt;更多选项-&gt;打开链接视图&lt;/code&gt;，可以找到局部的关系图，而在界面左侧的 &lt;code&gt;查看关系图谱&lt;/code&gt; 中则可以找到全局的关系图谱。&lt;/p&gt;
&lt;p&gt;双向链接的双向主要体现在，不仅可以统计链接的出链，也可以统计其反向链接数量，这使得可以获得这篇文档被引用的来源。&lt;/p&gt;
&lt;p&gt;通过双向链接可以很方便的直接获得思维视图，而这对于整理整体的知识谱系，使得知识融会贯通有着很重要的意义。&lt;/p&gt;
&lt;p&gt;在笔记的整理过程中，这种双向链接的功能可以使得回顾之前的知识或者掌握拓展的知识更加轻松，这使得比如：在一篇复习资料中，对于某一拓展知识，我们可以将其通过双向链接进行链接，而不是直接插入在复习资料之中，使得资料简洁的同时兼顾拓展性；同时假如在一篇公式与符号众多的资料中，某一处提及了较靠前的内容，通过双向链接也可以准确的回到那一处内容进行回顾，而不是如同 PDF 文档亦或者是纸质笔记一样需要花费大量的时间对知识点进行寻找。&lt;/p&gt;
&lt;p&gt;通过点击链接进行跳转之后，对于使用鼠标且具备侧键的用户来说，侧键可以直接进行「返回」操作，而对于其他的用户来说，可以点击文档左上角的的左向箭头，值得一提的是，这种返回操作完全不是 &lt;code&gt;ctrl + z&lt;/code&gt; 进行的撤销操作，请勿混淆。假如说因为前进与返回不方便而困扰，也可以在左下角的齿轮形状图标的设置中点击&lt;strong&gt;快捷键-返回&lt;/strong&gt;与&lt;strong&gt;快捷键-前进&lt;/strong&gt;，进行修改。&lt;/p&gt;
&lt;p&gt;使用双向链接这个过程十分简单，使用 &lt;code&gt;[[文件名|显示名称]]&lt;/code&gt; 或者 &lt;code&gt;[[文件名]]&lt;/code&gt; 就可以完成双向链接，其中后者是简化语法，链接的目标与显示的文本会相同（无后缀名）。值得一提的是，这种方式的链接很像 Markdown 基本的语法，即 Markdown 中的超链接语法，因此不难记忆。而二者的不同之处在于，超链接需要给出链接的完整的相对路径甚至绝对路径，而 Obsidian 则无需考虑完整的文件路径，仅需要输入文件名，而 Obsidian 的工作区则会维护剩余的部分。&lt;/p&gt;
&lt;p&gt;Obsidian 的双向链接同样支持索引到标题甚至段落，其使用的语法分别是 &lt;code&gt;[[文件名#标题名]]&lt;/code&gt; 以及 &lt;code&gt;[[文件名^段落标记]]&lt;/code&gt;，其中段落标记可以在输入 &lt;code&gt;^&lt;/code&gt; 之后选择了期望的段落后自主生成，其一般为形如 &lt;code&gt;^f6c831&lt;/code&gt; 的编码，或者可以使用 &lt;code&gt;^标记&lt;/code&gt; 来创建标记。标记的长度不限，但是只能由数字与字母组成，在编辑模式下可视，而在阅读模式下不可视。&lt;/p&gt;
&lt;h2&gt;插件安装&lt;/h2&gt;
&lt;p&gt;Obsidian 的插件安装分为使用插件市场进行安装或者手动安装，一般来说绝大多数的插件，读者仅需要使用插件市场便可以完成安装的事项。&lt;/p&gt;
&lt;p&gt;在选项中找到第三方插件，并且关闭安全模式之后，即可访问插件市场。值得一提的是，顺利的插件市场访问需要在可以顺利访问 Github 的网络环境下。&lt;/p&gt;
&lt;p&gt;在插件市场中找到自己心仪的插件，并且点击安装即可，十分的简单。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/image.2obgeegff0.webp&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在这里笔者推荐一款笔者最常用的插件，即 &lt;code&gt;Easy Typing&lt;/code&gt;，这个插件可以帮助你进行快速的格式化文档，使得你的文档规范统一。&lt;code&gt;Easy Typing&lt;/code&gt; 可以十分便捷的创建属于自己知识库的文本规范，同时可以对一些符号的输出进行自动的补全，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入两个 &lt;code&gt;【【&lt;/code&gt; 自动变成 &lt;code&gt;[[]]&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;输入 &lt;code&gt;（&lt;/code&gt; 自动变成 &lt;code&gt;（）&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;输入三个反引号自动补全为代码块。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;笔者分享了一些关于 Obsidian 的基础使用，包括仓库、双向链接以及插件安装等内容，希望对读者有所帮助。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Obsidian-Tutorial-zh.webp"/><enclosure url="https://picr2.axi404.top/Obsidian-Tutorial-zh.webp"/></item><item><title>Git 的常见操作</title><link>https://axi404.github.io/blog/git-tutorial</link><guid isPermaLink="true">https://axi404.github.io/blog/git-tutorial</guid><description>日常使用的 Git 操作，丝滑小连招。</description><pubDate>Wed, 03 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本篇内容写作的初衷，是由于，笔者在生活中的见闻。不少的 Git 的初学者，毫无疑问，是了解关于 Git 的大多数的基本操作的，但是对于其背后的流程却知之甚少。因此，在实际的操作中的时候，假如说进行正常的 &lt;code&gt;git clone&lt;/code&gt; 之后的&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;commit&lt;/code&gt;, &lt;code&gt;push&lt;/code&gt; 操作，那么多半问题不大，但是假如说遇到了更加复杂的需求，则难免束手无策。&lt;/p&gt;
&lt;p&gt;这便是笔者写作本内容的初衷，即尝试通过更加复杂的 Git 任务，尝试帮助读者了解一个更加完整的 Git 工作流，并丝滑地处理日常的一些基础内容之外的常见需求。&lt;/p&gt;
&lt;h2&gt;初始化 Github SSH&lt;/h2&gt;
&lt;p&gt;初始化 Github SSH 是每一个 Git 用户与 Github 进行交互的第一步，但是在这其中的不少流程往往引人迷惑，使得在后续的日常使用中，常常困惑于自己的配置是否合理。&lt;/p&gt;
&lt;p&gt;由于 Github 的更新，Github 的上传不再支持使用账号密码的身份验证，而是转为使用个人访问令牌或者 SSH 的方式，而其中毫无疑问，使用 SSH 是最为优雅的解决方案。SSH 生效的原理是，在本地生成的公钥私钥对，其中的公钥被上传至 Github，而在 SSH 之后，本地与 Github 建立安全连接，从而进行相关的操作。&lt;/p&gt;
&lt;p&gt;在这里首先给出初始化 Github SSH 的详细步骤，之后再进行解释，以解决部分初学者的误区。&lt;/p&gt;
&lt;h3&gt;详细步骤&lt;/h3&gt;
&lt;p&gt;首先，使用 SSH 创建密钥对：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ssh-keygen -t ed25519 -C &quot;your_email@example.com&quot;
# 或者 ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;ed25519&lt;/code&gt; 以及 &lt;code&gt;rsa&lt;/code&gt; 均是密钥生成的算法，其中 &lt;code&gt;ed25519&lt;/code&gt; 是更新的算法，假如说本地不支持，则可以使用 &lt;code&gt;rsa&lt;/code&gt;，本身的安全性均很高。输入之后默认回车即可，密钥会被生成至 &lt;code&gt;~/.ssh/&lt;/code&gt; 中。使用 &lt;code&gt;cat&lt;/code&gt; 指令可以进行查看：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cat ~/.ssh/id_ed25519.pub
# 或者 cat ~/.ssh/id_rsa.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将生成的公钥复制至 Github 中，在 Github 的 &lt;code&gt;Settings&lt;/code&gt; 中的 &lt;code&gt;SSH and GPG keys&lt;/code&gt; 中，点击 &lt;code&gt;Add SSH key&lt;/code&gt; 进行添加即可。&lt;/p&gt;
&lt;h3&gt;理解误区&lt;/h3&gt;
&lt;p&gt;在这一过程中，我们注意到，包括说互联网中绝大多数的常见教程，均会使用 &lt;code&gt;-C &quot;your_email@example.com&quot;&lt;/code&gt; 这一行指令，而与此同时，&lt;code&gt;git config&lt;/code&gt; 以及 Github 均存在邮箱地址这一配置内容，但是实际上这三者之间没有一点的关系。生成密钥使用的邮箱为注释性质的，本质上可以不添加；&lt;code&gt;git config&lt;/code&gt; 的邮箱为记录性质，每一条 commit 都需要记录用户以及邮箱；而 GitHub 的邮箱则是账号性质，是掌管 Github 权限的内容。&lt;/p&gt;
&lt;p&gt;因此也就不难解释一些奇妙的问题了，诸如自己的本地的 &lt;code&gt;push&lt;/code&gt; 在 Github 中显示的来源与自己的 Github 账号不一致。这完全是因为在 &lt;code&gt;git config&lt;/code&gt; 中配置的邮箱与 Github 中的邮箱不一致导致的，而信息与 &lt;code&gt;git config&lt;/code&gt; 中的内容保持一致，假如说想要纠正，重新设置 &lt;code&gt;git config&lt;/code&gt; 即可。&lt;/p&gt;
&lt;h2&gt;关联新建仓库&lt;/h2&gt;
&lt;p&gt;关联新建的仓库同样是在 Git 操作中很常见的一种，也就是应该如何让本地的 Git 与 Github 中的仓库之间建立远程链接，这其中最方便的一种便是使用 &lt;code&gt;git clone&lt;/code&gt; 指令。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone git@github.com:username/repository.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假如说已经在本地的仓库中创建了一些内容，则可以在 &lt;code&gt;git clone&lt;/code&gt; 之后将已经创建的内容统一复制到克隆出来的文件夹中即可。&lt;/p&gt;
&lt;p&gt;在这里需要指出的一种常见错误是，在 Github 中创建仓库时勾选了创建 &lt;code&gt;README.md&lt;/code&gt; 或者 &lt;code&gt;LICENSE&lt;/code&gt; 文件，而后使用大多数教程中推荐的 &lt;code&gt;git init&lt;/code&gt;, &lt;code&gt;git add .&lt;/code&gt;, &lt;code&gt;git commit -m &quot;initial&quot;&lt;/code&gt;, &lt;code&gt;git remote add origin git@github.com:username/repository.git&lt;/code&gt;, &lt;code&gt;git push -u origin main&lt;/code&gt;。这一流程常常导致报错，这是因为在 Github 中存在这些默认创建的文件，而本地的仓库中并没有这些文件，这会导致在 &lt;code&gt;git push&lt;/code&gt; 的时候出现错误，而如果已经进行过 &lt;code&gt;commit&lt;/code&gt;，也会因为 &lt;code&gt;commit&lt;/code&gt; 的历史不一致而在 &lt;code&gt;pull&lt;/code&gt; 以同步这些文件的时候报错。&lt;/p&gt;
&lt;p&gt;因此，正确的流程是，在 Github 中创建仓库时，不勾选这些默认创建的文件，而在本地创建这些文件，再进行 &lt;code&gt;git push&lt;/code&gt; 即可。或者使用上述的 &lt;code&gt;git clone&lt;/code&gt; 流程。&lt;/p&gt;
&lt;p&gt;假如说非要在这种情况下使用 &lt;code&gt;git init&lt;/code&gt; 的流程，则可以使用以下的脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git init
git add .
git commit -m &quot;initial&quot;
git branch # 查看当前分支名称
git branch -m main # 当前分支重命名为 main
git remote add origin git@github.com:username/repository.git
git pull origin main --allow-unrelated-histories
git push -u origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中的精髓在于使用 &lt;code&gt;--allow-unrelated-histories&lt;/code&gt;，这使得在 &lt;code&gt;pull&lt;/code&gt; 的时候，允许两个不同的仓库进行合并，从而避免报错。&lt;/p&gt;
&lt;h2&gt;废弃当前 Github 仓库分支并更新 main 分支&lt;/h2&gt;
&lt;p&gt;对于部分的仓库的重构需求，例如将本仓库不再使用 Hugo，而是使用 VitePress 进行搭建，那么需要将本仓库的 main 分支进行完全的重建，同时出于保险起见，还需要将之前的分支进行备份，也就是将其置入一个废弃分支。&lt;/p&gt;
&lt;p&gt;首先先备份当前的分支：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git checkout main
git checkout -b deprecated-main
git push origin deprecated-main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后重建当前的 main 分支：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git branch -D main
git checkout --orphan main
git rm -rf .
# 将新的文件添加到当前目录
git add .
git commit -m &quot;Rebuild main branch&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后在 push 的时候使用 &lt;code&gt;-f&lt;/code&gt;，也就是 force，进行强制推送：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git push -f origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;假如说本仓库存在一个 gh-pages 分支，有可能会需要删除这个分支，使用以下指令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git push origin --delete gh-pages
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;以上内容总结了部分的笔者在日常使用中经常会用到的 Git 相关的使用技巧，这些内容是维护一个仓库的过程中十分常见的。同时，同样需要注意的有诸如在修改仓库内容之前先进行 &lt;code&gt;git pull&lt;/code&gt; 此类日常习惯，这样才可以保证内容的一致性。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Git-Tutorial-zh.webp"/><enclosure url="https://picr2.axi404.top/Git-Tutorial-zh.webp"/></item><item><title>Github Actions and Pages 教程</title><link>https://axi404.github.io/blog/github-actions-and-pages-tutorials</link><guid isPermaLink="true">https://axi404.github.io/blog/github-actions-and-pages-tutorials</guid><description>关于 Actions 与 Pages 的教程，暨 CSBAOYANDDL 技术分享。</description><pubDate>Tue, 02 Jul 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本教程是关于 Actions 以及 Pages 的一些分享，最近一段时间在 Github 上面发烧了一阵子，狠狠的制作了一些开源的项目，自然也存在一些摸索性质的内容，而摸索出了答案，也就是时候出一篇教程来写一写，正好，本博客风格赏心悦目，值得一试。&lt;/p&gt;
&lt;h2&gt;关于 Github Pages&lt;/h2&gt;
&lt;p&gt;首先需要提及的是 Github Pages，简单来说，这是一种 Github 提供的静态网站托管服务，至于什么是静态网站，一种简易的理解是，至少上面不会有一个数据库，网站也不存在任何对数据的读写操作，一切的风格化变化只来自于网站提前搭建好的框架。&lt;/p&gt;
&lt;p&gt;基于这种特性，不难理解的是，Github Pages 尤其擅长处理一些类似于博客、文档、教程一类的网站，甚至说我们的 &lt;a href=&quot;https://cs-baoyan.github.io/CSBAOYANDDL/&quot;&gt;CSBAOYANDDL&lt;/a&gt; 也通过一种取巧的方式，可以通过 Github Actions 维护一个类似于数据库的内容。&lt;/p&gt;
&lt;p&gt;在提供静态网页这件事情上，Github 可以说是十分慷慨的，每一个用户可以创建近乎无限的仓库，而每一个仓库都可以对应一个 Github Pages————只要你知道如何设置（而这一点我们会在后面提及）。&lt;/p&gt;
&lt;h2&gt;关于 Github Actions&lt;/h2&gt;
&lt;p&gt;Github Actions 是一个持续集成和持续部署（CI/CD）平台，它允许用户在 Github 上创建自动化流程，用于构建、测试和部署项目，其支持使用 YAML 文件定义工作流程，并且可以与 Github 上的其他服务进行交互，换句话来说，Github Actions 支持我们在 Github 这个理论来说静态的平台上面运行一个脚本。&lt;/p&gt;
&lt;p&gt;通过上述的内容，细心的读者应当不难发现，Pages 提供静态服务，而 Actions 则可以运行脚本，二者的互补之下，很多内容都成为了可能，不过本篇中不得不遗憾地告诉读者，Actions 在其中并不发挥着过多的作用，大多数的内容仅是基于现如今成熟的网页模板以及 Github Pages 的静态网站部署，便已经结束了。&lt;/p&gt;
&lt;h2&gt;部署你的第一个网站&lt;/h2&gt;
&lt;h3&gt;回顾&lt;/h3&gt;
&lt;p&gt;在很多的教程中，往往都会教学如何建立一个自己的博客或者主页，通过 Github Pages 的方式。然而这些方法往往问题很大，即会让读者产生一种错觉，一个账户只能创建一个静态网站。&lt;/p&gt;
&lt;p&gt;让我们回顾一下这些教程说的内容，首先，在自己的账户中创建一个仓库，这个仓库的名字需要是 &lt;code&gt;username.github.io&lt;/code&gt;，对于笔者来说，也就是创建一个名为 &lt;code&gt;Axi404.github.io&lt;/code&gt; 的仓库。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/githubio_create.4917dtqb1l.webp&quot; alt=&quot;create github.io&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后在其中使用某些模板或者其他的内容进行进一步操作。这看上去确实正规，但是不免让人产生了怀疑，那么我的仓库名是不是只能叫做 &lt;code&gt;username.github.io&lt;/code&gt; 呢？&lt;/p&gt;
&lt;p&gt;事实上在创造一个仓库的时候，你的仓库中存在一个选项，即 Github Pages，在进行了一些操作之后，便可以让你创建一个网站，而这个网站一般来说其域名为 &lt;code&gt;username.github.io/reponame&lt;/code&gt;。具体进行的操作先按下不表，但这里也就不难发现了，实际上创建名为 &lt;code&gt;username.github.io&lt;/code&gt; 看上去确实特殊，但是并不意味着你只能创建一个仓库：事实上 Github Pages 对于这个仓库名称进行了特殊处理，使用该名称创建的仓库，其域名直接为 &lt;code&gt;username.github.io&lt;/code&gt;，但除此以外，并不限制你创建其他的仓库。&lt;/p&gt;
&lt;p&gt;让我们来简单的了解一下创建一个网站的流程。&lt;/p&gt;
&lt;p&gt;按照常规的流程来说，我们都知道，Web 网站是由 Web 三大件共同创建的，其中 html 负责创建网页的框架，css 负责创建网页的样式，而 js 负责创建网页的交互。而在大多数的网站中，&lt;code&gt;index.html&lt;/code&gt; 绝对是重中之重。在 Github Pages，其在部署阶段，网站会自动寻找在某一目录下的 &lt;code&gt;index.html&lt;/code&gt; 文件，并且将其作为网站的主页，同时将全部的内容部署到静态网页中。&lt;/p&gt;
&lt;p&gt;因此这一流程也也就不难想象了，创建一个 &lt;code&gt;index.html&lt;/code&gt;，在其中写入一些内容，然后将这个文件部署到 Github Pages 中，便可以得到一个网站，简单地好似将大象放进冰箱里。&lt;/p&gt;
&lt;h3&gt;实例&lt;/h3&gt;
&lt;p&gt;在这里给出一个小小的实例，读者可以跟着进行一下尝试，在这里我们假设读者已经在本地完成了 Git 以及 Github 相关的一切配置，并且拥有了一个仓库，例如名为 &lt;code&gt;MyExample&lt;/code&gt;。&lt;strong&gt;以下均会采用我的用户名进行操作，这是因为每一次使用 &lt;code&gt;username&lt;/code&gt; 的时候总会存在读者不解并不将其替换，使用本人的用户名应当会更加明显一些，表明替换的必要性。读者在使用的时候将我的用户名替换为自己的即可。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone git@github.com:Axi404/MyExample.git
cd MyExample
vim index.html
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;index.html&lt;/code&gt; 中写入以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;#x3C;!DOCTYPE html&gt;
&amp;#x3C;html lang=&quot;en&quot;&gt;
&amp;#x3C;head&gt;
    &amp;#x3C;meta charset=&quot;UTF-8&quot;&gt;
    &amp;#x3C;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &amp;#x3C;title&gt;My First Website&amp;#x3C;/title&gt;
&amp;#x3C;/head&gt;
&amp;#x3C;body&gt;
    &amp;#x3C;h1&gt;Welcome to My First Website&amp;#x3C;/h1&gt;
    &amp;#x3C;p&gt;This is a simple HTML page hosted on GitHub Pages.&amp;#x3C;/p&gt;
&amp;#x3C;/body&gt;
&amp;#x3C;/html&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更进一步来说，你可能愿意为其添加一些 CSS 样式以及 JS 脚本，这也同样不难：&lt;/p&gt;
&lt;p&gt;首先创建一个 CSS 文件名为 &lt;code&gt;styles.css&lt;/code&gt;，在其中写入一些代码。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;body {
    font-family: Arial, sans-serif;
    margin: 0;
    padding: 0;
    background-color: #f4f4f4;
}

h1 {
    color: #333;
    text-align: center;
    margin-top: 50px;
}

p {
    color: #666;
    text-align: center;
    margin-top: 20px;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后创建一个JS文件名为 &lt;code&gt;script.js&lt;/code&gt;，并且在其中输入一些代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;document.addEventListener(&apos;DOMContentLoaded&apos;, function() {
    alert(&apos;Welcome to My First Website!&apos;);
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后再对 &lt;code&gt;index.html&lt;/code&gt; 进行一些修改以导入这些内容，包括在 head 中加入 &lt;code&gt;&amp;#x3C;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt;&lt;/code&gt; 以及在 body 的末尾加入 &lt;code&gt;&amp;#x3C;script src=&quot;script.js&quot;&gt;&amp;#x3C;/script&gt;&lt;/code&gt;，这样便大功告成了。&lt;/p&gt;
&lt;p&gt;假如你使用的是 VS Code 之类的编辑器，使用 &lt;code&gt;Live Server&lt;/code&gt; 可以对这个页面进行实时阅览，十分好用，或者正常的 Linux 命令行，使用 &lt;code&gt;xdg-open&lt;/code&gt; 打开文件进行预览也是可以的（指在具有桌面 GUI 以及默认浏览器的系统中）。&lt;/p&gt;
&lt;p&gt;接下来可以将这些内容上传到 Github 了：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git add . 
git commit -m &quot;initial commit&quot;
git push
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后前往 &lt;code&gt;Github&lt;/code&gt; 上面，依次添加 &lt;code&gt;Setting -&gt; Pages -&gt; None -&gt; main -&gt; save&lt;/code&gt;，完成设置，流程可以如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/github_pages.51e2vk6wru.webp&quot; alt=&quot;github pages setting&quot;&gt;&lt;/p&gt;
&lt;p&gt;不难发现后方的名为 &lt;code&gt;/(root)&lt;/code&gt; 的选项，即你的 &lt;code&gt;index.html&lt;/code&gt; 所在的目录，我们这里使用默认的根目录即可，后续我们会知道，使用自定义的 Github Actions 也可以做到相同的效果。&lt;/p&gt;
&lt;p&gt;在点击 save 之后可以点击上方的 Actions 看到一个 deployment 的 action 正在 &lt;code&gt;queue&lt;/code&gt; 或者正在 &lt;code&gt;Pending&lt;/code&gt;，等待部署结束即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/actions_start.45m1pt4yo.webp&quot; alt=&quot;github actions pending&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时再次回到 Pages 的界面，可以看到页面已经部署，并且给出了 url 链接。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://picr2.axi404.top/pages_deploy_ready.9gwi0tjazu.webp&quot; alt=&quot;github pages ready&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后再次进行的部署流程会比这个简单很多，只需要在修改了内容之后重新 commit 并且 push 即可，剩下的内容 Github Actions 会帮助你完成，这是得力于这个 Action 对你的 push 操作的检测（被触发）。&lt;/p&gt;
&lt;p&gt;部署诸如 Hugo 以及 mkdocs 等内容与直接的 html 文件稍有不同，在后续的拓展内容中会陆续更新这两部分的介绍。&lt;/p&gt;
&lt;h2&gt;完成你的第一个 Github Actions&lt;/h2&gt;
&lt;p&gt;你已经完成了一个正常的网页的部署了，一般来说，假如说你是正常的手写的 &lt;code&gt;index.html&lt;/code&gt; 类型的静态网页，此时任务便已经结束了，不过很不幸，你可能还有更多的需求，所以需要一个自己的 Github Actions 来进行更多的个性化操作。&lt;/p&gt;
&lt;p&gt;笔者将给出两个示例来进行示范，其中之一是部署 vue 项目，众所周知 vue 项目需要顺利编译才可以成为正常的静态网页，而优雅的方式之中并不包括本地编译之后手动推送。如何在 Github 中使用 Github Actions 来自动化完成这一流程便成为了刚需。同时，笔者也将给出另一个示例，也就是 CSDDL 的另一关键组成：BoardCaster。BoardCaster 是保管在另一仓库中的 JSON 格式的保研信息数据库，如何进行定时的订阅以及对于当前仓库的定时更新？这也同样可以使用 Github Actions 做到。&lt;/p&gt;
&lt;h3&gt;部署 vue 项目&lt;/h3&gt;
&lt;p&gt;首先先通过正常的方式安装 vue3，并且已经完成了一个项目的新建。例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm install -g @vue/cli
vue create cs-baoyan-ddl
cd cs-baoyan-ddl
npm install
npm install gh-pages --save-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;并且进行了一些内容的创建。&lt;/p&gt;
&lt;p&gt;之后需要进行若干的设置操作，虽然这些并不包括在 Github Actions 之中，但是为了后续的部署，这些是必要内容，假如仅讲解 Github Actions 未免写得过于的空洞。&lt;/p&gt;
&lt;p&gt;首先修改 &lt;code&gt;vue.config.js&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-js&quot;&gt;module.exports = {
  publicPath: process.env.NODE_ENV === &apos;production&apos;
    ? &apos;/cs-baoyan-ddl/&apos; // your repo&apos;s name
    : &apos;/&apos;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此处值得一提的是，在此之前包括 fetch 的内容，如 &lt;code&gt;fetch(&apos;/config/schools.json&apos;)&lt;/code&gt;，需要修改为类似于 &lt;code&gt;fetch(&apos;/cs-baoyan-ddl/config/schools.json&apos;)&lt;/code&gt; 的格式。&lt;/p&gt;
&lt;p&gt;之后修改 &lt;code&gt;package.json&lt;/code&gt;，加入 deploy部分：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;&quot;scripts&quot;: {
  &quot;build&quot;: &quot;vue-cli-service build&quot;,
  &quot;serve&quot;: &quot;vue-cli-service serve&quot;,
  &quot;deploy&quot;: &quot;gh-pages -d dist&quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;gh-pages&lt;/code&gt; 是一个十分强大的工具，可以在 build 的时候为你 build 原内容到分支 &lt;code&gt;gh-pages&lt;/code&gt; 中。&lt;/p&gt;
&lt;p&gt;之后在 &lt;code&gt;.github/workflows/deploy.yml&lt;/code&gt; 中添加以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yml&quot;&gt;name: Deploy to GitHub Pages

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  build-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Node.js
        uses: actions/setup-node@v2
        with:
          node-version: &apos;16&apos;

      - name: Install dependencies
        run: npm install

      - name: Build project
        run: npm run build

      - name: Configure Git
        run: |
          git config --global user.name &apos;github-actions&apos;
          git config --global user.email &apos;github-actions@github.com&apos;

      - name: Deploy to GitHub Pages
        run: |
          git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
          npm run deploy
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;遂讲解一下这个 Github Actions 的内容。&lt;/p&gt;
&lt;p&gt;一般来说，Github Actions 一共包括三个部分，分别是 &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;on&lt;/code&gt; 以及 &lt;code&gt;jobs&lt;/code&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;：name 的含义应该不难理解，也就是你的 Actions 的名字，你在 Github 之中全流程都是可视化的，name 作为选择执行不同 Actions 的依据十分的直观。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;on&lt;/code&gt;：on 的含义是触发条件，也就是当什么事件发生时，你的 Actions 才会被触发。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;jobs&lt;/code&gt;：jobs 的含义是任务，也就是你的 Actions 具体要执行什么操作。
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;build-deploy&lt;/code&gt;：本代码中的 &lt;code&gt;build-deploy&lt;/code&gt; 是这个 Actions 之中唯一的任务，这串字符也就是这个任务的名称。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;runs-on: ubuntu-latest&lt;/code&gt;：&lt;code&gt;runs-on&lt;/code&gt; 的含义是运行环境，也就是你的 Actions 会在什么环境下运行。一般来说使用最新的 ubuntu 环境即可，即 &lt;code&gt;ubuntu-latest&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;steps&lt;/code&gt;：&lt;code&gt;steps&lt;/code&gt; 的含义是步骤，也就是你的 Actions 具体要执行什么操作。
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;actions/checkout@v2&lt;/code&gt;：&lt;code&gt;actions/checkout@v2&lt;/code&gt; 是一个 Github 官方提供的 Actions，其作用是检出仓库。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;actions/setup-node@v2&lt;/code&gt;：&lt;code&gt;actions/setup-node@v2&lt;/code&gt; 是一个 Github 官方提供的 Actions，其作用是安装 Node.js。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run: npm install&lt;/code&gt;：这个步骤会执行 &lt;code&gt;npm install&lt;/code&gt; 命令，以安装项目所需的所有依赖包。这个步骤通常是必要的，因为在构建和部署之前，所有的依赖包都需要被安装到项目中，这其中包括了关键内容，即 &lt;code&gt;vue&lt;/code&gt; 以及 &lt;code&gt;gh-pages&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run: npm run build&lt;/code&gt;：这个步骤会执行 &lt;code&gt;npm run build&lt;/code&gt; 命令，以构建项目。这通常会生成一个用于生产环境的优化后的静态文件集，例如 HTML、CSS 和 JavaScript 文件。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run: git config --global user.name &apos;github-actions&apos;&lt;/code&gt; 和 &lt;code&gt;run: git config --global user.email &apos;github-actions@github.com&apos;&lt;/code&gt;：这两个步骤用于配置 Git 用户名和电子邮件地址，以便后续的 Git 操作可以顺利进行。这里设置的用户名和电子邮件是为了让 GitHub Actions 可以以一个虚拟用户的身份进行提交操作。换句话说，这里的 name 以及 email 可以随意设置，只是为了一个名称而已，具体的权限由 &lt;code&gt;secrets.GITHUB_TOKEN&lt;/code&gt; 提供。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run: git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git&lt;/code&gt;：这一行命令会更新 Git 远程仓库的 URL，以便使用 GitHub 提供的访问令牌进行身份验证。&lt;code&gt;${{ secrets.GITHUB_TOKEN }}&lt;/code&gt; 是一个 GitHub 提供的自动生成的访问令牌，它存储在 GitHub Secrets 中，用于确保安全的身份验证。只有使用了 &lt;code&gt;secrets.GITHUB_TOKEN&lt;/code&gt;，指令才有与 Github 仓库交互的权限。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run: npm run deploy&lt;/code&gt;：这个步骤会执行 &lt;code&gt;npm run deploy&lt;/code&gt; 命令，以将构建后的文件部署到 GitHub Pages 上。由于安装了 &lt;code&gt;gh-pages&lt;/code&gt;，在部署的过程中会自动在 Git 上进行操作，将静态文件推送到 &lt;code&gt;gh-pages&lt;/code&gt; 分支，从而触发 GitHub Pages 的部署。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上便不难理解 Github Actions 的基本工作原理了，绝大多数的静态网站生成方案都会给出自己的 Github Actions 配置文件，而你只需要在理解了文件的组成之后在他们的基础上进行略微的修改即可。&lt;/p&gt;
&lt;h3&gt;定时更新仓库文件&lt;/h3&gt;
&lt;p&gt;CSDDL 的另一关键组成便是其数据库，也就是 BoardCaster。众所周知，静态网站并不存在后端，也就不存在可以持续更新维护与访问的后端数据库，但是使用 JSON 文件以及一种类似于订阅效果的操作，可以完成一个丐版的数据库，这并不困难。&lt;/p&gt;
&lt;p&gt;梳理一下思路，我们的需求包括，&lt;code&gt;git clone&lt;/code&gt; 另一仓库的内容，将另一仓库的内容截取需要的部分复制到本仓库对应位置，正常的 &lt;code&gt;add&lt;/code&gt;, &lt;code&gt;commit&lt;/code&gt;, &lt;code&gt;push&lt;/code&gt; 流程。最后，这个脚本需要定期执行。&lt;/p&gt;
&lt;p&gt;不难给出 Github Actions 脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yml&quot;&gt;name: Update JSON from BoardCaster

on:
  schedule:
    - cron: &apos;*/15 * * * *&apos;  # 每15分钟运行一次
  workflow_dispatch:  # 手动触发

jobs:
  update-readme:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Listener repository
      uses: actions/checkout@v2

    - name: Clone BoardCaster repository
      run: git clone https://github.com/CS-BAOYAN/BoardCaster.git

    - name: Copy and rename data.json to public/config/schools.json
      run: |
        cp BoardCaster/data.json public/config/schools.json

    - name: Commit and push changes if there are any
      run: |
        git config --global user.name &apos;github-actions&apos;
        git config --global user.email &apos;github-actions@github.com&apos;
        git add public/config/schools.json
        if git diff-index --quiet HEAD; then
          echo &quot;No changes to commit&quot;
        else
          git commit -m &quot;Update public/config/schools.json from BoardCaster&quot;
          git push
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;大多数内容应该不难理解，大量的篇幅是常规的 Git 操作，在 GITHUB_TOKEN 的环境下执行，其中唯一需要指出的细节是先比较 HEAD 之后再决定是否 commit，这是因为若无修改而 commit 会导致 Git 报错，虽然实际上无伤大雅，但是依然不够优雅。&lt;/p&gt;
&lt;p&gt;同样需要指出的是定时触发的语法，&lt;code&gt;schedule&lt;/code&gt; 中的 &lt;code&gt;cron&lt;/code&gt; 语法是 Github Actions 提供的定时触发语法，&lt;code&gt;*/15 * * * *&lt;/code&gt; 表示每 15 分钟触发一次。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;* * * * *
| | | | |
| | | | +---- 星期几 (0 - 7) (星期天为0或7)
| | | +------ 月份 (1 - 12)
| | +-------- 日期 (1 - 31)
| +---------- 小时 (0 - 23)
+------------ 分钟 (0 - 59)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cron 是一种用于调度任务的时间表表达式，广泛应用于类 Unix 操作系统的任务调度工具中。GitHub Actions 也支持使用 Cron 表达式来定时触发工作流。Cron 表达式由五个字段组成，分别表示分钟、小时、日期、月份和星期几。&lt;/p&gt;
&lt;p&gt;其中需要著名的特殊字符包括以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;星号（*）&lt;/strong&gt;：表示匹配任何值。例如，&lt;code&gt;* * * * *&lt;/code&gt; 表示每分钟执行一次。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;逗号（,）&lt;/strong&gt;：用于分隔多个值。例如，&lt;code&gt;0,15,30,45 * * * *&lt;/code&gt; 表示每小时的第 0、15、30 和 45 分钟执行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;连字符（-）&lt;/strong&gt;：用于指定一个范围。例如，&lt;code&gt;0-5 * * * *&lt;/code&gt; 表示每小时的第 0 到 5 分钟执行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;斜杠（/）&lt;/strong&gt;：用于指定步长。例如，&lt;code&gt;*/15 * * * *&lt;/code&gt; 表示每 15 分钟执行一次。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时，需要注意的是 Github Actions 由于流量过大的问题，所以说对于定期触发的 Actions 并不会按照准确的时间执行，而是大概率会出现延后，在这里开通了 &lt;code&gt;workflow_dispatch&lt;/code&gt;，可以在紧急情况下选择手动触发。&lt;/p&gt;
&lt;h2&gt;结语&lt;/h2&gt;
&lt;p&gt;以上便是本次介绍的全部内容了，在此限于篇幅，也要告一段落了，更多的静态网站部署工具，笔者多半也有使用过，或许改天会开一个合集，讲一下踩过的坑。&lt;/p&gt;
&lt;p&gt;Github Pages 与 Github Actions 可以帮助开发者构建个人网站，更重要的是，其完全免费。使用它们吧，创作属于自己的内容。&lt;/p&gt;</content:encoded><h:img src="https://picr2.axi404.top/Github-Actions-and-Pages-Tutorials-zh.webp"/><enclosure url="https://picr2.axi404.top/Github-Actions-and-Pages-Tutorials-zh.webp"/></item></channel></rss>